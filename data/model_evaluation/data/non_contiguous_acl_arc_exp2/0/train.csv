unique_id,citing_id,citing_title,cited_title,cited_authors,section_title,cited_abstract,citation_context,cite_context_paragraph,citation_class_label,dynamic_contexts_combined
CC388,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,finding terminology translations from nonparallel corpora,"['Pascale Fung', 'Kathleen McKeown']",introduction,"We present a statistical word feature, the Word Relation Matrix, which can be used to find translated pairs of words and terms from non-parallel corpora, across language groups. Online dictionary entries are used as seed words to generate Word Relation Matrices for the unknown words according to correlation measures. Word Relation Matrices are then mapped across the corpora to find translation pairs. Translation accuracies are around 30% when only the top candidate is counted. Nevertheless, top 20 candidate output give a 50.9% average increase in accuracy on human translator performance.","#AUTHOR_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora .","['All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', '#AUTHOR_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora .', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:']",0,"['There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', '#AUTHOR_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora .', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.']"
CC389,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,the selfextending phrasal lexicon,"['Uri Zernik', 'Michael Dyer']",introduction,,"â¢ Learnability ( #AUTHOR_TAG ) â¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â¢ Speech generation ( Rayner and Carter 1997 ) â¢ Localization ( Sch Â¨ aler 1996 )","['â\x80¢ Learnability ( #AUTHOR_TAG ) â\x80¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( Rayner and Carter 1997 ) â\x80¢ Localization ( Sch Â¨ aler 1996 )']",0,"['â\x80¢ Learnability ( #AUTHOR_TAG ) â\x80¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( Rayner and Carter 1997 ) â\x80¢ Localization ( Sch Â¨ aler 1996 )']"
CC390,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,a languageneutral sparsedata algorithm for extracting translation patterns,"['Kevin McTait', 'Arturo Trujillo']",introduction,,"Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , #AUTHOR_TAG , Carl ( 1999 ) , and Brown ( 2000 ) , inter alia .","['Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', 'Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', 'Watanabe (1993) combines lexical and dependency mappings to form his generalizations.', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , #AUTHOR_TAG , Carl ( 1999 ) , and Brown ( 2000 ) , inter alia .']",0,"['Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', 'Watanabe (1993) combines lexical and dependency mappings to form his generalizations.', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , #AUTHOR_TAG , Carl ( 1999 ) , and Brown ( 2000 ) , inter alia .']"
CC391,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,generating language with a phrasal lexicon,['Edward Hovy'],introduction,"In this paper, we ask: How should language be represented in a generator program? In particular, how do the concepts the generator must express, the grammar it is to use, and the words and phrases with which it must express them, relate? The answer presented here is that all linguistic knowledge -- all language -- should be contained in the lexicon. The argument is the following: A generator performs three types of task to produce text (deciding what material to include; ordering the parts within paragraphs and sentences; and expressing the parts as appropriate phrases and parts of speech). It gets the information it requires to do these tasks from three sources: from the grammar, from partially frozen phrases (including multi-predicate phrasal patterns), and from certain words. In a functionally organized system, there is no reason why an a priori distinction should be made between the contents of the lexicon and the contents of the grammar. From the generator's perspective, the difference between these sources is not important. Rules of grammar, multi-predicate phrases, and phrasal and verb predicate patterns can all be viewed as phrases, frozen to a greater or lesser degree, and should all be part of the lexicon. Some such ""phrases"" can be quite complex, prescribing a series of actions and tests to perform the three tasks: these can be thought of as specialist procedures. Others can be very simple: templates. This paper also describes the elements that constitute the lexicon of a phrasal generator program and the way the elements are used.","â¢ Learnability ( Zernik and Dyer 1987 ) â¢ Text generation ( #AUTHOR_TAG ; Milosavljevic , Tulloch , and Dale 1996 ) â¢ Speech generation ( Rayner and Carter 1997 ) â¢ Localization ( Sch Â¨ aler 1996 )","['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation ( #AUTHOR_TAG ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( Rayner and Carter 1997 ) â\x80¢ Localization ( Sch Â¨ aler 1996 )']",0,"['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation ( #AUTHOR_TAG ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( Rayner and Carter 1997 ) â\x80¢ Localization ( Sch Â¨ aler 1996 )']"
CC392,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,toward memorybased translation,"['Satoshi Sato', 'Makoto Nagao']",introduction,,"Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #AUTHOR_TAG ; Veale and Way 1997 ; Carl 1999 ) .7","['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #AUTHOR_TAG ; Veale and Way 1997 ; Carl 1999 ) .7']",0,"['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #AUTHOR_TAG ; Veale and Way 1997 ; Carl 1999 ) .7']"
CC393,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,what’s been forgotten in translation memory in envisioning machine translation in the information future,"['Elliott Macklovitch', 'Graham Russell']",introduction,,"From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one ( #AUTHOR_TAG ) .","[""In quite a short space of time, translation memory (TM) systems have become a very useful tool in the translator's armory."", 'TM systems store a set of source, target translation pairs in their databases.', 'If a new input string cannot be found exactly in the translation database, a search is conducted for close (or ""fuzzy"") matches of the input string, and these are retrieved together with their translations for the translator to manipulate into the final, output translation.', 'From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one ( #AUTHOR_TAG ) .']",0,"[""In quite a short space of time, translation memory (TM) systems have become a very useful tool in the translator's armory."", 'TM systems store a set of source, target translation pairs in their databases.', 'From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one ( #AUTHOR_TAG ) .']"
CC394,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,aligning clauses in parallel texts,"['Sotiris Boutsis', 'Stelios Piperidis']",introduction,"This paper describes a method for the automatic alignment of parallel texts at clause level. The method features statistical techniques coupled with shallow linguistic processing. It presupposes a parallel bilingual corpus and identifies alignments between the clauses of the source and target language sides of the corpus. Parallel texts are first statistically aligned at sentence level and then tagged with their part-of-speech categories. Regular grammars functioning on tags, recognize clauses on both sides of the parallel text. A probabilistic model is applied next, operating on the basis of word occurrence and co-occurrence probabilities and character lengths. Depending on sentence size, possible alignments arc fed into a dynamic progranuning framework or a simulated annealing system in order to find or approxim~te the best alignment. 1he method has been tested on a Small Eng~ lish-Greek corpus consisting of texts relevant to software systems and has produced promising results in terms of correctly identified clause alignments.",#AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities .,"['All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', '#AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities .', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:']",0,['#AUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities .']
CC395,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,gaijin a bootstrapping templatedriven approach to examplebased machine translation,"['Tony Veale', 'Andy Way']",introduction,,"â¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( Juola 1998 ) â¢ grammar optimization ( Juola 1994 ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; #AUTHOR_TAG ; Gough , Way , and Hearne 2002 )","['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; #AUTHOR_TAG ; Gough , Way , and Hearne 2002 )']",0,"['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; #AUTHOR_TAG ; Gough , Way , and Hearne 2002 )']"
CC396,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,the necessity of syntax markers two experiments with artificial languages,['Thomas Green'],introduction,,"â¢ language learning ( #AUTHOR_TAG ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( Juola 1998 ) â¢ grammar optimization ( Juola 1994 ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )","['â\x80¢ language learning ( #AUTHOR_TAG ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']",0,"['â\x80¢ language learning ( #AUTHOR_TAG ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']"
CC397,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,integrating translations from multiple sources with the pangloss mark iii machine translation system,"['Robert Frederking', 'Sergei Nirenburg', 'David Farwell', 'Steven Helmreich', 'Eduard Hovy', 'Kevin Knight', 'Stephen Beale', 'Constantin Domashnev', 'Donna Attardo', 'Dean Grannes', 'Ralf Brown']",conclusion,"Since MT systems, whatever translation method they employ, do not reach an optimum output on free text; each method handles some problems better than others. The PANGLOSS Mark III system is an MT environment that uses the best results from a variety of independent MT systems or engines working simultaneously within a single framework on the same text. This paper describes the method used to combine the outputs of the engines into a single text.","Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by Frederking and Nirenburg ( 1994 ) , #AUTHOR_TAG , and Hogan and Frederking ( 1998 ) .","['Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by Frederking and Nirenburg ( 1994 ) , #AUTHOR_TAG , and Hogan and Frederking ( 1998 ) .']",1,"['Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by Frederking and Nirenburg ( 1994 ) , #AUTHOR_TAG , and Hogan and Frederking ( 1998 ) .']"
CC398,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,subsentential exploitation of translation memories,"['Michel Simard', 'Philippe Langlais']",introduction,Laboratoire de recherche appliquee en linguistique informatique (RALI) Departement d&apos;Informatique et recherche operationnelle Universite de Montrea,"More recently , #AUTHOR_TAG have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , and Carl ( 2003 , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .","['More recently , #AUTHOR_TAG have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , and Carl ( 2003 , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .', 'This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from _source, target_ phrasal segments, and from there they suggest that �it is a reasonably short step to enabling an automated solution via the recombination element of EBMT systems such as those described in [Carl and Way 2003].�']",0,"['More recently , #AUTHOR_TAG have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , and Carl ( 2003 , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .']"
CC399,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,further experiments in bilingual text alignment,['Harold Somers'],introduction,"We describe and experimentally evaluate an alternative algorithm for aligning and extracting vocabulary from parallel texts using recency vectors and a similarity measure based on Levenshtein distance. The work is largely inspired by Fung and McKeown 's DK-vec, though we use a simpler algorithm. The technique is tested on two sets of parallel corpora involving English, French, German, Dutch, Spanish, and Japanese. We attempt to evaluate the importance of parameters such as frequency of words chosen as candidates, the effect of different language pairings, and differences between the two corpora.",#AUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance .,"['All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', '#AUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance .', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:']",0,"['There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Roscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', '#AUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance .', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.']"
CC400,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,a psycholinguistic approach to corpusbased machine translation,['Patrick Juola'],introduction,,"#AUTHOR_TAG , 1997 ) assumes that words ending in - ed are verbs .","['#AUTHOR_TAG , 1997 ) assumes that words ending in - ed are verbs .', 'However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category.', 'Instead, we take advantage of the fact that the initial phrasal chunks correspond to rule right-hand sides.', 'That is, for a rule in the Penn Treebank VP −→ VBG, NP, PP, we are certain (if the annotators have done their job correctly) that the first word in each of the strings corresponding to this right-hand side is a VBG, that is, a present participle.', 'Given this information, in such cases we tag such words with the <LEX> tag.', 'Taking expanding the board to 14 members −→ augmente le conseilà 14 membres as an example, we extract the chunks in ( 24 We ignore here the trivially true lexical chunk ""<QUANT> 14 : 14.""']",1,"['#AUTHOR_TAG , 1997 ) assumes that words ending in - ed are verbs .', 'Instead, we take advantage of the fact that the initial phrasal chunks correspond to rule right-hand sides.']"
CC401,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,the weighted majority algorithm,"['Nick Littlestone', 'Manfred Warmuth']",introduction,"AbstractWe study the construction of prediction algorithms in a situation in which a learner faces a sequence of trials, with a prediction to be made in each, and the goal of the learner is to make few mistakes. We are interested in the case where the learner has reason to believe that one of some pool of known algorithms will perform well, but the learner does not know which one. A simple and effective method, based on weighted voting, is introduced for constructing a compound algorithm in such a circumstance. We call this method the Weighted Majority Algorithm. We show that this algorithm is robust in the presence of errors in the data. We discuss various versions of the Weighted Majority Algorithm and prove mistake bounds for them that are closely related to the mistake bounds of the best algorithms of the pool. For example, given a sequence of trials, if there is an algorithm in the pool A that makes at most m mistakes then the Weighted Majority Algorithm will make at most c(log |A| + m) mistakes on that sequence, where c is fixed constant","We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm ( #AUTHOR_TAG ) .","['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks, as is usual in most EBMT systems (cf.', 'Sato and Nagao 1990;Veale and Way 1997;Carl 1999). 7', 's an example, consider the translation into French of the house collapsed.', 'Assume the conditional probabilities in ( 33  These mistranslations are all caused by boundary friction.', 'Each of the translations in ( 37) and ( 38) would be output with an associated weight and ranked by the system.', 'We would like to incorporate into our model a procedure whereby translation chunks extracted from the phrasal and marker lexicons are more highly regarded than those constructed by inserting words from the word-level lexicon into generalized marker chunks.', 'That is, we want to allocate a larger portion of the probability space to the phrasal and marker lexicons than to the generalized or wordlevel lexicons.', 'We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm ( #AUTHOR_TAG ) .']",3,"['We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm ( #AUTHOR_TAG ) .']"
CC402,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,gaijin a bootstrapping templatedriven approach to examplebased machine translation,"['Tony Veale', 'Andy Way']",introduction,,"Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; #AUTHOR_TAG ; Carl 1999 ) .7","['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; #AUTHOR_TAG ; Carl 1999 ) .7']",0,"['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; #AUTHOR_TAG ; Carl 1999 ) .7']"
CC403,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,the world wide web as a resource for examplebased machine translation tasks,['Gregory Grefenstette'],experiments,"The WWW is two orders of magnitude larger than the largest corpora. Although noisy, web text presents language as it is used, and statistics derived from the Web can have practical uses in many NLP applications. For this reason, the WWW should be seen and studied as any other computationally available linguistic resource. In this article, we illustrate this by showing that an Example-Based approach to lexical choice for machine translation can use the Web as an adequate and free resource.","However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #AUTHOR_TAG .","['The problem of boundary friction is clearly visible here: We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP.', 'However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #AUTHOR_TAG .', 'Grefenstette shows that the Web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often.', 'Rather than search for competing candidates, we select the ""best"" translation and have its morphological variants searched for on-line.', ""In the example above, namely, the personal computers, we search for les ordinateurs personnels versus the wrong alternatives le/la/l'ordinateurs personnels."", 'Interestingly, using Lycos, and setting the search language to French, the correct form les ordinateurs personnels is uniquely preferred over the other alternatives, as it is found 2,454 times, whereas the others are not found at all.', 'In this case, this translation overrides the highest-ranked translation (50) and is output as the final translation.', 'In fact, in checking the translations obtained for NPs using system combination ABC, we noted that 251 NPs out of the test set of 500 could be improved.', 'Of these 251, 207 (82.5%) were improved post hoc via the Web, with no improvement for the remaining 43 cases.', 'We consider this to be quite a significant result.']",5,"['However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #AUTHOR_TAG .', 'Grefenstette shows that the Web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often.', 'Rather than search for competing candidates, we select the ""best"" translation and have its morphological variants searched for on-line.', 'Of these 251, 207 (82.5%) were improved post hoc via the Web, with no improvement for the remaining 43 cases.']"
CC404,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,a psycholinguistic approach to corpusbased machine translation,['Patrick Juola'],introduction,,"#AUTHOR_TAG , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â > French and English â > Urdu .","['Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', '#AUTHOR_TAG , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â\x88\x92 > French and English â\x88\x92 > Urdu .', 'For the English −→ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data.', 'For English −→ Urdu, Juola (1997, page 213) notes that ""the system learned the original training corpus . . .', 'perfectly and could reproduce it without errors""; that is, it scored 100% accuracy when tested against the training corpus.', 'On novel test sentences, he gives results of 72% correct translation.', 'In their Gaijin system, Veale and Way (1997) give a result of 63% accurate translations obtained for English −→ German on a test set of 791 sentences from CorelDRAW manuals.']",0,"['#AUTHOR_TAG , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â\x88\x92 > French and English â\x88\x92 > Urdu .', 'For English -- Urdu, Juola (1997, page 213) notes that ""the system learned the original training corpus . . .', 'On novel test sentences, he gives results of 72% correct translation.', 'In their Gaijin system, Veale and Way (1997) give a result of 63% accurate translations obtained for English -- German on a test set of 791 sentences from CorelDRAW manuals.']"
CC405,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,a psycholinguistic approach to corpusbased machine translation,['Patrick Juola'],introduction,,"â¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( Juola 1998 ) â¢ grammar optimization ( #AUTHOR_TAG ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )","['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( #AUTHOR_TAG ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']",0,"['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( #AUTHOR_TAG ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']"
CC406,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,examplebased incremental synchronous interpretation,['Hans-Ulrich Block'],introduction,"This article describes a new approach to example based incremental translation for automatic interpretation systems developed in Verbmobil. The translation module is completely learned from a bilingual corpus. The training phase combines statistical word alignment with precomputation of translation ""chunks"" and contextual clustering of syntactic equivalence classes (word classes). The system gives incremental output for every piece of input being it words or sequences of words. It thus tries to mimic the behaviour of a human synchronous interpreter. If a larger context leads to the need for reformulation the system utters a correction marker like I mean, and restarts the output from the starting position of the reformulation. The system is currently effective for German = English. German = Chinese and German a Japanese are under construction. In the Verbmobil evaluation, this approach reached 79% of approximately correct translations on speech recognition output.","In a final processing stage , we generalize over the marker lexicon following a process found in #AUTHOR_TAG .","['In a final processing stage , we generalize over the marker lexicon following a process found in #AUTHOR_TAG .', ""In Block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool."", 'In a subsequent stage, chunk pairs are extracted, which are then generalized to produce a set of translation templates for each source, target segment.']",5,"['In a final processing stage , we generalize over the marker lexicon following a process found in #AUTHOR_TAG .', ""In Block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool.""]"
CC407,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,examplebased incremental synchronous interpretation,['Hans-Ulrich Block'],introduction,"This article describes a new approach to example based incremental translation for automatic interpretation systems developed in Verbmobil. The translation module is completely learned from a bilingual corpus. The training phase combines statistical word alignment with precomputation of translation ""chunks"" and contextual clustering of syntactic equivalence classes (word classes). The system gives incremental output for every piece of input being it words or sequences of words. It thus tries to mimic the behaviour of a human synchronous interpreter. If a larger context leads to the need for reformulation the system utters a correction marker like I mean, and restarts the output from the starting position of the reformulation. The system is currently effective for German = English. German = Chinese and German a Japanese are under construction. In the Verbmobil evaluation, this approach reached 79% of approximately correct translations on speech recognition output.","Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #AUTHOR_TAG to permit a limited form of insertion in the translation process .","['• language learning (Green 1979;Mori and Moeser 1983;Morgan, Meier, and Newport 1989) • monolingual grammar induction (Juola 1998) • grammar optimization (Juola 1994) • insights into universal grammar (Juola 1998) • machine translation (Juola 1994(Juola , 1997Veale and Way 1997;Gough, Way, and Hearne 2002) With respect to translation, a potential problem in using the marker hypothesis is that some languages do not have marker words such as articles, for instance.', ""Green's (1979) work showed that artificial languages, both with and without specific marker words, may be learned more accurately and quickly if such psycholinguistic cues exist."", 'The research of Mori and Moeser (1983) showed a similar effect due to case marking on pseudowords in such artificial languages, and Morgan, Meier, and Newport (1989) demonstrated that languages that do not permit pronouns as substitutes for phrases also provide evidence in favor of the marker hypothesis.', 'Juola\'s (1994Juola\'s ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to ""marker-normal form.""', 'However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.', 'Nevertheless, Juola (1998, page 23) observes that ""a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item (for example, a word and its case-marking), can capture this sort of result quite handily.""', 'Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated.', 'Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #AUTHOR_TAG to permit a limited form of insertion in the translation process .', 'As a byproduct of the chosen methodology, we also derive a standard ""word-level"" translation lexicon.', 'These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input.']",5,"['* language learning (Green 1979;Mori and Moeser 1983;Morgan, Meier, and Newport 1989) * monolingual grammar induction (Juola 1998) * grammar optimization (Juola 1994) * insights into universal grammar (Juola 1998) * machine translation (Juola 1994(Juola , 1997Veale and Way 1997;Gough, Way, and Hearne 2002) With respect to translation, a potential problem in using the marker hypothesis is that some languages do not have marker words such as articles, for instance.', ""Green's (1979) work showed that artificial languages, both with and without specific marker words, may be learned more accurately and quickly if such psycholinguistic cues exist."", 'Juola\'s (1994Juola\'s ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to ""marker-normal form.""', 'However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.', 'Nevertheless, Juola (1998, page 23) observes that ""a slightly more general mapping, where two adjacent terminal symbols can be merged into a single lexical item (for example, a word and its case-marking), can capture this sort of result quite handily.""', 'Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated.', 'Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #AUTHOR_TAG to permit a limited form of insertion in the translation process .', 'As a byproduct of the chosen methodology, we also derive a standard ""word-level"" translation lexicon.', 'These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input.']"
CC408,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,gaijin a bootstrapping templatedriven approach to examplebased machine translation,"['Tony Veale', 'Andy Way']",introduction,,"In their Gaijin system , #AUTHOR_TAG give a result of 63 % accurate translations obtained for English â > German on a test set of 791 sentences from CorelDRAW manuals .","['Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', 'Juola (1994Juola ( , 1997 conducts some small experiments using his METLA system to show the viability of this approach for English −→ French and English −→ Urdu.', 'For the English −→ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data.', 'For English −→ Urdu, Juola (1997, page 213) notes that ""the system learned the original training corpus . . .', 'perfectly and could reproduce it without errors""; that is, it scored 100% accuracy when tested against the training corpus.', 'On novel test sentences, he gives results of 72% correct translation.', 'In their Gaijin system , #AUTHOR_TAG give a result of 63 % accurate translations obtained for English â\x88\x92 > German on a test set of 791 sentences from CorelDRAW manuals .']",1,"['In their Gaijin system , #AUTHOR_TAG give a result of 63 % accurate translations obtained for English â\x88\x92 > German on a test set of 791 sentences from CorelDRAW manuals .']"
CC409,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,the role of syntax markers and semantic referents in learning an artificial language,"['Kazuo Mori', 'Shannon Moeser']",introduction,,"â¢ language learning ( Green 1979 ; #AUTHOR_TAG ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( Juola 1998 ) â¢ grammar optimization ( Juola 1994 ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )","['â\x80¢ language learning ( Green 1979 ; #AUTHOR_TAG ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']",0,"['â\x80¢ language learning ( Green 1979 ; #AUTHOR_TAG ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']"
CC410,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,on psycholinguistic grammars,['Patrick Juola'],introduction,"It has long been known that language acquisition is only possible if information is available above and beyond the mere presence of a set of strings in the language. One commonly postulated source of such information is a (possibly innate) constraint on the syntactic forms that a grammar can take. This paper develops and presents a set of formalisms based on the Marker Hypothesis that natural languages are ""marked"" for complex syntactic structure at surface form. It further compares the expressivity and restrictedness of these formalisms and shows that, first, not all constraints are actually restrictive, and second, that the Marker Hypothesis, and its implicit function/content word distinction, provide strong restrictions on the form of allowable grammars. These restrictions may in turn provide evidence about its actual psychological reality and salience. In particular, the class of strongly marked languages can be demonstrated not to admit all finite languages and thus not be subject to the hangman's noose of Gold's learnability proofs, and it is conjectured that these languages may provide a computable method of inferring human-like languages.","â¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â¢ monolingual grammar induction ( #AUTHOR_TAG ) â¢ grammar optimization ( Juola 1994 ) â¢ insights into universal grammar ( Juola 1998 ) â¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )","['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( #AUTHOR_TAG ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']",0,"['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( #AUTHOR_TAG ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']"
CC411,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,corpusbased acquisition of transfer functions using psycholinguistic principles,['Patrick Juola'],introduction,,"For English â\x88\x92 > Urdu , #AUTHOR_TAG , page 213 ) notes that ""the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus .","['Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of _source, target_ chunks.', 'Juola (1994, 1997) conducts some small experiments using his METLA system to show the viability of this approach for English __ French and English __ Urdu.', 'For the English __ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data.', 'For English â\x88\x92 > Urdu , #AUTHOR_TAG , page 213 ) notes that ""the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus .', 'On novel test sentences, he gives results of 72% correct translation.', 'In their Gaijin system, Veale and Way (1997) give a result of 63% accurate translations obtained for English __ German on a test set of 791 sentences from CorelDRAW manuals.']",0,"['For English â\\x88\\x92 > Urdu , #AUTHOR_TAG , page 213 ) notes that ""the system learned the original training corpus ... perfectly and could reproduce it without errors  ; that is , it scored 100 % accuracy when tested against the training corpus .']"
CC412,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,inducing translation templates for examplebased machine translation,['Michael Carl'],introduction,"This paper describes an example-based machine translation (EBMT) system which relays on various knowledge resources. Morphologic analyses abstract the surface forms of the languages to be translated. A shallow syntactic rule formalism is used to percolate features in derivation trees. Translation examples serve the decomposition of the text to be translated and determine the transfer of lexical values into the target language. Translation templates determine the word order of the target language and the type of phrases (e.g. noun phrase, prepositional phase, ...) to be generated in the target language. An induction mechanism generalizes translation templates from translation examples. The paper outlines the basic idea underlying the EBMT system and investigates the possibilities and limits of the translation template induction process.","Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , #AUTHOR_TAG , and Brown ( 2000 ) , inter alia .","['Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', 'Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', 'Watanabe (1993) combines lexical and dependency mappings to form his generalizations.', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , #AUTHOR_TAG , and Brown ( 2000 ) , inter alia .']",0,"['Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', 'Watanabe (1993) combines lexical and dependency mappings to form his generalizations.', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , #AUTHOR_TAG , and Brown ( 2000 ) , inter alia .']"
CC413,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,examplebased incremental synchronous interpretation,['Hans-Ulrich Block'],introduction,"This article describes a new approach to example based incremental translation for automatic interpretation systems developed in Verbmobil. The translation module is completely learned from a bilingual corpus. The training phase combines statistical word alignment with precomputation of translation ""chunks"" and contextual clustering of syntactic equivalence classes (word classes). The system gives incremental output for every piece of input being it words or sequences of words. It thus tries to mimic the behaviour of a human synchronous interpreter. If a larger context leads to the need for reformulation the system utters a correction marker like I mean, and restarts the output from the starting position of the reformulation. The system is currently effective for German = English. German = Chinese and German a Japanese are under construction. In the Verbmobil evaluation, this approach reached 79% of approximately correct translations on speech recognition output.","This is then generalized , following a methodology based on #AUTHOR_TAG , to generate the `` generalized marker lexicon . ''","['In this section, we describe how the memory of our EBMT system is seeded with a set of translations obtained from Web-based MT systems.', 'From this initial resource, we subsequently derive a number of different databases that together allow many new input sentences to be translated that it would not be possible to translate in other systems.', 'First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon.', ""This is then generalized , following a methodology based on #AUTHOR_TAG , to generate the `` generalized marker lexicon . ''"", 'Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www.up.univ-mrs.fr/˜veronis/biblios/ptp.htm.', 'methodology chosen, we automatically derive a fourth resource, namely, a ""word-level lexicon.""']",5,"['From this initial resource, we subsequently derive a number of different databases that together allow many new input sentences to be translated that it would not be possible to translate in other systems.', 'First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon.', ""This is then generalized , following a methodology based on #AUTHOR_TAG , to generate the `` generalized marker lexicon . ''"", 'Finally, as a result of the We refer the interested reader to the excellent and comprehensive bibliography on parallel text processing available at http://www.up.univ-mrs.fr/~veronis/biblios/ptp.htm.', 'methodology chosen, we automatically derive a fourth resource, namely, a ""word-level lexicon.""']"
CC414,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,hybrid language processing in the spoken language translator,"['Manny Rayner', 'David Carter']",introduction,"We present an overview of the Spoken Language Translator (SLT) system's hybrid language-processing architecture, focusing on the way in which rule-based and statistical methods are combined to achieve robust and efficient performance within a linguistically motivated framework. In general, we argue that rules are desirable in order to encode domain-independent linguistic constraints and achieve high-quality grammatical output, while corpus-derived statistics are needed if systems are to be efficient and robust; further, that hybrid architectures are superior from the point of view of portability to architectures which only make use of one type of information. We address the topics of ""multi-engine"" strategies for robust translation; robust bottom-up parsing using pruning and grammar specialization; rational development of linguistic rule-sets using balanced domain corpora; and efficient supervised training by interactive disambiguation. All work described is fully implemented in the current version of the SLT-2 system.","â¢ Learnability ( Zernik and Dyer 1987 ) â¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â¢ Speech generation ( #AUTHOR_TAG ) â¢ Localization ( Sch Â¨ aler 1996 )","['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( #AUTHOR_TAG ) â\x80¢ Localization ( Sch Â¨ aler 1996 )']",0,"['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( #AUTHOR_TAG ) â\x80¢ Localization ( Sch Â¨ aler 1996 )']"
CC415,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,the necessity of syntax markers two experiments with artificial languages,['Thomas Green'],introduction,,"Each set of translations is stored separately , and for each set the `` marker hypothesis '' ( #AUTHOR_TAG ) is used to segment the phrasal lexicon into a `` marker lexicon . ''","[""Each set of translations is stored separately , and for each set the `` marker hypothesis '' ( #AUTHOR_TAG ) is used to segment the phrasal lexicon into a `` marker lexicon . ''"", 'The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are ""marked"" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes.', 'That is, a basic phrase-level segmentation of an input sentence can be achieved by exploiting a closed list of known marker words to signal the start and end of each segment.']",5,"[""Each set of translations is stored separately , and for each set the `` marker hypothesis '' ( #AUTHOR_TAG ) is used to segment the phrasal lexicon into a `` marker lexicon . ''"", 'The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are ""marked"" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes.']"
CC416,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,examplebased incremental synchronous interpretation,['Hans-Ulrich Block'],introduction,"This article describes a new approach to example based incremental translation for automatic interpretation systems developed in Verbmobil. The translation module is completely learned from a bilingual corpus. The training phase combines statistical word alignment with precomputation of translation ""chunks"" and contextual clustering of syntactic equivalence classes (word classes). The system gives incremental output for every piece of input being it words or sequences of words. It thus tries to mimic the behaviour of a human synchronous interpreter. If a larger context leads to the need for reformulation the system utters a correction marker like I mean, and restarts the output from the starting position of the reformulation. The system is currently effective for German = English. German = Chinese and German a Japanese are under construction. In the Verbmobil evaluation, this approach reached 79% of approximately correct translations on speech recognition output.","That is, where #AUTHOR_TAG substitutes variables for various words in his templates, we replace certain lexical items with their marker tag.","['That is, where #AUTHOR_TAG substitutes variables for various words in his templates, we replace certain lexical items with their marker tag.', 'Given that examples such as ��<DET> a : un� are likely to exist in the word-level lexicon, they may be inserted at the point indicated by the marker tag to form the correct translation un bon homme.', 'We thus cluster on marker words to improve the coverage of our system (see Section 5 for results that show exactly how clustering on marker words helps); others (notably Brown [2000, 2003]) use clustering techniques to determine equivalence classes of individual words that can occur in the same context, and in so doing derive translation templates from individual translation examples.']",1,"['That is, where #AUTHOR_TAG substitutes variables for various words in his templates, we replace certain lexical items with their marker tag.', 'Given that examples such as <DET> a : un are likely to exist in the word-level lexicon, they may be inserted at the point indicated by the marker tag to form the correct translation un bon homme.', 'We thus cluster on marker words to improve the coverage of our system (see Section 5 for results that show exactly how clustering on marker words helps); others (notably Brown [2000, 2003]) use clustering techniques to determine equivalence classes of individual words that can occur in the same context, and in so doing derive translation templates from individual translation examples.']"
CC417,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,on psycholinguistic grammars,['Patrick Juola'],introduction,"It has long been known that language acquisition is only possible if information is available above and beyond the mere presence of a set of strings in the language. One commonly postulated source of such information is a (possibly innate) constraint on the syntactic forms that a grammar can take. This paper develops and presents a set of formalisms based on the Marker Hypothesis that natural languages are ""marked"" for complex syntactic structure at surface form. It further compares the expressivity and restrictedness of these formalisms and shows that, first, not all constraints are actually restrictive, and second, that the Marker Hypothesis, and its implicit function/content word distinction, provide strong restrictions on the form of allowable grammars. These restrictions may in turn provide evidence about its actual psychological reality and salience. In particular, the class of strongly marked languages can be demonstrated not to admit all finite languages and thus not be subject to the hangman's noose of Gold's learnability proofs, and it is conjectured that these languages may provide a computable method of inferring human-like languages.","Nevertheless , #AUTHOR_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''","['• language learning (Green 1979;Mori and Moeser 1983;Morgan, Meier, and Newport 1989) • monolingual grammar induction (Juola 1998) • grammar optimization (Juola 1994) • insights into universal grammar (Juola 1998) • machine translation (Juola 1994(Juola , 1997Veale and Way 1997;Gough, Way, and Hearne 2002) With respect to translation, a potential problem in using the marker hypothesis is that some languages do not have marker words such as articles, for instance.', ""Green's (1979) work showed that artificial languages, both with and without specific marker words, may be learned more accurately and quickly if such psycholinguistic cues exist."", 'The research of Mori and Moeser (1983) showed a similar effect due to case marking on pseudowords in such artificial languages, and Morgan, Meier, and Newport (1989) demonstrated that languages that do not permit pronouns as substitutes for phrases also provide evidence in favor of the marker hypothesis.', 'Juola\'s (1994Juola\'s ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to ""marker-normal form.""', 'However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.', ""Nevertheless , #AUTHOR_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''"", 'Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated.', 'Following construction of the marker lexicon, the source, target chunks are generalized further using a methodology based on Block (2000) to permit a limited form of insertion in the translation process.', 'As a byproduct of the chosen methodology, we also derive a standard ""word-level"" translation lexicon.', 'These various resources render the set of original translation pairs far more useful in deriving translations of previously unseen input.']",0,"[""Green's (1979) work showed that artificial languages, both with and without specific marker words, may be learned more accurately and quickly if such psycholinguistic cues exist."", ""Nevertheless , #AUTHOR_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''""]"
CC418,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,automated generalization of translation examples,['Ralf Brown'],introduction,"Previous work has shown that adding generalization of the examples in the corpus of an example-based machine translation (EBMT) system can reduce the required amount of pretranslated example text by as much as an order of magnitude for Spanish-English and FrenchEnglish EBMT. Using word clustering to automatically generalize the example corpus can provide the majority of this improvement for French-English with no manual intervention; the prior work required a large bilingual dictionary tagged with parts of speech and the manual creation of grammar rules. By seeding the clustering with a small amount of manuallycreated information, even better performance can be achieved. This paper describes a method whereby bilingual word clustering can be performed using standard monolingual document clustering techniques, and its effectiveness at reducing the size of the example corpus required.  1 Introduction  Example-Based Machine Translation (EBMT) relies on a collection of textual units (usuall..","Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , Carl ( 1999 ) , and #AUTHOR_TAG , inter alia .","['Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', 'Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', 'Watanabe (1993) combines lexical and dependency mappings to form his generalizations.', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , Carl ( 1999 ) , and #AUTHOR_TAG , inter alia .']",0,"['Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', 'Watanabe (1993) combines lexical and dependency mappings to form his generalizations.', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , McTait and Trujillo ( 1999 ) , Carl ( 1999 ) , and #AUTHOR_TAG , inter alia .']"
CC419,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,the phrasal lexicon,['Joseph Becker'],introduction,"Theoretical linguists have in recent years concentrated their attention on the productive aspect of language, wherein utterances are formed combinatorically from units the size of words or smaller. This paper will focus on the contrary aspect of language, wherein utterances are formed by repetition, modification, and concatenation of previously-known phrases consisting of more than one word. I suspect that we speak mostly by stitching together swatches of text that we have heard before; productive processes have the secondary role of adapting the old phrases to the new situation. The advantage of this point of view is that it has the potential to account for the observed linguistic behavior of native speakers, rather than discounting their actual behavior as irrelevant to their language. In particular, this point of view allows us to concede that most utterances are produced in stereotyped social situations, where the communicative and ritualistic functions of language demand not novelty, but rather an appropriate combination of formulas, cliches, idioms, allusions, slogans, and so forth. Language must have originated in such constrained social contexts, and they are still the predominant arena for language production. Therefore an understanding of the use of phrases is basic to the understanding of language as a whole.You are currently reading a much-abridged version of a paper that will be published elsewhere later.","More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :","['All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext.', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :']",0,"['More specifically , the notion of the phrasal lexicon ( used first by #AUTHOR_TAG ) has been used successfully in a number of areas :']"
CC420,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,a framework of a mechanical translation between japanese and english by analogy principle,['Makoto Nagao'],introduction,"Problems inherent in current machine translation systems have been reviewed and have been shown to be inherently inconsistent. The present paper defines a model based on a series of human language processing and in particular the use of analogical thinking. Machine translation systems developed so far have a kind of inherent contradiction in themselves. The more detailed a system has become by the additional improvements, the clearer the limitation and the boundary will be for the translation ability. To break through this difficulty we have to think about the mechanism of human translation, and have to build a model based on the fundamental function of language processing in the human brain. The following is an attempt to do this based on the ability of analogy finding in human beings.","All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .","['All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:']",0,"['All EBMT systems , from the initial proposal by #AUTHOR_TAG to the recent collection of Carl and Way ( 2003 ) , are premised on the availability of subsentential alignments derived from the input bitext .', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', ' Kay and Roscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', 'Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora.', 'Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', 'Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities.', 'The respective lengths of the putative alignments in terms of characters is also an important factor.', 'Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking.', 'Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics.', 'More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas:']"
CC421,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,a method for extracting translation patterns from translation examples,['Hideo Watanabe'],introduction,,#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .,"['Block distinguishes chunks from ""patterns,"" as we do: His chunks are similar to our marker chunks, and his patterns are similar to our generalized marker chunks.', 'Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', '#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .', 'Other similar approaches include those of Cicekli and Güvenir (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000, inter alia.']",0,"['Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', 'Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns.', '#AUTHOR_TAG combines lexical and dependency mappings to form his generalizations .']"
CC422,J04-3001,Sample Selection for Statistical Parsing,elements of information theory,"['Thomas M Cover', 'Joy A Thomas']",,"Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index.","Further details about the properties of entropy can be found in textbooks on information theory ( e.g. , #AUTHOR_TAG ) .","['where V is a random variable that can take any possible outcome in set V, and p(v) = Pr(V = v) is the density function.', 'Further details about the properties of entropy can be found in textbooks on information theory ( e.g. , #AUTHOR_TAG ) .', 'Determining the parse tree for a sentence from a set of possible parses can be viewed as assigning a value to a random variable.', 'Thus, a direct application of the entropy definition to the probability distribution of the parses for sentence w in G computes its tree entropy, TE(w, G), the expected number of bits needed to encode the distribution of possible parses for w.', 'However, we may not wish to compare two sentences with different numbers of parses by their entropy directly.', 'If the parse probability distributions for both sentences are uniform, the sentence with more parses will have a higher entropy.', 'Because longer sentences typically have more parses, using entropy directly would result in a bias toward selecting long sentences.', 'To normalize for the number of parses, the uncertainty-based evaluation function, f unc , is defined as a measurement of similarity between the actual probability distribution of the parses and a hypothetical uniform distribution for that set of parses.', 'In particular, we divide the tree entropy by the log of the number of parses: 10']",0,"['Further details about the properties of entropy can be found in textbooks on information theory ( e.g. , #AUTHOR_TAG ) .']"
CC423,J04-3001,Sample Selection for Statistical Parsing,applying cotraining methods to statistical parsing,['Anoop Sarkar'],related work,"We propose a novel Co-Training method for statistical parsing. The algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text. The algorithm iteratively labels the entire data set with parse trees. Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly out-performs training only on the labeled data.","The work of #AUTHOR_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing .","['Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', 'For example, Henderson and Brill (2002) consider the case in which acquiring additional human-annotated training data is not possible.', 'They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.', 'This approach assumes that there are enough existing labeled data to train the individual parsers.', 'Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another.', 'The work of #AUTHOR_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing .', 'Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training.', 'Similar approaches are being explored for parsing Hwa et al. 2003).']",0,"['Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', 'They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.', 'This approach assumes that there are enough existing labeled data to train the individual parsers.', 'The work of #AUTHOR_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing .', 'Similar approaches are being explored for parsing Hwa et al. 2003).']"
CC424,J04-3001,Sample Selection for Statistical Parsing,heterogeneous uncertainty sampling for supervised learning,"['David D Lewis', 'Jason Catlett']",related work,"Uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances. These methods can greatly reduce the number of instances that an expert need label. One problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances. We test the use of one classifier (a highly efficient probabilistic one) to select examples for training another (the C4.5 rule induction program). Despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger.","Some examples include text categorization ( #AUTHOR_TAG ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .","['Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization ( #AUTHOR_TAG ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .']",0,"['Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization ( #AUTHOR_TAG ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .']"
CC425,J04-3001,Sample Selection for Statistical Parsing,combining labeled and unlabeled data with cotraining,"['Avrim Blum', 'Tom Mitchell']",related work,,"Another technique for making better use of unlabeled data is cotraining ( #AUTHOR_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another .","['Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', 'For example, Henderson and Brill (2002) consider the case in which acquiring additional human-annotated training data is not possible.', 'They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.', 'This approach assumes that there are enough existing labeled data to train the individual parsers.', 'Another technique for making better use of unlabeled data is cotraining ( #AUTHOR_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another .', 'The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing.', 'Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training.', 'Similar approaches are being explored for parsing Hwa et al. 2003).']",0,"['Another technique for making better use of unlabeled data is cotraining ( #AUTHOR_TAG ) , in which two sufficiently different learners help each other learn by labeling training data for one another .']"
CC426,J04-3001,Sample Selection for Statistical Parsing,stochastic lexicalized contextfree grammar,"['Yves Schabes', 'Richard Waters']",,"Stochastic lexicalized context-free grammar (SLCFG) is an attractive compromise between the parsing efficiency of stochastic context-free grammar (SCFG) and the lexical sensitivity of stochastic lexicalized tree-adjoining grammar (SLTAG) . SLCFG is a restricted form of SLTAG that can only generate context-free languages and can be parsed in cubic time. However, SLCFG retains the lexical sensitivity of SLTAG and is therefore a much better basis for capturing distributional information about words than SCFG.","Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #AUTHOR_TAG ; Hwa 1998 ) , and Collins 's Model 2 parser ( 1997 ) .","[""In applying sample selection to training a PP-attachment model, we have observed that all effective evaluation functions make use of the model's current hypothesis in estimating the training utility of the candidates."", 'Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor.', 'In this section, we investigate whether these observations hold true for training statistical parsing models as well.', ""Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #AUTHOR_TAG ; Hwa 1998 ) , and Collins 's Model 2 parser ( 1997 ) ."", 'Although both models are lexicalized, statistical parsers, their learning algorithms are different.', 'The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data.', ""In contrast, PLTIG's expectation-maximization-based induction algorithm is partially supervised; the model's parameters are estimated indirectly from the training data.""]",5,"[""Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( #AUTHOR_TAG ; Hwa 1998 ) , and Collins 's Model 2 parser ( 1997 ) ."", 'Although both models are lexicalized, statistical parsers, their learning algorithms are different.']"
CC427,J04-3001,Sample Selection for Statistical Parsing,prepositional phrase attachment through a backedoff model,"['Michael Collins', 'James Brooks']",,"Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity. Typically, ambiguous verb phrases of the form v np1 p np2 are resolved through a model which considers values of the four head words (v, n1, p and n2). This paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable. Results on Wall Street Journal data of 84.5% accuracy are obtained using this method. A surprising result is the importance of low-count events -- ignoring events which occur less than 5 times in training data reduces performance to 81.6%.","Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( #AUTHOR_TAG ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .","['One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( #AUTHOR_TAG ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', 'Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.', 'We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier.']",0,"['One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( #AUTHOR_TAG ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', 'Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.']"
CC428,J04-3001,Sample Selection for Statistical Parsing,scaling to very very large corpora for natural language disambiguation,"['Michele Banko', 'Eric Brill']",related work,"The amount of readily available online  text has reached hundreds of  billions of words and continues to  grow. Yet for most core natural  language tasks, algorithms continue  to be optimized, tested and compared  after training on corpora consisting  of only one million words or less. In  this paper, we evaluate the  performance of different learning  methods on a prototypical natural  language disambiguation task,  confusion set disambiguation, when  trained on orders of magnitude more  labeled data than has previously been  used. We are fortunate that for this  particular application, correctly  labeled training data is free. Since  this will often not be the case, we  examine methods for effectively  exploiting very large corpora when  labeled data comes at a cost","Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( #AUTHOR_TAG ) , and word sense disambiguation ( Fujii et al. 1998 ) .","['Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( #AUTHOR_TAG ) , and word sense disambiguation ( Fujii et al. 1998 ) .']",0,"['Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( #AUTHOR_TAG ) , and word sense disambiguation ( Fujii et al. 1998 ) .']"
CC429,J04-3001,Sample Selection for Statistical Parsing,the estimation of stochastic contextfree grammars using the insideoutside algorithm computer speech and language,"['Karim A Lari', 'Steve J Young']",,,"Using the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities ( #AUTHOR_TAG ) , we can efficiently compute the probability of the sentence , P ( w | G ) .","['Using the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities ( #AUTHOR_TAG ) , we can efficiently compute the probability of the sentence , P ( w | G ) .', 'Similarly, the algorithm can be modified to compute the quantity']",5,"['Using the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities ( #AUTHOR_TAG ) , we can efficiently compute the probability of the sentence , P ( w | G ) .']"
CC430,J04-3001,Sample Selection for Statistical Parsing,insideoutside reestimation from partially bracketed corpora,"['Fernando C N Pereira', 'Yves Schabes']",,"The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modelling of hierarchical structure than the original one. In particular, over 90% of the constituents in the most likely analyses of a test set are compatible with test set constituents for a grammar trained on a corpus of 700 hand-parsed part-of-speech strings for ATIS sentences.",Our algorithm is similar to the approach taken by #AUTHOR_TAG for inducing PCFG parsers .,"['In the first experiment, we use an induction algorithm (Hwa 2001a) based on the expectation-maximization (EM) principle that induces parsers for PLTIGs.', ""The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar's likelihood of generating the training data."", 'In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing.', 'Here, we consider a partially supervised variant in which we assume that the learner is given the phrasal boundaries of the training sentences but not the label of the constituent units.', 'For example, the sentence Several fund managers expect a rough market this morning before prices stabilize.', 'would be labeled as ""((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)""', 'Our algorithm is similar to the approach taken by #AUTHOR_TAG for inducing PCFG parsers .']",1,"['In the first experiment, we use an induction algorithm (Hwa 2001a) based on the expectation-maximization (EM) principle that induces parsers for PLTIGs.', ""The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar's likelihood of generating the training data."", 'In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing.', 'Our algorithm is similar to the approach taken by #AUTHOR_TAG for inducing PCFG parsers .']"
CC431,J04-3001,Sample Selection for Statistical Parsing,insideoutside reestimation from partially bracketed corpora,"['Fernando C N Pereira', 'Yves Schabes']",introduction,"The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modelling of hierarchical structure than the original one. In particular, over 90% of the constituents in the most likely analyses of a test set are compatible with test set constituents for a grammar trained on a corpus of 700 hand-parsed part-of-speech strings for ATIS sentences.","For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( #AUTHOR_TAG ) .","['Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data.', 'For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( #AUTHOR_TAG ) .', 'Current state-of-the-art statistical parsers (Collins 1999;Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993).', 'However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest.', 'For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora.', 'Because supervised training demands significant human involvement (e.g., annotating the syntactic structure of each sentence by hand), creating a new corpus is a labor-intensive and time-consuming endeavor.', ""The goal of this work is to minimize a system's reliance on annotated training data.""]",0,"['For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data ( #AUTHOR_TAG ) .']"
CC432,J04-3001,Sample Selection for Statistical Parsing,rule writing or annotation costefficient resource usage for base noun phrase chunking,"['Grace Ngai', 'David Yarowsky']",related work,"This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive real-time human annotation. Several novel variations on active learning are investigated, and underlying cost models for cross-modal machine learning comparison are presented and explored. Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment.","Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( #AUTHOR_TAG ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .","['Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( #AUTHOR_TAG ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .']",0,"['Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( #AUTHOR_TAG ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .']"
CC433,J04-3001,Sample Selection for Statistical Parsing,an empirical evaluation of probabilistic lexicalized tree insertion grammars,['Rebecca Hwa'],,"We present an empirical study of the applicability of Probabilistic Lexicalized Tree Insertion Grammars (PLTIG), a lexicalized counterpart to Probabilistic Context-Free Grammars (PCFG), to problems in stochastic natural-language processing. Comparing the performance of PLTIGs, with non-hierarchical N-gram models and PCFGs, we show that PLTIG combines the best aspects of both, with language modeling capability comparable to N-gram models and PCFGs, we show that PLTIG combines the best aspects of both, with language modeling capability comparable to N-grams, and improved parsing performance over its nonlexicalized counterpart. Furthermore, training of PLTIGs displays faster convergence than PCFGs.","Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters 1993 ; #AUTHOR_TAG ) , and Collins 's Model 2 parser ( 1997 ) .","[""In applying sample selection to training a PP-attachment model, we have observed that all effective evaluation functions make use of the model's current hypothesis in estimating the training utility of the candidates."", 'Although knowledge about the problem space seems to help sharpening the learning curve initially, overall, it is not a good predictor.', 'In this section, we investigate whether these observations hold true for training statistical parsing models as well.', ""Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters 1993 ; #AUTHOR_TAG ) , and Collins 's Model 2 parser ( 1997 ) ."", 'Although both models are lexicalized, statistical parsers, their learning algorithms are different.', 'The Collins Parser is a fully supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data.', ""In contrast, PLTIG's expectation-maximization-based induction algorithm is partially supervised; the model's parameters are estimated indirectly from the training data.""]",5,"[""Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters 1993 ; #AUTHOR_TAG ) , and Collins 's Model 2 parser ( 1997 ) .""]"
CC434,J04-3001,Sample Selection for Statistical Parsing,limitations of cotraining for natural language learning from large datasets,"['David Pierce', 'Claire Cardie']",related work,"Co-Training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifiers using separate views of the same data. This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data. This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels. Using base noun phrase bracketing as a case study, we find that co-training reduces by 36% the difference in error between co-trained classifiers and fully supervised classifiers trained on a labeled version of all available data. However, degradation in the quality of the bootstrapped data arises as an obstacle to further improvement. To address this, we propose a moderately supervised variant of cotraining in which a human corrects the mistakes made during automatic labeling. Our analysis suggests that corrected co-training and similar moderately supervised methods may help cotraining scale to large natural language learning tasks.","#AUTHOR_TAG have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training .","['Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', 'For example, Henderson and Brill (2002) consider the case in which acquiring additional human-annotated training data is not possible.', 'They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.', 'This approach assumes that there are enough existing labeled data to train the individual parsers.', 'Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another.', 'The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing.', '#AUTHOR_TAG have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training .', 'Similar approaches are being explored for parsing Hwa et al. 2003).']",0,"['For example, Henderson and Brill (2002) consider the case in which acquiring additional human-annotated training data is not possible.', '#AUTHOR_TAG have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training .']"
CC435,J04-3001,Sample Selection for Statistical Parsing,headdriven statistical models for natural language parsing,['Michael Collins'],introduction,"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.","Current state-of-the-art statistical parsers ( #AUTHOR_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) .","['Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data.', 'For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992).', 'Current state-of-the-art statistical parsers ( #AUTHOR_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) .', 'However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest.', 'For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora.', 'Because supervised training demands significant human involvement (e.g., annotating the syntactic structure of each sentence by hand), creating a new corpus is a labor-intensive and time-consuming endeavor.', ""The goal of this work is to minimize a system's reliance on annotated training data.""]",0,"['Current state-of-the-art statistical parsers ( #AUTHOR_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) .']"
CC436,J04-3001,Sample Selection for Statistical Parsing,a rule based approach to pp attachment disambiguation,"['Eric Brill', 'Philip S Resnik']",,,"Some well-known approaches include rule-based models ( #AUTHOR_TAG ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .","['One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( #AUTHOR_TAG ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', 'Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.', 'We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier.']",0,"['One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( #AUTHOR_TAG ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', 'Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.']"
CC437,J04-3001,Sample Selection for Statistical Parsing,learning probabilistic lexicalized grammars for natural language processing,['Rebecca Hwa'],,"A good representation of language is essential to building natural language processing (NLP) applications. In recent years, the growing availability of machine-readable text corpora has popularized the use of corpus-trained probabilistic grammars to represent languages in NLP systems. Although automatically inducing grammars from large corpora is an appealing idea, it faces several challenges. First, the trained grammar must capture the complexity and ambiguities inherent in human languages. Second, to be useful in practical applications, the grammar must be computationally tractable. Third, although there exists an abundance of raw text, the induction of high-quality grammars depends on manually annotated training data, which are scarce; therefore, the learning algorithm must be able to generalize well. Finally, there are inherent trade-offs in attempting to meet all three challenges; a meta-level challenge is to find a good compromise between the competing factors.  To address these challenges, this thesis presents a partially supervised induction algorithm based on the Expectation-Maximization principle for the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism. Using the lexical properties of the PLTIG formalism in the learning algorithm, we show that it is possible to automatically induce a grammar for a natural language that adequately resolves ambiguities and manages domain complexity at a tractable computational cost. By augmenting the basic learning algorithm with training techniques such as grammar adaptation and sample-selection, we show that the induction's dependency on annotated training data can be significantly reduced. Our empirical studies indicate that even with a 36% reduction in annotated training data, the learning algorithm can nonetheless induce grammars without degrading their quality.","In the first experiment , we use an induction algorithm ( #AUTHOR_TAGa ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .","['In the first experiment , we use an induction algorithm ( #AUTHOR_TAGa ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .', ""The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar's likelihood of generating the training data."", 'In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing.', 'Here, we consider a partially supervised variant in which we assume that the learner is given the phrasal boundaries of the training sentences but not the label of the constituent units.', 'For example, the sentence Several fund managers expect a rough market this morning before prices stabilize.', 'would be labeled as ""((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)""', 'Our algorithm is similar to the approach taken by Pereira and Schabes (1992) for inducing PCFG parsers.']",5,"['In the first experiment , we use an induction algorithm ( #AUTHOR_TAGa ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .', ""The algorithm performs heuristic search through an iterative reestimation procedure to find local optima: sets of values for the grammar parameters that maximizes the grammar's likelihood of generating the training data."", 'In principle, the algorithm supports unsupervised learning; however, because the search space has too many local optima, the algorithm tends to converge on a model that is unsuitable for parsing.', 'Here, we consider a partially supervised variant in which we assume that the learner is given the phrasal boundaries of the training sentences but not the label of the constituent units.']"
CC438,J04-3001,Sample Selection for Statistical Parsing,heterogeneous uncertainty sampling for supervised learning,"['David D Lewis', 'Jason Catlett']",,"Uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances. These methods can greatly reduce the number of instances that an expert need label. One problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances. We test the use of one classifier (a highly efficient probabilistic one) to select examples for training another (the C4.5 rule induction program). Despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger.","That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example ( #AUTHOR_TAG ) .","['Performance of the hypothesis: Testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly.', 'That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example ( #AUTHOR_TAG ) .', 'The underlying assumption is that an uncertain output is likely to be wrong.']",0,"['Performance of the hypothesis: Testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly.', 'That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example ( #AUTHOR_TAG ) .', 'The underlying assumption is that an uncertain output is likely to be wrong.']"
CC439,J04-3001,Sample Selection for Statistical Parsing,corrected cotraining for statistical parsers,"['Rebecca Hwa', 'Miles Osborne', 'Anoop Sarkar', 'Mark Steedman']",related work,,"Similar approaches are being explored for parsing ( Steedman , #AUTHOR_TAG ; Hwa et al. 2003 ) .","['Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', 'For example, Henderson and Brill (2002) consider the case in which acquiring additional human-annotated training data is not possible.', 'They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers.', 'This approach assumes that there are enough existing labeled data to train the individual parsers.', 'Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another.', 'The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing.', 'Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training.', 'Similar approaches are being explored for parsing ( Steedman , #AUTHOR_TAG ; Hwa et al. 2003 ) .']",0,"['The work of Sarkar (2001) and  suggests that co-training can be helpful for statistical parsing.', 'Similar approaches are being explored for parsing ( Steedman , #AUTHOR_TAG ; Hwa et al. 2003 ) .']"
CC440,J04-3001,Sample Selection for Statistical Parsing,natural language parsing as statistical pattern recognition,['David Magerman'],,"Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules.  In this work, I propose an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process. Based on distributionally-derived and linguistically-based features of language, this parser acquires a set of statistical decision trees which assign a probability distribution on the space of parse trees given the input sentence. These decision trees take advantage of significant amount of contextual information, potentially including all of the lexical information in the sentence, to produce highly accurate statistical models of the disambiguation process. By basing the disambiguation criteria selection on entropy reduction rather than human intuition, this parser development method is able to consider more sentences than a human grammarian can when making individual disambiguation rules.  In experiments between a parser, acquired using this statistical framework, and a grammarian's rule-based parser, developed over a ten-year period, both using the same training material and test sentences, the decision tree parser significantly outperformed the grammar-based parser on the accuracy measure which the grammarian was trying to maximize, achieving an accuracy of 78% compared to the grammar-based parser's 69%.",The head words can be automatically extracted using a heuristic table lookup in the manner described by #AUTHOR_TAG .,"['The Collins-Brooks PP-attachment classification algorithm.', 'preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification.', 'For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb).', 'The head words can be automatically extracted using a heuristic table lookup in the manner described by #AUTHOR_TAG .', 'For this learning problem, the supervision is the one-bit information of whether p should attach to v or to n.', 'In order to learn the attachment preferences of prepositional phrases, the system builds attachment statistics for each the characteristic tuple of all training examples.', 'A characteristic tuple is some subset of the four head words in the example, with the condition that one of the elements must be the preposition.', 'Each training example forms eight characteristic tuples:']",5,['The head words can be automatically extracted using a heuristic table lookup in the manner described by #AUTHOR_TAG .']
CC441,J04-3001,Sample Selection for Statistical Parsing,statistical models for unsupervised prepositional phrase attachment,['Adwait Ratnaparkhi'],,"We present several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task. Our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information. It is therefore less resource-intensive and more portable than previous corpus-based algorithms proposed for this task. We present results for prepositional phrase attachment in both English and Spanish.Comment: uses colacl.st","Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( #AUTHOR_TAG ) .","['One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( #AUTHOR_TAG ) .', 'Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.', 'We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier.']",0,"['One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( #AUTHOR_TAG ) .', 'Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model.']"
CC442,J04-3001,Sample Selection for Statistical Parsing,the estimation of stochastic contextfree grammars using the insideoutside algorithm computer speech and language,"['Karim A Lari', 'Steve J Young']",,,We follow the notation convention of #AUTHOR_TAG .,"['For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form. 14', 'The basic idea is to compose the tree entropy of the entire sentence from the tree entropy of the subtrees.', 'The process is similar to that for computing the probability of the entire sentence from the probabilities of substrings (called Inside Probabilities).', 'We follow the notation convention of #AUTHOR_TAG .']",5,"['For illustrative purposes, we describe the computation process using a PCFG expressed in Chomsky normal form. 14', 'The basic idea is to compose the tree entropy of the entire sentence from the tree entropy of the subtrees.', 'The process is similar to that for computing the probability of the entire sentence from the probabilities of substrings (called Inside Probabilities).', 'We follow the notation convention of #AUTHOR_TAG .']"
CC443,J04-3001,Sample Selection for Statistical Parsing,selective sampling using the query by committee algorithm,"['Yoav Freund', 'H Sebastian Seung', 'Eli Shamir', 'Naftali Tishby']",,"We analyze the ""query by committee"" algorithm, a method for filtering informative queries from a random stream of inputs. We show that if the two-member committee algorithm achieves information gain with positive lower bound, then the prediction error decreases exponentially with the number of queries. We show that, in particular, this exponential decrease holds for query learning of perceptrons.","The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .","['Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'There are two types of selection algorithms: committee-based and single learner.', 'A committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem).', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.']",0,"['Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'There are two types of selection algorithms: committee-based and single learner.', 'A committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem).', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #AUTHOR_TAG ) .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.']"
CC444,J05-3003,Gaussian coordinates and the large scale universe,duden—das stilworterbuch duden—the style dictionary,['editor Dudenredaktion'],,,"She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden ( #AUTHOR_TAG ) .","['Carroll and Rooth (1998) achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each.', 'Their system recognizes 15 frames, and these do not contain details of subcategorizedfor prepositions.', 'Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. Sarkar and Zeman (2000) evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88%.', 'However, their evaluation does not examine the extracted subcategorization frames but rather the argument-adjunct distinctions posited by their system.', 'The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden ( #AUTHOR_TAG ) .', 'We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.']",0,"['Carroll and Rooth (1998) achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each.', 'The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden ( #AUTHOR_TAG ) .', 'We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.']"
CC445,J05-3003,Gaussian coordinates and the large scale universe,automatic extraction of subcategorization from corpora,"['Edward Briscoe', 'John Carroll']",,"We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount1.","#AUTHOR_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .","['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above, it is well documented (Roland and Jurafsky 1998) that subcategorization frames (and their frequencies) vary across domains.', 'We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ.', 'For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon.', 'It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases.', 'This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs.', 'Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples.', 'As a generalization, Briscoe (2001) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall.', '#AUTHOR_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .', 'Precision was quite high (95%), but recall was low (84%).', 'This has an effect on both the precision and recall scores of our system against COMLEX.', 'In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.', 'We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as ""correct"" or ""incorrect.""', 'Of the 80 fps, 33 were manually judged to be legitimate subcategorization frames.', 'For example, as Table 26 shows, there are a number of correct transitive verbs ([subj,obj]) in our automatically induced lexicon which are not included in COMLEX.', 'This examination was also useful in highlighting to us the frame types on which the lexical extraction procedure was performing poorly, in our case, those containing XCOMPs and those containing OBJ2S.', 'Out of 80 fns, 14 were judged to be incorrect when manually examined.', 'These can be broken down as follows: one intransitive frame, three ditransitive frames, three frames containing a COMP, and seven frames containing an oblique were found to be invalid.']",1,"['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above, it is well documented (Roland and Jurafsky 1998) that subcategorization frames (and their frequencies) vary across domains.', 'For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon.', 'This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs.', '#AUTHOR_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .', 'Of the 80 fps, 33 were manually judged to be legitimate subcategorization frames.']"
CC446,J05-3003,Gaussian coordinates and the large scale universe,natural language parsing as statistical pattern recognition,['David Magerman'],related work,"Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules.  In this work, I propose an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process. Based on distributionally-derived and linguistically-based features of language, this parser acquires a set of statistical decision trees which assign a probability distribution on the space of parse trees given the input sentence. These decision trees take advantage of significant amount of contextual information, potentially including all of the lexical information in the sentence, to produce highly accurate statistical models of the disambiguation process. By basing the disambiguation criteria selection on entropy reduction rather than human intuition, this parser development method is able to consider more sentences than a human grammarian can when making individual disambiguation rules.  In experiments between a parser, acquired using this statistical framework, and a grammarian's rule-based parser, developed over a ten-year period, both using the same training material and test sentences, the decision tree parser significantly outperformed the grammar-based parser on the accuracy measure which the grammarian was trying to maximize, achieving an accuracy of 78% compared to the grammar-based parser's 69%.","The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #AUTHOR_TAG and Collins ( 1997 ) .","['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', ""The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #AUTHOR_TAG and Collins ( 1997 ) .', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997).', 'Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach, those of Xia (1999) and Hockenmaier, Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the Penn-II trees.', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of ""inverse schemata.""']",0,"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #AUTHOR_TAG and Collins ( 1997 ) .', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.']"
CC447,J05-3003,Gaussian coordinates and the large scale universe,tree adjoining grammars in,['Aravind Joshi'],introduction,"The VERBMOBIL project is developing a translation system that can assist a face-to-face dialogue between two non-native english speakers. Instead of having continiously speak english, the dialogue partners have the option to switch to their respective mother tongues (currently german or japanese) in cases where they can't find the required word, phrase or sentence. In such situations, the users activate VERBMOBIL to translate their utterances into english.  A very important requirement for such a system is realtime processing. Realtime processing is essentially necessary, if such a system is to be smoothly integrated into an ongoing communication. This can be achieved by the use of anytime processing, which always provides a result. The quality of the result however, depends on the computation time given to the system. Early interruptions can only produce shallow results. Aiming at such a processing mode, methods for fast but preliminary translation must be integrated into the system assisted by others that refine these results. In this case we suggest structural translation with Synchronous Tree Adjoining Grammars (S-TAGs), which can serve as a fast and shallow realisation of all steps necessary during translation, i.e. analysis, transfer and generation, in a system capable of running anytime methods. This mode is especially adequate for standardized speech acts and simple sentences. Furthermore, it provides a result for early interruptions of the translation process. By building an explicit linguistic structure, methods for refining the result can rearrange the structure in order to increase the quality of the translation given extended execution time. This paper describes the formalism of S-TAGs and the parsing algorithm implemented in VERBMOBIL. Furthermore the language covered by the german grammar is described. Finally we list examples together with the execution time required for their processing","In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ #AUTHOR_TAG ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ #AUTHOR_TAG ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']",0,"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ #AUTHOR_TAG ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']"
CC448,J05-3003,Gaussian coordinates and the large scale universe,lexical functional grammar volume 34 of syntax and semantics,['Mary Dalrymple'],,,"While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , #AUTHOR_TAG questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level .","['While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , #AUTHOR_TAG questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level .', 'LFG argues that subcategorization requirements are best stated at the f-structure level, in functional rather than phrasal terms.', 'This is because of the assumption that abstract grammatical functions are primitive concepts as opposed to derivatives of phrase structural position.', 'In LFG, the subcategorization requirements of a particular predicate are expressed by its semantic form: FOCUS (↑ SUBJ)(↑ OBL on ) in Figure 1.']",0,"['While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , #AUTHOR_TAG questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level .', 'This is because of the assumption that abstract grammatical functions are primitive concepts as opposed to derivatives of phrase structural position.']"
CC449,J05-3003,Gaussian coordinates and the large scale universe,automatic acquisition of a large subcategorisation dictionary from corpora,['Christopher Manning'],related work,,#AUTHOR_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", '#AUTHOR_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']",0,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', '#AUTHOR_TAG attempts to improve on the approach of Brent ( 1993 ) by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.']"
CC450,J05-3003,Gaussian coordinates and the large scale universe,automatic extraction of subcategorization from corpora,"['Edward Briscoe', 'John Carroll']",experiments,"We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount1.","#AUTHOR_TAG , by comparison , employ 163 distinct predefined frames .","['Applying an absolute threshold of five occurrences, we still generate 162 frame types  from Penn-II and 221 from Penn-III.', '#AUTHOR_TAG , by comparison , employ 163 distinct predefined frames .']",0,"['Applying an absolute threshold of five occurrences, we still generate 162 frame types  from Penn-II and 221 from Penn-III.', '#AUTHOR_TAG , by comparison , employ 163 distinct predefined frames .']"
CC451,J05-3003,Gaussian coordinates and the large scale universe,extracting tree adjoining grammars from bracketed corpora,['Fei Xia'],related work,"ExtractingTreeAdjoiningGrammarsfromBracketedCorp oraFeiXiaDepartmentofComputerandInformationScienceUniversityofPennsylvania3401WalnutStreet,Suite400APhiladelphiaPA19104,USAfxia@linc.cis.upenn.eduAbstractInthispap er,werep ortourorkonextractinglexi-calizedtreeadjoininggrammars(LTAGs)frompar-tiallybracketedcorp ora.Thealgorithm rstfullybracketsthecorp ora,thenextractselementarytrees(etrees),and nally ltersoutinvalidusinglinguisticknowledge.Weshowthatthesetofex-tractedetreesmaynotb ecompleteenoughtocoverthewholelanguage,butthiswillnothaveabigim-pactonparsing.1Intro ductionLexicalizedTreeAdjoiningGrammar(LAG)isatree-rewritingformalism.Itismoreexpressivethanacontext-freegrammar(CFG),1andthereforeb et-terformalismforrepresentingvariousphenomenainnaturallanguages.Inthelastdecade,ithasb eenappliedtovariousNLPtaskssuchasparsing(Srini-vas,1997),machinetranslation(Palmeretal.,1998),informationretrieval(ChandrasekarandSrinivas,1997),generation(StoneandDoran,1997;McCoyetal.,1992),andsummarizationapplications(Bald-winetal.,1997).Awide-coverageLTGforapar-ticularnaturallanguageoftencontainsthousandsoftreesandtakesyearstobuild.Therehasb eenworkonextractingCFGs(Shi-raietal.,1995;Charniak,1996;Krotovandoth-ers,1998)andlexicalizedtreegrammars(Neumann,1998;Srinivas,1997)frombracketedcorp ora.Inthispap er,weprop oseanewmetho dforlearningLTAGsfromsuchcorp ora.22PennTreebankandLAG2.1PennTreebankInthispap er,weusetheEnglishPennTreebankasourbracketedcorpus,whichincludesab out1millionTheauthorwishestothankChung-hyeHan,AravindJoshi,MarthaPalmer,CarlosProlo,Ano opSarkar,andthreeanonymousreviewersformanyhelpfulcomments.1LTAGismoreexpressivethanCFformalismb othinweakandstronggenerativecapacity,e.g.itcanhandlecrossdep endencyelegantly.2Arelatedworkis(Srinivas,1997),butitsgoalnottolearnanewLTAGbuttoextracttheusefulinformation,suchasdep endencyandfrequencyoftrees,foranexistingLTAG.","Unlike our approach , those of #AUTHOR_TAG and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees .","['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', ""The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997).', 'Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach , those of #AUTHOR_TAG and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees .', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of ""inverse schemata.""']",1,"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach , those of #AUTHOR_TAG and Hockenmaier , Bierner , and Baldridge ( 2004 ) include a substantial initial correction and clean-up of the Penn-II trees .', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.']"
CC452,J05-3003,Gaussian coordinates and the large scale universe,automated extraction of tags from the penn treebank,"['John Chen', 'K Vijay-Shanker']",introduction,"The accuracy of statistical parsing models can be improved with the use of lexical information. Statistical parsing using Lexicalized tree adjoining grammar (LTAG), a kind of lexicalized grammar, has remained relatively unexplored. We believe that is largely in part due to the absence of large corpora accurately bracketed in terms of a perspicuous yet broad coverage LTAG. Our work attempts to alleviate this difficulty. We extract different LTAGs from the Penn Treebank. We show that certain strategies yield an improved extracted LTAG in terms of compactness, broad coverage, and supertagging accuracy. Furthermore, we perform a preliminary investigation in smoothing these grammars by means of an external linguistic resource, namely, the tree families of an XTAG grammar, a hand built grammar of English.","Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #AUTHOR_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .","['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #AUTHOR_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', 'However, our approach also generalizes to CFG category-based approaches.', 'In LFG, subcategorization requirements are enforced through semantic forms specifying which grammatical functions are required by a particular predicate.', 'Our approach is based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and Way 1999) and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures (Cahill et al. 2002;Cahill, McCarthy, et al. 2004).', 'Our technique requires a treebank annotated with LFG functional schemata.', 'In the early approach of van Genabith, Sadler, and Way (1999), this was provided by manually annotating the rules extracted from the publicly available subset of the AP Treebank to automatically produce corresponding f-structures.', 'If the f-structures are of high quality, reliable LFG semantic forms can be generated quite simply by recursively reading off the subcategorizable grammatical functions for each local PRED value at each level of embedding in the f-structures.', 'The work reported in van Genabith, Sadler, and Way (1999) was small scale (100 trees) and proof of concept and required considerable manual annotation work.', 'It did not associate frames with probabilities, discriminate between frames for active and passive constructions, properly reflect the effects of long-distance dependencies (LDDs), or include CFG category information.', 'In this article we show how the extraction process can be scaled to the complete Wall Street Journal (WSJ) section of the Penn-II Treebank, with about one million words in 50,000 sentences, based on the automatic LFG f-structure annotation algorithm described in Cahill et al. (2002) and Cahill, McCarthy, et al. (2004).', 'More recently we have extended the extraction approach to the larger, domain-diverse Penn-III Treebank.', 'Aside from the parsed WSJ section, this version of the treebank contains parses for a subsection of the Brown corpus (almost 385,000 words in 24,000 trees) taken from a variety of text genres. 1 In addition to extracting grammatical-function-For the remainder of this work, when we refer to the Penn-II Treebank, we mean the parse-annotated WSJ, and when we refer to the Penn-III Treebank, we mean the parse-annotated WSJ and Brown corpus combined.']",0,"['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( #AUTHOR_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'Our approach is based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and Way 1999) and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures (Cahill et al. 2002;Cahill, McCarthy, et al. 2004).', 'Our technique requires a treebank annotated with LFG functional schemata.', 'In the early approach of van Genabith, Sadler, and Way (1999), this was provided by manually annotating the rules extracted from the publicly available subset of the AP Treebank to automatically produce corresponding f-structures.', 'The work reported in van Genabith, Sadler, and Way (1999) was small scale (100 trees) and proof of concept and required considerable manual annotation work.', 'In this article we show how the extraction process can be scaled to the complete Wall Street Journal (WSJ) section of the Penn-II Treebank, with about one million words in 50,000 sentences, based on the automatic LFG f-structure annotation algorithm described in Cahill et al. (2002) and Cahill, McCarthy, et al. (2004).', 'More recently we have extended the extraction approach to the larger, domain-diverse Penn-III Treebank.', 'Aside from the parsed WSJ section, this version of the treebank contains parses for a subsection of the Brown corpus (almost 385,000 words in 24,000 trees) taken from a variety of text genres. 1 In addition to extracting grammatical-function-For the remainder of this work, when we refer to the Penn-II Treebank, we mean the parse-annotated WSJ, and when we refer to the Penn-III Treebank, we mean the parse-annotated WSJ and Brown corpus combined.']"
CC453,J05-3003,Gaussian coordinates and the large scale universe,automatic extraction of subcategorization from corpora,"['Edward Briscoe', 'John Carroll']",related work,"We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount1.","#AUTHOR_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection .","['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', '#AUTHOR_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection .', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']",0,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', '#AUTHOR_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection .', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.']"
CC454,J05-3003,Gaussian coordinates and the large scale universe,the tiger treebank,"['Sabine Brants', 'Stefanie Dipper', 'Silvia Hansen', 'Wolfgang Lezius', 'George Smith']",,"Proceedings of the 16th Nordic Conference   of Computational Linguistics NODALIDA-2007.  Editors: Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek and Mare Koit.  University of Tartu, Tartu, 2007.  ISBN 978-9985-4-0513-0 (online)  ISBN 978-9985-4-0514-7 (CD-ROM)  pp. 81-88","We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank ( #AUTHOR_TAG ) and Penn Chinese Treebank (Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (Cahill et al. 2003) and Chinese (Burke, Lam, et al. 2004).","['We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank ( #AUTHOR_TAG ) and Penn Chinese Treebank (Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (Cahill et al. 2003) and Chinese (Burke, Lam, et al. 2004).', 'The lexical resources, however, have not yet been evaluated. This, and much else, has to await further research.']",5,"['We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank ( #AUTHOR_TAG ) and Penn Chinese Treebank (Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (Cahill et al. 2003) and Chinese (Burke, Lam, et al. 2004).', 'The lexical resources, however, have not yet been evaluated. This, and much else, has to await further research.']"
CC455,J05-3003,Gaussian coordinates and the large scale universe,lexical functional grammar a formal system for grammatical representation,"['Ronald Kaplan', 'Joan Bresnan']",,In learning their native language children develop a remarkable set of capabilities They acquire knowledge and skills that enable them to pro duce and comprehend an inde nite number of novel utterances and to make quite subtle judgments about certain of their properties The ma jor goal of psycholinguistic research is to devise an explanatory account of the mental operations that underlie these linguistic abilities In pursuing this goal we have adopted what we call the Competence Hypothesis as a methodological principle We assume that an explana tory model of human language performance will incorporate a theoreti cally justi ed representation of the native speaker s linguistic knowledge a grammar as a component separate both from the computational mech anisms that operate on it a processor and from other nongrammatical processing parameters that might in uence the processor s behavior To a certain extent the various components that we postulate can be studied independently guided where appropriate by the well established methods and evaluation standards of linguistics computer science and experimen tal psychology However the requirement that the various components ultimately must t together in a consistent and coherent model imposes even stronger constraints on their structure and operation,The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure ( #AUTHOR_TAG ) : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs .,['The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure ( #AUTHOR_TAG ) : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs .'],0,['The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure ( #AUTHOR_TAG ) : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs .']
CC456,J05-3003,Gaussian coordinates and the large scale universe,lexical functional grammar volume 34 of syntax and semantics,['Mary Dalrymple'],,,Lexical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG ) is a member of the family of constraint-based grammars .,"['Lexical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'C-structure takes the form of phrase structure trees and is defined in terms of CFG rules and lexical entries.', 'F-structure is produced from functional annotations on the nodes of the c-structure and implemented in terms of recursive feature structures (attribute-value matrices).', 'This is exemplified by the analysis of the string The inquiry soon focused on the judge (wsj 0267 72) using the grammar in Figure 1, which results in the annotated c-structure and f-structure in Figure 2.']",0,['Lexical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG ) is a member of the family of constraint-based grammars .']
CC457,J05-3003,Gaussian coordinates and the large scale universe,lexicalfunctional syntax,['Joan Bresnan'],,,Lexical functional grammar ( Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .,"['Lexical functional grammar ( Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'C-structure takes the form of phrase structure trees and is defined in terms of CFG rules and lexical entries.', 'F-structure is produced from functional annotations on the nodes of the c-structure and implemented in terms of recursive feature structures (attribute-value matrices).', 'This is exemplified by the analysis of the string The inquiry soon focused on the judge (wsj 0267 72) using the grammar in Figure 1, which results in the annotated c-structure and f-structure in Figure 2.']",0,['Lexical functional grammar ( Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .']
CC458,J05-3003,Gaussian coordinates and the large scale universe,lexical functional grammar volume 34 of syntax and semantics,['Mary Dalrymple'],,,"According to #AUTHOR_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .","['The value of the PRED attribute in an f-structure is a semantic form Π gf 1 , gf 2 , . . .', ', gf n , where Π is a lemma and gf a grammatical function.', 'The semantic form provides an argument list gf 1 ,gf 2 , . . .', ',gf n specifying the governable grammatical functions (or arguments) required by the predicate to form a grammatical construction.', 'In Figure 1 the verb FOCUS requires a subject and an oblique object introduced by the preposition on: FOCUS (↑ SUBJ)(↑ OBL on ) .', 'The argument list can be empty, as in the PRED value for judge in Figure 1.', 'According to #AUTHOR_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .', 'OBJ θ and OBL θ represent families of grammatical functions indexed by their semantic role, represented by the theta subscript.', 'This list of grammatical functions is divided into governable (subcategorizable) grammatical functions (arguments) and nongovernable (nonsubcategorizable) grammatical functions (modifiers/adjuncts), as summarized in Table 1.']",0,"['The semantic form provides an argument list gf 1 ,gf 2 , . . .', ',gf n specifying the governable grammatical functions (or arguments) required by the predicate to form a grammatical construction.', 'According to #AUTHOR_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .', 'OBJ th and OBL th represent families of grammatical functions indexed by their semantic role, represented by the theta subscript.', 'This list of grammatical functions is divided into governable (subcategorizable) grammatical functions (arguments) and nongovernable (nonsubcategorizable) grammatical functions (modifiers/adjuncts), as summarized in Table 1.']"
CC459,J05-3003,Gaussian coordinates and the large scale universe,automatic extraction of subcategorization frames for czech,"['Anoop Sarkar', 'Daniel Zeman']",,"We present some novel machine learning techniques for the identification of subcategorization information for verbs in Czech. We compare three different statistical techniques applied to this problem. We show how the learning algorithm can be used to discover previously unknown subcategorization frames from the Czech Prague Dependency Treebank. The algorithm can then be used to label dependents of a verb in the Czech treebank as either arguments or adjuncts. Using our techniques, we are able to achieve 88 % precision on unseen parsed text.",#AUTHOR_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .,"['Carroll and Rooth (1998) achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each.', 'Their system recognizes 15 frames, and these do not contain details of subcategorized- for prepositions.', 'Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3.', '#AUTHOR_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .', 'However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument�adjunct distinctions posited by their sys- tem.', 'The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (Dudenredaktion 2001).', 'We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.']",1,"['Their system recognizes 15 frames, and these do not contain details of subcategorized- for prepositions.', 'Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3.', '#AUTHOR_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .', 'However, their evaluation does not examine the extracted subcatego- rization frames but rather the argumentadjunct distinctions posited by their sys- tem.', 'The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German.']"
CC460,J05-3003,Gaussian coordinates and the large scale universe,the automatic acquisition of frequencies of verb subcategorization frames from tagged corpora,"['Akira Ushioda', 'David Evans', 'Ted Gibson', 'Alex Waibel']",method,"We describe a mechanism for automatically acquiring verb subcategorization frames and their frequencies in a large corpus. A tagged corpus is first partially parsed to identify noun phrases and then a finear grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus. In an experiment involving the identification of six fixed subcategorization frames, our current system showed more than 80% accuracy. In addition, a new statistical approach substantially improves the accuracy of the frequency estimation.","Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( #AUTHOR_TAG ) .","['The syntactic functions COMP and XCOMP refer to clausal complements with different predicate control patterns as described in Section 2. However, as it stands, neither of these functions betrays anything about the syntactic nature of the constructs in question.', 'Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( #AUTHOR_TAG ) .', 'With only a slight modification, our system, along with the details provided by the automatically generated f-structures, allows us to extract frames with an equivalent level of detail.', 'For example, to identify a that-clause, we use']",0,"['Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause ( #AUTHOR_TAG ) .']"
CC461,J05-3003,Gaussian coordinates and the large scale universe,statistical decision tree models for parsing,['David Magerman'],related work,"Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to text-processing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result. This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing {$n$}-gram modeling techniques are inadequate for parsing models. In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser. Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATTER achieves 86 % precision, 86 % recall, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91 % precision, 90 % recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length.Comment: uses aclap.sty, psfig.tex (v1.9), postscript figure",The extraction procedure utilizes a head percolation table as introduced by #AUTHOR_TAG in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct .,"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', ""The extraction procedure utilizes a head percolation table as introduced by #AUTHOR_TAG in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct ."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997).', 'Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach, those of Xia (1999) and Hockenmaier, Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the Penn-II trees.', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of ""inverse schemata.""']",0,"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', ""The extraction procedure utilizes a head percolation table as introduced by #AUTHOR_TAG in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct ."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.']"
CC462,J05-3003,Gaussian coordinates and the large scale universe,from grammar to lexicon unsupervised learning of lexical syntax,['Michael Brent'],related work,"Imagine a language that is completely unfamiliar; the only means of studying it are an ordinary grammar book and a very large corpus of text. No dictionary is available. How can easily recognized, surface grammatical facts be used to extract from a corpus as much syntactic information as possible about individual words? This paper describes an approach based on two principles. First, rely on local morpho-syntactic cues to structure rather than trying to parse entire sentences. Second, treat these cues as probabilistic rather than absolute indicators of syntactic structure. Apply inferential statistics to the data collected using the cues, rather than drawing a categorical conclusion from a single occurrence of a cue. The effectiveness of this approach for inferring the syntactic frames of verbs is supported by experiments on an English corpus using a program called Lerner. Lerner starts out with no knowledge of content words---it bootstraps from determiners, auxiliaries, modals, prepositions, pronouns, complementizers, coordinating conjunctions, and punctuation.","The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #AUTHOR_TAG .","['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #AUTHOR_TAG .', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']",0,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #AUTHOR_TAG .', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.']"
CC463,J05-3003,Gaussian coordinates and the large scale universe,lexical functional grammar a formal system for grammatical representation,"['Ronald Kaplan', 'Joan Bresnan']",introduction,In learning their native language children develop a remarkable set of capabilities They acquire knowledge and skills that enable them to pro duce and comprehend an inde nite number of novel utterances and to make quite subtle judgments about certain of their properties The ma jor goal of psycholinguistic research is to devise an explanatory account of the mental operations that underlie these linguistic abilities In pursuing this goal we have adopted what we call the Competence Hypothesis as a methodological principle We assume that an explana tory model of human language performance will incorporate a theoreti cally justi ed representation of the native speaker s linguistic knowledge a grammar as a component separate both from the computational mech anisms that operate on it a processor and from other nongrammatical processing parameters that might in uence the processor s behavior To a certain extent the various components that we postulate can be studied independently guided where appropriate by the well established methods and evaluation standards of linguistics computer science and experimen tal psychology However the requirement that the various components ultimately must t together in a consistent and coherent model imposes even stronger constraints on their structure and operation,"We applied lexical-redundancy rules ( #AUTHOR_TAG ) to automatically convert the active COMLEX frames to their passive counterparts: For example, subjects are demoted to optional by oblique agents, and direct objects become subjects.","['We applied lexical-redundancy rules ( #AUTHOR_TAG ) to automatically convert the active COMLEX frames to their passive counterparts: For example, subjects are demoted to optional by oblique agents, and direct objects become subjects.', 'The resulting precision was very high (from 72.3% to 80.2%), and there was the expected drop in recall when prepositional details were included (from 54.7% to 29.3%).']",5,"['We applied lexical-redundancy rules ( #AUTHOR_TAG ) to automatically convert the active COMLEX frames to their passive counterparts: For example, subjects are demoted to optional by oblique agents, and direct objects become subjects.']"
CC464,J05-3003,Gaussian coordinates and the large scale universe,a comparison of evaluation metrics for a broad coverage parser,"['Richard Crouch', 'Ron Kaplan', 'Tracy King', 'Stefan Riezler']",method,,Both use the evaluation software and triple encoding presented in #AUTHOR_TAG .,"['In order to ensure the quality of the semantic forms extracted by our method, we must first ensure the quality of the f-structure annotations.', 'The results of two different evaluations of the automatically generated f-structures are presented in Table 2.', 'Both use the evaluation software and triple encoding presented in #AUTHOR_TAG .', 'The first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-structures']",5,"['The results of two different evaluations of the automatically generated f-structures are presented in Table 2.', 'Both use the evaluation software and triple encoding presented in #AUTHOR_TAG .', 'The first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-structures']"
CC465,J05-3003,Gaussian coordinates and the large scale universe,the parc 700 dependency bank,"['Tracy Holloway King', 'Richard Crouch', 'Stefan Riezler', 'Mary Dalrymple', 'Ronald Kaplan']",method,"An automatic method for annotating the Penn-II Treebank (Marcus et al., 1994) with high-level Lexical Functional Grammar (Kaplan and Bresnan, 1982; Bresnan, 2001; Dalrymple, 2001) f-structure representations is described in (Cahill et al., 2002; Cahill et al., 2004a; Cahill et al., 2004b; O'Donovan et al., 2004). The annotation algorithm and the automatically-generated f-structures are the basis for the automatic acquisition of wide-coverage and robust probabilistic approximations of LFG grammars (Cahill et al., 2002; Cahill et al., 2004a) and for the induction of LFG semantic forms (O'Donovan et al., 2004). The quality of the annotation algorithm and the f-structures it generates is, therefore, extremely important. To date, annotation quality has been measured in terms of precision and recall against the DCU 105. The annotation algorithm currently achieves an f-score of 96.57% for complete f-structures and 94.3% for preds-only f-structures. There are a number of problems with evaluating against a gold standard of this size, most notably that of overfitting. There is a risk of assuming that the gold standard is a complete and balanced representation of the linguistic phenomena in a language and basing design decisions on this. It is, therefore, preferable to evaluate against a more extensive, external standard. Although the DCU 105 is publicly available, 1 a larger well-established external standard can provide a more widely-recognised benchmark against which the quality of the f-structure annotation algorithm can be evaluated. For these reasons, we present an evaluation of the f-structure annotation algorithm of (Cahill et al., 2002; Cahill et al., 2004a; Cahill et al., 2004b; O'Donovan et al., 2004) against the PARC 700 Dependency Bank (King et al., 2003). Evaluation against an external gold standard is a non-trivial task as linguistic analyses may differ systematically between the gold standard and the output to be evaluated as regards feature geometry and nomenclature. We present conversion software to automatically account for many (but not all) of the systematic differences. Currently, we achieve an f-score of 87.31% for the f-structures generated from the original Penn-II trees and an f-score of 81.79% for f-structures from parse trees produced by Charniak's (2000) parser in our pipeline parsing architecture against the PARC 700","More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank ( #AUTHOR_TAG ) , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators .","['from Section 23 of the Penn Treebank as described in Cahill, McCarthy, et al. (2004).', 'For the full set of annotations they achieve precision of over 96.5% and recall of over 96.6%.', 'There is, however, a risk of overfitting when evaluation is limited to a gold standard of this size.', 'More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank ( #AUTHOR_TAG ) , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators .', 'They report precision of over 88.5% and recall of over 86% (Table 2).', 'The PARC 700 Dependency Bank differs substantially from both the DCU 105 f-structure bank and the automatically generated f-structures in regard to the style of linguistic analysis, feature nomenclature, and feature geometry.', 'Some, but not all, of these differences are captured by automatic conversion software.', 'A detailed discussion of the issues inherent in this process and a full analysis of results is presented in Burke, Cahill, et al. (2004a).', 'Results broken down by grammatical function for the DCU 105 evaluation are presented in Table 3. OBL (prepositional phrase) arguments are traditionally difficult to annotate reliably.', 'The results show, however, that with respect to obliques, the annotation algorithm, while slightly conservative (recall of 82%), is very accurate: 96% of the time it annotates an oblique, the annotation is correct.']",0,"['More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank ( #AUTHOR_TAG ) , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators .', 'The PARC 700 Dependency Bank differs substantially from both the DCU 105 f-structure bank and the automatically generated f-structures in regard to the style of linguistic analysis, feature nomenclature, and feature geometry.']"
CC466,J05-3003,Gaussian coordinates and the large scale universe,natural language parsing as statistical pattern recognition,['David Magerman'],related work,"Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules.  In this work, I propose an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process. Based on distributionally-derived and linguistically-based features of language, this parser acquires a set of statistical decision trees which assign a probability distribution on the space of parse trees given the input sentence. These decision trees take advantage of significant amount of contextual information, potentially including all of the lexical information in the sentence, to produce highly accurate statistical models of the disambiguation process. By basing the disambiguation criteria selection on entropy reduction rather than human intuition, this parser development method is able to consider more sentences than a human grammarian can when making individual disambiguation rules.  In experiments between a parser, acquired using this statistical framework, and a grammarian's rule-based parser, developed over a ten-year period, both using the same training material and test sentences, the decision tree parser significantly outperformed the grammar-based parser on the accuracy measure which the grammarian was trying to maximize, achieving an accuracy of 78% compared to the grammar-based parser's 69%.","Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #AUTHOR_TAG and Collins ( 1997 ) ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.","['Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii (2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #AUTHOR_TAG and Collins ( 1997 ) ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of �inverse schemata.�']",5,"['Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #AUTHOR_TAG and Collins ( 1997 ) ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.']"
CC467,J05-3003,Gaussian coordinates and the large scale universe,on the order of words,"['Anthony Ades', 'Mark Steedman']",introduction,"Visual referring expression recognition is a challenging task that requires natural language understanding in the context of an image. We critically examine RefCOCOg, a standard benchmark for this task, using a human study and show that 83.7% of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the word order doesn't matter. To measure the true progress of existing models, we split the test set into two sets, one which requires reasoning on linguistic structure and the other which doesn't. Additionally, we create an out-of-distribution dataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that the target object changes. Using these datasets, we empirically show that existing methods fail to exploit linguistic structure and are 12% to 23% lower in performance than the established progress for this task. We also propose two methods, one based on contrastive learning and the other based on multi-task learning, to increase the robustness of ViLBERT, the current state-of-the-art model for this task. Our datasets are publicly available at https://github.com/aws/aws-refcocog-advComment: ACL 202","In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ #AUTHOR_TAG ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ #AUTHOR_TAG ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']",0,"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ #AUTHOR_TAG ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']"
CC468,J05-3003,Gaussian coordinates and the large scale universe,automatic fstructure annotation of treebank trees,['Anette Frank'],method,,"Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ; #AUTHOR_TAG ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .","['The first step in the application of our methodology is the production of a treebank annotated with LFG f-structure information.', 'F-structures are attribute-value structures which represent abstract syntactic information, approximating to basic predicate-argument-modifier structures.', 'Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ; #AUTHOR_TAG ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .', 'However, more recent work (Cahill et al. 2002;Cahill, McCarthy, et al. 2004) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank (Marcus et al. 1994), containing more than 1,000,000 words and 49,000 sentences.']",0,"['The first step in the application of our methodology is the production of a treebank annotated with LFG f-structure information.', 'Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ; #AUTHOR_TAG ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .']"
CC469,J05-3003,Gaussian coordinates and the large scale universe,subcategorization acquisition as,['Anna Korhonen'],related work,"Evaluation of word sense disambiguation (WSD) systems is often based on machine-readable dictionaries (MRDs). Such evaluation typically employs a set of fine-grained dictionary senses and considers them all to be equally important. In this paper, we propose a novel evaluation method for WSD systems in the context of automatic subcategorization acquisition. Building on an extant subcategorization acquisition system, we show that the system would benefit from WSD and propose modifications which allow it to make use of WSD. The enhanced subcategorization acquisition system can then be used as a task-based evaluation method for WSD systems where both the notion of sense and the sense's relevance to the evaluation process is determined by the application itself. 1",Recent work by #AUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection .,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by #AUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection .', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']",0,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by #AUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection .', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.']"
CC470,J05-3003,Gaussian coordinates and the large scale universe,compacting the penn treebank grammar,"['Alexander Krotov', 'Mark Hepple', 'Robert Gaizauskas', 'Yorick Wilks']",,"Treebanks, such as the Penn Treebank (PTB), offer a simple approach to obtaining a broad coverage grammar: one can simply read the grammar off the parse trees in the treebank. While such a grammar is easy to obtain, a square-root rate of growth of the rule set with corpus size suggests that the derived grammar is far from complete and that much more treebanked text would be required to obtain a complete grammar, if one exists at some limit. However, we offer an alternative explanation in terms of the underspecification of structures within the treebank. This hypothesis is explored by applying an algorithm to compact the derived grammar by eliminating redundant rules - rules whose right hand sides can be parsed by other rules. The size of the resulting compacted grammar, which is significantly less than that of the full treebank grammar, is shown to approach a limit. However, such a compacted grammar does not yield very good performance figures. A version of the compaction algorithm taking rule probabilities into account is proposed, which is argued to be more linguistically motivated. Combined with simple thresholding, this method can be used to give a 58% reduction in grammar size without significant change in parsing performance, and can produce a 69% reduction with some gain in recall, but a loss in precision.","In Charniak ( 1996 ) and #AUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .","['The rate of accession may also be represented graphically.', 'In Charniak ( 1996 ) and #AUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity.', 'Figure 8 graphs the rate of induction of semantic form and CFG rule types from Penn-III (the WSJ and parse-annotated Brown corpus combined).', 'Because of the variation in the size of sections between the Brown and the WSJ, we plotted accession against word count.', 'The first part of the graph (up to 1,004,414 words)']",0,"['In Charniak ( 1996 ) and #AUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .']"
CC471,J05-3003,Gaussian coordinates and the large scale universe,three generative lexicalised models for statistical parsing,['Michael Collins'],related work,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on Magerman ( 1994 ) and #AUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.","['Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii (2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on Magerman ( 1994 ) and #AUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of �inverse schemata.�']",5,"['Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on Magerman ( 1994 ) and #AUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.']"
CC472,J05-3003,Gaussian coordinates and the large scale universe,identifying verb arguments and their syntactic function in the penn treebank,"['Alexandra Kinyon', 'Carlos Prolo']",related work,"In this paper, we present a tool that allows one to automatically extract verb argument-structure from the Penn Treebank as well as from other corpora annotated with the Penn Treebank release 2 conventions. More specifically, we examine each possible sequence of tags, both functional and categorial and determine whether such a sequence indicates an obligatory argument, an optional argument or a modifier. We argue that this approach is more fine-grained and thus more satisfactory than the existing approaches which have aimed at determining argumenthood in the Penn Treebank. The goal of this work is to provide a set of sufficiently general and fine-grained rules as well as an implementation which will be reusable and freely available to the research community. 1",#AUTHOR_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .,"['Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data.', '#AUTHOR_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arguments were then mapped to traditional syntactic functions.', 'For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject.', 'In general, argumenthood was preferred over adjuncthoood.', 'As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is.', 'Sarkar and Zeman (2000) present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank (Hajic 1998).', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', 'This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score.', 'The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more).', 'Marinov and Hemming (2004) present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank (Simov, Popova, and Osenova 2002).', ""In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming's system collects both arguments and adjuncts."", 'It then uses the binomial log-likelihood ratio to filter incorrect frames.', 'The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees.', 'The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences.']",0,['#AUTHOR_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .']
CC473,J05-3003,Gaussian coordinates and the large scale universe,automatic extraction of subcategorization frames from the bulgarian tree bank,"['Svetoslav Marinov', 'Cecilia Hemming']",related work,"(1) a. Teodora opened the door. b. *Arto looked the door. In (1-a) the verb open takes as an obligatory argument an NP and therefore differs from look in (1-b), which is an ill-formed sentence, because look require","#AUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) .","['Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data.', 'Kinyon and Prolo (2002) describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank.', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arguments were then mapped to traditional syntactic functions.', 'For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject.', 'In general, argumenthood was preferred over adjuncthoood.', 'As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is.', 'Sarkar and Zeman (2000) present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank (Hajic 1998).', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', 'This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score.', 'The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more).', '#AUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) .', ""In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming's system collects both arguments and adjuncts."", 'It then uses the binomial log-likelihood ratio to filter incorrect frames.', 'The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees.', 'The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences.']",0,"['#AUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) .']"
CC474,J05-3003,Gaussian coordinates and the large scale universe,extracting tree adjoining grammars from bracketed corpora,['Fei Xia'],related work,"ExtractingTreeAdjoiningGrammarsfromBracketedCorp oraFeiXiaDepartmentofComputerandInformationScienceUniversityofPennsylvania3401WalnutStreet,Suite400APhiladelphiaPA19104,USAfxia@linc.cis.upenn.eduAbstractInthispap er,werep ortourorkonextractinglexi-calizedtreeadjoininggrammars(LTAGs)frompar-tiallybracketedcorp ora.Thealgorithm rstfullybracketsthecorp ora,thenextractselementarytrees(etrees),and nally ltersoutinvalidusinglinguisticknowledge.Weshowthatthesetofex-tractedetreesmaynotb ecompleteenoughtocoverthewholelanguage,butthiswillnothaveabigim-pactonparsing.1Intro ductionLexicalizedTreeAdjoiningGrammar(LAG)isatree-rewritingformalism.Itismoreexpressivethanacontext-freegrammar(CFG),1andthereforeb et-terformalismforrepresentingvariousphenomenainnaturallanguages.Inthelastdecade,ithasb eenappliedtovariousNLPtaskssuchasparsing(Srini-vas,1997),machinetranslation(Palmeretal.,1998),informationretrieval(ChandrasekarandSrinivas,1997),generation(StoneandDoran,1997;McCoyetal.,1992),andsummarizationapplications(Bald-winetal.,1997).Awide-coverageLTGforapar-ticularnaturallanguageoftencontainsthousandsoftreesandtakesyearstobuild.Therehasb eenworkonextractingCFGs(Shi-raietal.,1995;Charniak,1996;Krotovandoth-ers,1998)andlexicalizedtreegrammars(Neumann,1998;Srinivas,1997)frombracketedcorp ora.Inthispap er,weprop oseanewmetho dforlearningLTAGsfromsuchcorp ora.22PennTreebankandLAG2.1PennTreebankInthispap er,weusetheEnglishPennTreebankasourbracketedcorpus,whichincludesab out1millionTheauthorwishestothankChung-hyeHan,AravindJoshi,MarthaPalmer,CarlosProlo,Ano opSarkar,andthreeanonymousreviewersformanyhelpfulcomments.1LTAGismoreexpressivethanCFformalismb othinweakandstronggenerativecapacity,e.g.itcanhandlecrossdep endencyelegantly.2Arelatedworkis(Srinivas,1997),butitsgoalnottolearnanewLTAGbuttoextracttheusefulinformation,suchasdep endencyandfrequencyoftrees,foranexistingLTAG.",#AUTHOR_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank .,"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', ""The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', '#AUTHOR_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank .', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997).', 'Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach, those of Xia (1999) and Hockenmaier, Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the Penn-II trees.', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of ""inverse schemata.""']",0,"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', '#AUTHOR_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank .', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.']"
CC475,J05-3003,Gaussian coordinates and the large scale universe,lexical functional grammar a formal system for grammatical representation,"['Ronald Kaplan', 'Joan Bresnan']",introduction,In learning their native language children develop a remarkable set of capabilities They acquire knowledge and skills that enable them to pro duce and comprehend an inde nite number of novel utterances and to make quite subtle judgments about certain of their properties The ma jor goal of psycholinguistic research is to devise an explanatory account of the mental operations that underlie these linguistic abilities In pursuing this goal we have adopted what we call the Competence Hypothesis as a methodological principle We assume that an explana tory model of human language performance will incorporate a theoreti cally justi ed representation of the native speaker s linguistic knowledge a grammar as a component separate both from the computational mech anisms that operate on it a processor and from other nongrammatical processing parameters that might in uence the processor s behavior To a certain extent the various components that we postulate can be studied independently guided where appropriate by the well established methods and evaluation standards of linguistics computer science and experimen tal psychology However the requirement that the various components ultimately must t together in a consistent and coherent model imposes even stronger constraints on their structure and operation,"In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']",0,"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']"
CC476,J05-3003,Gaussian coordinates and the large scale universe,how verb subcategorization frequencies are affected by corpus choice,"['Douglas Roland', 'Daniel Jurafsky']",,"The probabilistic relation between verbs and their arguments plays an important role in modern statistical parsers and supertaggers, and in psychological theories of language processing. But these probabilities are computed in very different ways by the two sets of researchers. Computational linguists compute verb subcategorization probabilities from large corpora while psycholinguists compute them from psychological studies (sentence production and completion tasks). Recent studies have found differences between corpus frequencies and psycholinguistic measures. We analyze subcategorization frequencies from four different corpora: psychological sentence production data (Connine et al. 1984), written text (Brown and WSJ), and telephone conversation data (Switchboard). We find two different sources for the differences. Discourse influence is a result of how verb use is affected by different discourse types such as narrative, connected discourse, and single sentence productions. Semantic influence is a result of different corpora using different senses of verbs, which have different subcategorization frequencies. We conclude that verb sense and discourse type play an important role in the frequencies observed in different experimental and corpus based sources of verb subcategorization frequencies.",It has been shown ( #AUTHOR_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains .,"['Recently we have applied our methodology to the Penn-III Treebank, a more balanced corpus resource with a number of text genres.', 'Penn-III consists of the WSJ section from Penn-II as well as a parse-annotated subset of the Brown corpus.', 'The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor.', 'It has been shown ( #AUTHOR_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains .', 'Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur.', 'The f-structure annotation algorithm was extended with only minor amendments to cover the parsed Brown corpus.', 'The most important of these was the way in which we distinguish between oblique and adjunct.', 'We noted in Section 4 that our method of assigning an oblique annotation in Penn-II was precise, albeit conservative.', 'Because of a change of annotation policy in Penn-III, the -CLR tag (indicating a close relationship between a PP and the local syntactic head), information which we had previously exploited, is no longer used.', 'For Penn-III the algorithm annotates all PPs which do not carry a Penn adverbial functional tag (such as -TMP or -LOC) and occur as the sisters of the verbal head of a VP as obliques.']",4,['It has been shown ( #AUTHOR_TAG ) that the subcategorization tendencies of verbs vary across linguistic domains .']
CC477,J05-3003,Gaussian coordinates and the large scale universe,extracting tree adjoining grammars from bracketed corpora,['Fei Xia'],introduction,"ExtractingTreeAdjoiningGrammarsfromBracketedCorp oraFeiXiaDepartmentofComputerandInformationScienceUniversityofPennsylvania3401WalnutStreet,Suite400APhiladelphiaPA19104,USAfxia@linc.cis.upenn.eduAbstractInthispap er,werep ortourorkonextractinglexi-calizedtreeadjoininggrammars(LTAGs)frompar-tiallybracketedcorp ora.Thealgorithm rstfullybracketsthecorp ora,thenextractselementarytrees(etrees),and nally ltersoutinvalidusinglinguisticknowledge.Weshowthatthesetofex-tractedetreesmaynotb ecompleteenoughtocoverthewholelanguage,butthiswillnothaveabigim-pactonparsing.1Intro ductionLexicalizedTreeAdjoiningGrammar(LAG)isatree-rewritingformalism.Itismoreexpressivethanacontext-freegrammar(CFG),1andthereforeb et-terformalismforrepresentingvariousphenomenainnaturallanguages.Inthelastdecade,ithasb eenappliedtovariousNLPtaskssuchasparsing(Srini-vas,1997),machinetranslation(Palmeretal.,1998),informationretrieval(ChandrasekarandSrinivas,1997),generation(StoneandDoran,1997;McCoyetal.,1992),andsummarizationapplications(Bald-winetal.,1997).Awide-coverageLTGforapar-ticularnaturallanguageoftencontainsthousandsoftreesandtakesyearstobuild.Therehasb eenworkonextractingCFGs(Shi-raietal.,1995;Charniak,1996;Krotovandoth-ers,1998)andlexicalizedtreegrammars(Neumann,1998;Srinivas,1997)frombracketedcorp ora.Inthispap er,weprop oseanewmetho dforlearningLTAGsfromsuchcorp ora.22PennTreebankandLAG2.1PennTreebankInthispap er,weusetheEnglishPennTreebankasourbracketedcorpus,whichincludesab out1millionTheauthorwishestothankChung-hyeHan,AravindJoshi,MarthaPalmer,CarlosProlo,Ano opSarkar,andthreeanonymousreviewersformanyhelpfulcomments.1LTAGismoreexpressivethanCFformalismb othinweakandstronggenerativecapacity,e.g.itcanhandlecrossdep endencyelegantly.2Arelatedworkis(Srinivas,1997),butitsgoalnottolearnanewLTAGbuttoextracttheusefulinformation,suchasdep endencyandfrequencyoftrees,foranexistingLTAG.","Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ; #AUTHOR_TAG ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .","['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ; #AUTHOR_TAG ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', 'However, our approach also generalizes to CFG category-based approaches.', 'In LFG, subcategorization requirements are enforced through semantic forms specifying which grammatical functions are required by a particular predicate.', 'Our approach is based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and Way 1999) and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures (Cahill et al. 2002;Cahill, McCarthy, et al. 2004).', 'Our technique requires a treebank annotated with LFG functional schemata.', 'In the early approach of van Genabith, Sadler, and Way (1999), this was provided by manually annotating the rules extracted from the publicly available subset of the AP Treebank to automatically produce corresponding f-structures.', 'If the f-structures are of high quality, reliable LFG semantic forms can be generated quite simply by recursively reading off the subcategorizable grammatical functions for each local PRED value at each level of embedding in the f-structures.', 'The work reported in van Genabith, Sadler, and Way (1999) was small scale (100 trees) and proof of concept and required considerable manual annotation work.', 'It did not associate frames with probabilities, discriminate between frames for active and passive constructions, properly reflect the effects of long-distance dependencies (LDDs), or include CFG category information.', 'In this article we show how the extraction process can be scaled to the complete Wall Street Journal (WSJ) section of the Penn-II Treebank, with about one million words in 50,000 sentences, based on the automatic LFG f-structure annotation algorithm described in Cahill et al. (2002) and Cahill, McCarthy, et al. (2004).', 'More recently we have extended the extraction approach to the larger, domain-diverse Penn-III Treebank.', 'Aside from the parsed WSJ section, this version of the treebank contains parses for a subsection of the Brown corpus (almost 385,000 words in 24,000 trees) taken from a variety of text genres. 1 In addition to extracting grammatical-function-For the remainder of this work, when we refer to the Penn-II Treebank, we mean the parse-annotated WSJ, and when we refer to the Penn-III Treebank, we mean the parse-annotated WSJ and Brown corpus combined.']",0,"['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ; #AUTHOR_TAG ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'Our approach is based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and Way 1999) and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures (Cahill et al. 2002;Cahill, McCarthy, et al. 2004).', 'Our technique requires a treebank annotated with LFG functional schemata.', 'In the early approach of van Genabith, Sadler, and Way (1999), this was provided by manually annotating the rules extracted from the publicly available subset of the AP Treebank to automatically produce corresponding f-structures.', 'More recently we have extended the extraction approach to the larger, domain-diverse Penn-III Treebank.', 'Aside from the parsed WSJ section, this version of the treebank contains parses for a subsection of the Brown corpus (almost 385,000 words in 24,000 trees) taken from a variety of text genres. 1 In addition to extracting grammatical-function-For the remainder of this work, when we refer to the Penn-II Treebank, we mean the parse-annotated WSJ, and when we refer to the Penn-III Treebank, we mean the parse-annotated WSJ and Brown corpus combined.']"
CC478,J05-3003,Gaussian coordinates and the large scale universe,the derivation of a grammatically indexed lexicon from the longman dictionary of contemporary english,"['Branimir Boguraev', 'Edward Briscoe', 'John Carroll', 'David Carter', 'Claire Grover']",related work,"We describe a methodology and associated software system for the construction of a large lexicon from an existing machine-readable (published) dictionary. The lexicon serves as a component of an English morphological and syntactic analyser and contains entries with grammatical definitions compatible with the word and sentence grammar employed by the analyser. We describe a software system with two integrated components. One of these is capable of extracting syntactically rich, theory-neutral lexical templates from a suitable machine-readable source. The second supports interactive and semi-automatic generation and testing of target lexical entries in order to derive a sizeable, accurate and consistent lexicon from the source dictionary which contains partial (and occasionally in-accurate) information. Finally, we evaluate the utility of the Longman Dictionary of Contemporary English as a suitable source dictionary for the target lexicon.","Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #AUTHOR_TAG ) dictionaries and adding around 30 frames found by manual inspection .","['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #AUTHOR_TAG ) dictionaries and adding around 30 frames found by manual inspection .', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']",0,"['Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll ( 1997 ) predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( #AUTHOR_TAG ) dictionaries and adding around 30 frames found by manual inspection .', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.']"
CC479,J05-3003,Gaussian coordinates and the large scale universe,natural language parsing as statistical pattern recognition,['David Magerman'],method,"Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules.  In this work, I propose an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process. Based on distributionally-derived and linguistically-based features of language, this parser acquires a set of statistical decision trees which assign a probability distribution on the space of parse trees given the input sentence. These decision trees take advantage of significant amount of contextual information, potentially including all of the lexical information in the sentence, to produce highly accurate statistical models of the disambiguation process. By basing the disambiguation criteria selection on entropy reduction rather than human intuition, this parser development method is able to consider more sentences than a human grammarian can when making individual disambiguation rules.  In experiments between a parser, acquired using this statistical framework, and a grammarian's rule-based parser, developed over a ten-year period, both using the same training material and test sentences, the decision tree parser significantly outperformed the grammar-based parser on the accuracy measure which the grammarian was trying to maximize, achieving an accuracy of 78% compared to the grammar-based parser's 69%.","The annotation procedure is dependent on locating the head daughter , for which an amended version of #AUTHOR_TAG is used .","['We utilize the automatic annotation algorithm of Cahill et al. (2002) and Cahill, McCarthy, et al. (2004) to derive a version of Penn-II in which each node in each tree is annotated with LFG functional annotations in the form of attribute-value structure equations.', 'The algorithm uses categorial, configurational, local head, and Penn-II functional and trace information.', 'The annotation procedure is dependent on locating the head daughter , for which an amended version of #AUTHOR_TAG is used .', 'The head is annotated with the LFG equation ↑=↓.', 'Linguistic generalizations are provided over the left (the prefix) and the right (suffix) context of the head for each syntactic category occurring as the mother nodes of such heads.', ""To give a simple example, the rightmost NP to the left of a VP head under an S is likely to be the subject of the sentence (↑ SUBJ =↓), while the leftmost NP to the right of the V head of a VP is most probably the verb's object (↑ OBJ =↓)."", 'Cahill, McCarthy, et al. (2004) provide four classes of annotation principles: one for noncoordinate configurations, one for coordinate configurations, one for traces (long-distance dependencies), and a final ""catch all and clean up"" phase.']",5,"['We utilize the automatic annotation algorithm of Cahill et al. (2002) and Cahill, McCarthy, et al. (2004) to derive a version of Penn-II in which each node in each tree is annotated with LFG functional annotations in the form of attribute-value structure equations.', 'The algorithm uses categorial, configurational, local head, and Penn-II functional and trace information.', 'The annotation procedure is dependent on locating the head daughter , for which an amended version of #AUTHOR_TAG is used .', 'The head is annotated with the LFG equation |=|.', 'Linguistic generalizations are provided over the left (the prefix) and the right (suffix) context of the head for each syntactic category occurring as the mother nodes of such heads.', 'Cahill, McCarthy, et al. (2004) provide four classes of annotation principles: one for noncoordinate configurations, one for coordinate configurations, one for traces (long-distance dependencies), and a final ""catch all and clean up"" phase.']"
CC480,J05-3003,Gaussian coordinates and the large scale universe,lexical functional grammar volume 34 of syntax and semantics,['Mary Dalrymple'],introduction,,"In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']",0,"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; #AUTHOR_TAG ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']"
CC481,J05-3003,Gaussian coordinates and the large scale universe,the penn treebank annotating predicate argument structure,"['Mitchell Marcus', 'Grace Kim', 'Mary Ann Marcinkiewicz', 'Robert MacIntyre', 'Mark Ferguson', 'Karen Katz', 'Britta Schasberger']",method,"The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides a set of coindexed null elements in what can be thought of as ""underlying"" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides some non-context free annotational mechanism to allow the structure of discontinuous constituents to be easily recovered, and allows for a clear, concise tagging system for some semantic roles.","However , more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank ( #AUTHOR_TAG ) , containing more than 1,000,000 words and 49,000 sentences .","['The first step in the application of our methodology is the production of a treebank annotated with LFG f-structure information.', 'F-structures are attribute-value structures which represent abstract syntactic information, approximating to basic predicate-argument-modifier structures.', 'Most of the early work on automatic f-structure annotation (e.g., van Genabith, Way, and Sadler 1999;Frank 2000;Sadler, van Genabith, and Way 2000) was applied only to small data sets (fewer than 200 sentences) and was largely proof of concept.', 'However , more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank ( #AUTHOR_TAG ) , containing more than 1,000,000 words and 49,000 sentences .']",0,"['However , more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank ( #AUTHOR_TAG ) , containing more than 1,000,000 words and 49,000 sentences .']"
CC482,J05-3003,Gaussian coordinates and the large scale universe,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan Sag']",introduction,,"In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ #AUTHOR_TAG ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ #AUTHOR_TAG ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']",0,"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ #AUTHOR_TAG ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']"
CC483,J05-3003,Gaussian coordinates and the large scale universe,extracting tree adjoining grammars from bracketed corpora,['Fei Xia'],,"ExtractingTreeAdjoiningGrammarsfromBracketedCorp oraFeiXiaDepartmentofComputerandInformationScienceUniversityofPennsylvania3401WalnutStreet,Suite400APhiladelphiaPA19104,USAfxia@linc.cis.upenn.eduAbstractInthispap er,werep ortourorkonextractinglexi-calizedtreeadjoininggrammars(LTAGs)frompar-tiallybracketedcorp ora.Thealgorithm rstfullybracketsthecorp ora,thenextractselementarytrees(etrees),and nally ltersoutinvalidusinglinguisticknowledge.Weshowthatthesetofex-tractedetreesmaynotb ecompleteenoughtocoverthewholelanguage,butthiswillnothaveabigim-pactonparsing.1Intro ductionLexicalizedTreeAdjoiningGrammar(LAG)isatree-rewritingformalism.Itismoreexpressivethanacontext-freegrammar(CFG),1andthereforeb et-terformalismforrepresentingvariousphenomenainnaturallanguages.Inthelastdecade,ithasb eenappliedtovariousNLPtaskssuchasparsing(Srini-vas,1997),machinetranslation(Palmeretal.,1998),informationretrieval(ChandrasekarandSrinivas,1997),generation(StoneandDoran,1997;McCoyetal.,1992),andsummarizationapplications(Bald-winetal.,1997).Awide-coverageLTGforapar-ticularnaturallanguageoftencontainsthousandsoftreesandtakesyearstobuild.Therehasb eenworkonextractingCFGs(Shi-raietal.,1995;Charniak,1996;Krotovandoth-ers,1998)andlexicalizedtreegrammars(Neumann,1998;Srinivas,1997)frombracketedcorp ora.Inthispap er,weprop oseanewmetho dforlearningLTAGsfromsuchcorp ora.22PennTreebankandLAG2.1PennTreebankInthispap er,weusetheEnglishPennTreebankasourbracketedcorpus,whichincludesab out1millionTheauthorwishestothankChung-hyeHan,AravindJoshi,MarthaPalmer,CarlosProlo,Ano opSarkar,andthreeanonymousreviewersformanyhelpfulcomments.1LTAGismoreexpressivethanCFformalismb othinweakandstronggenerativecapacity,e.g.itcanhandlecrossdep endencyelegantly.2Arelatedworkis(Srinivas,1997),butitsgoalnottolearnanewLTAGbuttoextracttheusefulinformation,suchasdep endencyandfrequencyoftrees,foranexistingLTAG.","Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #AUTHOR_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .","['In addition to evaluating the quality of our extracted semantic forms, we also examined the rate at which they are induced.', 'This can be expressed as a measure of the coverage of the induced lexicon on new data.', 'Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #AUTHOR_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .', 'We then compare this to a test lexicon from Section 23.', 'Table 27 shows the results of the evaluation of the coverage of an induced lexicon for verbs only.', 'There is a corresponding semantic form in the reference lexicon for 89.89% of the verbs in Section 23. 10.11% of the entries in the test lexicon did not appear in the reference lexicon.', 'Within this group, we can distinguish between known words, which have an entry in the reference lexicon, and unknown words, which do not exist at all in the reference lexicon.', 'In the same way we make the distinction  between known frames and unknown frames.', 'There are, therefore, four different cases in which an entry may not appear in the reference lexicon.', 'Table 27 shows that the most common case is that of known verbs occurring with a different, although known, subcategorization frame (7.85%).']",5,"['In addition to evaluating the quality of our extracted semantic forms, we also examined the rate at which they are induced.', 'Following Hockenmaier , Bierner , and Baldridge ( 2002 ) , #AUTHOR_TAG , and Miyao , Ninomiya , and Tsujii ( 2004 ) , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .']"
CC484,J05-3003,Gaussian coordinates and the large scale universe,from grammar to lexicon unsupervised learning of lexical syntax,['Michael Brent'],related work,"Imagine a language that is completely unfamiliar; the only means of studying it are an ordinary grammar book and a very large corpus of text. No dictionary is available. How can easily recognized, surface grammatical facts be used to extract from a corpus as much syntactic information as possible about individual words? This paper describes an approach based on two principles. First, rely on local morpho-syntactic cues to structure rather than trying to parse entire sentences. Second, treat these cues as probabilistic rather than absolute indicators of syntactic structure. Apply inferential statistics to the data collected using the cues, rather than drawing a categorical conclusion from a single occurrence of a cue. The effectiveness of this approach for inferring the syntactic frames of verbs is supported by experiments on an English corpus using a program called Lerner. Lerner starts out with no knowledge of content words---it bootstraps from determiners, auxiliaries, modals, prepositions, pronouns, complementizers, coordinating conjunctions, and punctuation.",#AUTHOR_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', '#AUTHOR_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes.', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']",0,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', '#AUTHOR_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.']"
CC485,J05-3003,Gaussian coordinates and the large scale universe,lexical functional grammar a formal system for grammatical representation,"['Ronald Kaplan', 'Joan Bresnan']",,In learning their native language children develop a remarkable set of capabilities They acquire knowledge and skills that enable them to pro duce and comprehend an inde nite number of novel utterances and to make quite subtle judgments about certain of their properties The ma jor goal of psycholinguistic research is to devise an explanatory account of the mental operations that underlie these linguistic abilities In pursuing this goal we have adopted what we call the Competence Hypothesis as a methodological principle We assume that an explana tory model of human language performance will incorporate a theoreti cally justi ed representation of the native speaker s linguistic knowledge a grammar as a component separate both from the computational mech anisms that operate on it a processor and from other nongrammatical processing parameters that might in uence the processor s behavior To a certain extent the various components that we postulate can be studied independently guided where appropriate by the well established methods and evaluation standards of linguistics computer science and experimen tal psychology However the requirement that the various components ultimately must t together in a consistent and coherent model imposes even stronger constraints on their structure and operation,Lexical functional grammar ( #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .,"['Lexical functional grammar ( #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'C-structure takes the form of phrase structure trees and is defined in terms of CFG rules and lexical entries.', 'F-structure is produced from functional annotations on the nodes of the c-structure and implemented in terms of recursive feature structures (attribute-value matrices).', 'This is exemplified by the analysis of the string The inquiry soon focused on the judge (wsj 0267 72) using the grammar in Figure 1, which results in the annotated c-structure and f-structure in Figure 2.']",0,['Lexical functional grammar ( #AUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .']
CC486,J05-3003,Gaussian coordinates and the large scale universe,automatic extraction of subcategorization frames for czech,"['Anoop Sarkar', 'Daniel Zeman']",related work,"We present some novel machine learning techniques for the identification of subcategorization information for verbs in Czech. We compare three different statistical techniques applied to this problem. We show how the learning algorithm can be used to discover previously unknown subcategorization frames from the Czech Prague Dependency Treebank. The algorithm can then be used to label dependents of a verb in the Czech treebank as either arguments or adjuncts. Using our techniques, we are able to achieve 88 % precision on unseen parsed text.",#AUTHOR_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic,"['Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data.', 'Kinyon and Prolo (2002) describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank.', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arguments were then mapped to traditional syntactic functions.', 'For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject.', 'In general, argumenthood was preferred over adjuncthoood.', 'As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is.', '#AUTHOR_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', 'This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score.', 'The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more).', 'Marinov and Hemming (2004) present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank (Simov, Popova, and Osenova 2002).', ""In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming's system collects both arguments and adjuncts."", 'It then uses the binomial log-likelihood ratio to filter incorrect frames.', 'The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees.', 'The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences.']",0,"['Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data.', 'Kinyon and Prolo (2002) describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank.', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arguments were then mapped to traditional syntactic functions.', '#AUTHOR_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', 'The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more).', 'Marinov and Hemming (2004) present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank (Simov, Popova, and Osenova 2002).', ""In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemming's system collects both arguments and adjuncts."", 'The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees.']"
CC487,J05-3003,Gaussian coordinates and the large scale universe,three generative lexicalised models for statistical parsing,['Michael Collins'],related work,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #AUTHOR_TAG .","['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', ""The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #AUTHOR_TAG .', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997).', 'Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach, those of Xia (1999) and Hockenmaier, Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the Penn-II trees.', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of ""inverse schemata.""']",0,"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', 'Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing.', 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman ( 1994 ) and #AUTHOR_TAG .', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.']"
CC488,J05-3003,Gaussian coordinates and the large scale universe,lexical functional grammar volume 34 of syntax and semantics,['Mary Dalrymple'],method,,"#AUTHOR_TAG argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .","['In order to capture CFG-based categorial information, we add a CAT feature to the f-structures automatically generated from the Penn-II and Penn-III Treebanks.', 'Its value is the syntactic category of the lexical item whose lemma gives rise to the PRED value at that particular level of embedding.', 'This makes it possible to classify words and their semantic forms based on their syntactic category and reduces the risk of inaccurate assignment of subcategorization frame frequencies due to POS ambiguity, distinguishing, for example, between the nominal and verbal occurrences of the lemma fight.', 'With this, the output for the verb impose in Figure 4 is impose (v,[subj, obj, obl:on]).', 'For some of our experiments, we conflate the different verbal (and other) tags used in the Penn Treebanks to a single verbal marker (Table 4).', 'As a further extension, the extraction procedure reads off the syntactic category of the head of each of the subcategorized syntactic functions: impose(v,[subj(n),obj(n),obl:on]). 3 In this way, our methodology is able to produce surface syntactic as well as abstract functional subcategorization details.', '#AUTHOR_TAG argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .', 'In contrast to much of the work reviewed in Section 3, which limits itself to the extraction of surface syntactic subcategorization details, our system can provide this information as well as details of grammatical function.']",4,"['In order to capture CFG-based categorial information, we add a CAT feature to the f-structures automatically generated from the Penn-II and Penn-III Treebanks.', 'Its value is the syntactic category of the lexical item whose lemma gives rise to the PRED value at that particular level of embedding.', 'This makes it possible to classify words and their semantic forms based on their syntactic category and reduces the risk of inaccurate assignment of subcategorization frame frequencies due to POS ambiguity, distinguishing, for example, between the nominal and verbal occurrences of the lemma fight.', 'As a further extension, the extraction procedure reads off the syntactic category of the head of each of the subcategorized syntactic functions: impose(v,[subj(n),obj(n),obl:on]). 3 In this way, our methodology is able to produce surface syntactic as well as abstract functional subcategorization details.', '#AUTHOR_TAG argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .', 'In contrast to much of the work reviewed in Section 3, which limits itself to the extraction of surface syntactic subcategorization details, our system can provide this information as well as details of grammatical function.']"
CC489,J05-3003,Gaussian coordinates and the large scale universe,from treebank to propbank,"['Paul Kingsbury', 'Martha Palmer']",,,"In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( #AUTHOR_TAG ) .","['We have presented an algorithm for the extraction of semantic forms (or subcategorization frames) from the Penn-II and Penn-III Treebanks, automatically annotated with LFG f-structures.', 'In contrast to many other approaches, ours does not predefine the subcategorization frames we extract.', ""We have applied the algorithm to the WSJ sections of Penn-II (50,000 trees) (O' Donovan et al. 2004) and to the parse-annotated Brown corpus of Penn-III (almost 25,000 additional trees)."", 'We extract syntactic-function-based subcategorization frames (LFG semantic forms) and traditional CFG category-based frames, as well as mixed-function-category-based frames.', 'Unlike many other approaches to subcategorization frame extraction, our system properly reflects the effects of long-distance dependencies.', 'Also unlike many approaches, our method distinguishes between active and passive frames.', 'Finally, our system associates conditional probabilities with the frames we extract.', 'Making the distinction between the behavior of verbs in active and passive contexts is particularly important for the accurate assignment of probabilities to semantic forms.', 'We carried out an extensive evaluation of the complete induced lexicon against the full COMLEX resource.', 'To our knowledge, this is the most extensive qualitative evaluation of subcategorization extraction in English.', 'The only evaluation of a similar scale is that carried out by Schulte im Walde (2002b) for German.', ""The results reported here for Penn-II compare favorably against the baseline and, in fact, are an improvement on those reported in O' Donovan et al. (2004)."", 'The results for the larger, more domain-diverse Penn-III lexicon are very encouraging, in some cases almost 15% above the baseline.', 'We believe our semantic forms are fine-grained, and by choosing to evaluate against COMLEX, we set our sights high: COMLEX is considerably more detailed than the OALD or LDOCE used for other earlier evaluations.', 'Our error analysis also revealed some interesting issues associated with using an external standard such as COMLEX.', 'In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( #AUTHOR_TAG ) .']",3,"[""We have applied the algorithm to the WSJ sections of Penn-II (50,000 trees) (O' Donovan et al. 2004) and to the parse-annotated Brown corpus of Penn-III (almost 25,000 additional trees)."", 'In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( #AUTHOR_TAG ) .']"
CC490,J05-3003,Gaussian coordinates and the large scale universe,how verb subcategorization frequencies are affected by corpus choice,"['Douglas Roland', 'Daniel Jurafsky']",,"The probabilistic relation between verbs and their arguments plays an important role in modern statistical parsers and supertaggers, and in psychological theories of language processing. But these probabilities are computed in very different ways by the two sets of researchers. Computational linguists compute verb subcategorization probabilities from large corpora while psycholinguists compute them from psychological studies (sentence production and completion tasks). Recent studies have found differences between corpus frequencies and psycholinguistic measures. We analyze subcategorization frequencies from four different corpora: psychological sentence production data (Connine et al. 1984), written text (Brown and WSJ), and telephone conversation data (Switchboard). We find two different sources for the differences. Discourse influence is a result of how verb use is affected by different discourse types such as narrative, connected discourse, and single sentence productions. Semantic influence is a result of different corpora using different senses of verbs, which have different subcategorization frequencies. We conclude that verb sense and discourse type play an important role in the frequencies observed in different experimental and corpus based sources of verb subcategorization frequencies.","As noted above , it is well documented ( #AUTHOR_TAG ) that subcategorization frames ( and their frequencies ) vary across domains .","['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above , it is well documented ( #AUTHOR_TAG ) that subcategorization frames ( and their frequencies ) vary across domains .', 'We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ.', 'For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon.', 'It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases.', 'This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs.', 'Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples.', 'As a generalization, Briscoe (2001) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall.', 'Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX.', 'Precision was quite high (95%), but recall was low (84%).', 'This has an effect on both the precision and recall scores of our system against COMLEX.', 'In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.', 'We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as ""correct"" or ""incorrect.""', 'Of the 80 fps, 33 were manually judged to be legitimate subcategorization frames.', 'For example, as Table 26 shows, there are a number of correct transitive verbs ([subj,obj]) in our automatically induced lexicon which are not included in COMLEX.', 'This examination was also useful in highlighting to us the frame types on which the lexical extraction procedure was performing poorly, in our case, those containing XCOMPs and those containing OBJ2S.', 'Out of 80 fns, 14 were judged to be incorrect when manually examined.', 'These can be broken down as follows: one intransitive frame, three ditransitive frames, three frames containing a COMP, and seven frames containing an oblique were found to be invalid.']",4,"['As noted above , it is well documented ( #AUTHOR_TAG ) that subcategorization frames ( and their frequencies ) vary across domains .', 'Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX.']"
CC491,J05-3003,Gaussian coordinates and the large scale universe,automated extraction of tags from the penn treebank,"['John Chen', 'K Vijay-Shanker']",related work,"The accuracy of statistical parsing models can be improved with the use of lexical information. Statistical parsing using Lexicalized tree adjoining grammar (LTAG), a kind of lexicalized grammar, has remained relatively unexplored. We believe that is largely in part due to the absence of large corpora accurately bracketed in terms of a perspicuous yet broad coverage LTAG. Our work attempts to alleviate this difficulty. We extract different LTAGs from the Penn Treebank. We show that certain strategies yield an improved extracted LTAG in terms of compactness, broad coverage, and supertagging accuracy. Furthermore, we perform a preliminary investigation in smoothing these grammars by means of an external linguistic resource, namely, the tree families of an XTAG grammar, a hand built grammar of English.",#AUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .,"['Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG.', 'As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar.', '#AUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .', ""The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins's (1997) approach to the differentiation between complement and adjunct."", 'This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question.', 'The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996.', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', 'The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099.', 'Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank.', 'For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner.', 'The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997).', 'Each node is subsequently assigned the relevant category based on its constituent type and surface configuration.', 'The algorithm handles ""like"" coordination and exploits the traces used in the treebank in order to interpret LDDs.', 'Unlike our approach, those of Xia (1999) and Hockenmaier, Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the Penn-II trees.', 'Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii ( 2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', 'In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of ""inverse schemata.""']",0,"['#AUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .', 'Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank.', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997).']"
CC492,J05-3003,Gaussian coordinates and the large scale universe,treebank grammars in,['Eugene Charniak'],,,"In #AUTHOR_TAG and Krotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .","['The rate of accession may also be represented graphically.', 'In #AUTHOR_TAG and Krotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity.', 'Figure 8 graphs the rate of induction of semantic form and CFG rule types from Penn-III (the WSJ and parse-annotated Brown corpus combined).', 'Because of the variation in the size of sections between the Brown and the WSJ, we plotted accession against word count.', 'The first part of the graph (up to 1,004,414 words)']",0,"['In #AUTHOR_TAG and Krotov et al. ( 1998 ) , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .']"
CC493,J05-3003,Gaussian coordinates and the large scale universe,automatic acquisition of a large subcategorisation dictionary from corpora,['Christopher Manning'],introduction,,"#AUTHOR_TAG argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .","['One important type of lexical information is the subcategorization requirements of an entry (i.e., the arguments a predicate must take in order to form a grammatical construction).', 'Lexicons, including subcategorization details, were traditionally produced by hand.', 'However, as the manual construction of lexical resources is time consuming, error prone, expensive, and rarely ever complete, it is often the case that the limitations of NLP systems based on lexicalized approaches are due to bottlenecks in the lexicon component.', 'In addition, subcategorization requirements may vary across linguistic domain or genre (Carroll and Rooth 1998).', '#AUTHOR_TAG argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .', 'Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue.']",4,"['One important type of lexical information is the subcategorization requirements of an entry (i.e., the arguments a predicate must take in order to form a grammatical construction).', 'Lexicons, including subcategorization details, were traditionally produced by hand.', 'However, as the manual construction of lexical resources is time consuming, error prone, expensive, and rarely ever complete, it is often the case that the limitations of NLP systems based on lexicalized approaches are due to bottlenecks in the lexicon component.', 'In addition, subcategorization requirements may vary across linguistic domain or genre (Carroll and Rooth 1998).', '#AUTHOR_TAG argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .', 'Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue.']"
CC494,J05-3003,Gaussian coordinates and the large scale universe,lexicalfunctional syntax,['Joan Bresnan'],introduction,,"In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']",0,"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #AUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']"
CC495,J05-3003,Gaussian coordinates and the large scale universe,the automatic acquisition of frequencies of verb subcategorization frames from tagged corpora,"['Akira Ushioda', 'David Evans', 'Ted Gibson', 'Alex Waibel']",related work,"We describe a mechanism for automatically acquiring verb subcategorization frames and their frequencies in a large corpus. A tagged corpus is first partially parsed to identify noun phrases and then a finear grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus. In an experiment involving the identification of six fixed subcategorization frames, our current system showed more than 80% accuracy. In addition, a new statistical approach substantially improves the accuracy of the frequency estimation.",#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .', 'The experiment is limited by the fact that all prepositional phrases are treated as adjuncts.', ""Ushioda et al. (1993) employ an additional statistical method based on log-linear models and Bayes' theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames."", 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993).', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'The frames incorporate control information and details of specific prepositions.', 'Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames.', 'Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.', 'They perform a mapping between their frames and those of the OALD, resulting in 15 frame types.', 'These do not contain details of specific prepositions.']",0,"['We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input.', 'Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon.', 'Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames.', '#AUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .', 'Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur.', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb.', 'Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection.', 'Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns.', 'The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate.']"
CC496,J05-3003,Gaussian coordinates and the large scale universe,from dictionary to corpus to selforganizing dictionary learning valency associations in the face of variation and change,['Edward Briscoe'],,"ing over specific lexically-governed particles and prepositions and specific predicate selectional preferences, but including some `derived' / `alternant' semi-productive, and therefore only semipredictable, bounded dependency constructions, such as particle or dative movement, there are at least 163 valency frames associated with verbal predicates in (current) English (Briscoe, 2000). In this paper, I will review the work that my colleagues and I have done to learn (semi-)automatically this very large number of associations between individual verbal predicates and valency frames. Access to a comprehensive and accurate valency lexicon is critical for the development of robust and accurate parsing technology capable of recovering predicate-argument relations (and thus logical forms) from free text or transcribed speech. Without this information it is possible to `chunk' input into phrases but not to distinguish arguments from adjuncts or resolve most phrasal attachment ambiguities. Furthermore, for statistical parsers it is not enough to know the associations of predicates to valency frames, it is also critical to know the relative frequency of such associations given a specific predicate. Such information is a core component of that required to `lexicalize' a probabilistic parser, and it is now well-established that lexicalization is essential for accurate disambiguation (e.g. Collins, 1997, Carroll et al, 1998). While state-of-the-art wide-coverage grammars of English, capable of recovering predicateargument structure and expressed as a unification-based phrase structure grammar, have on the order of 1000 rules, it is clear that the number of associations between valency frames and predicates needed in a lexicon for such a grammar will be much higher.","As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .","['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above, it is well documented (Roland and Jurafsky 1998) that subcategorization frames (and their frequencies) vary across domains.', 'We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ.', 'For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon.', 'It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases.', 'This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs.', 'Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples.', 'As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .', 'Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX.', 'Precision was quite high (95%), but recall was low (84%).', 'This has an effect on both the precision and recall scores of our system against COMLEX.', 'In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26.', 'We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as ""correct"" or ""incorrect.""', 'Of the 80 fps, 33 were manually judged to be legitimate subcategorization frames.', 'For example, as Table 26 shows, there are a number of correct transitive verbs ([subj,obj]) in our automatically induced lexicon which are not included in COMLEX.', 'This examination was also useful in highlighting to us the frame types on which the lexical extraction procedure was performing poorly, in our case, those containing XCOMPs and those containing OBJ2S.', 'Out of 80 fns, 14 were judged to be incorrect when manually examined.', 'These can be broken down as follows: one intransitive frame, three ditransitive frames, three frames containing a COMP, and seven frames containing an oblique were found to be invalid.']",0,"['As a generalization , #AUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .']"
CC497,J05-4005,Chinese Word Segmentation and Named Entity Recognition: A Pragmatic Approach,a stochastic finitestate wordsegmentation algorithm for chinese,"['Richard Sproat', 'Chilin Shih', 'William Gale', 'Nancy Chang']",related work,,"A previous work along this line is #AUTHOR_TAG , which is based on weighted finite-state transducers ( FSTs ) .","['We believe that the identification of OOV words should not be treated as a problem separate from word segmentation.', 'We propose a unified approach that solves both problems simultaneously.', 'A previous work along this line is #AUTHOR_TAG , which is based on weighted finite-state transducers ( FSTs ) .', 'Our approach is similarly motivated but is based on a different mechanism: linear mixture models.', 'As we shall see, the models provide a more flexible framework for incorporating various kinds of lexical and statistical information.', ""Many types of OOV words that are not covered in Sproat's system can be dealt with in our system."", 'The linear models we used are originally derived from linear discriminant functions widely used for pattern classification (Duda, Hart, and Stork 2001) and have been recently introduced into NLP tasks by Collins and Duffy (2001).', 'Other frameworks of Chinese word segmentation, which are similar to the linear models, include maximum entropy models (Xue 2003) and conditional random fields (Peng, Feng, and McCallum 2004).', 'They also use a unified approach to word breaking and OOV identification.']",1,"['We believe that the identification of OOV words should not be treated as a problem separate from word segmentation.', 'A previous work along this line is #AUTHOR_TAG , which is based on weighted finite-state transducers ( FSTs ) .', 'As we shall see, the models provide a more flexible framework for incorporating various kinds of lexical and statistical information.', ""Many types of OOV words that are not covered in Sproat's system can be dealt with in our system."", 'Other frameworks of Chinese word segmentation, which are similar to the linear models, include maximum entropy models (Xue 2003) and conditional random fields (Peng, Feng, and McCallum 2004).']"
CC498,J06-2002,Generating Referring Expressions that Involve Gradable Properties,computational interpretations of the gricean maximes in the generation of referring expressions,"['Robbert Dale', 'Ehud Reiter']",experiments,"We examine the problem of generating definite noun phrases that are appropriate referring expressions; that is, noun phrases that (a) successfully identify the intended referent to the hearer whilst (b) not conveying to him or her any false conversational implicatures (Grice, 1975). We review several possible computational interpretations of the conversational implicature maxims, with different computational costs, and argue that the simplest may be the best, because it seems to be closest to what human speakers do. We describe our recommended algorithm in detail, along with a specification of the resources a host system must provide in order to make use of the algorithm, and an implementation used in the natural language generation component of the IDAS system.","#AUTHOR_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) .","['Common sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication.', 'We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.', '#AUTHOR_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) .', 'They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 "" bolt, human assemblers ""unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt"" (Dale and Reiter 1995). 6 Our own experiments (van Deemter 2004) point in the same direction.', 'In one experiment, for example, 34 students at the University of Brighton were shown six pieces of paper, each of which showed two isosceles and approximately equilateral triangles.', 'Triangles of three sizes were shown, with bases of 5, 8, and 16 mm respectively.', 'On each sheet, one of the two triangles had been circled with a pencil.', 'We asked subjects to imagine themselves on the phone to someone who held a copy of the same sheet, but not necessarily with the same orientation (e.g., possibly upside down), and to complete the answers in the following: Q: Which triangle on this sheet was circled?', 'A: The ............ triangle.']",0,"['Common sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication.', 'We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.', '#AUTHOR_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) .', 'They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 "" bolt, human assemblers ""unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt"" (Dale and Reiter 1995). 6 Our own experiments (van Deemter 2004) point in the same direction.', 'Triangles of three sizes were shown, with bases of 5, 8, and 16 mm respectively.', 'On each sheet, one of the two triangles had been circled with a pencil.', 'A: The ............ triangle.']"
CC499,J06-2002,Generating Referring Expressions that Involve Gradable Properties,situations and attitudes,"['Jon Barwise', 'John Perry']",introduction,"In this provocative book, Barwise and Perry tackle the slippery subject of ""meaning, "" a subject that has long vexed linguists, language philosophers, and logicians.","Viewed in this way , gradable adjectives are an extreme example of the ""efficiency of language"" ( #AUTHOR_TAG ) : Far from meaning something concrete like ""larger than 8 cm"" -- a concept that would have very limited applicability -- or even something more general like ""larger than the average N , ""a word like large is applicable across a wide range of different situations .","['Viewed in this way , gradable adjectives are an extreme example of the ""efficiency of language"" ( #AUTHOR_TAG ) : Far from meaning something concrete like ""larger than 8 cm"" -- a concept that would have very limited applicability -- or even something more general like ""larger than the average N , ""a word like large is applicable across a wide range of different situations .']",1,"['Viewed in this way , gradable adjectives are an extreme example of the ""efficiency of language"" ( #AUTHOR_TAG ) : Far from meaning something concrete like ""larger than 8 cm"" -- a concept that would have very limited applicability -- or even something more general like ""larger than the average N , ""a word like large is applicable across a wide range of different situations .']"
CC500,J06-2002,Generating Referring Expressions that Involve Gradable Properties,projecting the adjective the syntax and semantics of gradability and comparison,['Christopher Kennedy'],introduction,,"Clearly , what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , #AUTHOR_TAG ) .","['Clearly , what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , #AUTHOR_TAG ) .', 'The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about the standards employed (Kyburg and Morreau 2000;DeVault and Stone 2004): (3), for example, implies a standard that counts 10 cm as large and 8 cm as not large.', 'Our own proposal will abstract away from the effects of linguistic context.', 'We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words.', 'This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of']",0,"['Clearly , what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , #AUTHOR_TAG ) .']"
CC501,J06-2002,Generating Referring Expressions that Involve Gradable Properties,welfare economics and social choice theory,['Allan M Feldman'],experiments,"Preferences and Utility.- Barter Exchange.- Welfare Properties of Market Exchange.- Welfare Properties of ""Jungle"" Exchange.- Economies with Production.- Uncertainty in Exchange.- Externalities.- Public Goods.- Compensation Criteria.- Fairness and the Rawls Criterion.- Life and Death Choices.- Majority Voting.- Arrow's Impossibility Theorem.- Dominant-Strategy Implementation.- Nash Implementation.- Bayesian Implementation.- Epilogue.","Cases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. , #AUTHOR_TAG ) was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such that","['Cases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. , #AUTHOR_TAG ) was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such that']",0,"['Cases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. , #AUTHOR_TAG ) was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such that']"
CC502,J06-2002,Generating Referring Expressions that Involve Gradable Properties,ordering among premodifiers,"['James Shaw', 'Vasileios Hatzivassiloglou']",,"We present a corpus-based study of the sequential ordering among premodifiers in noun phrases. This information is important for the fluency of generated text in practical applications. We propose and evaluate three approaches to identify sequential order among premodifiers: direct evidence, transitive closure, and clustering. Our implemented system can make over 94 % of such ordering decisions correctly, as evaluated on a large, previously unseen test corpus.","One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #AUTHOR_TAG ; Malouf 2000 ) .","['One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #AUTHOR_TAG ; Malouf 2000 ) .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others.', 'Quirk et al. (1985), for example, report that ""adjectives denoting size, length, and height normally precede other nonderived adjectives"" (e.g., the small round table is usually preferred to the round small table).', 'Semantically, this does not come as a surprise.', 'In a noun phrase of the form the three small (-est) [N], for example, the words preceding N select the three smallest elements of [N].', 'It follows that, to denote the three smallest elements of the set of round tables, the only option is to say the three small round tables, rather than the three round small tables.', 'The latter would mean something else, namely, the three round ones among the n small(est) tables (where n is not specified).', ""It actually seems quite possible to say this, but only when some set of small tables is contextually salient (e.g., I don't mean those small tables, I mean the three round ones)."", 'Given that n is unspecified, the noun phrase would tend to be very unclear in any other context.']",0,"['One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #AUTHOR_TAG ; Malouf 2000 ) .', 'Interestingly, vague properties tend to be realized before others.', 'Quirk et al. (1985), for example, report that ""adjectives denoting size, length, and height normally precede other nonderived adjectives"" (e.g., the small round table is usually preferred to the round small table).', 'Semantically, this does not come as a surprise.', 'In a noun phrase of the form the three small (-est) [N], for example, the words preceding N select the three smallest elements of [N].', 'The latter would mean something else, namely, the three round ones among the n small(est) tables (where n is not specified).', ""It actually seems quite possible to say this, but only when some set of small tables is contextually salient (e.g., I don't mean those small tables, I mean the three round ones)."", 'Given that n is unspecified, the noun phrase would tend to be very unclear in any other context.']"
CC503,J06-2002,Generating Referring Expressions that Involve Gradable Properties,modern engineering mathematics second edition,"['Glyn James', 'David Burley', 'Dick Clements', 'Phil Dyke', 'John Searl', 'Jerry Wright']",introduction,,"3 The degree of precision of the measurement ( #AUTHOR_TAG , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size .","['3 The degree of precision of the measurement ( #AUTHOR_TAG , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size .']",0,"['3 The degree of precision of the measurement ( #AUTHOR_TAG , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size .']"
CC504,J06-2002,Generating Referring Expressions that Involve Gradable Properties,efficient contextsensitive generation of referring expressions,"['Emiel Krahmer', 'Mari¨et Theune']",,3 A Modification of the Algorithm Based on Salience 5 3.1 Motivation: Determining the Context Set : : : : : : : : : : : : : 5 3.2 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3.3 Outline of the Modified Algorithm : : : : : : : : : : : : : : : : : 7 3.4 Examples : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9 3.5 Discussion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11,"Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ; #AUTHOR_TAG ) .","['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ; #AUTHOR_TAG ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', 'Consider, for instance, the list of properties L = size > 3 cm, size < 9 cm .', 'If interleaving forced us to realize the two properties in L one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one (if the facts in the KB support it), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size < 9 cm).', 'Clearly, sophisticated use of gradable adjectives requires a separation between CD and linguistic realization, unless one is willing to complicate linguistic realization considerably.']",1,"['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ; #AUTHOR_TAG ) .']"
CC505,J06-2002,Generating Referring Expressions that Involve Gradable Properties,sorites paradox,['Dominic Hyde'],,,"This is the strongest version of the sorites paradox ( e.g. , #AUTHOR_TAG ) .","['NLG has to do more than select a distinguishing description (i.e., one that unambiguously denotes its referent; Dale 1989): The selected expression should also be felicitous.', 'Consider the question, discussed in the philosophical logic literature, of whether it is legitimate, for a gradable adjective, to distinguish between ""observationally indifferent"" entities: Suppose two objects x and y, are so similar that it is impossible to distinguish their sizes; can it ever be reasonable to say that x is large and y is not?', 'A positive answer would not be psychologically plausible, since x and y are indistinguishable; but a negative answer would prohibit any binary distinction between objects that are large and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it.', 'This is the strongest version of the sorites paradox ( e.g. , #AUTHOR_TAG ) .']",0,"['This is the strongest version of the sorites paradox ( e.g. , #AUTHOR_TAG ) .']"
CC506,J06-2002,Generating Referring Expressions that Involve Gradable Properties,efficient contextsensitive generation of referring expressions,"['Emiel Krahmer', 'Mari¨et Theune']",introduction,3 A Modification of the Algorithm Based on Salience 5 3.1 Motivation: Determining the Context Set : : : : : : : : : : : : : 5 3.2 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3.3 Outline of the Modified Algorithm : : : : : : : : : : : : : : : : : 7 3.4 Examples : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9 3.5 Discussion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11,"We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 , #AUTHOR_TAG ) .","['We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 , #AUTHOR_TAG ) .', 'Before we do this, consider the tractability of the original IA.', 'If the running time of FindBestValue(r, A i ) is a constant times the number of Values of the Attribute A i , then the worst-case running time of IA (and IA Plur ) is O(n v n a ), where n a equals the number of Attributes in the language and n v the average number of Values of all Attributes.', 'This is because, in the worst case, all Values of all Attributes need to be attempted (van Deemter 2002).', 'As for the new algorithm, we focus on the crucial phases 2, 4, and 5.']",0,"['We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 , #AUTHOR_TAG ) .']"
CC507,J06-2002,Generating Referring Expressions that Involve Gradable Properties,what do we mean by ‘usually’,['John H Toogood'],,,"This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #AUTHOR_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) .","['Sometimes we are forced to be vague because the information we have (e.g., based on perception or verbal reports) is itself inexact.', 'Such cases can be modeled by letting NLG systems take vague information (e.g., Rain[Wednesday] = heavy) as their input.', 'We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD.', 'This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #AUTHOR_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) .', 'We shall therefore focus-unlike earlier computational accounts-on vague descriptions, that is, vague expressions in definite descriptions.', 'Here, the context tends to obliterate the vagueness associated with the adjective.', ""Suppose you enter a vet's surgery in the company of two dogs: a big one on a leash, and a tiny one in your arms."", 'The vet asks ""Who\'s the patient?"", and you answer ""the big dog.""', 'This answer will allow the vet to pick out the patient just as reliably as if you had said ""the one on the leash""; the fact that big is a vague term is irrelevant.', 'You omit the exact size of the dog, just like some of its other properties (e.g., the leash), because they do not improve the description.', 'This shows how vague properties can contribute to the precise task of identifying a referent.']",0,"['Sometimes we are forced to be vague because the information we have (e.g., based on perception or verbal reports) is itself inexact.', 'This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people ( #AUTHOR_TAG ) , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) .']"
CC508,J06-2002,Generating Referring Expressions that Involve Gradable Properties,psychologie der objektbenennung,"['Tony Hermann', 'Roland Deutsch']",introduction,,"#AUTHOR_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking .","['Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely.', 'Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x.', 'Which of these should come first?', '#AUTHOR_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking .', ""In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say 'the tall candle' when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to 'the fat candle.'"", ""Hermann and Deutsch's findings may be implemented as follows."", 'First, the Values of the different Attributes should be normalized to make them comparable.', 'Second, preference order should be calculated dynamically (i.e., based on the current value of C, and taking the target into account), preferring larger gaps over smaller ones.', '(It is possible, e.g., that width is most suitable for singling out a black cat, but height for singling out a white cat.)', 'The rest of the algorithm remains unchanged.']",0,"['Which of these should come first?', '#AUTHOR_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking .', ""Hermann and Deutsch's findings may be implemented as follows.""]"
CC509,J06-2002,Generating Referring Expressions that Involve Gradable Properties,the effects of redundant communications on listeners when more is less child development,['Susan Sonnenschein'],experiments,,"While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ; #AUTHOR_TAG ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .","['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ; #AUTHOR_TAG ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation (Sedivy et al. 1999): Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known.', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']",0,"['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ; #AUTHOR_TAG ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']"
CC510,J06-2002,Generating Referring Expressions that Involve Gradable Properties,principles of categorization,['Eleanor Rosch'],introduction,,"The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , #AUTHOR_TAG ) .","[""FindBestValue selects the 'best value' from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity."", 'The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , #AUTHOR_TAG ) .', 'IA Plur can refer to individuals as well as sets, since reference to a target individual r can be modeled as reference to the singleton set {r}.']",0,"[""FindBestValue selects the 'best value' from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity."", 'The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , #AUTHOR_TAG ) .', 'IA Plur can refer to individuals as well as sets, since reference to a target individual r can be modeled as reference to the singleton set {r}.']"
CC511,J06-2002,Generating Referring Expressions that Involve Gradable Properties,efficient contextsensitive generation of referring expressions,"['Emiel Krahmer', 'Mari¨et Theune']",experiments,3 A Modification of the Algorithm Based on Salience 5 3.1 Motivation: Determining the Context Set : : : : : : : : : : : : : 5 3.2 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3.3 Outline of the Modified Algorithm : : : : : : : : : : : : : : : : : 7 3.4 Examples : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9 3.5 Discussion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11,"#AUTHOR_TAG have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available .","['We shall see that a natural treatment of salience falls automatically out of our treatment of vague descriptions.', 'As we shall see, this will allow us to simplify the structure of GRE algorithms, and it will explain why many definite descriptions that look as if they were distinguishing descriptions are actually ambiguous.', '9.4.1 A New Perspective on Salience.', ""#AUTHOR_TAG have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available ."", 'In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse.', 'Now suppose we let GRE treat salience just like other gradable Attributes.', 'Suppose there are ten mice, five of which are black, whose degrees of salience are 1, 1, 3, 4, and 5 (the last one being most salient), while the other objects in the domain (cats, white mice) all have a higher salience.', 'Then our algorithm might generate this list of properties: L = mouse, black, salience > 4 .', 'This is a distinguishing description for the black mouse whose salience is 5: the most salient black mouse.', 'The simpler description the black mouse can be derived by stipulating that the property of being most salient can be left implicit in English.', 'The salience Attribute has to be taken into account by CD, however, and this can be ensured in various ways.', 'For example, instead of testing whether C ∩']",0,"['We shall see that a natural treatment of salience falls automatically out of our treatment of vague descriptions.', 'As we shall see, this will allow us to simplify the structure of GRE algorithms, and it will explain why many definite descriptions that look as if they were distinguishing descriptions are actually ambiguous.', '9.4.1 A New Perspective on Salience.', ""#AUTHOR_TAG have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available ."", 'Then our algorithm might generate this list of properties: L = mouse, black, salience > 4 .', 'The simpler description the black mouse can be derived by stipulating that the property of being most salient can be left implicit in English.', 'The salience Attribute has to be taken into account by CD, however, and this can be ensured in various ways.', 'For example, instead of testing whether C ']"
CC512,J06-2002,Generating Referring Expressions that Involve Gradable Properties,children’s use of context in interpreting ”big” and ”little” child development,"['K S Ebeling', 'S A Gelman']",,,Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month ( #AUTHOR_TAG ) .,"['Gradability is especially widespread in adjectives.', 'A search of the British National Corpus (BNC), for example, shows at least seven of the ten most frequent adjectives (last, other, new, good, old, great, high, small, different, large) to be gradable.', 'Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month ( #AUTHOR_TAG ) .', 'These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?, when two hats of different sizes are visible).']",0,"['Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month ( #AUTHOR_TAG ) .', 'These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?, when two hats of different sizes are visible).']"
CC513,J06-2002,Generating Referring Expressions that Involve Gradable Properties,fitting words vague language in context linguistics and philosophy,"['Alice Kyburg', 'Michael Morreau']",introduction,,"The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #AUTHOR_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .","['Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999).', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #AUTHOR_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', 'Our own proposal will abstract away from the effects of linguistic context.', 'We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words.', 'This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of']",0,"['The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( #AUTHOR_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', 'Our own proposal will abstract away from the effects of linguistic context.']"
CC514,J06-2002,Generating Referring Expressions that Involve Gradable Properties,object reference in a shared domain of conversation pragmatics and cognition,"['Robbert-Jan Beun', 'Anita Cremers']",introduction,,"We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( #AUTHOR_TAG , Krahmer and Theune 2002 ) .","['We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( #AUTHOR_TAG , Krahmer and Theune 2002 ) .', 'Before we do this, consider the tractability of the original IA.', 'If the running time of FindBestValue(r, A i ) is a constant times the number of Values of the Attribute A i , then the worst-case running time of IA (and IA Plur ) is O(n v n a ), where n a equals the number of Attributes in the language and n v the average number of Values of all Attributes.', 'This is because, in the worst case, all Values of all Attributes need to be attempted (van Deemter 2002).', 'As for the new algorithm, we focus on the crucial phases 2, 4, and 5.']",0,"['We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( #AUTHOR_TAG , Krahmer and Theune 2002 ) .']"
CC515,J06-2002,Generating Referring Expressions that Involve Gradable Properties,speaking from intention to articulation,['William J M Levelt'],introduction,"In Speaking, Willem ""Pim"" Levelt, Director of the Max-Planck-Institut fur Psycholinguistik, accomplishes the formidable task of covering the entire process of speech production, from constraints on conversational appropriateness to articulation and self-monitoring of speech. Speaking is unique in its balanced coverage of all major aspects of the production of speech, in the completeness of its treatment of the entire speech process, and in its strategy of exemplifying rather than formalizing theoretical issues.","Hermann and Deutsch ( 1976 ; also reported in #AUTHOR_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking .","['Even if comparative properties are at the bottom of the preference order, while stronger inequalities precede weaker ones, the order is not fixed completely.', 'Suppose, for example, that the KB contains information about height as well as width, then we have inequalities of the forms (a) height > x, (b) height < x, (c) width > x, and (d) width < x.', 'Which of these should come first?', 'Hermann and Deutsch ( 1976 ; also reported in #AUTHOR_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking .', ""In experiments involving candles of different heights and widths, if the referent is both the tallest and the fattest candle, subjects tended to say 'the tall candle' when the tallest candle is much taller than all others whereas the same candle is only slightly wider than the others; if the reverse is the case, the preference switches to 'the fat candle.'"", ""Hermann and Deutsch's findings may be implemented as follows."", 'First, the Values of the different Attributes should be normalized to make them comparable.', 'Second, preference order should be calculated dynamically (i.e., based on the current value of C, and taking the target into account), preferring larger gaps over smaller ones.', '(It is possible, e.g., that width is most suitable for singling out a black cat, but height for singling out a white cat.)', 'The rest of the algorithm remains unchanged.']",0,"['Which of these should come first?', 'Hermann and Deutsch ( 1976 ; also reported in #AUTHOR_TAG ) show that greater differences are most likely to be chosen , presumably because they are more striking .', ""Hermann and Deutsch's findings may be implemented as follows.""]"
CC516,J06-2002,Generating Referring Expressions that Involve Gradable Properties,computational interpretations of the gricean maximes in the generation of referring expressions,"['Robbert Dale', 'Ehud Reiter']",experiments,"We examine the problem of generating definite noun phrases that are appropriate referring expressions; that is, noun phrases that (a) successfully identify the intended referent to the hearer whilst (b) not conveying to him or her any false conversational implicatures (Grice, 1975). We review several possible computational interpretations of the conversational implicature maxims, with different computational costs, and argue that the simplest may be the best, because it seems to be closest to what human speakers do. We describe our recommended algorithm in detail, along with a specification of the resources a host system must provide in order to make use of the algorithm, and an implementation used in the natural language generation component of the IDAS system.","The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ; #AUTHOR_TAG ) .","['The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ; #AUTHOR_TAG ) .', 'But IA may be replaced by any other reasonable GRE algorithm, for example, one that always exactly minimizes the number of properties expressed, or one that always ""greedily"" selects the property that removes the maximum number of distractors.', 'Let G be any such GRE algorithm, then we can proceed as follows:']",0,"['The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ; #AUTHOR_TAG ) .']"
CC517,J06-2002,Generating Referring Expressions that Involve Gradable Properties,how to refer with vague descriptions,['Manfred Pinkal'],,"This paper deals with the question how reference with vague descriptions should be analysed formally, and gives a proposal for a solution. More precisely, it suggests a formal treatment for the reference identifying function of a special type of vague expressions, as they are used in a special type of definite descriptions. The results of the analysis, however, seem to be of more general importance. The paper consists of:    (i)    some remarks on vagueness and the formal treatment of one type of vagueness;          (ii)    some remarks on definite descriptions and the formal treatment of one kind of descriptions;          (iii)    a short outline of the formal frame which is used to describe both phenomena;          (iv)    an exemplary interpretation of one sentence containing a definite description with a vague predicate within this frame;          (v)    some remarks on the results of the formal analysis concerning reference, vagueness and their mutual relations1.","Following #AUTHOR_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .","['Vague or gradable expressions pose problems to models of language, caused by their context dependence, and by the fact that they are applicable to different degrees.', 'This article focuses on gradable adjectives, also called degree adjectives. 1 More specifically, we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program.', 'Following #AUTHOR_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .', 'It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective.', 'Vague descriptions are worth studying because they use vagueness in a comparatively transparent way, often combining clarity of reference with indeterminacy of meaning; as a result, they allow us to make inroads into the difficult area of research on vagueness.', 'Generation offers an interesting perspective because it forces one to ask when it is a good idea to use these descriptions, in addition to asking what they mean.']",5,"['Following #AUTHOR_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .', 'Vague descriptions are worth studying because they use vagueness in a comparatively transparent way, often combining clarity of reference with indeterminacy of meaning; as a result, they allow us to make inroads into the difficult area of research on vagueness.']"
CC518,J06-2002,Generating Referring Expressions that Involve Gradable Properties,achieving incremental semantic interpretation through contextual representation,"['Julie Sedivy', 'Michael Tanenhaus', 'Craig Chambers', 'Gregory Carlson']",experiments,"While much work has been done investigating the role of context in the incremental processing of syntactic indeterminacies, relatively little is known about online semantic interpretation. The experiments in this article made use of the eye-tracking paradigm with spoken language and visual contexts in order to examine how, and when listeners make use of contextually-defined contrast in interpreting simple prenominal adjectives. Experiment 1 focused on intersective adjectives. Experiment 1A provided further evidence that intersective adjectives are processed incrementally. Experiment 1B compared response times to follow instructions such as 'Pick up the blue comb' under conditions where there were two blue objects (e.g. a blue pen and a blue comb), but only one of these objects had a contrasting member in the display. Responses were faster to objects with a contrasting member, establishing that the listeners initially assume a contrastive interpretation for intersective adjectives. Experiments 2 and 3 focused on vague scalar adjectives examining the time course with which listeners establish contrast for scalar adjectives such as tall using information provided by the head noun (e.g. glass) and information provided by the visual context. Use of head-based information was examined by manipulating the typicality of the target object (e.g. whether it was a good or poor example of a tall glass. Use of context-dependent contrast was examined by either having only a single glass in the display (the no contrast condition) or a contrasting object (e.g. a smaller glass). The pattern of results indicated that listeners interpreted the scalar adjective incrementally taking into account context-specific contrast prior to encountering the head. Moreover, the presence of a contrasting object, sharply reduced, and in some conditions completely eliminated, typicality effects. The results suggest a language processing system in which semantic interpretation, as well as syntactic processing, is conducted incrementally, with early integration of contextual information.","A similar problem is discussed in the psycholinguistics of interpretation ( #AUTHOR_TAG ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known .","['While IA is generally thought to be consistent with findings on human language production (Hermann and Deutsch 1976;Levelt 1989;Pechmann 1989;Sonnenschein 1982), the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates.', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation ( #AUTHOR_TAG ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known .', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']",1,"['We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'A similar problem is discussed in the psycholinguistics of interpretation ( #AUTHOR_TAG ) : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known .', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']"
CC519,J06-2002,Generating Referring Expressions that Involve Gradable Properties,data structures and algorithms,"['Alfred V Aho', 'John E Hopcroft', 'Jeffrey D Ullman']",introduction,"Multi-adaptive Galerkin methods are extensions of the standard continuous and discontinuous Galerkin methods for the numerical solution of initial value problems for ordinary or partial differential equations. In particular, the multi-adaptive methods allow individual and adaptive time steps to be used for different components or in different regions of space. We present algorithms for efficient multi-adaptive time-stepping, including the recursive construction of time slabs and adaptive time step selection. We also present data structures for efficient storage and interpolation of the multi-adaptive solution. The efficiency of the proposed algorithms and data structures is demonstrated for a series of benchmark problems.Comment: ACM Transactions on Mathematical Software 35(3), 24 pages (2008","Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #AUTHOR_TAG ] Chapter 8 ) .","['If computing the intersection of two sets takes constant time then this makes the complexity of interpreting non-vague descriptions linear: O(nd), where nd is the number of properties used.', 'In a vague description, the property last added to the description is context dependent.', 'Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #AUTHOR_TAG ] Chapter 8 ) .', 'Once again, the most time-consuming part of the calculation can be performed off-line, since it is the same for all referring expressions.']",0,"['Worst case , calculating the set corresponding with such a property , of the form size ( x ) = maxm , for example , involves sorting the distractors as to their size , which may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #AUTHOR_TAG ] Chapter 8 ) .']"
CC520,J06-2002,Generating Referring Expressions that Involve Gradable Properties,logic and conversation,['Paul Grice'],experiments,"Die Studie befasst sich mit dem Unterschied zwischen dem sozialen Alltagsgesprach und der experimentellen Befragungssituation. Es wird angenommen, dass das Alltagsgesprach nach bestimmten Prinzipien ('co-operative' and 'relevance' principles) ablauft, die fur beide Gesprachspartner in verbindlicher Weise einen relevanten, wahrheitsgetreuen und gehaltvollen Informationsaustausch garantieren. Diese Prinzipien werden im experimentellen Gesprach durch den Forscher verletzt. Zwei Experimente, die mit 44 Studenten an der Universitat Heidelberg und 48 Studenten der University of Illinois durchgefuhrt wurden, sollten beweisen, dass Versuchspersonen, denen weder informative noch relevante Inhalte vermittelt wurden, sich bei ihrem Antwortverhalten von den durch Psychologen vermittelten personlichen Informationen und nicht von statistischen Informationen leiten liessen, selbst wenn die statistischen Daten einen hoheren diagnostischen Wert besassen. Dies entspricht ganz den Regeln der sozialen Gesprachsfuhrung (denn nur Psychologen, nicht Computer vermogen sich den Konversationsregeln anzupassen) und kann nur in der Weise interpretiert werden, dass Versuchspersonen der ihnen dargebotenen Information eine Relevanz aus dem sozialen Kontext der Situation heraus zuordnen. (ML)'According to the co-operative principle of social discourse, listeners expect speakers to be relevant, truthful, and informative. The apparent overreliance of individuals on non-diagnostic person information at the expense of base-rate information is shown to be, in part, due to the violation of this principle in experiments on judgmental biases. In these experiments, subjects are presented information that is neither informative nor relevant, in a communicative context that suggests otherwise. Accordingly, subjects relied more on individuating personality information and less on base-rate information when the personality information was presented by a psychologist rather than compiled by a computer, presumably because a human communicator but not a computer is supposed to conform to conversational norms. Moreover, subjects relied more on individuating information when the framing of the task implied that psychologists provided correct estimates than when it implied that statisticians provided correct estimates; and when the individuating rather than the base-rate information was varied as a within subjects factor.' (author's abstract",Common sense ( as well as the Gricean maxims ; #AUTHOR_TAG ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication .,"['Common sense ( as well as the Gricean maxims ; #AUTHOR_TAG ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication .', 'We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.', 'Dale and Reiter (1995), for example, discussed the transcripts of a dialogue between people who assemble a piece of garden furniture (originally recorded by Candy Sidner).', 'They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 "" bolt, human assemblers ""unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt"" (Dale and Reiter 1995). 6 Our own experiments (van Deemter 2004) point in the same direction.', 'In one experiment, for example, 34 students at the University of Brighton were shown six pieces of paper, each of which showed two isosceles and approximately equilateral triangles.', 'Triangles of three sizes were shown, with bases of 5, 8, and 16 mm respectively.', 'On each sheet, one of the two triangles had been circled with a pencil.', 'We asked subjects to imagine themselves on the phone to someone who held a copy of the same sheet, but not necessarily with the same orientation (e.g., possibly upside down), and to complete the answers in the following: Q: Which triangle on this sheet was circled?', 'A: The ............ triangle.']",0,['Common sense ( as well as the Gricean maxims ; #AUTHOR_TAG ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication .']
CC521,J06-2002,Generating Referring Expressions that Involve Gradable Properties,two theories about adjectives,['Hans Kamp'],experiments,,Similar things hold for multifaceted properties like intelligence ( #AUTHOR_TAG ) .,"['). Multidimensionality can also slip in through the backdoor.', 'Consider big, for example, when applied to 3D shapes.', 'If there exists a formula for mapping three dimensions into one (e.g., length × width × height) then the result is one dimension (overall-size), and the algorithm of Section 4 can be applied verbatim.', 'But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation.', 'Similar things hold for multifaceted properties like intelligence ( #AUTHOR_TAG ) .']",0,"['). Multidimensionality can also slip in through the backdoor.', 'Similar things hold for multifaceted properties like intelligence ( #AUTHOR_TAG ) .']"
CC522,J06-2002,Generating Referring Expressions that Involve Gradable Properties,the semantics of gradation,['Manfred Bierwisch'],introduction,"The meaning of a complex expression is a function of the lexical meanings of its components and the syntactic structure of the whole. Regularity of semantic composition The meaning of a syntactically regular expression derives from the meanings of its components in a regular way. PoC&gt; The doctrine&gt; Sub-compositionality&gt; Verb gradation&gt; Semantics&gt; Cognition&gt; Life 2 1.2 Syntactic composition and semantic composition Syntactic composition (in terms of constituency or in terms of dependency, or both) follows grammatical rules. * The rules of syntactic composition are in terms of syntactic types (""syntactic categories"") of expressions to be composed, and of the results. * The rules are constrained by principles which, at least partly, may be assumed to apply to syntactic composition only: constraints due to the requirements such as linearization, parsability, syntactic interpretability. (Other constraints, such as economy and faithfulness apply more generally.","For some adjectives , including the ones that #AUTHOR_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate .","['What we said above has also disregarded elements of the ""global"" (i.e., not immediately available) context.', 'For some adjectives , including the ones that #AUTHOR_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate .', 'He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms.', 'For example (after Bierwisch 1989),']",0,"['For some adjectives , including the ones that #AUTHOR_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate .', 'For example (after Bierwisch 1989),']"
CC523,J06-2002,Generating Referring Expressions that Involve Gradable Properties,logic and conversation,['Paul Grice'],,"Die Studie befasst sich mit dem Unterschied zwischen dem sozialen Alltagsgesprach und der experimentellen Befragungssituation. Es wird angenommen, dass das Alltagsgesprach nach bestimmten Prinzipien ('co-operative' and 'relevance' principles) ablauft, die fur beide Gesprachspartner in verbindlicher Weise einen relevanten, wahrheitsgetreuen und gehaltvollen Informationsaustausch garantieren. Diese Prinzipien werden im experimentellen Gesprach durch den Forscher verletzt. Zwei Experimente, die mit 44 Studenten an der Universitat Heidelberg und 48 Studenten der University of Illinois durchgefuhrt wurden, sollten beweisen, dass Versuchspersonen, denen weder informative noch relevante Inhalte vermittelt wurden, sich bei ihrem Antwortverhalten von den durch Psychologen vermittelten personlichen Informationen und nicht von statistischen Informationen leiten liessen, selbst wenn die statistischen Daten einen hoheren diagnostischen Wert besassen. Dies entspricht ganz den Regeln der sozialen Gesprachsfuhrung (denn nur Psychologen, nicht Computer vermogen sich den Konversationsregeln anzupassen) und kann nur in der Weise interpretiert werden, dass Versuchspersonen der ihnen dargebotenen Information eine Relevanz aus dem sozialen Kontext der Situation heraus zuordnen. (ML)'According to the co-operative principle of social discourse, listeners expect speakers to be relevant, truthful, and informative. The apparent overreliance of individuals on non-diagnostic person information at the expense of base-rate information is shown to be, in part, due to the violation of this principle in experiments on judgmental biases. In these experiments, subjects are presented information that is neither informative nor relevant, in a communicative context that suggests otherwise. Accordingly, subjects relied more on individuating personality information and less on base-rate information when the personality information was presented by a psychologist rather than compiled by a computer, presumably because a human communicator but not a computer is supposed to conform to conversational norms. Moreover, subjects relied more on individuating information when the framing of the task implied that psychologists provided correct estimates than when it implied that statisticians provided correct estimates; and when the individuating rather than the base-rate information was varied as a within subjects factor.' (author's abstract","In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles ( #AUTHOR_TAG ) .","['Minimality.', 'Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form.', 'In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles ( #AUTHOR_TAG ) .']",0,"['Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form.', 'In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles ( #AUTHOR_TAG ) .']"
CC524,J06-2002,Generating Referring Expressions that Involve Gradable Properties,understanding shortcuts in nlg systems,['Chris Mellish'],experiments,,"In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #AUTHOR_TAG ) .","['If the usefulness of NLG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail.', 'In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #AUTHOR_TAG ) .', 'The only practical alternative is to provide the generator with ""crisp"" (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database.', 'It is this avenue that we have explored in this article, in combination with various (incremental and other) approaches to GRE.', 'Far from being a peculiarity of a few adjectives, vagueness is widespread.', 'We believe that our approach can be applied to a variety of situations in which vagueness affects referring expressions including, for example, r color terms (Section 9.3); r nouns that allow different degrees of strictness (Section 9.5); r degrees of salience (Section 9.4); and r imprecise pointing (Section 9.5).']",4,"['If the usefulness of NLG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail.', 'In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( #AUTHOR_TAG ) .']"
CC525,J06-2002,Generating Referring Expressions that Involve Gradable Properties,efficient contextsensitive generation of referring expressions,"['Emiel Krahmer', 'Mari¨et Theune']",experiments,3 A Modification of the Algorithm Based on Salience 5 3.1 Motivation: Determining the Context Set : : : : : : : : : : : : : 5 3.2 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3.3 Outline of the Modified Algorithm : : : : : : : : : : : : : : : : : 7 3.4 Examples : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9 3.5 Discussion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11,"CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( #AUTHOR_TAG , Section 8.6.2 ) : Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say , size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed .","['Some generalizations of our method are fairly straightforward. For example, consider a relational description (cf., Dale and Haddock 1991) involving a gradable adjective, as in the dog in the large shed.', 'CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( #AUTHOR_TAG , Section 8.6.2 ) : Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say , size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed .', 'Note that this is felicitous even if the shed is not the largest one in the domain, as is true for d2 in the following situation (contains-a=b means that a is contained by b):']",0,"['Some generalizations of our method are fairly straightforward. For example, consider a relational description (cf., Dale and Haddock 1991) involving a gradable adjective, as in the dog in the large shed.', 'CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm ( #AUTHOR_TAG , Section 8.6.2 ) : Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size ( say , size 5 ) ; if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed .']"
CC526,J06-2002,Generating Referring Expressions that Involve Gradable Properties,incremental speech production and referential overspecification,['Thomas Pechmann'],experiments,"Speech is often produced incrementally: speakers start to articulate an utterance before knowing exactly what they are going to say. The time devoted to articulating first parts of an utterance is simultaneously used to process further information which is to be incorporated into that utterance. Empirical evidence is presented which supports the assumption that referential noun phrases are indeed often produced incrementally. This assumption explains several phenomena: first, speakers' production of overspecified ('redundant') utterances; second, irregularities concerning the ordering ofprenominal adjectives; and third, the way acoustic stress is assigned in referential noun phrases. The basic problem that a speaker faces in what is usually called 'referential communication' is that he has to refer unambiguously to a target object in the context of other objects in such a way that a listener is able to single it out from all relevant alternatives. In order to characterize a target object unambiguously in the context of others, the speaker must determine some appropriate set of distinguishing features, that is, those features that distinguish the target from all relevant alternatives. Consider the simple example depicted in Figure 1. Suppose a referential domain consists of three objects: a black and white triangle and a white circle. Suppose the white circle is the target object on which the speaker wants to focus the listener's attention. In this case the distinguishing feature is the object class. The information 'circle' is sufficient for the listener to know which object is meant by the speaker. In contrast, color would be nondistinguishing information in this case, since color does not help the listener to differentiate the circle from the white triangle. Research on referential communication has primarily been oriented toward developmental questions (for overviews, see Dickson 1981; Glucksberg et al. 1975; Shatz 1978). For most researchers, success or failure in the child's referential-communication tasks has served as an Linguistics 27 (1989), 89-110 0024-3949/89/0027-0089 $2.00 (c) Mouton de Gruyter, Amsterdam","While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; #AUTHOR_TAG ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .","['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; #AUTHOR_TAG ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation (Sedivy et al. 1999): Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known.', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']",0,"['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; #AUTHOR_TAG ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .']"
CC527,J06-2002,Generating Referring Expressions that Involve Gradable Properties,games and information an introduction to game theory,['Eric Rasmusen'],experiments,"List of Figures. List of Tables. List of Games. Preface. Contents and Purpose. Changes in the Second Edition (1994). Changes in the Third Edition (2001). Changes in the Fourth Edition (2006). Using the Book. The Level of Mathematics. Other Books. Contact Information. Acknowledgements. Introduction. History. Game Theory's Method. Exemplifying Theory. This Book's Style. Notes. PART 1: GAME THEORY. 1. The Rules of the Game. Definitions. Dominated and Dominant Strategies: The Prisoner's Dilemma. Iterated Dominance: The Battle of the Bismarck Sea. Nash Equilibrium: Boxed Pigs, The Battle of the Sexes and Ranked Coordination. Focal Points. Notes. Problems. Classroom Game. 2. Information. The Strategic and Extensive Forms of a Game. Information Sets. Perfect, Certain, Symmetric, and Complete Information. The Harsanyi Transformation and Bayesian Games. Example: The Png Settlement Game. Notes. Problems. Classroom Game. 3. Mixed and Continuous Strategies. Mixed Strategies: The Welfare Game. The Payoff-equating Method and Games of Timing. Mixed Strategies with General Parameters and N Players: The Civic Duty Game. Randomizing is not Always Mixing: The Auditing Game. Continuous Strategies: The Cournot Game. Continuous Strategies: The Bertrand Game, Strategic Complements, and Strategic. Substitutes. Existence of Equilibrium. Notes. Problems. Classroom Game. 4. Dynamic Games with Symmetric Information. Subgame Perfectness. An Example of Perfectness: Entry Deterrence I. Credible Threats, Sunk Costs, and the Open-Set Problem in the Game of Nuisance Suits. Recoordination to Pareto-dominant Equilibria in Subgames: Pareto Perfection. Notes. Problems. Classroom Game. 5. Reputation and Repeated Games with Symmetric Information. Finitely Repeated Games and the Chainstore Paradox. Infinitely Repeated Games, Minimax Punishments, and the Folk Theorem. Reputation: The One-sided Prisoner's Dilemma. Product Quality in an Infinitely Repeated Game. Markov Equilibria and Overlapping Generations: Customer Switching Costs. Evolutionary Equilibrium: The Hawk-Dove Game. Notes. Problems. Classroom Game. 6. Dynamic Games with Incomplete Information. Perfect Bayesian Equilibrium: Entry Deterrence II and III. Refining Perfect Bayesian Equilibrium in the Entry Deterrence and PhD Admissions Games. The Importance of Common Knowledge: Entry Deterrence IV and V. Incomplete Information in the Repeated Prisoner's Dilemma: The Gang of Four Model. The Axelrod Tournament. Credit and the Age of the Firm: The Diamond Model. Notes. Problems. Classroom Game. PART 2: ASYMMETRIC INFORMATION. 7. Moral Hazard: Hidden Actions. Categories of Asymmetric Information Models. A Principal-agent Model: The Production Game. The Incentive Compatibility and Participation Constraints. Optimal Contracts: The Broadway Game. Notes. Problems. Classroom Game. 8. Further Topics in Moral Hazard. Efficiency Wages. Tournaments. Institutions and Agency Problems. Renegotiation: The Repossession Game. State-space Diagrams: Insurance Games I and II. Joint Production by Many Agents: The Holmstrom Teams Model. The Multitask Agency Problem. Notes. Problems. Classroom Game. 9. Adverse Selection. Introduction: Production Game VI. Adverse Selection under Certainty: Lemons I and II. Heterogeneous Tastes: Lemons III and IV. Adverse Selection under Uncertainty: Insurance Game III. Market Microstructure. A Variety of Applications. Adverse Selection and Moral Hazard Combined: Production Game VII. Notes. Problems. Classroom Game. 10. Mechanism Design and Postcontractual Hidden Knowledge. Mechanisms, Unravelling, Cross Checking, and the Revelation Principle. Myerson Mechanism Design. An Example of Postcontractual Hidden Knowledge: The Salesman Game. The Groves Mechanism. Price Discrimination. Rate-of-return Regulation and Government Procurement. Notes. Problems. Classroom Game. 11. Signalling. The Informed Player Moves First: Signalling. Variants on the Signalling Model of Education. General Comments on Signalling in Education. The Informed Player Moves Second: Screening. Two Signals: The Game of Underpricing New Stock Issues. Signal Jamming and Limit Pricing. Countersignalling. Notes. Problems. Classroom Game. PART 3: APPLICATIONS. 12. Bargaining. The Basic Bargaining Problem: Splitting a Pie. The Nash Bargaining Solution. Alternating Offers over Finite Time. Alternating Offers over Infinite Time. Incomplete Information. Setting Up a Way to Bargain: The Myerson-Satterthwaite Mechanism. Notes. Problems. Classroom Game. 13. Auctions. Values Private and Common, Continuous and Discrete. Optimal Strategies under Different Rules in Private-value Auctions. Revenue Equivalence, Risk Aversion, and Uncertainty. Reserve Prices and the Marginal Revenue Approach. Common-value Auctions and the Winner's Curse. Asymmetric Equilibria, Affiliation, and Linkage: The Wallet Game. Notes. Problems. Classroom Game. 14. Pricing. Quantities as Strategies: Cournot Equilibrium Revisited. Capacity Constraints: The Edgeworth Paradox. Location Models. Comparative Statics and Supermodular Games. Vertical Differentiation. Durable Monopoly. Notes. Problems. Classroom Game. Mathematical Appendix. Notation. The Greek Alphabet. Glossary. Formulas and Functions. Probability Distributions. Supermodularity. Fixed Point Theorems. Genericity. Discounting. Risk. References and Name Index. Subject Index","When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #AUTHOR_TAG ) .","['Generalizations to complex Boolean descriptions involving negation and disjunction (van Deemter 2004) appear to be largely straightforward, except for issues to do with opposites and markedness.', 'For example, the generator will have to decide whether to say the patients that are old or the patients that are not young.', '9.3 Multidimensionality 9.3.1 Combinations of Adjectives.', 'When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #AUTHOR_TAG ) .', 'Let us focus on references to an individual referent r, starting with a description that contains more than one gradable adjective.', 'The NP the tall fat giraffe, for example, can safely refer to an element b in a situation like the one below, where b is the only element that exceeds all distractors with respect to some dimension (a different one for a than for c, as it happens) while not being exceeded by any distractors in any dimension: Cases like this would be covered if the decision-theoretic property of Pareto optimality (e.g., Feldman 1980) was used as the sole criterion: Formally, an object r ∈ C has a Pareto-optimal combination of Values V iff there is no other x ∈ C such that 1. ∃V i ∈ V : V i (x) > V i (r) and 2. ¬∃V j ∈ V : V j (x) < V j (r)']",0,"['When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #AUTHOR_TAG ) .']"
CC528,J06-2002,Generating Referring Expressions that Involve Gradable Properties,speaking from intention to articulation,['William J M Levelt'],experiments,"In Speaking, Willem ""Pim"" Levelt, Director of the Max-Planck-Institut fur Psycholinguistik, accomplishes the formidable task of covering the entire process of speech production, from constraints on conversational appropriateness to articulation and self-monitoring of speech. Speaking is unique in its balanced coverage of all major aspects of the production of speech, in the completeness of its treatment of the entire speech process, and in its strategy of exemplifying rather than formalizing theoretical issues.","While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; #AUTHOR_TAG ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .","['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; #AUTHOR_TAG ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation (Sedivy et al. 1999): Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known.', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']",0,"['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; #AUTHOR_TAG ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']"
CC529,J06-2002,Generating Referring Expressions that Involve Gradable Properties,achieving incremental semantic interpretation through contextual representation,"['Julie Sedivy', 'Michael Tanenhaus', 'Craig Chambers', 'Gregory Carlson']",experiments,"While much work has been done investigating the role of context in the incremental processing of syntactic indeterminacies, relatively little is known about online semantic interpretation. The experiments in this article made use of the eye-tracking paradigm with spoken language and visual contexts in order to examine how, and when listeners make use of contextually-defined contrast in interpreting simple prenominal adjectives. Experiment 1 focused on intersective adjectives. Experiment 1A provided further evidence that intersective adjectives are processed incrementally. Experiment 1B compared response times to follow instructions such as 'Pick up the blue comb' under conditions where there were two blue objects (e.g. a blue pen and a blue comb), but only one of these objects had a contrasting member in the display. Responses were faster to objects with a contrasting member, establishing that the listeners initially assume a contrastive interpretation for intersective adjectives. Experiments 2 and 3 focused on vague scalar adjectives examining the time course with which listeners establish contrast for scalar adjectives such as tall using information provided by the head noun (e.g. glass) and information provided by the visual context. Use of head-based information was examined by manipulating the typicality of the target object (e.g. whether it was a good or poor example of a tall glass. Use of context-dependent contrast was examined by either having only a single glass in the display (the no contrast condition) or a contrasting object (e.g. a smaller glass). The pattern of results indicated that listeners interpreted the scalar adjective incrementally taking into account context-specific contrast prior to encountering the head. Moreover, the presence of a contrasting object, sharply reduced, and in some conditions completely eliminated, typicality effects. The results suggest a language processing system in which semantic interpretation, as well as syntactic processing, is conducted incrementally, with early integration of contextual information.","(For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #AUTHOR_TAG 1999, discussed in Section 7.2).","['We could require that the referent of an evaluative description fall into the correct segment of the relevant dimension.', '(For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #AUTHOR_TAG 1999, discussed in Section 7.2).', 'Be this as it may, we shall henceforth focus on local context, assuming that additional requirements on the global context can be made if necessary.']",0,"['We could require that the referent of an evaluative description fall into the correct segment of the relevant dimension.', '(For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #AUTHOR_TAG 1999, discussed in Section 7.2).', 'Be this as it may, we shall henceforth focus on local context, assuming that additional requirements on the global context can be made if necessary.']"
CC530,J06-2002,Generating Referring Expressions that Involve Gradable Properties,cooking up referring expressions,['Robert Dale'],,"This paper describes the referring expression generation mechanisms used in EPICURE, a computer program which produces natural language descriptions of cookery recipes. Major features of the system include: an underlying ontology which permits the representation of non-singular entities; a notion of discriminatory power, to determine what properties should be used in a description; and a PATR-like unification grammar to produce surface linguistic strings.","NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #AUTHOR_TAG ) : The selected expression should also be felicitous .","['NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #AUTHOR_TAG ) : The selected expression should also be felicitous .', 'Consider the question, discussed in the philosophical logic literature, of whether it is legitimate, for a gradable adjective, to distinguish between ""observationally indifferent"" entities: Suppose two objects x and y, are so similar that it is impossible to distinguish their sizes; can it ever be reasonable to say that x is large and y is not?', 'A positive answer would not be psychologically plausible, since x and y are indistinguishable; but a negative answer would prohibit any binary distinction between objects that are large and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it.', 'This is the strongest version of the sorites paradox (e.g., Hyde 2002).']",0,"['NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #AUTHOR_TAG ) : The selected expression should also be felicitous .']"
CC531,J06-2002,Generating Referring Expressions that Involve Gradable Properties,the order of prenominal adjectives in natural language generation,['Rob Malouf'],,The order of prenominal adjectival modifiers in English is governed by complex and difficult to describe constraints which straddle the boundary between competence and performance. This paper describes and compares a number of statistical and machine learning techniques for ordering sequences of adjectives in the context of a natural language generation system.,"One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ; #AUTHOR_TAG ) .","['One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ; #AUTHOR_TAG ) .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others.', 'Quirk et al. (1985), for example, report that ""adjectives denoting size, length, and height normally precede other nonderived adjectives"" (e.g., the small round table is usually preferred to the round small table).', 'Semantically, this does not come as a surprise.', 'In a noun phrase of the form the three small (-est) [N], for example, the words preceding N select the three smallest elements of [N].', 'It follows that, to denote the three smallest elements of the set of round tables, the only option is to say the three small round tables, rather than the three round small tables.', 'The latter would mean something else, namely, the three round ones among the n small(est) tables (where n is not specified).', ""It actually seems quite possible to say this, but only when some set of small tables is contextually salient (e.g., I don't mean those small tables, I mean the three round ones)."", 'Given that n is unspecified, the noun phrase would tend to be very unclear in any other context.']",0,"['One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ; #AUTHOR_TAG ) .']"
CC532,J06-2002,Generating Referring Expressions that Involve Gradable Properties,computational interpretations of the gricean maximes in the generation of referring expressions,"['Robbert Dale', 'Ehud Reiter']",introduction,"We examine the problem of generating definite noun phrases that are appropriate referring expressions; that is, noun phrases that (a) successfully identify the intended referent to the hearer whilst (b) not conveying to him or her any false conversational implicatures (Grice, 1975). We review several possible computational interpretations of the conversational implicature maxims, with different computational costs, and argue that the simplest may be the best, because it seems to be closest to what human speakers do. We describe our recommended algorithm in detail, along with a specification of the resources a host system must provide in order to make use of the algorithm, and an implementation used in the natural language generation component of the IDAS system.","4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX , #AUTHOR_TAG ) .","['4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX , #AUTHOR_TAG ) .', 'VAGUE uses both of these devices.']",0,"['4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX , #AUTHOR_TAG ) .', 'VAGUE uses both of these devices.']"
CC533,J06-2002,Generating Referring Expressions that Involve Gradable Properties,understanding complex visually referring utterances,"['Peter Gorniak', 'Deb Roy']",experiments,"We propose a computational model of visually-grounded spatial language understanding, based on a study of how people verbally describe objects in visual scenes. We describe our implementation of word level visually-grounded semantics and their embedding in a compositional parsing framework. The implemented system selects the correct referents in response to a broad range of referring expressions for a large percentage of test cases. In an analysis of the system's successes and failures we reveal how visual context influences the semantics of utterances and propose future extensions to the model that take such context into account.","The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX #AUTHOR_TAG ; Thorisson 1994 , for other plans ) .","['In our example, b is the only object that has a Pareto-optimal combination of Values, predicting correctly that b can be called the tall fat giraffe.', 'It seems likely, however, that people use doubly graded descriptions more liberally.', 'For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe.', 'Many alternative strategies are possible.', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX #AUTHOR_TAG ; Thorisson 1994 , for other plans ) .']",0,"['It seems likely, however, that people use doubly graded descriptions more liberally.', 'Many alternative strategies are possible.', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX #AUTHOR_TAG ; Thorisson 1994 , for other plans ) .']"
CC534,J06-2002,Generating Referring Expressions that Involve Gradable Properties,interpreting vague utterances in context,"['David DeVault', 'Matthew Stone']",introduction,"We use the interpretation of vague scalar predicates  like small as an illustration of how systematic  semantic models of dialogue context enable  the derivation of useful, fine-grained utterance  interpretations from radically underspecified  semantic forms. Because dialogue context  suffices to determine salient alternative scales  and relevant distinctions along these scales,  we can infer implicit standards of comparison  for vague scalar predicates through completely  general pragmatics, yet closely constrain the intended  meaning to within a natural range","The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #AUTHOR_TAG ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .","['Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999).', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #AUTHOR_TAG ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', 'Our own proposal will abstract away from the effects of linguistic context.', 'We shall ask how noun phrases like the ones in (3) and (4) can be generated, without asking how they constrain, and are constrained by, other uses of large and related words.', 'This will allow us to make the following simplification: In a definite description that expresses only properties that are needed for singling out a referent, we take the base form of']",0,"['Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999).', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #AUTHOR_TAG ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', 'Our own proposal will abstract away from the effects of linguistic context.']"
CC535,J06-2002,Generating Referring Expressions that Involve Gradable Properties,computational interpretations of the gricean maximes in the generation of referring expressions,"['Robbert Dale', 'Ehud Reiter']",introduction,"We examine the problem of generating definite noun phrases that are appropriate referring expressions; that is, noun phrases that (a) successfully identify the intended referent to the hearer whilst (b) not conveying to him or her any false conversational implicatures (Grice, 1975). We review several possible computational interpretations of the conversational implicature maxims, with different computational costs, and argue that the simplest may be the best, because it seems to be closest to what human speakers do. We describe our recommended algorithm in detail, along with a specification of the resources a host system must provide in order to make use of the algorithm, and an implementation used in the natural language generation component of the IDAS system.","Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX , #AUTHOR_TAG ) .","['The representation of inequalities is not entirely trivial.', 'For one thing, it is convenient to view properties of the form size(x) < α as belonging to a different Attribute than those of the form size(x) > α, because this causes the Values of an Attribute to be linearly ordered: Being larger than 12 cm implies being larger than 10 cm, and so on.', 'More importantly, it will now become normal for an object to have many Values for the same Attribute; c 4 , for example, has the Values > 6 cm, > 10 cm, and > 12 cm.', 'Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX , #AUTHOR_TAG ) .', ""If we abstract away from the role of basic-level Values, then Dale and Reiter's FindBestValue chooses the most general Value that removes the maximal number of distractors, as we have seen."", 'Thus, size(x) > m is preferred over size(x) > n iff m > n; conversely, size(x) < m is preferred over size(x) < n iff m < n.)', 'This is reflected by the order in which the properties are listed above: Once a sizerelated property is selected, later size-related properties do not remove any distractors and will therefore not be included in the description.']",0,"['The representation of inequalities is not entirely trivial.', 'Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX , #AUTHOR_TAG ) .', ""If we abstract away from the role of basic-level Values, then Dale and Reiter's FindBestValue chooses the most general Value that removes the maximal number of distractors, as we have seen.""]"
CC536,J06-2002,Generating Referring Expressions that Involve Gradable Properties,generating referring expressions containing relations,"['Robbert Dale', 'Nickolas Haddock']",experiments,"Recent work on the Generation of Referring Expressions has increased the generating capability of algorithms in this area. This paper asks whether the models underlying these proposals can still be used if even more complex referring expressions are generated. To discuss this issue, we will investigate a variety of referring expressions that pose difficulties to current generation algorithms. In particular, we will discuss the difficulties associated with quantified referring expressions (such as 'those women who have fewer than two children', or 'the people who work for exactly 2 employers') and explain how they can be generated by extending the inference-based approach described in (Varges, 2004).","For example , consider a relational description ( cfXXX , #AUTHOR_TAG ) involving a gradable adjective , as in the dog in the large shed .","['Some generalizations of our method are fairly straightforward.', 'For example , consider a relational description ( cfXXX , #AUTHOR_TAG ) involving a gradable adjective , as in the dog in the large shed .', 'CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm (Krahmer and Theune 2002, Section 8.6.2):', 'Suppose an initial description is generated describing the set of all those dogs that are in sheds over a given size (say, size 5); if this description happens to distinguish an individual dog then this legitimizes the use of the noun phrase the dog in the large shed.', ""Note that this is felicitous even if the shed is not the largest one in the domain, as is true for d 2 in the following situation (contains-a=b means that a is contained by b): In other words, the dog in the large shed denotes 'the dog such that there is no other shed that is equally large or larger and that contains a dog'."", 'Note that it would be odd, in the above-sketched situation, to say the dog in the largest shed.']",0,"['For example , consider a relational description ( cfXXX , #AUTHOR_TAG ) involving a gradable adjective , as in the dog in the large shed .', 'CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm (Krahmer and Theune 2002, Section 8.6.2):']"
CC537,J06-2002,Generating Referring Expressions that Involve Gradable Properties,the bargaining problem,['John Nash'],experiments,"AbstractConsider the problem of partitioning n items among d players where the utility of each player for bundles of items is additive; so, player r has utility vri for item i and the utility of that player for a bundle of items is the sum of the vir's over the items i in his/her bundle. Each partition S of the items is then associated with a d-dimensional utility vector VS whose coordinates are the utilities that the players assign to the bundles they get under S. Also, lotteries over partitions are associated with the corresponding expected utility vectors. We model the problem as a Nash bargaining game over the set of lotteries over partitions and provide methods for computing the corresponding Nash solution, to prescribed accuracy, with effort that is polynomial in n. In particular, we show that points in the pareto-optimal set of the corresponding bargaining set correspond to lotteries over partitions under which each item, with the possible exception of at most d(d-1)/2 items, is assigned in the same way","The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( #AUTHOR_TAG ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans ) .","['In our example, b is the only object that has a Pareto-optimal combination of Values, predicting correctly that b can be called the tall fat giraffe.', 'It seems likely, however, that people use doubly graded descriptions more liberally.', 'For example, if the example is modified by letting width(a) = 3.1 m, making a slightly fatter than b, then b might still be the only reasonable referent of the tall fat giraffe.', 'Many alternative strategies are possible.', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( #AUTHOR_TAG ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans ) .']",0,"['Many alternative strategies are possible.', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( #AUTHOR_TAG ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans ) .']"
CC538,J06-2002,Generating Referring Expressions that Involve Gradable Properties,building natural language generation systems,"['Ehud Reiter', 'Robert Dale']",experiments,"This book explains how to build Natural Language Generation (NLG) systems - computer software systems which use techniques from artificial intelligence and computational linguistics to automatically generate understandable texts in English or other human languages, either in isolation or as part of multimedia documents, Web pages, and speech output systems","The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. , #AUTHOR_TAG ) .","['The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. , #AUTHOR_TAG ) .', 'An example of such an inference rule is the one that transforms a list of the form mouse, >10 cm into one of the form mouse, size(x) = max 2 if only two mice are larger than 10 cm.', 'The same issues also make it difficult to interleave CD and linguistic realization as proposed by various authors, because properties may need to be combined before they are expressed.']",0,"['The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. , #AUTHOR_TAG ) .', 'An example of such an inference rule is the one that transforms a list of the form mouse, >10 cm into one of the form mouse, size(x) = max 2 if only two mice are larger than 10 cm.', 'The same issues also make it difficult to interleave CD and linguistic realization as proposed by various authors, because properties may need to be combined before they are expressed.']"
CC539,J06-2002,Generating Referring Expressions that Involve Gradable Properties,should corpora texts be gold standards for nlg,"['Ehud Reiter', 'Somayajulu Sripada']",,,"A more flexible approach is used by #AUTHOR_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .","['Some NLG systems produce gradable adjectives. The FOG weather-forecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994).', 'FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead.', 'A more flexible approach is used by #AUTHOR_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .', 'A third approach was implemented in Dial Your Disc (DYD), where the extension of a gradable adjective like famous was computed rather than specified by hand (van Deemter and Odijk 1997).', 'To determine, for example, whether one of Mozart�s piano sonatas could be called a famous sonata, the system looked up the number x of compact disc recordings of this sonata (as listed in an encyclopedia) and compared it to the average number y of CD recordings of each of Mozart�s sonatas.', 'The sonata was called a famous sonata if x >> y. Like DYD, the work reported in this article will abandon the use of fixed boundary values for gradable adjectives, letting these values depend on the context in which the adjective is used.']",0,"['Some NLG systems produce gradable adjectives. The FOG weather-forecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994).', 'FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead.', 'A more flexible approach is used by #AUTHOR_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .']"
CC540,J06-2002,Generating Referring Expressions that Involve Gradable Properties,textual economy through close coupling of syntax and semantics,"['Matthew Stone', 'Bonnie Webber']",,"We focus on the production of efficient descriptions of objects, actions and events. We define a type of  efficiency, textual economy, that exploits the hearer&apos;s recognition of inferential links to material elsewhere  within a sentence. Textual economy leads to efficient descriptions because the material that supports such  inferences has been included to satisfy independent communicative goals, and is therefore overloaded  in the sense of Pollack [18]. We argue that achieving textual economy imposes strong requirements  on the representation and reasoning used in generating sentences. The representation must support the  generator&apos;s simultaneous consideration of syntax and semantics. Reasoningmust enable the generator  to assess quickly and reliably at any stage how the hearer will interpret the current sentence, with its  &apos;-(inc6mplete)syntax and&apos;semantics. We show that these representational and reasoning requirements are  met in the SPUD system for sentence planning and realization","Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #AUTHOR_TAG ; Krahmer and Theune 2002 ) .","['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #AUTHOR_TAG ; Krahmer and Theune 2002 ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', 'Consider, for instance, the list of properties L = size > 3 cm, size < 9 cm .', 'If interleaving forced us to realize the two properties in L one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one (if the facts in the KB support it), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size < 9 cm).', 'Clearly, sophisticated use of gradable adjectives requires a separation between CD and linguistic realization, unless one is willing to complicate linguistic realization considerably.']",1,"['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( #AUTHOR_TAG ; Krahmer and Theune 2002 ) .']"
CC541,J06-2002,Generating Referring Expressions that Involve Gradable Properties,psychologie der objektbenennung,"['Tony Hermann', 'Roland Deutsch']",experiments,,"While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .","['While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This question is especially pertinent in the case of vague expressions, since gradable properties are selected last, but realized first (Section 6).', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation (Sedivy et al. 1999): Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known.', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']",0,"['While IA is generally thought to be consistent with findings on human language production ( #AUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', ""(Wildly redundant descriptions can result if the 'wrong' preference order are chosen.)"", 'We shall see that vague descriptions pose particular challenges to incrementality.', 'One question emerges when the IA is combined with findings on word order and incremental interpretation.', 'If human speakers and/or writers perform CD incrementally, then why are properties not expressed in the same order in which they were selected?', 'This means that the linguistic realization cannot start until CD is concluded, contradicting eye-tracking experiments suggesting that speakers start speaking while still scanning distractors (Pechmann 1989).', 'A similar problem is discussed in the psycholinguistics of interpretation (Sedivy et al. 1999): Interpretation is widely assumed to proceed incrementally, but vague descriptions resist strict incrementality, since an adjective in a vague description can only be fully interpreted when its comparison set is known.', 'Sedivy and colleagues resolve this quandary by allowing a kind of revision, whereby later words allow hearers to refine their interpretation of gradable adjectives.', 'Summarizing the situation in generation and interpretation, it is clear that the last word on incrementality has not been said.']"
CC542,J06-2002,Generating Referring Expressions that Involve Gradable Properties,basic color terms,"['Brent Berlin', 'Paul Kay']",experiments,,"A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .","['Color terms are a case apart.', 'If color is modeled in terms of saturation, hue, and luminosity, for instance, then an object a may be classified as greener than b on one dimension (e.g., saturation), but less green than b on another (e.g., hue).', 'This would considerably complicate the application of our algorithm to color terms, which is otherwise mostly straighforward (Section 9.3).', '(The green chair, said in the presence of two greenish chairs, would refer to the one that is closest to prototypical green.)', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .', '(Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent.', 'Red as in red hair, e.g., differs from red as in red chair.)', 'Different attitudes towards multidimensionality are possible.', 'One possibility is to be cautious and to keep aiming for distinguishing descriptions in the strict sense.', 'In this case, the program should limit the use of vague descriptions to situations where there exists a referent that has a Pareto-optimal combination of Values.', 'Alternatively, one could allow referring expressions to be ambiguous.', 'It would be consistent with this attitude, for example, to map multiple dimensions into one overall dimension, perhaps by borrowing from principles applied in perceptual grouping, where different perceptual dimensions are mapped into one (e.g., Thorisson 1994).', 'The empirical basis of this line of work, however, is still somewhat weak, so the risk of referential unclarity looms large.', 'Also, this attitude would go against the spirit of GRE, where referring expressions have always been assumed to be distinguishing.']",0,"['Color terms are a case apart.', 'If color is modeled in terms of saturation, hue, and luminosity, for instance, then an object a may be classified as greener than b on one dimension (e.g., saturation), but less green than b on another (e.g., hue).', 'This would considerably complicate the application of our algorithm to color terms, which is otherwise mostly straighforward (Section 9.3).', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension ( #AUTHOR_TAG , pages 10 -- 12 ) .', '(Ideally, GRE should also take into account that the meaning of color words can differ across different types of referent.']"
CC543,J06-2002,Generating Referring Expressions that Involve Gradable Properties,achieving incremental semantic interpretation through contextual representation,"['Julie Sedivy', 'Michael Tanenhaus', 'Craig Chambers', 'Gregory Carlson']",experiments,"While much work has been done investigating the role of context in the incremental processing of syntactic indeterminacies, relatively little is known about online semantic interpretation. The experiments in this article made use of the eye-tracking paradigm with spoken language and visual contexts in order to examine how, and when listeners make use of contextually-defined contrast in interpreting simple prenominal adjectives. Experiment 1 focused on intersective adjectives. Experiment 1A provided further evidence that intersective adjectives are processed incrementally. Experiment 1B compared response times to follow instructions such as 'Pick up the blue comb' under conditions where there were two blue objects (e.g. a blue pen and a blue comb), but only one of these objects had a contrasting member in the display. Responses were faster to objects with a contrasting member, establishing that the listeners initially assume a contrastive interpretation for intersective adjectives. Experiments 2 and 3 focused on vague scalar adjectives examining the time course with which listeners establish contrast for scalar adjectives such as tall using information provided by the head noun (e.g. glass) and information provided by the visual context. Use of head-based information was examined by manipulating the typicality of the target object (e.g. whether it was a good or poor example of a tall glass. Use of context-dependent contrast was examined by either having only a single glass in the display (the no contrast condition) or a contrasting object (e.g. a smaller glass). The pattern of results indicated that listeners interpreted the scalar adjective incrementally taking into account context-specific contrast prior to encountering the head. Moreover, the presence of a contrasting object, sharply reduced, and in some conditions completely eliminated, typicality effects. The results suggest a language processing system in which semantic interpretation, as well as syntactic processing, is conducted incrementally, with early integration of contextual information.",#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .,"['It is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input.', 'Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects.', '#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .', 'Consider the tall cup.', 'The relevant scene would contain three distractors: (1) a less tall object of the same type as the target (e.g., a cup that is less tall), (2) a different kind of object that previous studies had shown to be intermediate in height (e.g., a pitcher that, while being taller than both cups, was neither short nor tall for a pitcher), and (3) a different type of object to which the adjective is inapplicable (e.g., a door key).', ""Across the different conditions under which the experiment was done (e.g., allowing subjects to study the domain before or after the onset of speech), it was found not to matter much whether the adjective applied 'intrinsically' to the target object (i.e., whether the target was tall for a cup): Hearers identifed the target without problems in both types of situations."", 'The time subjects took before looking at the target for the first time was measured, and although these latency times were somewhat greater when the referent were not intrinsically tall than when they were, the average difference was tiny at 554 versus 538 miliseconds.', 'Since latency times are thought to be sensitive to most of the problems that hearers may have in processing a text, these results suggest that, for dimensional adjectives, it is forgivable to disregard global context.']",0,"['It is normally perhaps unlikely that people produce language on the basis of the kind of numerical representations that our algorithm has used as input.', 'Although psychological plausibility is not our aim, it is worth noting that the inequalities computed as step 2 of the algorithm of Section 4 might be psychologically more plausible, since they are essentially no more than comparisons between objects.', '#AUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .']"
CC544,J06-2002,Generating Referring Expressions that Involve Gradable Properties,efficient contextsensitive generation of referring expressions,"['Emiel Krahmer', 'Mari¨et Theune']",experiments,3 A Modification of the Algorithm Based on Salience 5 3.1 Motivation: Determining the Context Set : : : : : : : : : : : : : 5 3.2 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 3.3 Outline of the Modified Algorithm : : : : : : : : : : : : : : : : : 7 3.4 Examples : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9 3.5 Discussion : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11,"It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .","['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .', 'We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions.', 'Yet, by giving gradable properties a low ranking, we might cause the algorithm to underuse them, for example, in situations where gradable properties are highly relevant to the purpose of the discourse (e.g., a fist fight between people of very different sizes).', 'Luckily, there are no semantic or algorithmic reasons for giving gradables a low ranking.', 'Let us see how things would work if they were ranked more highly.']",1,"['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( #AUTHOR_TAG ) .', 'We have seen in Section 4.3 that generation and interpretation of vague descriptions does have a slightly higher computational complexity than that of nonvague descriptions.', 'Yet, by giving gradable properties a low ranking, we might cause the algorithm to underuse them, for example, in situations where gradable properties are highly relevant to the purpose of the discourse (e.g., a fist fight between people of very different sizes).', 'Let us see how things would work if they were ranked more highly.']"
CC545,J06-2002,Generating Referring Expressions that Involve Gradable Properties,two theories about adjectives,['Hans Kamp'],introduction,,"2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .","['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .']",0,"['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions ( #AUTHOR_TAG ; also Section 8.1 of the present article ) .']"
CC546,J06-2002,Generating Referring Expressions that Involve Gradable Properties,situations and attitudes,"['Jon Barwise', 'John Perry']",introduction,"In this provocative book, Barwise and Perry tackle the slippery subject of ""meaning, "" a subject that has long vexed linguists, language philosophers, and logicians.","In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .","['IA Plur deals with vague properties in essentially the same way as FOG: Attributes like size are treated as if they were not context dependent: Their Values always apply to the same objects, regardless of what other properties occur in the description.', 'In this way, IA could never describe the same animal as the large chihuahua and the small brown dog, for example.', 'This approach does not do justice to gradable adjectives, whether they are used in the base form, the superlative, or the comparative.', 'Suppose, for example, one set a fixed quantitative boundary, making the word large true of everything above it, and false of everything below it.', 'Then IA would tend to have little use for this property at all since, presumably, every chihuahua would be small and every alsatian large, making each of the combinations {large, chihuahua} (which denotes the empty set) and {large, alsatian} (the set of all alsatians) useless.', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .""]",0,"['IA Plur deals with vague properties in essentially the same way as FOG: Attributes like size are treated as if they were not context dependent: Their Values always apply to the same objects, regardless of what other properties occur in the description.', 'This approach does not do justice to gradable adjectives, whether they are used in the base form, the superlative, or the comparative.', 'Suppose, for example, one set a fixed quantitative boundary, making the word large true of everything above it, and false of everything below it.', 'Then IA would tend to have little use for this property at all since, presumably, every chihuahua would be small and every alsatian large, making each of the combinations {large, chihuahua} (which denotes the empty set) and {large, alsatian} (the set of all alsatians) useless.', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account ( #AUTHOR_TAG ; see our Section 2 ) .""]"
CC547,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,the wellbuilt clinical question a key to evidencebased decisions,"['W Scott Richardson', 'Mark C Wilson', 'James Nishikawa', 'Robert S Hayward']",,,The following four components have been identified as the key elements of a question related to patient care ( #AUTHOR_TAG ) :,"['The second facet is independent of the clinical task and pertains to the structure of a well-built clinical question.', 'The following four components have been identified as the key elements of a question related to patient care ( #AUTHOR_TAG ) :']",0,"['The second facet is independent of the clinical task and pertains to the structure of a well-built clinical question.', 'The following four components have been identified as the key elements of a question related to patient care ( #AUTHOR_TAG ) :']"
CC548,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,modern information retrieval,"['Ricardo Baeza-Yates', 'Berthier Ribeiro-Neto']",,"Digital technologies can help scholars to navigate the vast quantities of musical data and source materials now available to them, but an imaginative leap is needed in order to conceptualize the kinds of musicological research questions we might ask of electronic corpora. In particular, our data-rich digital world offers enormous potential for the exploration of musical transmission and relatedness.  In this article, we explore the 16th- and 17th-century instrumental battaglia (battle piece), a genre with a very distinctive collective identity arising from the use of numerous shared ingredients (including melodic, motivic, textural, harmonic and rhythmic features). However, a battaglia is not defined by the presence of a core set of essential features, and exact concordance between these pieces is often remarkably low. This kind of musical 'family resemblance' (formulated after Wittgenstein) poses a serious challenge to both traditional musicological apparatus (for example, finding aids such as thematic catalogues) and Music Information Retrieval (which has often privileged melodic similarity at the expense of other kinds of musical relatedness).  This case study provides a stimulus for rethinking the complex nature of musical similarity. In doing this, we outline a set of requirements for digital tools that could support the discovery, exploration and representation of these kinds of relationships",Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved ( #AUTHOR_TAG ).,"['Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved ( #AUTHOR_TAG ).', 'It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall.']",5,"['Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved ( #AUTHOR_TAG ).', 'It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall.']"
CC549,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,formulating the question,['Andrew Booth'],related work,To know something -- as the first part of our inquiry showed -- is to designate facts by means of judgments in such a way as to obtain a unique correlation while using the smallest possible number of concepts.,"Although originally developed as a tool to assist in query formulation , #AUTHOR_TAG pointed out that PICO frames can be employed to structure IR results for improving precision .","['The idea that clinical information systems should be sensitive to the practice of evidence-based medicine is not new.', 'Based on analyses of 4,000 MEDLINE citations, Mendonça and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology.', 'The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges Project (Haynes et al. 1994;Wilczynski, McKibbon, and Haynes 2001).', 'Cimino and Mendonça reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis.', 'Although originally developed as a tool to assist in query formulation , #AUTHOR_TAG pointed out that PICO frames can be employed to structure IR results for improving precision .', 'PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al. 1989).', 'The work of Hearst (1996) demonstrates that faceted queries can be converted into simple filtering constraints to boost precision.']",0,"['Based on analyses of 4,000 MEDLINE citations, Mendonca and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology.', 'Cimino and Mendonca reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis.', 'Although originally developed as a tool to assist in query formulation , #AUTHOR_TAG pointed out that PICO frames can be employed to structure IR results for improving precision .', 'The work of Hearst (1996) demonstrates that faceted queries can be converted into simple filtering constraints to boost precision.']"
CC550,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,a comparative study on feature selection in text categorization,"['Yiming Yang', 'Jan O Pedersen']",,This paper is a comparative study of feature selection methods in statistical learning of text categorization The focus is on aggres sive dimensionality reduction Five meth ods were evaluated including term selection based on document frequency DF informa tion gain IG mutual information MI a test CHI and term strength TS We found IG and CHI most e ective in our ex periments Using IG thresholding with a k nearest neighbor classi er on the Reuters cor pus removal of up to removal of unique terms actually yielded an improved classi cation accuracy measured by average preci sion DF thresholding performed similarly Indeed we found strong correlations between the DF IG and CHI values of a term This suggests that DF thresholding the simplest method with the lowest cost in computation can be reliably used instead of IG or CHI when the computation of these measures are too expensive TS compares favorably with the other methods with up to vocabulary reduction but is not competitive at higher vo cabulary reduction levels In contrast MI had relatively poor performance due to its bias towards favoring rare terms and its sen sitivity to probability estimation errors,"We first identified the most informative unigrams and bigrams using the information gain measure ( #AUTHOR_TAG ) , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) .","['The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features.', 'We first identified the most informative unigrams and bigrams using the information gain measure ( #AUTHOR_TAG ) , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) .', 'Diseasespecific terms, such as rheumatoid arthritis, were then manually removed.', 'Finally, the list of features was revised by the registered nurse who participated in the annotation effort.', 'This classifier also outputs the probability of a class assignment.']",5,"['The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features.', 'We first identified the most informative unigrams and bigrams using the information gain measure ( #AUTHOR_TAG ) , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) .', 'This classifier also outputs the probability of a class assignment.']"
CC551,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,customization in a unified framework for summarizing medical literature,"['Noemie Elhadad', 'Min-Yen Kan', 'Judith Klavans', 'Kathleen McKeown']",related work,"Objective: We present the summarization system in the PErsonalized Retrieval and Summarization of Images, Video and Language (PERSIVAL) medical digital library. Although we discuss the context of our summarization research within the PERSIVAL platform, the primary focus of this article is on strategies to define and generate customized summaries. Methods and material: Our summarizer employs a unified user model to create a tailored summary of relevant documents for either a physician or lay person. The approach takes advantage of regularities in medical literature text structure and content to fulfill identified user needs. Results: The resulting summaries combine both machine-generated text and extracted text that comes from multiple input documents. Customization includes both group-based modeling for two classes of users, physician and lay person, and individually driven models based on a patient record. Conclusions: Our research shows that customization is feasible in a medical digital library","The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ; #AUTHOR_TAG ) .","['In addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs.', ""The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ; #AUTHOR_TAG ) ."", 'Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine.', 'Patient information is no doubt important to answering clinical questions, and our work could certainly benefit from experiences gained in the PERSIVAL project.']",1,"['In addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs.', ""The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ; #AUTHOR_TAG ) .""]"
CC552,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,generative content models for structural analysis of medical abstracts,"['Jimmy Lin', 'Damianos Karakos', 'Dina Demner-Fushman', 'Sanjeev Khudanpur']",related work,"The ability to accurately model the content structure of text is important for many natural language processing applications. This paper describes experiments with generative models for analyzing the discourse structure of medical abstracts, which generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"". We demonstrate that Hidden Markov Models are capable of accurately capturing the structure of such texts, and can achieve classification accuracy comparable to that of discriminative techniques. In addition, generative approaches provide advantages that may make them preferable to discriminative techniques such as Support Vector Machines under certain conditions. Our work makes two contributions: at the application level, we report good performance on an interesting task in an important domain; more generally, our results contribute to an ongoing discussion regarding the tradeoffs between generative and discriminative techniques.","For example , McKnight and Srinivasan ( 2003 ) describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #AUTHOR_TAG ) .","['The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes.', 'For example , McKnight and Srinivasan ( 2003 ) describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #AUTHOR_TAG ) .', 'Tbahriti et al. (2006) have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance.', 'Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering.', 'In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative statements in MEDLINE abstracts, but once again, this work is not directly applicable to clinical question answering.']",0,"['The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes.', 'For example , McKnight and Srinivasan ( 2003 ) describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #AUTHOR_TAG ) .']"
CC553,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,automatically evaluating answers to definition questions,"['Jimmy Lin', 'Dina Demner-Fushman']",,"Following recent developments in the automatic evaluation of machine translation and document summarization, we present a similar approach, implemented in a measure called Pourpre, for automatically evaluating answers to definition questions. Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system's response. The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address with this work. Experiments with the TREC 2003 and TREC 2004 QA tracks indicate that rankings produced by our metric correlate highly with official rankings, and that Pourpre outperforms direct application of existing metrics.","We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( #AUTHOR_TAGb ) , but these features are also beyond the capabilities of current summarization systems .","['The most important characteristic of answers, as recommended by Ely et al. (2005) in their study of real-world physicians, is that they focus on bottom-line clinical advice-information that physicians can directly act on.', 'Ideally, answers should integrate information from multiple clinical studies, pointing out both similarities and differences.', 'The system should collate concurrences, that is, if multiple abstracts arrive at the same conclusion-it need not be repeated unless the physician wishes to ""drill down""; the system should reconcile contradictions, for example, if two abstracts disagree on a particular treatment because they studied different patient populations.', 'We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( #AUTHOR_TAGb ) , but these features are also beyond the capabilities of current summarization systems .']",1,"['We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization ( #AUTHOR_TAGb ) , but these features are also beyond the capabilities of current summarization systems .']"
CC554,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,using argumentation to retrieve articles with similar citations an inquiry into improving related articles search in the medline digital library,"['Imad Tbahriti', 'Christine Chichester', 'Fr´ed´erique Lisacek', 'Patrick Ruch']",related work,"The aim of this study is to investigate the relationships between citations and the scientific argumentation found abstracts. We design a related article search task and observe how the argumentation can affect the search results. We extracted citation lists from a set of 3200 full-text papers originating from a narrow domain. In parallel, we recovered the corresponding MEDLINE records for analysis of the argumentative moves. Our argumentative model is founded on four classes: PURPOSE, METHODS, RESULTS and CONCLUSION. A Bayesian classifier trained on explicitly structured MEDLINE abstracts generates these argumentative categories. The categories are used to generate four different argumentative indexes. A fifth index contains the complete abstract, together with the title and the list of Medical Subject Headings (MeSH) terms. To appraise the relationship of the moves to the citations, the citation lists were used as the criteria for determining relatedness of articles, establishing a benchmark; it means that two articles are considered as ""related"" if they share a significant set of co-citations. Our results show that the average precision of queries with the PURPOSE and CONCLUSION features is the highest, while the precision of the RESULTS and METHODS features was relatively low. A linear weighting combination of the moves is proposed, which significantly improves retrieval of related articles.",#AUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance .,"['The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes.', 'For example, McKnight and Srinivasan (2003) describe a machine learning approach to automatically label sentences as belonging to introduction, methods, results, or conclusion using structured abstracts as training data (see also Lin et al. 2006).', '#AUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance .', 'Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering.', 'In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative statements in MEDLINE abstracts, but once again, this work is not directly applicable to clinical question answering.']",0,"['The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes.', '#AUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance .', 'In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative statements in MEDLINE abstracts, but once again, this work is not directly applicable to clinical question answering.']"
CC555,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,answering clinical questions,"['M Lee Chambliss', 'Jennifer Conley']",introduction,"The combination of recent developments in question-answering research and the availability of unparalleled resources developed specifically for automatic semantic processing of text in the medical domain provides a unique opportunity to explore complex question answering in the domain of clinical medicine. This article presents a system designed to satisfy the information needs of physicians practicing evidence-based medicine. We have developed a series of knowledge extractors, which employ a combination of knowledge-based and statistical techniques, for automatically identifying clinically relevant aspects of MEDLINE abstracts. These extracted elements serve as the input to an algorithm that scores the relevance of citations with respect to structured representations of information needs, in accordance with the principles of evidencebased medicine. Starting with an initial list of citations retrieved by PubMed, our system can bring relevant abstracts into higher ranking positions, and from these abstracts generate responses that directly answer physicians ' questions. We describe three separate evaluations: one focused on the accuracy of the knowledge extractors, one conceptualized as a document reranking task, and finally, an evaluation of answers by two physicians. Experiments on a collection of real-world clinical questions show that our approach significantly outperforms the already competitive PubMed baseline. 1","However , studies have shown that existing systems for searching MEDLINE ( such as PubMed , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ; #AUTHOR_TAG ) .","['Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985;Gorman, Ash, and Wykoff 1994;Ely et al. 1999Ely et al. , 2005.', ""MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians' questions, and is commonly used in that capacity (Cogdill and Moore 1997;De Groote and Dorsch 2003)."", 'However , studies have shown that existing systems for searching MEDLINE ( such as PubMed , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ; #AUTHOR_TAG ) .', 'Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts.', 'Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology-better information systems to provide decision support for physicians have the potential to improve the quality of health care.']",0,"['Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985;Gorman, Ash, and Wykoff 1994;Ely et al. 1999Ely et al. , 2005.', ""MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians' questions, and is commonly used in that capacity (Cogdill and Moore 1997;De Groote and Dorsch 2003)."", 'However , studies have shown that existing systems for searching MEDLINE ( such as PubMed , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ; #AUTHOR_TAG ) .', 'Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology-better information systems to provide decision support for physicians have the potential to improve the quality of health care.']"
CC556,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,the nlm indexing initiative’s medical text indexer,"['Alan R Aronson', 'James G Mork', 'Clifford W Gay', 'Susanne M Humphrey', 'Willie J Rogers']",,,"Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content ( #AUTHOR_TAG ) .","['Additional metadata are associated with each MEDLINE citation.', 'The most important of these is the controlled vocabulary terms assigned by human indexers.', ""NLM's controlled vocabulary thesaurus, Medical Subject Headings (MeSH), 2 contains approximately 23,000 descriptors arranged in a hierarchical structure and more than 151,000 Supplementary Concept Records (additional chemical substance names) within a separate thesaurus."", ""Indexing is performed by approximately 100 indexers with at least bachelor's degrees in life sciences and formal training in indexing provided by NLM."", 'Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content ( #AUTHOR_TAG ) .', 'Nevertheless, the indexing process remains firmly human-centered.']",0,"['Additional metadata are associated with each MEDLINE citation.', 'The most important of these is the controlled vocabulary terms assigned by human indexers.', ""NLM's controlled vocabulary thesaurus, Medical Subject Headings (MeSH), 2 contains approximately 23,000 descriptors arranged in a hierarchical structure and more than 151,000 Supplementary Concept Records (additional chemical substance names) within a separate thesaurus."", ""Indexing is performed by approximately 100 indexers with at least bachelor's degrees in life sciences and formal training in indexing provided by NLM."", 'Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content ( #AUTHOR_TAG ) .', 'Nevertheless, the indexing process remains firmly human-centered.']"
CC557,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,what makes a good answer the role of context in question answering,"['Jimmy Lin', 'Dennis Quan', 'Vineet Sinha', 'Karun Bakshi', 'David Huynh', 'Boris Katz', 'David R Karger']",conclusion,"Question answering systems have proven to be helpful to users because they can provide succinct answers that do not require users to wade through a large number of documents. However, despite recent advances in the underlying question answering technology, the problem of designing effective interfaces has been largely unexplored. We conducted a user study to investigate this area and discovered that, overall, users prefer paragraph-sized chunks of text over just an exact phrase as the answer to their questions. Furthermore, users generally prefer answers embedded in context, regardless of the perceived reliability of the source documents. When researching a topic, increasing the amount of text returned to users significantly decreases the number of queries that they pose to the system, suggesting that users utilize supporting text to answer related questions. We believe that these results can serve to guide future developments in question answering interfaces.","Previously , a user study ( #AUTHOR_TAG ) has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance .","['The design and implementation of our current system leaves many open avenues for future exploration, one of which concerns our assumptions about the query interface.', 'Previously , a user study ( #AUTHOR_TAG ) has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance .', 'We have argued that a query interface based on structured PICO frames will yield better-formulated queries, although it is unclear whether physicians would invest the upfront effort necessary to accomplish this.', 'Issuing extremely short queries appears to be an ingrained habit of information seekers today, and the dominance of World Wide Web searches reinforce this behavior.', 'Given these trends, physicians may actually prefer the rapid back-and-forth interaction style that comes with short queries.', 'We believe that if systems can produce noticeably better results with richer queries, users will make more of an effort to formulate them.', 'This, however, presents a chicken-and-egg problem: One possible solution is to develop models that can automatically fill query frames given a couple of keywords-this would serve to kick-start the query generation process.']",1,"['Previously , a user study ( #AUTHOR_TAG ) has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance .']"
CC558,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,towards a medical questionanswering system a feasibility study,"['Pierre Jacquemart', 'Pierre Zweigenbaum']",related work,,"The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #AUTHOR_TAG , Rinaldi et al. 2004 ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .","['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #AUTHOR_TAG , Rinaldi et al. 2004 ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', 'Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine.']",0,"['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #AUTHOR_TAG , Rinaldi et al. 2004 ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .']"
CC559,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,the interaction of domain knowledge and linguistic structure in natural language processing interpreting hypernymic propositions in biomedical text,"['Thomas C Rindflesch', 'Marcelo Fiszman']",introduction,"Interpretation of semantic propositions in free-text documents such as MEDLINE citations would provide valuable support for biomedical applications, and several approaches to semantic interpretation are being pursued in the biomedical informatics community. In this paper, we describe a methodology for interpreting linguistic structures that encode hypernymic propositions, in which a more specific concept is in a taxonomic relationship with a more general concept. In order to effectively process these constructions, we exploit underspecified syntactic analysis and structured domain knowledge from the Unified Medical Language System (UMLS). After introducing the syntactic processing on which our system depends, we focus on the UMLS knowledge that supports interpretation of hypernymic propositions. We first use semantic groups from the Semantic Network to ensure that the two concepts involved are compatible; hierarchical information in the Metathesaurus then determines which concept is more general and which more specific. A preliminary evaluation of a sample based on the semantic group Chemicals and Drugs provides 83% precision. An error analysis was conducted and potential solutions to the problems encountered are presented. The research discussed here serves as a paradigm for investigating the interaction between domain knowledge and linguistic structure in natural language processing, and could also make a contribution to research on automatic processing of discourse structure. Additional implications of the system we present include its integration in advanced semantic interpretation processors for biomedical text and its use for information extraction in specific domains. The approach has the potential to support a range of applications, including information retrieval and ontology engineering.","Second , software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep ( #AUTHOR_TAG ) extracts relations between the concepts .","['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'This domain is well-suited for exploring the posed research questions for several reasons.', 'First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second , software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep ( #AUTHOR_TAG ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process.', 'The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']",0,"['First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second , software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep ( #AUTHOR_TAG ) extracts relations between the concepts .']"
CC560,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,issues in stacked generalization,"['Kai Ming Ting', 'Ian H Witten']",,"Stacked generalization is a general method of using a high-level model to combine lower-level models to achieve greater predictive accuracy. In this paper we address two crucial issues which have been considered to be a 'black art' in classification tasks ever since the introduction of stacked generalization in 1992 by Wolpert: the type of generalizer that is suitable to derive the higher-level model, and the kind of attributes that should be used as its input. We find that best results are obtained when the higher-level model combines the confidence (and not just the predictions) of the lower-level ones.    We demonstrate the effectiveness of stacked generalization for combining three different types of learning algorithms for classification tasks. We also compare the performance of stacked generalization with majority vote and published results of arcing and bagging.","The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification ( #AUTHOR_TAG ) , which can be described by the following equation:","['We attempted two approaches for assigning these weights. The first method relied on ad hoc weight selection based on intuition.', 'The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification ( #AUTHOR_TAG ) , which can be described by the following equation:']",5,"['The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification ( #AUTHOR_TAG ) , which can be described by the following equation:']"
CC561,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,analysis of semantic classes in medical text for question answering,"['Yun Niu', 'Graeme Hirst']",related work,"To answer questions from clinical-evidence texts, we identify occurrences of the semantic classes -- disease, medication, patient outcome -- that are candidate elements of the answer, and the relations among them. Additionally, we determine whether an outcome is positive or negative.",The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #AUTHOR_TAG .,"['The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #AUTHOR_TAG .', 'Their study also illustrates the importance of semantic classes and relations.', 'However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope).', 'Because secondary sources are often more consistently organized, it is possible to depend on certain surface cues for reliable extraction (which is not possible for MEDLINE abstracts in general).', 'Our study tackles outcome identification in primary medical sources and demonstrates that respectable performance is possible with a feature-combination approach.']",1,"['The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #AUTHOR_TAG .', 'Their study also illustrates the importance of semantic classes and relations.', 'Because secondary sources are often more consistently organized, it is possible to depend on certain surface cues for reliable extraction (which is not possible for MEDLINE abstracts in general).']"
CC562,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,effective mapping of biomedical text to the umls metathesaurus the metamap program,['Alan R Aronson'],introduction,"The UMLS (r) Metathesaurus(r), the largest thesaurus in the biomedical domain, provides a representation of biomedical knowledge consisting of concepts classi-fied by semantic type and both hierarchical and non-hierarchical relationships among the concepts. This knowledge has proved useful for many applications including decision support systems, management of patient records, information retrieval (IR) and data mining. Gaining effective access to the knowledge is critical to the success of these applications. This paper describes MetaMap, a program developed at the National Library of Medicine (NLM) to map bio-medical text to the Metathesaurus or, equivalently, to discover Metathesaurus concepts referred to in text. MetaMap uses a knowledge intensive approach based on symbolic, natural language processing (NLP) and computational linguistic techniques. Besides being applied for both IR and data mining applications, MetaMap is one of the foundations of NLM's Indexing Initiative System which is being applied to both semi-automatic and fully automatic indexing of the biomed-ical literature at the library","Second , software for utilizing this ontology already exists : MetaMap ( #AUTHOR_TAG ) identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts .","['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'This domain is well-suited for exploring the posed research questions for several reasons.', 'First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second , software for utilizing this ontology already exists : MetaMap ( #AUTHOR_TAG ) identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process.', 'The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']",0,"['First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second , software for utilizing this ontology already exists : MetaMap ( #AUTHOR_TAG ) identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.']"
CC563,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,improving fulltext precision on short queries using simple constraints,['Marti A Hearst'],related work,"We show that two simple constraints, when applied to short user queries (on the order of 5{10 words) can yield precision scores comparable to or better than those achieved using long queries (50{85 words) at low document cuto levels. These constraints are meant to detect documents that have subtopic passages that includes the most important components of the query. The constraints are: (i) a simple Boolean constraint which requires the user to specify the query as a list of topics; this list is converted into a conjunct of disjuncts by the system, and (ii) a subtopic-sized proximity constraint imposed over the Boolean constraint. The vector space model is used to rank the documents that satisfy both constraints. Experiments run over 45 TREC queries show signi cant, almost consistent improvements over rankings that use no constraints. These results have important rami cations for interactive systems intended for casual users, such as those searching on the World Wide Web.",The work of #AUTHOR_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision .,"['The idea that clinical information systems should be sensitive to the practice of evidence-based medicine is not new.', 'Based on analyses of 4,000 MEDLINE citations, Mendonça and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology.', 'The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges Project (Haynes et al. 1994;Wilczynski, McKibbon, and Haynes 2001).', 'Cimino and Mendonça reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis.', 'Although originally developed as a tool to assist in query formulation, Booth (2000) pointed out that PICO frames can be employed to structure IR results for improving precision.', 'PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al. 1989).', 'The work of #AUTHOR_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision .']",0,['The work of #AUTHOR_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision .']
CC564,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,automatically identifying health outcome information in medline records,"['Dina Demner-Fushman', 'Barbara Few', 'Susan E Hauser', 'George Thoma']",,"Objective: Understanding the effect of a given intervention on the patient's health outcome is one of the key elements in providing optimal patient care. This study presents a methodology for automatic identification of outcomes-related information in medical text and evaluates its potential in satisfying clinical information needs related to health care outcomes. Design: An annotation scheme based on an evidence-based medicine model for critical appraisal of evidence was developed and used to annotate 633 MEDLINE citations. Textual, structural, and meta-information features essential to outcome identification were learned from the created collection and used to develop an automatic system. Accuracy of automatic outcome identification was assessed in an intrinsic evaluation and in an extrinsic evaluation, in which ranking of MEDLINE search results obtained using PubMed Clinical Queries relied on identified outcome statements. Measurements: The accuracy and positive predictive value of outcome identification were calculated. Effectiveness of the outcome-based ranking was measured using mean average precision and precision at rank 10. Results: Automatic outcome identification achieved 88% to 93% accuracy. The positive predictive value of individual sentences identified as outcomes ranged from 30% to 37%. Outcome-based ranking improved retrieval accuracy, tripling mean average precision and achieving 389% improvement in precision at rank 10. Conclusion: Preliminary results in outcome-based document ranking show potential validity of the evidence-based medicine-model approach in timely delivery of information critical to clinical decision support at the point of service","The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine ( #AUTHOR_TAG ) .","['The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine ( #AUTHOR_TAG ) .', 'As will be described herein, the population, problem, and the intervention extractors are based largely on recognition of semantic types and a few manually constructed rules; the outcome extrac-tor, in contrast, is implemented as an ensemble of classifiers trained using supervised machine learning techniques ).', 'These two very different approaches can be attributed to differences in the nature of the frame elements: Whereas problems and interventions can be directly mapped to UMLS concepts, and populations easily mapped to patterns that include UMLS concepts, outcome statements follow no predictable pattern.', 'The initial goal of the annotation effort was to identify outcome statements in abstract text.', 'A physician, two registered nurses, and an engineering researcher manually identified sentences that describe outcomes in 633 MEDLINE abstracts; a post hoc analysis demonstrates good agreement (κ = 0.77).', 'The annotated abstracts were retrieved using PubMed and attempted to model different user behaviors ranging from naive to expert (where advanced search features were employed).', 'With the exception of 50 citations retrieved to answer a question about childhood immunization, the rest of the results were retrieved by querying on a disease, for example, diabetes.', 'Of the 633 citations, 100 abstracts were also fully annotated with population, problems, and interventions.', 'These 100 abstracts were set aside as a held-out test set.', 'Of the remaining citations, 275 were used for training and rule derivation, as described in the following sections.']",5,"['The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine ( #AUTHOR_TAG ) .']"
CC565,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,analysis of questions asked by family doctors regarding patient care,"['John W Ely', 'Jerome A Osheroff', 'Mark H Ebell', 'George R Bergus', 'Barcey T Levy', 'M Lee Chambliss', 'Eric R Evans']",introduction,"Abstract Objectives: To characterise the information needs of family doctors by collecting the questions they asked about patient care during consultations and to classify these in ways that would be useful to developers of knowledge bases. Design: Observational study in which investigators visited doctors for two half days and collected their questions. Taxonomies were developed to characterise the clinical topic and generic type of information sought for each question. Setting: Eastern Iowa. Participants: Random sample of 103 family doctors. Main outcome measures: Number of questions posed, pursued, and answered; topic and generic type of information sought for each question; time spent pursuing answers; information resources used. Results: Participants asked a total of 1101 questions. Questions about drug prescribing, obstetrics and gynaecology, and adult infectious disease were most common and comprised 36% of all questions. The taxonomy of generic questions included 69 categories; the three most common types, comprising 24% of all questions, were ""What is the cause of symptom X?"" ""What is the dose of drug X?"" and ""How should I manage disease or finding X?"" Answers to most questions (702, 64%) were not immediately pursued, but, of those pursued, most (318, 80%) were answered Doctors spent an average of less than 2 minutes pursuing an answer, and they used readily available print and human resources Only two questions led to a formal literature search. Conclusions: Family doctors in this study did not pursue answers to most of their questions. Questions about patient care can be organised into a limited number of generic types, which could help guide the efforts of knowledge base developers. Key messages Questions that doctors have about the care of their patients could help guide the content of medical information sources and medical training In this study of US family doctors, participants frequently had questions about patient care but did not pursue answers to most questions (64%) On average, participants spent less than 2 minutes seeking an answer to a question The most common resources used to answer questions included textbooks and colleagues; formal literature searches were rarely performed The most common generic questions were ""What is the cause of symptom X?"" ""What is the dose of drug X?"" and ""How should I manage disease or finding X?""","Furthermore , the need to answer questions related to patient care at the point of service has been well studied and documented ( Covell , Uman , and Manning 1985 ; Gorman , Ash , and Wykoff 1994 ; #AUTHOR_TAG , 2005 ) .","['Furthermore , the need to answer questions related to patient care at the point of service has been well studied and documented ( Covell , Uman , and Manning 1985 ; Gorman , Ash , and Wykoff 1994 ; #AUTHOR_TAG , 2005 ) .', ""MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answering physicians' questions, and is commonly used in that capacity (Cogdill and Moore 1997;De Groote and Dorsch 2003)."", 'However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994;Chambliss and Conley 1996).', 'Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts.', 'Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology-better information systems to provide decision support for physicians have the potential to improve the quality of health care.']",0,"['Furthermore , the need to answer questions related to patient care at the point of service has been well studied and documented ( Covell , Uman , and Manning 1985 ; Gorman , Ash , and Wykoff 1994 ; #AUTHOR_TAG , 2005 ) .']"
CC566,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,answer extraction semantic clustering and extractive summarization for clinical question answering,"['Dina Demner-Fushman', 'Jimmy Lin']",,"This paper presents a hybrid approach to question answering in the clinical domain that combines techniques from summarization and information retrieval. We tackle a frequently-occurring class of questions that takes the form ""What is the best drug treatment for X?"" Starting from an initial set of MEDLINE citations, our system first identifies the drugs under study. Abstracts are then clustered using semantic classes from the UMLS ontology. Finally, a short extractive summary is generated for each abstract to populate the clusters. Two evaluations---a manual one focused on short answers and an automatic one focused on the supporting abstract---demonstrate that our system compares favorably to PubMed, the search system most widely used by physicians today.","We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #AUTHOR_TAG and Lin (2006).","['Finally, answer generation remains an area that awaits further exploration, although we would have to first define what a good answer should be.', 'We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #AUTHOR_TAG and Lin (2006).', 'Furthermore, the current answer generator does not handle complex issues such as contradictory and inconsistent statements.', 'To address these very difficult challenges, finer-grained semantic analysis of medical texts is required.']",0,"['We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #AUTHOR_TAG and Lin (2006).', 'Furthermore, the current answer generator does not handle complex issues such as contradictory and inconsistent statements.', 'To address these very difficult challenges, finer-grained semantic analysis of medical texts is required.']"
CC567,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,automatically identifying health outcome information in medline records,"['Dina Demner-Fushman', 'Barbara Few', 'Susan E Hauser', 'George Thoma']",,"Objective: Understanding the effect of a given intervention on the patient's health outcome is one of the key elements in providing optimal patient care. This study presents a methodology for automatic identification of outcomes-related information in medical text and evaluates its potential in satisfying clinical information needs related to health care outcomes. Design: An annotation scheme based on an evidence-based medicine model for critical appraisal of evidence was developed and used to annotate 633 MEDLINE citations. Textual, structural, and meta-information features essential to outcome identification were learned from the created collection and used to develop an automatic system. Accuracy of automatic outcome identification was assessed in an intrinsic evaluation and in an extrinsic evaluation, in which ranking of MEDLINE search results obtained using PubMed Clinical Queries relied on identified outcome statements. Measurements: The accuracy and positive predictive value of outcome identification were calculated. Effectiveness of the outcome-based ranking was measured using mean average precision and precision at rank 10. Results: Automatic outcome identification achieved 88% to 93% accuracy. The positive predictive value of individual sentences identified as outcomes ranged from 30% to 37%. Outcome-based ranking improved retrieval accuracy, tripling mean average precision and achieving 389% improvement in precision at rank 10. Conclusion: Preliminary results in outcome-based document ranking show potential validity of the evidence-based medicine-model approach in timely delivery of information critical to clinical decision support at the point of service","After much exploration , #AUTHOR_TAG discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues .","['After much exploration , #AUTHOR_TAG discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues .', 'Consider the following segment:']",0,"['After much exploration , #AUTHOR_TAG discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues .']"
CC568,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,feature selection for unbalanced class distribution and naive bayes,"['Dunja Mladenic', 'Marko Grobelnik']",,"This paper describes an approach to feature subset selection that takes into account problem speciics and learning algorithm characteristics. It is developed for the Naive Bayesian classiier applied on text data, since it combines well with the addressed learning problems. We focus on domains with many features that also have a highly unbalanced class distribution and asymmetric misclassii-cation costs given only implicitly in the problem. By asymmetric misclassiication costs we mean that one of the class values is the target class value for which we want to get predictions and we prefer false positive over false negative. Our example problem is automatic document categorization using machine learning, where we want to identify documents relevant for the selected category. Usually, only about 1%-10% of examples belong to the selected category. Our experimental comparison of eleven feature scoring measures show that considering domain and algorithm characteristics signiicantly improves the results of classiication.","We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio ( #AUTHOR_TAG ) .","['The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features.', 'We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio ( #AUTHOR_TAG ) .', 'Diseasespecific terms, such as rheumatoid arthritis, were then manually removed.', 'Finally, the list of features was revised by the registered nurse who participated in the annotation effort.', 'This classifier also outputs the probability of a class assignment.']",5,"['The n-gram based classifier is also a naive Bayes classifier, but it operates on a different set of features.', 'We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio ( #AUTHOR_TAG ) .', 'This classifier also outputs the probability of a class assignment.']"
CC569,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,answer extraction semantic clustering and extractive summarization for clinical question answering,"['Dina Demner-Fushman', 'Jimmy Lin']",,"This paper presents a hybrid approach to question answering in the clinical domain that combines techniques from summarization and information retrieval. We tackle a frequently-occurring class of questions that takes the form ""What is the best drug treatment for X?"" Starting from an initial set of MEDLINE citations, our system first identifies the drugs under study. Abstracts are then clustered using semantic classes from the UMLS ontology. Finally, a short extractive summary is generated for each abstract to populate the clusters. Two evaluations---a manual one focused on short answers and an automatic one focused on the supporting abstract---demonstrate that our system compares favorably to PubMed, the search system most widely used by physicians today.","Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , #AUTHOR_TAG .","['It is clear that the type of answers desired by physicians require a level of semantic analysis that is beyond the current state of the art, even with the aid of existing medical ontologies.', 'For example, even the seemingly straightforward task of identifying similarities and differences in outcome statements is rendered exceedingly complex by the tremendous amount of background medical knowledge that must be brought to bear in interpreting clinical results and subtle differences in study design, objectives, and results; the closest analogous task in computational linguistics-redundancy detection for multi-document summarization-seems easy by comparison.', 'Furthermore, it is unclear if textual strings make ""good answers.""', 'Perhaps a graphical rendering of the semantic predicates present in relevant abstracts might more effectively convey the desired information; see, for example, Fiszman, Rindflesch, and Kilicoglu (2004).', 'Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , #AUTHOR_TAG .']",3,"['It is clear that the type of answers desired by physicians require a level of semantic analysis that is beyond the current state of the art, even with the aid of existing medical ontologies.', 'For example, even the seemingly straightforward task of identifying similarities and differences in outcome statements is rendered exceedingly complex by the tremendous amount of background medical knowledge that must be brought to bear in interpreting clinical results and subtle differences in study design, objectives, and results; the closest analogous task in computational linguistics-redundancy detection for multi-document summarization-seems easy by comparison.', 'Furthermore, it is unclear if textual strings make ""good answers.""', 'Perhaps a graphical rendering of the semantic predicates present in relevant abstracts might more effectively convey the desired information; see, for example, Fiszman, Rindflesch, and Kilicoglu (2004).', 'Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , #AUTHOR_TAG .']"
CC570,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,overview of the trec,['Ellen M Voorhees'],related work,,"Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions ( #AUTHOR_TAG ) .","['Finally, the evaluation of answers to complex questions remains an open research problem.', 'Although it is clear that measures designed for open-domain factoid questions are not appropriate, the community has not agreed on a methodology that will allow meaningful comparisons of results from different systems.', 'In Sections 9 and 10, we have discussed many of these issues.', 'Recently, there is a growing consensus that an evaluation methodology based on the notion of ""information nuggets"" may provide an appropriate framework for assessing the quality of answers to complex questions.', ""Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions ( #AUTHOR_TAG ) ."", 'A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed Demner-Fushman 2005a, 2006b).', 'However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken.']",0,"['Finally, the evaluation of answers to complex questions remains an open research problem.', 'In Sections 9 and 10, we have discussed many of these issues.', 'Recently, there is a growing consensus that an evaluation methodology based on the notion of ""information nuggets"" may provide an appropriate framework for assessing the quality of answers to complex questions.', ""Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions ( #AUTHOR_TAG ) ."", 'A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcomings of the original nugget scoring model, although a number of these issues have been recently addressed Demner-Fushman 2005a, 2006b).']"
CC571,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,the role of knowledge in conceptual retrieval a study in the domain of clinical medicine,"['Jimmy Lin', 'Dina Demner-Fushman']",related work,"Despite its intuitive appeal, the hypothesis that retrieval at the level of ""concepts"" should outperform purely term-based approaches remains unverified empirically. In addition, the use of ""knowledge"" has not consistently resulted in performance gains. After identifying possible reasons for previous negative results, we present a novel framework for ""conceptual retrieval"" that articulates the types of knowledge that are important for information seeking. We instantiate this general framework in the domain of clinical medicine based on the principles of evidence-based medicine (EBM). Experiments show that an EBM-based scoring algorithm dramatically outperforms a state-of-the-art baseline that employs only term statistics. Ablation studies further yield a better understanding of the performance contributions of different components. Finally, we discuss how other domains can benefit from knowledge-based approaches.","In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #AUTHOR_TAGa ) for a brief overview .","['Clinical question answering is an emerging area of research that has only recently begun to receive serious attention.', 'As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated.', 'In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems.', 'For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (Voorhees and Tice 1999).', 'In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #AUTHOR_TAGa ) for a brief overview .']",0,"['In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems.', 'In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #AUTHOR_TAGa ) for a brief overview .']"
CC572,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,automatically evaluating answers to definition questions,"['Jimmy Lin', 'Dina Demner-Fushman']",related work,"Following recent developments in the automatic evaluation of machine translation and document summarization, we present a similar approach, implemented in a measure called Pourpre, for automatically evaluating answers to definition questions. Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system's response. The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address with this work. Experiments with the TREC 2003 and TREC 2004 QA tracks indicate that rankings produced by our metric correlate highly with official rankings, and that Pourpre outperforms direct application of existing metrics.","A number of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed ( #AUTHOR_TAGa , 2006b ) .","['Finally, the evaluation of answers to complex questions remains an open research problem.', 'Although it is clear that measures designed for open-domain factoid questions are not appropriate, the community has not agreed on a methodology that will allow meaningful comparisons of results from different systems.', 'In Sections 9 and 10, we have discussed many of these issues.', 'Recently, there is a growing consensus that an evaluation methodology based on the notion of ""information nuggets"" may provide an appropriate framework for assessing the quality of answers to complex questions.', 'Nugget F-score has been employed as a metric in the TREC question-answering track since 2003, to evaluate so-called definition and ""other"" questions (Voorhees 2003).', 'A number of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed ( #AUTHOR_TAGa , 2006b ) .', 'However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken.']",0,"['Finally, the evaluation of answers to complex questions remains an open research problem.', 'Although it is clear that measures designed for open-domain factoid questions are not appropriate, the community has not agreed on a methodology that will allow meaningful comparisons of results from different systems.', 'In Sections 9 and 10, we have discussed many of these issues.', 'Recently, there is a growing consensus that an evaluation methodology based on the notion of ""information nuggets"" may provide an appropriate framework for assessing the quality of answers to complex questions.', 'Nugget F-score has been employed as a metric in the TREC question-answering track since 2003, to evaluate so-called definition and ""other"" questions (Voorhees 2003).', 'A number of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed ( #AUTHOR_TAGa , 2006b ) .']"
CC573,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,strength of recommendation taxonomy sort a patientcentered approach to grading evidence in the medical literature,"['Mark H Ebell', 'Jay Siwek', 'Barry D Weiss', 'Steven H Woolf', 'Jeffrey Susman', 'Bernard Ewigman', 'Marjorie Bowman']",,"A large number of taxonomies are used to rate the quality of an individual study and the strength of a recommendation based on a body of evidence. We have developed a new grading scale that will be used by several family medicine and primary care journals (required or optional), with the goal of allowing readers to learn one taxonomy that will apply to many sources of evidence. Our scale is called the Strength of Recommendation Taxonomy. It addresses the quality, quantity, and consistency of evidence and allows authors to rate individual studies or bodies of evidence. The taxonomy is built around the information mastery framework, which emphasizes the use of patient-oriented outcomes that measure changes in morbidity or mortality. An A-level recommendation is based on consistent and good-quality patient-oriented evidence; a B-level recommendation is based on inconsistent or limited-quality patient-oriented evidence; and a C-level recommendation is based on consensus, usual practice, opinion, disease-oriented evidence, or case series for studies of diagnosis, treatment, prevention, or screening. Levels of evidence from 1 to 3 for individual studies also are defined. We hope that consistent use of this taxonomy will improve the ability of authors and readers to communicate about the translation of research into practice.",Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy ( #AUTHOR_TAG ) .,"['The potential highest level of the strength of evidence for a given citation can be identified using the Publication Type (a metadata field) and MeSH terms pertaining to the type of the clinical study.', 'Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy ( #AUTHOR_TAG ) .']",5,['Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy ( #AUTHOR_TAG ) .']
CC574,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,firstyear medical students’ information needs and resource selection responses to a clinical scenario,"['Keith W Cogdill', 'Margaret E Moore']",introduction,"Etude ayant pour but une meilleure comprehension des besoins en information des etudiants en medecine de premiere annee, et de leurs perceptions des ressources appropriees( ressources telles que livres , MEDLINE ... )","MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity ( #AUTHOR_TAG ; De Groote and Dorsch 2003 ) .","['Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985;Gorman, Ash, and Wykoff 1994;Ely et al. 1999Ely et al. , 2005.', ""MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity ( #AUTHOR_TAG ; De Groote and Dorsch 2003 ) ."", 'However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994;Chambliss and Conley 1996).', 'Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts.', 'Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology-better information systems to provide decision support for physicians have the potential to improve the quality of health care.']",0,"['Furthermore, the need to answer questions related to patient care at the point of service has been well studied and documented (Covell, Uman, and Manning 1985;Gorman, Ash, and Wykoff 1994;Ely et al. 1999Ely et al. , 2005.', ""MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity ( #AUTHOR_TAG ; De Groote and Dorsch 2003 ) ."", 'However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994;Chambliss and Conley 1996).', 'Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of language processing and information retrieval technology-better information systems to provide decision support for physicians have the potential to improve the quality of health care.']"
CC575,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,the trec8 question answering track evaluation,"['Ellen M Voorhees', 'Dawn M Tice']",related work,,"For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start ( #AUTHOR_TAG ) .","['Clinical question answering is an emerging area of research that has only recently begun to receive serious attention.', 'As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated.', 'In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems.', 'For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start ( #AUTHOR_TAG ) .', 'In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for a brief overview.']",0,"['For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start ( #AUTHOR_TAG ) .']"
CC576,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,evidencebased medicine how to practice and teach ebm second edition churchill livingstone,"['David L Sackett', 'Sharon E Straus', 'W Scott Richardson', 'William Rosenberg', 'R Brian Haynes']",introduction,,"Third , the paradigm of evidence-based medicine ( #AUTHOR_TAG ) provides a task-based model of the clinical information-seeking process .","['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'This domain is well-suited for exploring the posed research questions for several reasons.', 'First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second, software for utilizing this ontology already exists: MetaMap (Aronson 2001) identifies concepts in free text, and SemRep (Rindflesch and Fiszman 2003) extracts relations between the concepts.', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third , the paradigm of evidence-based medicine ( #AUTHOR_TAG ) provides a task-based model of the clinical information-seeking process .', 'The PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system.', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']",0,"['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'Third , the paradigm of evidence-based medicine ( #AUTHOR_TAG ) provides a task-based model of the clinical information-seeking process .']"
CC577,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,natural language question answering the view from here,"['Lynette Hirschman', 'Robert Gaizauskas']",experiments,"As users struggle to navigate the wealth of on-line information now available, the need for automated question answering systems becomes more urgent. We need systems that allow a user to ask a question in everyday language and receive an answer quickly and succinctly, with sufficient context to validate the answer. Current search engines can return ranked lists of documents, but they do not deliver answers to the user. Question answering systems address this problem. Recent successes have been reported in a series of question-answering evaluations that started in 1999 as part of the Text Retrieval Conference (TREC). The best systems are now able to answer more than two thirds of factual questions in this evaluation.","As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ; #AUTHOR_TAG ) .","['Ideally, we would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors).', 'However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results.', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ; #AUTHOR_TAG ) .']",1,"['As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ; #AUTHOR_TAG ) .']"
CC578,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,the trec8 question answering track evaluation,"['Ellen M Voorhees', 'Dawn M Tice']",experiments,,"As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #AUTHOR_TAG ; Hirschman and Gaizauskas 2001 ) .","['Ideally, we would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors).', 'However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results.', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #AUTHOR_TAG ; Hirschman and Gaizauskas 2001 ) .']",1,"['As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( #AUTHOR_TAG ; Hirschman and Gaizauskas 2001 ) .']"
CC579,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,semantic characteristics of medline citations useful for therapeutic decisionmaking,"['Charles Sneiderman', 'Dina Demner-Fushman', 'Marcelo Fiszman', 'Thomas C Rindflesch']",,MEDLINE retrieval using several information retrieval algorithms was characterized for relevance to point-of-care therapeutic decisions for a sample of clinical queries in family practice. Evaluation methodology is described and preliminary results are presented.,"For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #AUTHOR_TAG .","['Although our problem extractor returns a list of clinical problems, we only evaluate performance on identification of the primary problem.', ""For some abstracts, MeSH headings can be used as ground truth, because one of the human indexers' tasks in assigning terms is to identify the main topic of the article (sometimes a disorder)."", 'For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #AUTHOR_TAG .']",5,"['For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #AUTHOR_TAG .']"
CC580,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,answering questions in the genomics domain,"['Fabio Rinaldi', 'James Dowdall', 'Gerold Schneider', 'Andreas Persidis']",related work,"In this paper we describe current efforts aimed at adapting an existing Question Answering system to a new document set, namely research papers in the genomics domain. The system has been originally developed for another restricted domain, however it has already proved its portability. Nevertheless, the process is not painless, and the specific purpose of  this paper is to describe the problems encountered.","The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .","['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', 'Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine.']",0,"['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #AUTHOR_TAG ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .']"
CC581,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,effective mapping of biomedical text to the umls metathesaurus the metamap program,['Alan R Aronson'],,"The UMLS (r) Metathesaurus(r), the largest thesaurus in the biomedical domain, provides a representation of biomedical knowledge consisting of concepts classi-fied by semantic type and both hierarchical and non-hierarchical relationships among the concepts. This knowledge has proved useful for many applications including decision support systems, management of patient records, information retrieval (IR) and data mining. Gaining effective access to the knowledge is critical to the success of these applications. This paper describes MetaMap, a program developed at the National Library of Medicine (NLM) to map bio-medical text to the Metathesaurus or, equivalently, to discover Metathesaurus concepts referred to in text. MetaMap uses a knowledge intensive approach based on symbolic, natural language processing (NLP) and computational linguistic techniques. Besides being applied for both IR and data mining applications, MetaMap is one of the foundations of NLM's Indexing Initiative System which is being applied to both semi-automatic and fully automatic indexing of the biomed-ical literature at the library","Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .","['Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', 'Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture higher-level generalizations about entities (e.g., CHEMICALS & DRUGS).', 'An additional feature we take advantage of (when present) is explicit section markers present in some abstracts.', 'These so-called structured abstracts were recommended by the Ad Hoc Working Group for Critical Appraisal of the Medical Literature (1987) to help humans assess the reliability and content of a publication and to facilitate the indexing and retrieval processes.', 'These abstracts loosely adhere to the introduction, methods, results, and conclusions format common in scientific writing, and delineate a study using explicitly marked sections with variations of the above headings.', 'Although many core clinical journals require structured abstracts, there is a great deal of variation in the actual headings.', 'Even when present, the headings are not organized in a manner focused on patient care.', 'In addition, abstracts of much high-quality work remain unstructured.', 'For these reasons, explicit section markers are not entirely reliable indicators for the various semantic elements we seek to extract, but must be considered along with other sources of evidence.']",5,"['Our knowledge extractors rely extensively on MetaMap ( #AUTHOR_TAG ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .']"
CC582,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,knowledge extraction for clinical question answering preliminary results,"['Dina Demner-Fushman', 'Jimmy Lin']",,"The combination of recent developments in question an-swering research and the unparalleled resources devel-oped specifically for automatic semantic processing of text in the medical domain provides a unique opportu-nity to explore complex question answering in the clin-ical domain. In this paper, we attempt to operationalize major aspects of evidence-based medicine in the form of knowledge extractors that serve as the fundamental building blocks of a clinical question answering sys-tem. Our evaluations demonstrate that domain-specific knowledge can be effectively leveraged to extract PICO frame elements from MEDLINE abstracts. Clinical in-formation systems in support of physicians ' decision-making process have the potential to improve the qual-ity of patient care in real-world settings","This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .","['The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an example of a completely annotated abstract, see Figure 2.', 'Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases.']",2,"['The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in #AUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an example of a completely annotated abstract, see Figure 2.', 'Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases.']"
CC583,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,the nlm indexing initiative’s medical text indexer,"['Alan R Aronson', 'James G Mork', 'Clifford W Gay', 'Susanne M Humphrey', 'Willie J Rogers']",,,"Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .","['The function α(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task.', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .']",3,"['Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( #AUTHOR_TAG ) .']"
CC584,J07-1005,Answering Clinical Questions with Knowledge-Based and Statistical Techniques,the wellbuilt clinical question a key to evidencebased decisions,"['W Scott Richardson', 'Mark C Wilson', 'James Nishikawa', 'Robert S Hayward']",introduction,,The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .,"['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'This domain is well-suited for exploring the posed research questions for several reasons.', 'First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993).', 'Second, software for utilizing this ontology already exists: MetaMap (Aronson 2001) identifies concepts in free text, and SemRep (Rindflesch and Fiszman 2003) extracts relations between the concepts.', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', 'The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and million concept names from more than 100 controlled vocabularies.', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process.', 'The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']",0,"['We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings.', 'Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process.', 'The PICO framework ( #AUTHOR_TAG ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', 'The confluence of these many factors makes clinical question answering a very exciting area of research.']"
CC585,J08-1003,Wide-Coverage Deep Statistical Parsing Using Automatic Dependency Structure Annotation,speed and accuracy in shallow and deep stochastic parsing,"['Ron Kaplan', 'Stefan Riezler', 'Tracy Holloway King', 'John T Maxwell', 'Alexander Vasserman', 'Richard Crouch']",introduction,"Abstract : This paper reports some experiments that Compare the accuracy and performance of two stochastic parsing systems. The currently popular Collins parser is a shallow parser whose output contains more detailed semantically relevant information than other such parsers. The XLE parser is a deep-parsing system that couples a Lexical Functional Grammar to a log- linear disambiguation component and provides much richer representations theory. We measured the accuracy of both systems against a gold standard of the PARC 700 dependency bank, and also measured their processing times. We found the deep-parsing system to be more accurate than the Collins parser with only a slight reduction in parsing speed.","Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; Preiss 2003 ; #AUTHOR_TAG ; Miyao and Tsujii 2004 ) .","['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; Preiss 2003 ; #AUTHOR_TAG ; Miyao and Tsujii 2004 ) .', 'Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.']",4,"['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; Preiss 2003 ; #AUTHOR_TAG ; Miyao and Tsujii 2004 ) .']"
CC586,J08-1003,Wide-Coverage Deep Statistical Parsing Using Automatic Dependency Structure Annotation,the parc 700 dependency bank,"['Tracy Holloway King', 'Richard Crouch', 'Stefan Riezler', 'Mary Dalrymple', 'Ron Kaplan']",introduction,"An automatic method for annotating the Penn-II Treebank (Marcus et al., 1994) with high-level Lexical Functional Grammar (Kaplan and Bresnan, 1982; Bresnan, 2001; Dalrymple, 2001) f-structure representations is described in (Cahill et al., 2002; Cahill et al., 2004a; Cahill et al., 2004b; O'Donovan et al., 2004). The annotation algorithm and the automatically-generated f-structures are the basis for the automatic acquisition of wide-coverage and robust probabilistic approximations of LFG grammars (Cahill et al., 2002; Cahill et al., 2004a) and for the induction of LFG semantic forms (O'Donovan et al., 2004). The quality of the annotation algorithm and the f-structures it generates is, therefore, extremely important. To date, annotation quality has been measured in terms of precision and recall against the DCU 105. The annotation algorithm currently achieves an f-score of 96.57% for complete f-structures and 94.3% for preds-only f-structures. There are a number of problems with evaluating against a gold standard of this size, most notably that of overfitting. There is a risk of assuming that the gold standard is a complete and balanced representation of the linguistic phenomena in a language and basing design decisions on this. It is, therefore, preferable to evaluate against a more extensive, external standard. Although the DCU 105 is publicly available, 1 a larger well-established external standard can provide a more widely-recognised benchmark against which the quality of the f-structure annotation algorithm can be evaluated. For these reasons, we present an evaluation of the f-structure annotation algorithm of (Cahill et al., 2002; Cahill et al., 2004a; Cahill et al., 2004b; O'Donovan et al., 2004) against the PARC 700 Dependency Bank (King et al., 2003). Evaluation against an external gold standard is a non-trivial task as linguistic analyses may differ systematically between the gold standard and the output to be evaluated as regards feature geometry and nomenclature. We present conversion software to automatically account for many (but not all) of the systematic differences. Currently, we achieve an f-score of 87.31% for the f-structures generated from the original Penn-II trees and an f-score of 81.79% for f-structures from parse trees produced by Charniak's (2000) parser in our pipeline parsing architecture against the PARC 700","Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; #AUTHOR_TAG ; Preiss 2003 ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .","['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; #AUTHOR_TAG ; Preiss 2003 ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .', 'Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.']",4,"['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; #AUTHOR_TAG ; Preiss 2003 ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .', 'Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.']"
CC587,J08-1003,Wide-Coverage Deep Statistical Parsing Using Automatic Dependency Structure Annotation,using grammatical relations to compare parsers,['Judita Preiss'],introduction,"We use the grammatical relations (GRs) described in Carroll et al. (1998) to compare a number of parsing algorithms. A first ranking of the parsers is provided by comparing the extracted GRs to a gold standard GR annotation of 500 Susanne sentences: this required an implementation of GR extraction software for Penn Treebank style parsers. In addition, we perform an experiment using the extracted GRs as input to the Lappin and Leass (1994) anaphora resolution algorithm. This produces a second ranking of the parsers, and we investigate the number of errors that are caused by the incorrect 'GRs.","Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; #AUTHOR_TAG ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .","['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; #AUTHOR_TAG ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .', 'Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.']",4,"['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; #AUTHOR_TAG ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .', 'Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.']"
CC588,J08-2002,A Global Joint Model for Semantic Role Labeling,discriminative reranking for natural language parsing,['Michael Collins'],introduction,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","Our re-ranking approach , like the approach to parse re-ranking of #AUTHOR_TAG , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes .","['Such a rich graphical model can represent many dependencies but there are two dangers-one is that the computational complexity of training the model and searching for the most likely labeling given the tree can be prohibitive, and the other is that if too many dependencies are encoded, the model will over-fit the training data and will not generalize well.', 'We propose a model which circumvents these two dangers and achieves significant performance gains over a similar local model that does not add any dependency arcs among the random variables.', 'To tackle the efficiency problem, we adopt dynamic programming and re-ranking algorithms.', 'To avoid overfitting we encode only a small set of linguistically motivated dependencies in features over sets of the random variables.', 'Our re-ranking approach , like the approach to parse re-ranking of #AUTHOR_TAG , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes .', 'The joint model is restricted to these n assignments and does not have to search the exponentially large space of all possible joint labelings.']",1,"['Our re-ranking approach , like the approach to parse re-ranking of #AUTHOR_TAG , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes .']"
CC589,J08-2002,A Global Joint Model for Semantic Role Labeling,discriminative reranking for natural language parsing,['Michael Collins'],,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #AUTHOR_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions .","['Motivation for Re-Ranking.', 'For argument identification, the number of possible assignments for a parse tree with n nodes is 2 n .', 'This number can run into the hundreds of billions for a normal-sized tree.', 'For argument labeling, the number of possible assignments is ≈ 20 m , if m is the number of arguments of a verb (typically between 2 and 5), and 20 is the approximate number of possible labels if considering both core and modifying arguments.', 'Training a model which has such a huge number of classes is infeasible if the model does not factorize due to strong independence assumptions.', 'Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #AUTHOR_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions .', 'We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments.', 'As can be seen from Figure 8(a), for relatively small values of n, our re-ranking approach does not present a serious bottleneck to performance.', 'We used a value of n = 10 for training.', 'In Figure 8(a) we can see that if we could pick, using an oracle, the best assignment out of the top 10 assignments according to the local model, we would achieve an F-Measure of 97.3 on all arguments.', 'Increasing the number of n to 30 results in a very small gain in the upper bound on performance and a large increase in memory requirements.', 'We therefore selected n = 10 as a good compromise.']",5,"['Motivation for Re-Ranking.', 'For argument identification, the number of possible assignments for a parse tree with n nodes is 2 n .', 'For argument labeling, the number of possible assignments is  20 m , if m is the number of arguments of a verb (typically between 2 and 5), and 20 is the approximate number of possible labels if considering both core and modifying arguments.', 'Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( #AUTHOR_TAG ) , which selects from likely assignments generated by a model which makes stronger independence assumptions .', 'We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments.', 'As can be seen from Figure 8(a), for relatively small values of n, our re-ranking approach does not present a serious bottleneck to performance.']"
CC590,J09-1003,Evaluating Centering for Information Ordering Using Corpora,stochastic text structuring using the principle of continuity,"['Nikiforos Karamanis', 'Hisar Maruli Manurung']",,"This paper explores the feasibility of implementing an evolutionary algorithm for text structuring using the heuristic of continuity as a fitness function, chosen over other more complicated metrics of text coherence. Using MCGONAGALL (Manurung et al., 2000) as our experimental platform, we show that by employing an elitist strategy for stochastic search it is possible to quickly reach the global optimum of minimal violations of continuity.","Following our previous work ( #AUTHOR_TAG ; Althaus , Karamanis , and Koller 2004 ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists .","['Following our previous work ( #AUTHOR_TAG ; Althaus , Karamanis , and Koller 2004 ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists .', 'A set of candidate orderings is produced by creating different permutations of these lists.', 'A metric of coherence uses features from centering to compute a score for each candidate ordering and select the highest scoring ordering as the output. 9', ""A wide range of metrics of coherence can be defined in centering's terms, simply on the basis of the work we reviewed in Section 3. To exemplify this, let us first assume that the ordering in Example (3), which is analyzed as a sequence of CF lists in Table 5, is a candidate ordering."", 'Table 6 summarizes the NOCBs, the violations of COHERENCE, SALIENCE, and CHEAPNESS, and the centering transitions for this ordering. 10', 'he candidate ordering contains two NOCBs in sentences (3e) and (3f).', 'Its score according to M.NOCB, the metric used by Karamanis and Manurung (2002) and Althaus, Karamanis, and Koller (2004), is 2. Another ordering with fewer NOCBs (should such an ordering exist) will be preferred over this candidate as the selected output of information ordering if M.NOCB is used to guide this process.', 'M.NOCB relies only on CONTINUITY.', 'Because satisfying this principle is a prerequisite for the computation of every other centering feature, M.NOCB is the simplest possible centering-based metric and will be used as the baseline in our experiments.']",2,"['Following our previous work ( #AUTHOR_TAG ; Althaus , Karamanis , and Koller 2004 ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists .', 'A set of candidate orderings is produced by creating different permutations of these lists.', 'A metric of coherence uses features from centering to compute a score for each candidate ordering and select the highest scoring ordering as the output. 9', ""A wide range of metrics of coherence can be defined in centering's terms, simply on the basis of the work we reviewed in Section 3. To exemplify this, let us first assume that the ordering in Example (3), which is analyzed as a sequence of CF lists in Table 5, is a candidate ordering."", 'Table 6 summarizes the NOCBs, the violations of COHERENCE, SALIENCE, and CHEAPNESS, and the centering transitions for this ordering. 10', 'M.NOCB relies only on CONTINUITY.']"
CC591,J09-1005,Unsupervised Type and Token Identification of Idiomatic Expressions,automatic identification of noncompositional phrases,['Dekang Lin'],,,"Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #AUTHOR_TAG .","['Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #AUTHOR_TAG .', 'We use the automatically built thesaurus of Lin (1998) to find words similar to each constituent, in order to automatically generate variants.', 'Variants are generated by replacing either the noun or the verb constituent of a pair with a semantically (and syntactically) similar word. 3', 'xamples of automatically generated variants for the pair spill, bean are pour, bean , stream, bean , spill, corn , and spill, rice .']",4,"['Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #AUTHOR_TAG .', 'We use the automatically built thesaurus of Lin (1998) to find words similar to each constituent, in order to automatically generate variants.', 'Variants are generated by replacing either the noun or the verb constituent of a pair with a semantically (and syntactically) similar word. 3']"
CC592,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,decision graphs—an extension of decision trees,['J J Oliver'],,"In this paper, we examine Decision Graphs, a generalization of decision trees. We present an inference scheme to construct decision graphs using the Minimum Message Length Principle. Empirical tests demonstrate that this scheme compares favourably with other decision tree inference schemes. This work provides a metric for comparing the relative merit of the decision tree and decision graph formalisms for a particular domain.","We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs ( #AUTHOR_TAG ) .","['In principle, we could have used a classification method to predict clusters from the values of the confidence measures for unseen cases.', 'We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs ( #AUTHOR_TAG ) .']",0,"['We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs ( #AUTHOR_TAG ) .']"
CC593,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,knowledge harvesting articulation and delivery the hewlettpackard journal,"['K A Delic', 'D Lahaix']",,,"Such technologies require significant human input , and are difficult to create and maintain ( #AUTHOR_TAG ) .","['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997).', 'Such technologies require significant human input , and are difficult to create and maintain ( #AUTHOR_TAG ) .', 'In contrast, the techniques examined in this article are corpus-based and data-driven.', 'The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus.']",0,"['Such technologies require significant human input , and are difficult to create and maintain ( #AUTHOR_TAG ) .']"
CC594,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,bridging the lexical chasm statistical approaches to answerfinding,"['A Berger', 'R Caruana', 'D Cohn', 'D Freitag', 'V Mittal']",method,,"â¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ; #AUTHOR_TAG ) .","['â\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ; #AUTHOR_TAG ) .']",1,"['â\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ; #AUTHOR_TAG ) .']"
CC595,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,an introduction to modern information retrieval,"['G Salton', 'M J McGill']",method,"Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here.","This method follows a traditional Information Retrieval paradigm ( #AUTHOR_TAG ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .","['As stated herein, we studied two document-based methods: Document Retrieval and Document Prediction.', '(Doc-Ret).', 'This method follows a traditional Information Retrieval paradigm ( #AUTHOR_TAG ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .', 'In our case, the query is a new request e-mail to be addressed by the system, and we have considered three views of the documents in the corpus:']",5,"['This method follows a traditional Information Retrieval paradigm ( #AUTHOR_TAG ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .']"
CC596,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,an introduction to modern information retrieval,"['G Salton', 'M J McGill']",method,"Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here.","For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #AUTHOR_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) .","['We carried out a preliminary experiment in order to compare the three variants of the Doc-Ret method.', 'The evaluation is performed by considering each request e-mail in turn, removing it and its response from the corpus, carrying out the retrieval process, and then comparing the retrieved response with the actual response (if there are several similar responses in the corpus, an appropriate response can still be retrieved).', 'The results of this experiment are shown in Table 1.', 'The first column shows which document retrieval variant is being evaluated.', 'The second column shows the proportion of requests for which one or more documents were retrieved (using our applicability threshold).', 'We see that matching on requests yields more retrieved documents than matching on responses, and that matching on request-response pairs yields even more retrieved documents.', 'For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #AUTHOR_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) .', 'The third column in Table 1 shows the proportion of requests for which this similarity is non-zero.', 'Again, the third variant (matching on request-response pairs) retrieves the highest proportion of responses that bear some similarity to the real responses.', 'The fourth column shows the average similarity between the top retrieved response and the real response for the cases where retrieval took place.', 'Here too the third variant yields the best similarity score (0.52).']",5,"['The results of this experiment are shown in Table 1.', 'The first column shows which document retrieval variant is being evaluated.', 'For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #AUTHOR_TAG ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) .', 'Here too the third variant yields the best similarity score (0.52).']"
CC597,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,an introduction to modern information retrieval,"['G Salton', 'M J McGill']",,"Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here.","This situation suggests a response-automation approach that follows the document retrieval paradigm ( #AUTHOR_TAG ) , where a new request is matched with existing response documents ( e-mails ) .","['The example in Figure 1(b) illustrates a situation where specific words in the request (docking station and install) are also mentioned in the response.', 'This situation suggests a response-automation approach that follows the document retrieval paradigm ( #AUTHOR_TAG ) , where a new request is matched with existing response documents ( e-mails ) .', 'However, specific words in the request do not always match a response well, and sometimes do not match a response at all, as demonstrated by the examples in Figures 1(a) and 1(c), respectively.']",0,"['This situation suggests a response-automation approach that follows the document retrieval paradigm ( #AUTHOR_TAG ) , where a new request is matched with existing response documents ( e-mails ) .', 'However, specific words in the request do not always match a response well, and sometimes do not match a response at all, as demonstrated by the examples in Figures 1(a) and 1(c), respectively.']"
CC598,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,expert systems a technology before its time ai expert available at wwwstanfordedugroup scipavsgtexpertsystemsaiexperthtml,"['A Barr', 'S Tessler']",introduction,,It is therefore no surprise that early attempts at response automation were knowledge-driven ( #AUTHOR_TAG ; Watson 1997 ; Delic and Lahaix 1998 ) .,"['However, even the automation of responses to the ""easy"" problems is a difficult task.', 'Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( #AUTHOR_TAG ; Watson 1997 ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']",0,"['However, even the automation of responses to the ""easy"" problems is a difficult task.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( #AUTHOR_TAG ; Watson 1997 ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']"
CC599,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,statistical and inductive inference by minimum message length,['C S Wallace'],,,We then use the program Snob ( Wallace and Boulton 1968 ; #AUTHOR_TAG ) to cluster these experiences .,"['We train the system by clustering the ""experiences"" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).', 'We then use the program Snob ( Wallace and Boulton 1968 ; #AUTHOR_TAG ) to cluster these experiences .', 'Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.', 'shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15', 'These clusters were chosen because they illustrate clearly three situations of interest.']",5,"['We train the system by clustering the ""experiences"" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).', 'We then use the program Snob ( Wallace and Boulton 1968 ; #AUTHOR_TAG ) to cluster these experiences .', 'Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.', 'shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15', 'These clusters were chosen because they illustrate clearly three situations of interest.']"
CC600,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,expert systems a technology before its time ai expert available at wwwstanfordedugroup scipavsgtexpertsystemsaiexperthtml,"['A Barr', 'S Tessler']",,,"The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #AUTHOR_TAG ) and case-based reasoning ( Watson 1997 ) .","['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #AUTHOR_TAG ) and case-based reasoning ( Watson 1997 ) .', 'Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998).', 'In contrast, the techniques examined in this article are corpus-based and data-driven.', 'The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus.']",1,"['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( #AUTHOR_TAG ) and case-based reasoning ( Watson 1997 ) .', 'Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998).', 'In contrast, the techniques examined in this article are corpus-based and data-driven.']"
CC601,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,libsvm a library for support vector machines software available at httpwwwcsie ntuedutw∼cjlinlibsvm,"['C C Chang', 'C J Lin']",method,,7 We employed the LIBSVM package ( #AUTHOR_TAG ) .,"[""We use a Support Vector Machine (SVM) with a Radial Basis Function kernel to predict SCs from users' requests. 7"", 'A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request.', 'During the For Sent-Pred we also experimented with grammatical and sentence-based syntactic features, such as number of syntactic phrases, grammatical mood, and grammatical person (Marom and Zukerman 2006), but the simple binary bag-of-lemmas representation yielded similar results.', '7 We employed the LIBSVM package ( #AUTHOR_TAG ) .', 'prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3.', 'We then apply the following steps.']",5,['7 We employed the LIBSVM package ( #AUTHOR_TAG ) .']
CC602,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,statistical and inductive inference by minimum message length,['C S Wallace'],method,,"In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #AUTHOR_TAG ) .","[""The idea behind the Doc-Pred method is similar to Bickel and Scheffer's (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected."", 'In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #AUTHOR_TAG ) .', 'We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2).', 'The input to Snob is a set of binary vectors, one vector per response document.', 'The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency).', 'The predictive model is a Decision Graph (Oliver 1993), which, like Snob, is based on the MML principle.', 'The Decision Graph is trained on unigram and bigram lemmas in the request as input features, 5 and the identifier of the response cluster that contains the actual response for the request as the target feature.', 'The model predicts which response cluster is most suitable for a given request, and returns the probability that this prediction is correct.', 'This probability is our indicator of whether the Doc-Pred method can address a new request.', 'As for the Doc-Ret method, an applicability threshold for this parameter is currently determined empirically (Table 3).']",5,"['In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #AUTHOR_TAG ) .']"
CC603,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,an introduction to modern information retrieval,"['G Salton', 'M J McGill']",method,"Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here.",We use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( van Rijsbergen 1979 ; #AUTHOR_TAG ) .,"['We use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( van Rijsbergen 1979 ; #AUTHOR_TAG ) .', 'Precision measures how much of the information in an automatically generated response is correct (i.e., appears in the model response), and F-score measures the overall similarity between the automatically generated response and the model response.', 'F-score is the harmonic mean of precision and recall, which measures how much of the information in the model response appears in the generated response.', 'We consider precision separately because it does not penalize missing information, enabling us to better assess our sentence-based methods.', 'Precision, recall, and F-score are calculated as follows using a word-by-word comparison (stop-words are excluded). 13', 'ecision =']",5,"['We use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( van Rijsbergen 1979 ; #AUTHOR_TAG ) .', 'F-score is the harmonic mean of precision and recall, which measures how much of the information in the model response appears in the generated response.', 'Precision, recall, and F-score are calculated as follows using a word-by-word comparison (stop-words are excluded). 13']"
CC604,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,applying casebased reasoning techniques for enterprise systems,['I Watson'],,"Chapter 1: What Is Case-Based Reasoning? Chapter 2: Understanding CBR Chapter 3: The Application of CBR Chapter 4: Industrial Applications of CBR Chapter 5: CBR and Customer Service Chapter 6: CBR Software Tools Chapter 7: Building a Diagnostic Case-Base Chapter 8: Building, Testing, and Maintaining Case-Bases Chapter 9: Conclusion Chapter 10: Bibliography","The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning ( #AUTHOR_TAG ) .","['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning ( #AUTHOR_TAG ) .', 'Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998).', 'In contrast, the techniques examined in this article are corpus-based and data-driven.', 'The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus.']",1,"['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning ( #AUTHOR_TAG ) .', 'In contrast, the techniques examined in this article are corpus-based and data-driven.']"
CC605,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,mercure towards an automatic email followup system,"['G Lapalme', 'L Kosseim']",,,"#AUTHOR_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering .","['There are very few reported attempts at corpus-based automation of help-desk responses (Carmel, Shtalhaim, and Soffer 2000;Lapalme and Kosseim 2003;Bickel and Scheffer 2004;Malik, Subramaniam, and Kaushik 2007).', 'eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), retrieves a list of request-response pairs and presents a ranked list of responses to the user.', 'If the user is unsatisfied with this list, an operator is asked to generate a new response.', 'The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses.', 'However, there is no attempt to automatically generate a single response.', 'Bickel and Scheffer (2004) compared the performance of document retrieval and document prediction for generating help-desk responses.', 'Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs.', 'Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes.', 'The generated response is the answer that is closest to the centroid of the cluster.', ""Bickel and Scheffer's results are consistent with ours, in the sense that the performance of the Doc-Ret method is significantly worse than that of Doc-Pred."", 'However, it is worth noting that their corpus is significantly smaller than ours (805 question-answer pairs), their questions seem to be much simpler and shorter than those in our corpus, and the replies shorter and more homogeneous.', 'Malik, Subramaniam, and Kaushik (2007) developed a system that builds questionanswer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer.', 'This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences.', 'In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates.', '#AUTHOR_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering .', 'Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed.', 'The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches.', 'However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts).']",1,"['#AUTHOR_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering .']"
CC606,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,the design implementation and use of computational linguistics volume 35 number 4 the ngram statistics package in cicling,"['S Banerjee', 'T Pedersen']",method,,"5 Significant bigrams are obtained using the n-gram statistics package NSP ( #AUTHOR_TAG ) , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) .","['5 Significant bigrams are obtained using the n-gram statistics package NSP ( #AUTHOR_TAG ) , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) .']",5,"['5 Significant bigrams are obtained using the n-gram statistics package NSP ( #AUTHOR_TAG ) , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) .']"
CC607,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,mercure towards an automatic email followup system,"['G Lapalme', 'L Kosseim']",introduction,,"Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .","['In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches.', 'An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries.', 'This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary.', 'Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', 'A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues.', 'Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses.', 'Our study is based on a large corpus of requestresponse e-mail dialogues between customers and operators at Hewlett-Packard.', 'Observations from this corpus have led us to consider several methods that implement different types of corpus-based strategies.', 'Specifically, we have investigated two types of methods (retrieval and prediction) applied at two levels of granularity (document and sentence).', 'In this article, we present these methods and compare their performance.']",0,"['An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries.', 'Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', 'A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues.', 'Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses.', 'Our study is based on a large corpus of requestresponse e-mail dialogues between customers and operators at Hewlett-Packard.', 'Observations from this corpus have led us to consider several methods that implement different types of corpus-based strategies.']"
CC608,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,automatic question answering using the web beyond the factoid,"['R Soricut', 'E Brill']",,"In this paper we describe and evaluate a Question Answering (QA) system that goes beyond answering factoid questions. Our approach to QA assumes no restrictions on the type of questions that are handled, and no assumption that the answers to be provided are factoids. We present an unsupervised approach for collecting question and answer pairs from FAQ pages, which we use to collect a corpus of 1 million question/answer pairs from FAQ pages available on the Web. This corpus is used to train various statistical models employed by our QA system: a statistical chunker used to transform a natural language-posed question into a phrase-based query to be submitted for exact match to an off-the-shelf search engine; an answer/question translation model, used to assess the likelihood that a proposed answer is indeed an answer to the posed question; and an answer language model, used to assess the likelihood that a proposed answer is a well-formed answer. We evaluate our QA system in a modular fashion, by comparing the performance of baseline algorithms against our proposed algorithms for various modules in our QA system. The evaluation shows that our system achieves reasonable performance in terms of answer accuracy for a large variety of complex, non-factoid questions.","Two applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000; Jijkoun and de Rijke 2005;  #AUTHOR_TAG ) .","['Two applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000; Jijkoun and de Rijke 2005;  #AUTHOR_TAG ) .', 'An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.', 'In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.', 'Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses.']",1,"['Two applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000; Jijkoun and de Rijke 2005;  #AUTHOR_TAG ) .']"
CC609,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,mercure towards an automatic email followup system,"['G Lapalme', 'L Kosseim']",method,,â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( #AUTHOR_TAG ; Roy and Subramaniam 2006 ) .,['â\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( #AUTHOR_TAG ; Roy and Subramaniam 2006 ) .'],1,['â\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( #AUTHOR_TAG ; Roy and Subramaniam 2006 ) .']
CC610,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,assessing agreement on classification tasks the kappa statistic,['J Carletta'],method,"Currently, computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics, none of which are easily interpretable or comparable to each other. Meanwhile, researchers in content analysis have already experienced the same difficulties and come up with a solution in the kappa statistic. We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science, and argue that we would be better off as a field adopting techniques from content analysis.","Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #AUTHOR_TAG ) .","['Each evaluation set contained 80 cases, randomly selected from the corpus in proportion to the size of each data set, where a case contained a request e-mail, the model response, and the responses generated by the two methods being compared.', 'Each judge was given of these cases, and was asked to assess the generated responses on the four criteria listed previously. 14', 'e maximized the coverage of this study by allocating different cases to each judge, thus avoiding a situation where a particularly good or bad set of cases is evaluated by all judges.', ""In addition, we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges' assessments would be comparable."", 'Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #AUTHOR_TAG ) .', 'However, it is still necessary to We asked the judges to leave a question unanswered if they felt they did not have the technical knowledge to make a judgment, but this did not occur.']",5,"['Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( #AUTHOR_TAG ) .']"
CC611,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,multidocument summarization by sentence extraction,"['J Goldstein', 'V Mittal', 'J Carbonell', 'M Kantrowitz']",method,,"In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( #AUTHOR_TAG ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 ) .","['As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses.', 'An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response.', 'This task can be cast as extractive multi-document summarization.', 'Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable.', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( #AUTHOR_TAG ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 ) .']",0,"['This task can be cast as extractive multi-document summarization.', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( #AUTHOR_TAG ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 ) .']"
CC612,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,hybrid recommender systems user modeling and useradapted interaction,['R Burke'],,,A common way to combine different models consists of selecting the model that is most confident regarding its decision ( #AUTHOR_TAG ) .,"['A common way to combine different models consists of selecting the model that is most confident regarding its decision ( #AUTHOR_TAG ) .', 'However, in our case, the individual confidence (applicability) measures employed by our response-generation methods are not comparable (e.g., the retrieval score in Doc-Ret is different in nature from the prediction probability in Doc-Pred).', 'Hence, prior to selecting the most confident method, we need to find a way to compare the different measures of confidence.', 'Because the performances of the different methods are comparable, we do this by establishing a link between confidence and performance.', 'In other words, our meta-level process learns to predict the performance of the different methods from their confidence levels on the basis of previous experience.', 'These predictions enable our system to recommend a particular method for handling a new (unseen) request (Marom, Zukerman, and Japkowicz 2007).']",1,"['A common way to combine different models consists of selecting the model that is most confident regarding its decision ( #AUTHOR_TAG ) .', 'However, in our case, the individual confidence (applicability) measures employed by our response-generation methods are not comparable (e.g., the retrieval score in Doc-Ret is different in nature from the prediction probability in Doc-Pred).', 'Because the performances of the different methods are comparable, we do this by establishing a link between confidence and performance.', 'In other words, our meta-level process learns to predict the performance of the different methods from their confidence levels on the basis of previous experience.', 'These predictions enable our system to recommend a particular method for handling a new (unseen) request (Marom, Zukerman, and Japkowicz 2007).']"
CC613,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,queryrelevant summarization using faqs,"['A Berger', 'V Mittal']",,"This paper introduces a statistical model for query-relevant summarization: succinctly characterizing the relevance of a document to a query. Learning parameter values for the proposed model requires a large collection of summarized documents, which we do not have, but as a proxy, we use a collection of FAQ (frequently-asked question) documents. Taking a learning approach enables a principled, quantitative evaluation of the proposed system, and the results of some initial experiments---on a collection of Usenet FAQs and on a FAQ-like set of customer-submitted questions to several large retail companies---suggest the plausibility of learning for summarization.","Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004;Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) ( #AUTHOR_TAG; Soricut and Brill 2006).","['Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004;Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) ( #AUTHOR_TAG; Soricut and Brill 2006).', 'An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.', 'In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.', 'Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses.']",1,"['Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004;Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) ( #AUTHOR_TAG; Soricut and Brill 2006).']"
CC614,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,building effective question answering characters,"['A Leuski', 'R Patel', 'D Traum', 'B Kennedy']",method,"In this paper, we describe methods for building and evaluation of limited domain question-answering characters. Several classification techniques are tested, including text classification using support vector machines, language-model based retrieval, and cross-language information retrieval techniques, with the latter having the highest success rate. We also evaluated the effect of speech recognition errors on performance with users, finding that retrieval is robust until recognition reaches over 50 % WER.","â¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( Feng et al. 2006 ; #AUTHOR_TAG ) .","['â\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( Feng et al. 2006 ; #AUTHOR_TAG ) .', 'The representativeness of the sample size was not discussed in any of these studies.']",1,"['â\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( Feng et al. 2006 ; #AUTHOR_TAG ) .']"
CC615,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,queryrelevant summarization using faqs,"['A Berger', 'V Mittal']",method,"This paper introduces a statistical model for query-relevant summarization: succinctly characterizing the relevance of a document to a query. Learning parameter values for the proposed model requires a large collection of summarized documents, which we do not have, but as a proxy, we use a collection of FAQ (frequently-asked question) documents. Taking a learning approach enables a principled, quantitative evaluation of the proposed system, and the results of some initial experiments---on a collection of Usenet FAQs and on a FAQ-like set of customer-submitted questions to several large retail companies---suggest the plausibility of learning for summarization.","â¢ Only an automatic evaluation was performed , which relied on having model responses ( #AUTHOR_TAG ; Berger et al. 2000 ) .","['In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries.', 'These systems addressed the evaluation issue as follows.', 'â\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( #AUTHOR_TAG ; Berger et al. 2000 ) .']",1,"['In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries.', 'These systems addressed the evaluation issue as follows.', 'â\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( #AUTHOR_TAG ; Berger et al. 2000 ) .']"
CC616,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,evaluation of a largescale email response system,"['Y Marom', 'I Zukerman']",method,,In #AUTHOR_TAGa ) we identified several systems that resemble ours in that they provide answers to queries .,"['In #AUTHOR_TAGa ) we identified several systems that resemble ours in that they provide answers to queries .', 'These systems addressed the evaluation issue as follows.', 'r Only an automatic evaluation was performed, which relied on having model responses .']",0,['In #AUTHOR_TAGa ) we identified several systems that resemble ours in that they provide answers to queries .']
CC617,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,an information measure for classification,"['C S Wallace', 'D M Boulton']",method,"1. The class to which each thing belongs. 2. The average properties of each class. 3. The deviations of each thing from the average properties of its parent class. If the things are found to be concentrated in a small area of the region of each class in the measurement space then the deviations will be small, and with reference to the average class properties most of the information about a thing is given by naming the class to which it belongs. In this case the information may be recorded much more briefly than if a classification had not been used. We suggest that the best classification is that which results in the briefest recording of all the attribute information. In this context, we will regard the measurements of each thing as being a message about that thing. Shannon (1948) showed that where messages may be regarded as each nominating the occurrence of a particular event among a universe of possible events, the information needed to record a series of such messages is minimised if the messages are encoded so that the length of each message is proportional to minus the logarithm of the relative frequency of occurrence of the event which it nominates. The information required is greatest when all frequencies are equal. The messages here nominate the positions in measurement space of the 5 1 points representing the attributes of the things. If the expected density of points in the measurement space is everywhere uniform, the positions of the points cannot be encoded more briefly than by a simple list of the measured values. However, if the expected density is markedly non-uniform, application","In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #AUTHOR_TAG ; Wallace 2005 ) .","[""The idea behind the Doc-Pred method is similar to Bickel and Scheffer's (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request's features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected."", 'In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #AUTHOR_TAG ; Wallace 2005 ) .', 'We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2).', 'The input to Snob is a set of binary vectors, one vector per response document.', 'The values of a vector correspond to the presence or absence of each (lemmatized) corpus word in the document in question (after removing stop-words and words with very low frequency).', 'The predictive model is a Decision Graph (Oliver 1993), which, like Snob, is based on the MML principle.', 'The Decision Graph is trained on unigram and bigram lemmas in the request as input features, 5 and the identifier of the response cluster that contains the actual response for the request as the target feature.', 'The model predicts which response cluster is most suitable for a given request, and returns the probability that this prediction is correct.', 'This probability is our indicator of whether the Doc-Pred method can address a new request.', 'As for the Doc-Ret method, an applicability threshold for this parameter is currently determined empirically (Table 3).']",5,"['In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( #AUTHOR_TAG ; Wallace 2005 ) .', 'The predictive model is a Decision Graph (Oliver 1993), which, like Snob, is based on the MML principle.']"
CC618,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,queryrelevant summarization using faqs,"['A Berger', 'V Mittal']",,"This paper introduces a statistical model for query-relevant summarization: succinctly characterizing the relevance of a document to a query. Learning parameter values for the proposed model requires a large collection of summarized documents, which we do not have, but as a proxy, we use a collection of FAQ (frequently-asked question) documents. Taking a learning approach enables a principled, quantitative evaluation of the proposed system, and the results of some initial experiments---on a collection of Usenet FAQs and on a FAQ-like set of customer-submitted questions to several large retail companies---suggest the plausibility of learning for summarization.","In FAQs , #AUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .","['In FAQs , #AUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .', 'They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', 'Jijkoun and de Rijke ( 2005) compared different variants of retrieval techniques.', 'Soricut and Brill (2006) compared a predictive approach (statistical translation), a retrieval approach based on a language-model, and a hybrid approach which combines statistical chunking and traditional retrieval.', 'Two significant differences between help-desk and FAQs are the following.']",0,"['In FAQs , #AUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', 'Jijkoun and de Rijke ( 2005) compared different variants of retrieval techniques.']"
CC619,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,an intelligent discussionbot for answering student queries in threaded discussions,"['D Feng', 'E Shaw', 'J Kim', 'E Hovy']",method,"This paper describes a discussion-bot that provides answers to students' discussion board questions in an unobtrusive and human-like way. Using information retrieval and natural language processing techniques, the discussion-bot identifies the questioner's interest, mines suitable answers from an annotated corpus of 1236 archived threaded discussions and 279 course documents and chooses an appropriate response. A novel modeling approach was designed for the analysis of archived threaded discussions to facilitate answer extraction. We compare a self-out and an all-in evaluation of the mined answers. The results show that the discussion-bot can begin to meet students' learning requests. We discuss directions that might be taken to increase the effectiveness of the question matching and answer extraction algorithms. The research takes place in the context of an undergraduate computer science course.","â¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( #AUTHOR_TAG ; Leuski et al. 2006 ) .","['â\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( #AUTHOR_TAG ; Leuski et al. 2006 ) .', 'The representativeness of the sample size was not discussed in any of these studies.']",1,"['â\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( #AUTHOR_TAG ; Leuski et al. 2006 ) .']"
CC620,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,learning from message pairs for automatic email answering,"['S Bickel', 'T Scheffer']",introduction,"We consider the problem of learning a mapping from question to answer messages. The training data for this problem consist of pairs of messages that have been received and sent in the past. We formulate the problem setting, discuss appropriate performance metrics, develop a solution and describe two baseline methods for comparison. We present a case study based on emails received and answered by the service center of a large online store.","Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .","['In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches.', 'An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries.', 'This indicates that help-desk customers may have also become more tolerant of inaccurate or incomplete automatically generated replies, provided these replies are still relevant to their problem, and so long as the customers can follow up with a request for human-generated responses if necessary.', 'Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', 'A major factor limiting this work is the dearth of corpora-help-desk e-mails tend to be proprietary and are subject to privacy issues.', 'Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses.', 'Our study is based on a large corpus of requestresponse e-mail dialogues between customers and operators at Hewlett-Packard.', 'Observations from this corpus have led us to consider several methods that implement different types of corpus-based strategies.', 'Specifically, we have investigated two types of methods (retrieval and prediction) applied at two levels of granularity (document and sentence).', 'In this article, we present these methods and compare their performance.']",0,"['An outcome of the recent proliferation of statistical approaches, in particular in recommender systems and search engines, is that people have become accustomed to responses that are not precisely tailored to their queries.', 'Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', 'Further, this application lacks the kind of benchmark data sets that are used in question-answering and text summarization. 2 In this article, we report on our experiments with corpus-based techniques for the automation of help-desk responses.', 'Our study is based on a large corpus of requestresponse e-mail dialogues between customers and operators at Hewlett-Packard.', 'Observations from this corpus have led us to consider several methods that implement different types of corpus-based strategies.', 'Specifically, we have investigated two types of methods (retrieval and prediction) applied at two levels of granularity (document and sentence).']"
CC621,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,statistical learning theory,['V N Vapnik'],method,"The traditional approach of statistical physics to supervised learning routinely assumes unrealistic generative models for the data: usually inputs are independent random variables, uncorrelated with their labels. Only recently, statistical physicists started to explore more complex forms of data, such as equally-labelled points lying on (possibly low dimensional) object manifolds. Here we provide a bridge between this recently-established research area and the framework of statistical learning theory, a branch of mathematics devoted to inference in machine learning. The overarching motivation is the inadequacy of the classic rigorous results in explaining the remarkable generalization properties of deep learning. We propose a way to integrate physical models of data into statistical learning theory, and address, with both combinatorial and statistical mechanics methods, the computation of the Vapnik-Chervonenkis entropy, which counts the number of different binary classifications compatible with the loss class. As a proof of concept, we focus on kernel machines and on two simple realizations of data structure introduced in recent physics literature: $k$-dimensional simplexes with prescribed geometric relations and spherical manifolds (equivalent to margin classification). Entropy, contrary to what happens for unstructured data, is nonmonotonic in the sample size, in contrast with the rigorous bounds. Moreover, data structure induces a novel transition beyond the storage capacity, which we advocate as a proxy of the nonmonotonicity, and ultimately a cue of low generalization error. The identification of a synaptic volume vanishing at the transition allows a quantification of the impact of data structure within replica theory, applicable in cases where combinatorial methods are not available, as we demonstrate for margin learning.Comment: 19 pages, 3 figure","Specifically , we used Decision Graphs ( Oliver 1993 ) for Doc-Pred , and SVMs ( #AUTHOR_TAG ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .","['The focus of our work is on the general applicability of the different response automa- tion methods, rather than on comparing the performance of particular implementa- tion techniques.', 'Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research.', 'Specifically , we used Decision Graphs ( Oliver 1993 ) for Doc-Pred , and SVMs ( #AUTHOR_TAG ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .', 'Because this variation was uniformly implemented for both approaches, it does not affect their relative performance.', 'These methodological variations are summarized in Table 2.']",5,"['Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research.', 'Specifically , we used Decision Graphs ( Oliver 1993 ) for Doc-Pred , and SVMs ( #AUTHOR_TAG ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .', 'Because this variation was uniformly implemented for both approaches, it does not affect their relative performance.', 'These methodological variations are summarized in Table 2.']"
CC622,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,eventbased extractive summarization,"['E Filatova', 'V Hatzivassiloglou']",method,,"After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #AUTHOR_TAG to penalize redundant sentences in cohesive clusters .","['Removing redundant sentences.', 'After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #AUTHOR_TAG to penalize redundant sentences in cohesive clusters .', 'This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.e., its score is not decremented).', 'Specifically, given a sentence s k in cluster SC l which contains a sentence with a higher or equal score, the contribution of SC l to Score(s k ) (= Pr(SC l ) × Pr(s k |SC l )) is subtracted from Score(s k ).', 'After applying these penalties, we retain only the sentences whose adjusted score is greater than zero (for a highly cohesive cluster, typically only one sentence remains).']",5,"['Removing redundant sentences.', 'After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #AUTHOR_TAG to penalize redundant sentences in cohesive clusters .']"
CC623,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,decision graphs—an extension of decision trees,['J J Oliver'],method,"In this paper, we examine Decision Graphs, a generalization of decision trees. We present an inference scheme to construct decision graphs using the Minimum Message Length Principle. Empirical tests demonstrate that this scheme compares favourably with other decision tree inference schemes. This work provides a metric for comparing the relative merit of the decision tree and decision graph formalisms for a particular domain.","The predictive model is a Decision Graph ( #AUTHOR_TAG ) , which , like Snob , is based on the MML principle .","['The idea behind the Doc-Pred method is similar to Bickel and Scheffer�s (2004): Response documents are grouped into clusters, one of these clusters is predicted for a new request on the basis of the request�s features, and the response that is most representative of the predicted cluster (closest to the centroid) is selected.', 'In our case, the clustering is performed by the program Snob, which implements mixture model- ing combined with model selection based on the Minimum Message Length (MML) criterion (Wallace and Boulton 1968; Wallace 2005).', 'We chose this program because the number of clusters does not have to be specified in advance, and it returns a probabilistic interpretation for its clusters (this interpretation is used by the Sent-Pred method, Section 3.2.2).', 'The input to Snob is a set of binary vectors, one vector per response document.', 'The values of a vector correspond to the presence or absence of each (lem- matized) corpus word in the document in question (after removing stop-words and words with very low frequency).4', 'The predictive model is a Decision Graph ( #AUTHOR_TAG ) , which , like Snob , is based on the MML principle .', 'The Decision Graph is trained on unigram and bigram lemmas in the request as input features,5 and the identifier of the response cluster that contains the actual response for the request as the target feature.', 'The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct.', 'This probability is our indicator of whether the Doc-Pred method can address a new request.', 'As for the Doc-Ret method, an applicability threshold for this parameter is currently determined empirically (Table 3).']",5,"['The predictive model is a Decision Graph ( #AUTHOR_TAG ) , which , like Snob , is based on the MML principle .']"
CC624,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,knowledge harvesting articulation and delivery the hewlettpackard journal,"['K A Delic', 'D Lahaix']",introduction,,It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; Watson 1997 ; #AUTHOR_TAG ) .,"['However, even the automation of responses to the ""easy"" problems is a difficult task.', 'Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; Watson 1997 ; #AUTHOR_TAG ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']",0,['It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; Watson 1997 ; #AUTHOR_TAG ) .']
CC625,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,mercure towards an automatic email followup system,"['G Lapalme', 'L Kosseim']",,,"There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; #AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .","['There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; #AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', 'eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), retrieves a list of request-response pairs and presents a ranked list of responses to the user.', 'If the user is unsatisfied with this list, an operator is asked to generate a new response.', 'The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses.', 'However, there is no attempt to automatically generate a single response.', 'Bickel and Scheffer (2004) compared the performance of document retrieval and document prediction for generating help-desk responses.', 'Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs.', 'Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes.', 'The generated response is the answer that is closest to the centroid of the cluster.', ""Bickel and Scheffer's results are consistent with ours, in the sense that the performance of the Doc-Ret method is significantly worse than that of Doc-Pred."", 'However, it is worth noting that their corpus is significantly smaller than ours (805 question-answer pairs), their questions seem to be much simpler and shorter than those in our corpus, and the replies shorter and more homogeneous.', 'Malik, Subramaniam, and Kaushik (2007) developed a system that builds questionanswer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer.', 'This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences.', 'In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates.', 'Lapalme and Kosseim (2003) investigated three approaches to the automatic generation of response e-mails: text classification, case-based reasoning, and question answering.', 'Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed.', 'The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches.', 'However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts).']",1,"['There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; #AUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', 'Lapalme and Kosseim (2003) investigated three approaches to the automatic generation of response e-mails: text classification, case-based reasoning, and question answering.', 'Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed.']"
CC626,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,learning from message pairs for automatic email answering,"['S Bickel', 'T Scheffer']",,"We consider the problem of learning a mapping from question to answer messages. The training data for this problem consist of pairs of messages that have been received and sent in the past. We formulate the problem setting, discuss appropriate performance metrics, develop a solution and describe two baseline methods for comparison. We present a case study based on emails received and answered by the service center of a large online store.","There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .","['There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', 'eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), retrieves a list of request-response pairs and presents a ranked list of responses to the user.', 'If the user is unsatisfied with this list, an operator is asked to generate a new response.', 'The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses.', 'However, there is no attempt to automatically generate a single response.', 'Bickel and Scheffer (2004) compared the performance of document retrieval and document prediction for generating help-desk responses.', 'Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs.', 'Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes.', 'The generated response is the answer that is closest to the centroid of the cluster.', ""Bickel and Scheffer's results are consistent with ours, in the sense that the performance of the Doc-Ret method is significantly worse than that of Doc-Pred."", 'However, it is worth noting that their corpus is significantly smaller than ours (805 question-answer pairs), their questions seem to be much simpler and shorter than those in our corpus, and the replies shorter and more homogeneous.', 'Malik, Subramaniam, and Kaushik (2007) developed a system that builds questionanswer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer.', 'This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences.', 'In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates.', 'Lapalme and Kosseim (2003) investigated three approaches to the automatic generation of response e-mails: text classification, case-based reasoning, and question answering.', 'Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed.', 'The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches.', 'However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts).']",1,"['There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #AUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', 'However, there is no attempt to automatically generate a single response.', 'Bickel and Scheffer (2004) compared the performance of document retrieval and document prediction for generating help-desk responses.', 'Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes.', 'The generated response is the answer that is closest to the centroid of the cluster.', 'However, it is worth noting that their corpus is significantly smaller than ours (805 question-answer pairs), their questions seem to be much simpler and shorter than those in our corpus, and the replies shorter and more homogeneous.', 'Malik, Subramaniam, and Kaushik (2007) developed a system that builds questionanswer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer.', 'Lapalme and Kosseim (2003) investigated three approaches to the automatic generation of response e-mails: text classification, case-based reasoning, and question answering.']"
CC627,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,automatic question answering using the web beyond the factoid,"['R Soricut', 'E Brill']",,"In this paper we describe and evaluate a Question Answering (QA) system that goes beyond answering factoid questions. Our approach to QA assumes no restrictions on the type of questions that are handled, and no assumption that the answers to be provided are factoids. We present an unsupervised approach for collecting question and answer pairs from FAQ pages, which we use to collect a corpus of 1 million question/answer pairs from FAQ pages available on the Web. This corpus is used to train various statistical models employed by our QA system: a statistical chunker used to transform a natural language-posed question into a phrase-based query to be submitted for exact match to an off-the-shelf search engine; an answer/question translation model, used to assess the likelihood that a proposed answer is indeed an answer to the posed question; and an answer language model, used to assess the likelihood that a proposed answer is a well-formed answer. We evaluate our QA system in a modular fashion, by comparing the performance of baseline algorithms against our proposed algorithms for various modules in our QA system. The evaluation shows that our system achieves reasonable performance in terms of answer accuracy for a large variety of complex, non-factoid questions.","#AUTHOR_TAG compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval .","['In FAQs,  employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document.', 'They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', 'Jijkoun and de Rijke ( 2005) compared different variants of retrieval techniques.', '#AUTHOR_TAG compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval .', 'Two significant differences between help-desk and FAQs are the following.']",1,"['In FAQs,  employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document.', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', 'Jijkoun and de Rijke ( 2005) compared different variants of retrieval techniques.', '#AUTHOR_TAG compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval .']"
CC628,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,automating helpdesk responses a comparative study of informationgathering approaches,"['Y Marom', 'I Zukerman']",method,,"6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( #AUTHOR_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results .","[""We use a Support Vector Machine (SVM) with a Radial Basis Function kernel to predict SCs from users' requests. 7"", 'A separate SVM is trained for each SC, with unigram and bigram lemmas in a request as input features, and a binary target feature specifying whether the SC contains a sentence from the response to this request.', '6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( #AUTHOR_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results .', '7 We employed the LIBSVM package (Chang and Lin 2001).', 'prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3.', 'We then apply the following steps.']",5,"[""We use a Support Vector Machine (SVM) with a Radial Basis Function kernel to predict SCs from users' requests. 7"", '6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( #AUTHOR_TAG ) , but the simple binary bag-of-lemmas representation yielded similar results .', '7 We employed the LIBSVM package (Chang and Lin 2001).', 'We then apply the following steps.']"
CC629,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,hybrid recommender systems user modeling and useradapted interaction,['R Burke'],,,"They also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #AUTHOR_TAG as follows .","['In addition to the different response-generation methods, we have proposed a metalevel strategy to combine them.', 'This kind of meta-learning is referred to as stacking by the Data Mining community (Witten and Frank 2000).', 'Lekakos and Giaglis (2007) implemented a supervised version of this approach for a recommender system, as opposed to our unsupervised version.', 'They also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #AUTHOR_TAG as follows .', ""The merging category corresponds to techniques where the individual methods affect each other in different ways (this category encompasses Burke's feature combination, cascade, feature augmentation, and meta-level sub-categories)."", ""The ensemble category corresponds to techniques where the predictions of the individual methods are combined to produce a final prediction (this category encompasses Burke's weighted, switching, and mixed sub-categories).""]",0,"['Lekakos and Giaglis (2007) implemented a supervised version of this approach for a recommender system, as opposed to our unsupervised version.', 'They also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #AUTHOR_TAG as follows .']"
CC630,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,evaluation of a largescale email response system,"['Y Marom', 'I Zukerman']",method,,"In Section 5 , we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system ( #AUTHOR_TAGb ) .","['Our experimental set-up is designed to evaluate the ability of the different responsegeneration methods to address unseen request e-mails.', 'In particular, we want to determine the applicability of our methods to different situations, namely, whether different requests are addressed only by some methods, or whether there is a significant overlap between the methods.', 'Our evaluation is performed by measuring the quality of the generated responses.', 'Quality is a subjective measure, which is best judged by the users of the system (i.e., the help-desk customers or operators).', 'In Section 5 , we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system ( #AUTHOR_TAGb ) .', 'However, our more comprehensive evaluation is an automatic one that treats the responses generated by the help-desk operators as model responses, and performs text-based comparisons between the model responses and the automatically generated ones.']",5,"['Our experimental set-up is designed to evaluate the ability of the different responsegeneration methods to address unseen request e-mails.', 'In Section 5 , we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system ( #AUTHOR_TAGb ) .']"
CC631,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,applying casebased reasoning techniques for enterprise systems,['I Watson'],introduction,"Chapter 1: What Is Case-Based Reasoning? Chapter 2: Understanding CBR Chapter 3: The Application of CBR Chapter 4: Industrial Applications of CBR Chapter 5: CBR and Customer Service Chapter 6: CBR Software Tools Chapter 7: Building a Diagnostic Case-Base Chapter 8: Building, Testing, and Maintaining Case-Bases Chapter 9: Conclusion Chapter 10: Bibliography",It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; #AUTHOR_TAG ; Delic and Lahaix 1998 ) .,"['However, even the automation of responses to the ""easy"" problems is a difficult task.', 'Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; #AUTHOR_TAG ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']",0,"['It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; #AUTHOR_TAG ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']"
CC632,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,bridging the lexical chasm statistical approaches to answerfinding,"['A Berger', 'R Caruana', 'D Cohn', 'D Freitag', 'V Mittal']",,,#AUTHOR_TAG compared two retrieval approaches ( TF.IDF and query expansion ) and two predictive approaches ( statistical translation and latent variable models ) .,"['In FAQs,  employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document.', 'They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', '#AUTHOR_TAG compared two retrieval approaches ( TF.IDF and query expansion ) and two predictive approaches ( statistical translation and latent variable models ) .', 'Jijkoun and de Rijke ( 2005) compared different variants of retrieval techniques.', 'Soricut and Brill (2006) compared a predictive approach (statistical translation), a retrieval approach based on a language-model, and a hybrid approach which combines statistical chunking and traditional retrieval.', 'Two significant differences between help-desk and FAQs are the following.']",0,"['In FAQs,  employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document.', '#AUTHOR_TAG compared two retrieval approaches ( TF.IDF and query expansion ) and two predictive approaches ( statistical translation and latent variable models ) .', 'Soricut and Brill (2006) compared a predictive approach (statistical translation), a retrieval approach based on a language-model, and a hybrid approach which combines statistical chunking and traditional retrieval.']"
CC633,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,a hybrid approach for improving predictive accuracy of collaborative filtering algorithms user modeling and useradapted interaction,"['G Lekakos', 'G M Giaglis']",,,"Following #AUTHOR_TAG , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .","['Following #AUTHOR_TAG , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .', 'However, in our situation, there is not always one single winner (two methods can perform similarly well for a given request), and there are different ways to pick winners (for example, based on F-score or precision).', 'Therefore, such an approach would require the utilization of subjective heuristics for creating labels, which would significantly influence what is being learned.', 'Instead, we adopt an unsupervised approach that finds patterns in the data-confidence values coupled with performance scores (Section 6.1)-and then attempts to fit unseen data to these patterns (Section 6.2).', 'Heuristics are still needed in order to decide which response-generation method to apply to an unseen case, but they are applied only after the learning is complete (Section 6.3).', 'In other words, the subjective process of setting performance criteria (which should be conducted by the organization running the helpdesk) does not influence the machine learning process.']",1,"['Following #AUTHOR_TAG , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .', 'However, in our situation, there is not always one single winner (two methods can perform similarly well for a given request), and there are different ways to pick winners (for example, based on F-score or precision).', 'Therefore, such an approach would require the utilization of subjective heuristics for creating labels, which would significantly influence what is being learned.', 'Instead, we adopt an unsupervised approach that finds patterns in the data-confidence values coupled with performance scores (Section 6.1)-and then attempts to fit unseen data to these patterns (Section 6.2).', 'Heuristics are still needed in order to decide which response-generation method to apply to an unseen case, but they are applied only after the learning is complete (Section 6.3).', 'In other words, the subjective process of setting performance criteria (which should be conducted by the organization running the helpdesk) does not influence the machine learning process.']"
CC634,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,automatic generation of domain models for callcenters from noisy transcriptions,"['S Roy', 'L V Subramaniam']",method,"Call centers handle customer queries from various domains such as computer sales and support, mobile phones, car rental, etc. Each such domain generally has a domain model which is essential to handle customer complaints. These models contain common problem categories, typical customer issues and their solutions, greeting styles. Currently these models are manually created over time. Towards this, we propose an unsupervised technique to generate domain models automatically from call transcriptions. We use a state of the art Automatic Speech Recognition system to transcribe the calls between agents and customers, which still results in high word error rates (40%) and show that even from these noisy transcriptions of calls we can automatically build a domain model. The domain model is comprised of primarily a topic taxonomy where every node is characterized by topic(s), typical Questions-Answers (Q&As), typical actions and call statistics. We show how such a domain model can be used for topic identification of unseen calls. We also propose applications for aiding agents while handling calls and for agent monitoring based on the domain model.",â¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .,['â\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .'],1,['â\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #AUTHOR_TAG ) .']
CC635,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,sentence fusion for multidocument news summarization,"['R Barzilay', 'K R McKeown']",method,"A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without extensive reading. In this article, we introduce sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents. Sentence fusion involves bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence. Sentence fusion moves the summarization field from the use of purely extractive methods to the generation of abstracts that contain sentences not found in any of the input documents and can synthesize information across sources.","In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .","['As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses.', 'An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response.', 'This task can be cast as extractive multi-document summarization.', 'Unlike a document reuse approach, sentence-level approaches need to consider issues of discourse coherence in order to ensure that the extracted combination of sentences is coherent or at least understandable.', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .']",0,"['This task can be cast as extractive multi-document summarization.', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #AUTHOR_TAG ) .']"
CC636,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,detection of questionanswer pairs in email conversations,"['L Shrestha', 'K R McKeown']",,,"Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).","['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).', 'An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries.', 'In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information.', 'Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses.']",1,"['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #AUTHOR_TAG ) , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).']"
CC637,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,decision graphs—an extension of decision trees,['J J Oliver'],method,"In this paper, we examine Decision Graphs, a generalization of decision trees. We present an inference scheme to construct decision graphs using the Minimum Message Length Principle. Empirical tests demonstrate that this scheme compares favourably with other decision tree inference schemes. This work provides a metric for comparing the relative merit of the decision tree and decision graph formalisms for a particular domain.","Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .","['The focus of our work is on the general applicability of the different response automation methods, rather than on comparing the performance of particular implementation techniques.', 'Hence, throughout the course of this project, the different methods had minor implementational variations, which do not affect the overall insights of this research.', 'Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .', 'Because this variation was uniformly implemented for both approaches, it does not affect their relative performance.', 'These methodological variations are summarized in Table 2.']",5,"['Specifically , we used Decision Graphs ( #AUTHOR_TAG ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .']"
CC638,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,automatic evaluation of summaries using ngram cooccurrence statistics,"['C Y Lin', 'E H Hovy']",method,,"13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .","['13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .']",5,"['13 We also employed sequence-based measures using the ROUGE tool set ( #AUTHOR_TAG ) , with similar results to those obtained with the word-by-word measures .']"
CC639,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,an information measure for classification,"['C S Wallace', 'D M Boulton']",,"1. The class to which each thing belongs. 2. The average properties of each class. 3. The deviations of each thing from the average properties of its parent class. If the things are found to be concentrated in a small area of the region of each class in the measurement space then the deviations will be small, and with reference to the average class properties most of the information about a thing is given by naming the class to which it belongs. In this case the information may be recorded much more briefly than if a classification had not been used. We suggest that the best classification is that which results in the briefest recording of all the attribute information. In this context, we will regard the measurements of each thing as being a message about that thing. Shannon (1948) showed that where messages may be regarded as each nominating the occurrence of a particular event among a universe of possible events, the information needed to record a series of such messages is minimised if the messages are encoded so that the length of each message is proportional to minus the logarithm of the relative frequency of occurrence of the event which it nominates. The information required is greatest when all frequencies are equal. The messages here nominate the positions in measurement space of the 5 1 points representing the attributes of the things. If the expected density of points in the measurement space is everywhere uniform, the positions of the points cannot be encoded more briefly than by a simple list of the measured values. However, if the expected density is markedly non-uniform, application",We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .,"['We train the system by clustering the ""experiences"" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).', 'We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', 'Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.', 'shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15', 'These clusters were chosen because they illustrate clearly three situations of interest.']",5,"['We train the system by clustering the ""experiences"" of the response-generation methods in addressing requests, where each experience is characterized by the value of the confidence measure employed by a method and its subsequent performance, reflected by precision and recall (Equations ( 7) and ( 8), respectively).', 'We then use the program Snob ( #AUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', 'Figure 8(a) is a projection of the centroids of the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (PCA)-these dimensions account for 95% of the variation in the data.', 'shows the (unprojected) centroid values of three of the clusters (the top part of the figure will be discussed subsequently). 15', 'These clusters were chosen because they illustrate clearly three situations of interest.']"
CC640,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,evaluation of a largescale email response system,"['Y Marom', 'I Zukerman']",method,,"In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .","['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .', 'Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator.', 'Our user study assessed the response-generation methods from the following perspectives, which yield information that is beyond the F-score and precision measures obtained in the automatic evaluation.']",5,"['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system ( #AUTHOR_TAGa ) .']"
CC641,J09-4010,An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain,in question answering two heads are better than one,"['J Chu-Carroll', 'K Czuba', 'J M Prager', 'A Ittycheriah']",,"Motivated by the success of ensemble methods  in machine learning and other areas of natural  language processing, we developed a multistrategy  and multi-source approach to question  answering which is based on combining the results  from different answering agents searching  for answers in multiple corpora. The answering  agents adopt fundamentally different strategies,  one utilizing primarily knowledge-based  mechanisms and the other adopting statistical  techniques. We present our multi-level answer  resolution algorithm that combines results from  the answering agents at the question, passage,  and/or answer levels. Experiments evaluating  the effectiveness of our answer resolution algorithm  show a 35.0% relative improvement over  our baseline system in the number of questions  correctly answered, and a 32.8% improvement  according to the average precision metric","The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) .","['Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome.', ""More specifically, it belongs to Burke's switching sub-category, where a single method is selected on a case-by-case basis."", ""A similar approach is taken in Rotaru and Litman's (2005) reading comprehension system, but their system does not perform any learning."", 'Instead it uses a voting mechanism to select the answer given by the majority of methods.', ""The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) ."", 'Because the results of all the methods are comparable, no learning is required: At each stage of the ""cascade of methods,"" the method that performs best is selected.', 'In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics.', 'Therefore, we need to learn from experience when to use each method.']",1,"[""The question answering system developed by #AUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) .""]"
CC642,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,paraphrasing with bilingual parallel corpora,"['Colin Bannard', 'Chris Callison-Burch']",introduction,"Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrase-based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments.","But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( #AUTHOR_TAG ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .","['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( #AUTHOR_TAG ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']",4,"['But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( #AUTHOR_TAG ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']"
CC643,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,nonlinear programming 2nd edition athena scientific,['Dimitri P Bertsekas'],,,"Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #AUTHOR_TAG ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .","['Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here η is an optimization precision, α is a step size chosen with the strong Wolfe's rule (Nocedal and Wright 1999)."", 'Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #AUTHOR_TAG ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .', 'In practice this only happens at the start of optimization and we use a sub-gradient for the first direction.']",5,"['We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here e is an optimization precision, a is a step size chosen with the strong Wolfe's rule (Nocedal and Wright 1999)."", 'Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( #AUTHOR_TAG ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .', 'In practice this only happens at the start of optimization and we use a sub-gradient for the first direction.']"
CC644,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,wordbased alignment phrasebased translation whats the link,"['Adam Lopez', 'Philip Resnik']",,,See #AUTHOR_TAG for further discussion .,"['We now investigate whether our alignments produce improvements in an end-to-end phrase-based machine translation system.', 'We use a state-of-the-art machine translation system,5 and follow the experimental setup used for the 2008 shared task on machine translation (ACL 2008 Third Workshop on Statistical Machine Translation).', 'The full pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and filter long sentences); (2) build language models; (3) create word alignments in each direction; (4) symmetrize directional word alignments; (5) build phrase table; (6) tune weights for the phrase table.', 'For more details consult the shared task description.6', 'To evaluate the quality of the produced alignments, we keep the pipeline unchanged, and use the models described earlier to generate the word alignments in Step 3.', 'For Step 4, we use the soft union symmetrization heuristic.', 'Symmetrization has almost no effect on alignments produced by S-HMM, but we use it for uniformity in the experiments.', 'We tested three values of the threshold (0.2, 0.4, 0.6) which try to capture different tradeoffs of precision vs. recall, and pick the best according to the translation performance on development data.', 'Table 2 summarizes the results for the different corpora.', 'For refer- ence we include IBM Model 4 as suggested in the task description.', 'PR training always outperforms EM training and outperforms IBM Model 4 in all but one experiment.', 'Differences in BLEU range from 0.2 to 0.9.', 'The two constraints help to a different extent for different corpora and translation directions, in a somewhat unpredictable manner.', 'In general our impression is that the connection between alignment quality and BLEU scores is complicated, and changes are difficult to explain and justify.', 'The number of iterations for MERT optimization to converge varied from 2 to 28; and the best choice of threshold on the development set did not always correspond to the best on the test set.', 'Contrary to conventional wisdom in the MT community, bigger phrase tables did not always perform better.', 'In 14 out of 18 cases, the threshold picked was 0.4 (medium size phrase tables) and the other four times 0.2 was picked (smaller phrase tables).', 'When we include only high confidence alignments, more phrases are extracted but many of these are erroneous.', 'Potentially this leads to a poor estimate of the phrase probabilities.', 'See #AUTHOR_TAG for further discussion .']",0,"['We now investigate whether our alignments produce improvements in an end-to-end phrase-based machine translation system.', 'We use a state-of-the-art machine translation system,5 and follow the experimental setup used for the 2008 shared task on machine translation (ACL 2008 Third Workshop on Statistical Machine Translation).', 'The full pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and filter long sentences); (2) build language models; (3) create word alignments in each direction; (4) symmetrize directional word alignments; (5) build phrase table; (6) tune weights for the phrase table.', 'For more details consult the shared task description.6', 'To evaluate the quality of the produced alignments, we keep the pipeline unchanged, and use the models described earlier to generate the word alignments in Step 3.', 'Symmetrization has almost no effect on alignments produced by S-HMM, but we use it for uniformity in the experiments.', 'We tested three values of the threshold (0.2, 0.4, 0.6) which try to capture different tradeoffs of precision vs. recall, and pick the best according to the translation performance on development data.', 'Table 2 summarizes the results for the different corpora.', 'The two constraints help to a different extent for different corpora and translation directions, in a somewhat unpredictable manner.', 'Contrary to conventional wisdom in the MT community, bigger phrase tables did not always perform better.', 'In 14 out of 18 cases, the threshold picked was 0.4 (medium size phrase tables) and the other four times 0.2 was picked (smaller phrase tables).', 'When we include only high confidence alignments, more phrases are extracted but many of these are erroneous.', 'Potentially this leads to a poor estimate of the phrase probabilities.', 'See #AUTHOR_TAG for further discussion .']"
CC645,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,a systematic comparison of various statistical alignment models,"['Franz Josef Och', 'Hermann Ney']",introduction,"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.","Many researchers use the GIZA + + software package ( #AUTHOR_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency .","['IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models, which produce the target sentence one target word at a time by choosing a source word and generating its translation.', 'IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deficient, intractable models that require local heuristic search and are difficult to implement and extend.', 'Many researchers use the GIZA + + software package ( #AUTHOR_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency .', 'All of the models are asymmetric (switching target and source languages produces drastically different results) and the simpler models (IBM Models 1, 2, and HMM) do not enforce bijectivity (the majority of words translating as a single word).', 'Although there are systematic translation phenomena where one cannot hope to obtain 1-to-1 alignments, we observe that in over 6 different European language pairs the majority of alignments are in fact 1-to-1 (86-98%).', 'This leads to the common practice of post-processing heuristics for intersecting directional alignments to produce nearly bijective and symmetric results (Koehn, Och, and Marcu 2003).']",0,"['Many researchers use the GIZA + + software package ( #AUTHOR_TAG ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency .']"
CC646,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,numerical optimization,"['Jorge Nocedal', 'Stephen J Wright']",,"Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems. For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side.","Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule ( #AUTHOR_TAG ) .","['Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule ( #AUTHOR_TAG ) ."", 'Here, β∇(λ) represents an ascent direction chosen as follows: For inequality constraints, it is the projected gradient (Bertsekas 1999); for equality constraints with slack, we use conjugate gradient (Nocedal and Wright 1999), noting that when λ = 0, the objective is not differentiable.', 'In practice this only happens at the start of optimization and we use a sub-gradient for the first direction.']",5,"['We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule ( #AUTHOR_TAG ) ."", 'Here, b(l) represents an ascent direction chosen as follows: For inequality constraints, it is the projected gradient (Bertsekas 1999); for equality constraints with slack, we use conjugate gradient (Nocedal and Wright 1999), noting that when l = 0, the objective is not differentiable.', 'In practice this only happens at the start of optimization and we use a sub-gradient for the first direction.']"
CC647,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,getting the structure right for word alignment leaf,"['Alexander Fraser', 'Daniel Marcu']",,"Word alignment is the problem of annotating parallel text with translational correspondence. Previous generative word alignment models have made structural assumptions such as the 1-to-1, 1-to-N, or phrase-based consecutive word assumptions, while previous discriminative models have either made such an assumption directly or used features derived from a generative model making one of these assumptions. We present a new generative alignment model which avoids these structural limitations, and show that it is effective when trained using both unsupervised and semi-supervised training methods.","This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #AUTHOR_TAG ) .","['Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments.', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #AUTHOR_TAG ) .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', ""Instead, we propose to use a learning framework called Posterior Regularization (Graça, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model's posteriors."", 'The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model).', 'Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior.', 'On the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable.', 'For example, enforcing that each hidden state of an HMM model should be used at most once per sentence would break the Markov property and make the model intractable.', 'In contrast, we will show how to enforce the constraint that each hidden state is used at most once in expectation.', 'The underlying model remains unchanged, but the learning method changes.', 'During learning, our method is similar to the EM algorithm with the addition of solving an optimization problem similar to a maximum entropy problem inside the E Step.', 'The following subsections present the Posterior Regularization framework, followed by a description of how to encode two pieces of prior information aimed at solving the problems described at the end of Section 2.']",1,"['Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments.', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model ( #AUTHOR_TAG ) .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', ""Instead, we propose to use a learning framework called Posterior Regularization (Graca, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model's posteriors."", 'The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model).', 'Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior.', 'On the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable.', 'For example, enforcing that each hidden state of an HMM model should be used at most once per sentence would break the Markov property and make the model intractable.', 'In contrast, we will show how to enforce the constraint that each hidden state is used at most once in expectation.', 'The underlying model remains unchanged, but the learning method changes.']"
CC648,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,moses open source toolkit for statistical machine translation,"['Hieu Hoang', 'Alexandra Birch', 'Chris Callison-Burch', 'Richard Zens', 'Rwth Aachen', 'Alexandra Constantin', 'Marcello Federico', 'Nicola Bertoldi', 'Chris Dyer', 'Brooke Cowan', 'Wade Shen', 'Christine Moran', 'Ondrej Bojar']",,"We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.",5 The open source Moses ( #AUTHOR_TAG ) toolkit from www.statmt.org/moses/ .,['5 The open source Moses ( #AUTHOR_TAG ) toolkit from www.statmt.org/moses/ .'],5,['5 The open source Moses ( #AUTHOR_TAG ) toolkit from www.statmt.org/moses/ .']
CC649,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,building a multilingual parallel subtitle corpus,['J¨org Tiedemann'],,"In this paper on-going work of creating an extensive multilingual parallel corpus of movie subtitles is presented. The corpus currently contains roughly 23,000 pairs of aligned subtitles covering about 2,700 movies in 29 languages. Subtitles mainly consist of transcribed speech, sometimes in a very condensed way. Insertions, deletions and paraphrases are very frequent which makes them a challenging data set to work with especially when applying automatic sentence alignment. Standard alignment approaches rely on translation consistency either in terms of length or term translations or a combination of both. In the paper, we show that these approaches are not applicable for subtitles and we propose a new alignment approach based on time overlaps specifically designed for subtitles. In our experiments we obtain a significant improvement of alignment accuracy compared to standard length-based","results are based on a corpus of movie subtitles ( #AUTHOR_TAG ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .","['In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'We used the system proposed by Ganchev, Gillenwater, and Taskar (2009).', 'This system uses a word-aligned corpus and a parser for a resource-rich language (source language) in order to create a parser for a resource-poor language (target language).', 'We consider a parse tree on the source language as a set of dependency edges to be transferred.', 'For each such edge, if both end points are aligned to words in the target language, then the edge is transferred.', 'These edges are then used as weak supervision when training a generative or discriminative dependency parser.', 'In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised parse trees on the target side.', 'By changing the threshold in MBR decoding of alignments, we can trade off accuracy of the transferred edges vs. transferring more edges.', 'We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish.', 'Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es).', 'results are based on a corpus of movie subtitles ( #AUTHOR_TAG ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .', 'We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.']",5,"['In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'results are based on a corpus of movie subtitles ( #AUTHOR_TAG ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .']"
CC650,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,the hiero machine translation system extensions evaluation and analysis,"['David Chiang', 'Adam Lopez', 'Nitin Madnani', 'Christof Monz', 'Philip Resnik', 'Michael Subotin']",introduction,"Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has been largely absent from the best performing machine translation systems in recent community-wide evaluations. In this paper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems.","Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ Galley et al. 2004 ; #AUTHOR_TAG ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).","['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ Galley et al. 2004 ; #AUTHOR_TAG ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001;Hwa et al. 2005;Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).']",0,"['Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ Galley et al. 2004 ; #AUTHOR_TAG ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).']"
CC651,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,minimum bayesrisk word alignments of bilingual texts,"['Shankar Kumar', 'William Byrne']",introduction,"We present Minimum Bayes-Risk word alignment for machine translation. This statistical, model-based approach attempts to minimize the expected risk of alignment errors under loss functions that measure alignment quality. We describe various loss functions, including some that incorporate linguistic analysis as can be obtained from parse trees, and show that these approaches can improve alignments of the English-French Hansards.","Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding ( #AUTHOR_TAG ; Liang , Taskar , and Klein 2006 ; Ganchev , and Taskar 2007 ) .","['Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding ( #AUTHOR_TAG ; Liang , Taskar , and Klein 2006 ; Ganchev , and Taskar 2007 ) .', 'Using this decoding we include an alignment link i − j if the posterior probability that word i aligns to word j is above some threshold.', 'This allows the accumulation of probability from several low-scoring alignments that agree on one alignment link.', 'The threshold is tuned on some small amount of labeled data-in our case the development set-to minimize some loss.', 'Kumar and Byrne (2002) study different loss functions that incorporate linguistic knowledge, and show significant improvement over likelihood decoding.', 'Note that this could potentially result in an alignment having zero probability under the model, as many-to-many alignments can be produced in this way.', 'MBR decoding has several advantages over Viterbi decoding.', 'First, independently of the particular choice of the loss function, by picking a specific threshold we can trade off precision and recall of the predicted word alignments.', 'In fact, in this work when comparing different alignment sets we do not commit to any loss function but instead compare precision vs recall curves, by generating alignments for different thresholds (0..1).', 'Second, with this method we can ignore the null word probabilities, which tend to be poorly estimated.']",5,"['Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding ( #AUTHOR_TAG ; Liang , Taskar , and Klein 2006 ; Ganchev , and Taskar 2007 ) .', 'Using this decoding we include an alignment link i - j if the posterior probability that word i aligns to word j is above some threshold.', 'This allows the accumulation of probability from several low-scoring alignments that agree on one alignment link.', 'The threshold is tuned on some small amount of labeled data-in our case the development set-to minimize some loss.', 'Kumar and Byrne (2002) study different loss functions that incorporate linguistic knowledge, and show significant improvement over likelihood decoding.', 'Note that this could potentially result in an alignment having zero probability under the model, as many-to-many alignments can be produced in this way.', 'First, independently of the particular choice of the loss function, by picking a specific threshold we can trade off precision and recall of the predicted word alignments.', 'In fact, in this work when comparing different alignment sets we do not commit to any loss function but instead compare precision vs recall curves, by generating alignments for different thresholds (0..1).', 'Second, with this method we can ignore the null word probabilities, which tend to be poorly estimated.']"
CC652,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,a systematic comparison of various statistical alignment models,"['Franz Josef Och', 'Hermann Ney']",,"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.","We used a standard implementation of IBM Model 4 ( #AUTHOR_TAG ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves .","['The intent of this experimental section is to evaluate the gains from using constraints during learning, hence the main comparison is between HMM trained with normal EM vs. trained with PR plus constraints.', 'We also report results for IBM Model 4, because it is often used as the default word alignment model, and can be used as a reference.', 'However, we would like to note that IBM Model 4 is a more complex model, able to capture more structure, albeit at the cost of intractable inference.', 'Because our approach is orthogonal to the base model used, the constraints described here could be applied in principle to IBM Model 4 if exact inference was efficient, hopefully yielding similar improvements.', 'We used a standard implementation of IBM Model 4 ( #AUTHOR_TAG ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves .', 'We trained IBM Model 4 using the default configuration of the']",5,"['We also report results for IBM Model 4, because it is often used as the default word alignment model, and can be used as a reference.', 'However, we would like to note that IBM Model 4 is a more complex model, able to capture more structure, albeit at the cost of intractable inference.', 'Because our approach is orthogonal to the base model used, the constraints described here could be applied in principle to IBM Model 4 if exact inference was efficient, hopefully yielding similar improvements.', 'We used a standard implementation of IBM Model 4 ( #AUTHOR_TAG ) and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves .', 'We trained IBM Model 4 using the default configuration of the']"
CC653,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,unsupervised multilingual learning for morphological segmentation,"['Benjamin Snyder', 'Regina Barzilay']",introduction,"For centuries, the deep connection between languages has brought about major discoveries about human communication. In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning. In particular, we study the task of morphological segmentation of multiple languages. We present a nonparametric Bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies cross-lingual morpheme patterns, or abstract morphemes. We apply our model to three Semitic languages: Arabic, Hebrew, Aramaic, as well as to English. Our results demonstrate that learning morphological models in tandem reduces error by up to 24% relative to monolingual models. Furthermore, we provide evidence that our joint model achieves better performance when applied to languages from the same family.","But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( #AUTHOR_TAG ) .","['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( #AUTHOR_TAG ) .']",4,"['But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( #AUTHOR_TAG ) .']"
CC654,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,a systematic comparison of various statistical alignment models,"['Franz Josef Och', 'Hermann Ney']",,"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.",The standard approach is to train two models independently and then intersect their predictions ( #AUTHOR_TAG ) .,"['The directional nature of the generative models used to recover word alignments conflicts with their interpretation as translations.', 'In practice, we see that the choice of which language is source versus target matters and changes the mistakes made by the model (the first row of panels in Figure 1).', 'The standard approach is to train two models independently and then intersect their predictions ( #AUTHOR_TAG ) .', 'However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree.', 'Let the directional models be defined as: − → p ( − → z ) (source-target) and ← − p ( ← − z ) (target-source).', 'We suppress dependence on x and y for brevity.', 'Define z to range over the union of all possible directional alignments − → Z ∪ ← − Z .', 'We define a mixture model p']",1,"['The directional nature of the generative models used to recover word alignments conflicts with their interpretation as translations.', 'The standard approach is to train two models independently and then intersect their predictions ( #AUTHOR_TAG ) .', 'However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree.', 'We suppress dependence on x and y for brevity.', 'Define z to range over the union of all possible directional alignments - - Z  - - Z .']"
CC655,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,a new view of the em algorithm that justifies incremental sparse and other variants,"['Radford M Neal', 'Geoffrey E Hinton']",introduction,"The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.","EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) ( #AUTHOR_TAG ) :","['Because of the latent alignment variables z, the log-likelihood function for the HMM model is not concave, and the model is fit using the Expectation Maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).', 'EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) ( #AUTHOR_TAG ) :']",5,"['Because of the latent alignment variables z, the log-likelihood function for the HMM model is not concave, and the model is fit using the Expectation Maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).', 'EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) ( #AUTHOR_TAG ) :']"
CC656,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,a systematic comparison of various statistical alignment models,"['Franz Josef Och', 'Hermann Ney']",,"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.","This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #AUTHOR_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) .","['Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments.', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #AUTHOR_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', ""Instead, we propose to use a learning framework called Posterior Regularization (Graça, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model's posteriors."", 'The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model).', 'Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior.', 'On the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable.', 'For example, enforcing that each hidden state of an HMM model should be used at most once per sentence would break the Markov property and make the model intractable.', 'In contrast, we will show how to enforce the constraint that each hidden state is used at most once in expectation.', 'The underlying model remains unchanged, but the learning method changes.', 'During learning, our method is similar to the EM algorithm with the addition of solving an optimization problem similar to a maximum entropy problem inside the E Step.', 'The following subsections present the Posterior Regularization framework, followed by a description of how to encode two pieces of prior information aimed at solving the problems described at the end of Section 2.']",1,"['Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #AUTHOR_TAG ) , and more recently by the LEAF model ( Fraser and Marcu 2007 ) .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', ""Instead, we propose to use a learning framework called Posterior Regularization (Graca, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model's posteriors."", 'The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model).', 'Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior.', 'On the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable.', 'In contrast, we will show how to enforce the constraint that each hidden state is used at most once in expectation.', 'The underlying model remains unchanged, but the learning method changes.', 'During learning, our method is similar to the EM algorithm with the addition of solving an optimization problem similar to a maximum entropy problem inside the E Step.', 'The following subsections present the Posterior Regularization framework, followed by a description of how to encode two pieces of prior information aimed at solving the problems described at the end of Section 2.']"
CC657,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,hmm word and phrase alignment for statistical machine translation,"['Yonggang Deng', 'William Byrne']",related work,"Estimation and alignment procedures for word and phrase alignment hidden Markov models (HMMs) are developed for the alignment of parallel text. The development of these models is motivated by an analysis of the desirable features of IBM Model 4, one of the original and most effective models for word alignment. These models are formulated to capture the desirable aspects of Model 4 in an HMM alignment formalism. Alignment behavior is analyzed and compared to human-generated reference alignments, and the ability of these models to capture different types of alignment phenomena is evaluated. In analyzing alignment performance, Chinese-English word alignments are shown to be comparable to those of IBM Model 4 even when models are trained over large parallel texts. In translation performance, phrase-based statistical machine translation systems based on these HMM alignments can equal and exceed systems based on Model 4 alignments, and this is shown in Arabic-English and Chinese-English translation. These alignment models can also be used to generate posterior statistics over collections of parallel text, and this is used to refine and extend phrase translation tables with a resulting improvement in translation quality.","In the context of word alignment , #AUTHOR_TAG use a state-duration HMM in order to model word-to-phrase translations .","['The idea of introducing constraints over a model to better guide the learning process has appeared before.', 'In the context of word alignment , #AUTHOR_TAG use a state-duration HMM in order to model word-to-phrase translations .', 'The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant η > 1.', 'This encourages more transitions and hence shorter phrases.', 'For the task of unsupervised dependency parsing, Smith and Eisner (2006) add a constraint of the form ""the average length of dependencies should be X"" to capture the locality of syntax (at least half of the dependencies are between adjacent words), using a scheme they call structural annealing.', ""They modify the model's distribution over trees p θ (y) by a penalty term as: p θ (y) ∝ p θ (y)e (δ e∈y length(e)) , where length(e) is the surface length of edge e."", 'The factor δ changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training.', 'These two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take.', 'However, the approaches differ substantially from PR. Smith and Eisner (2006) make a statement of the form ""scale the total length of edges"", which depending on the value of δ will prefer to have more shorter/longer edges.', 'Such statements are not data dependent.', 'Depending on the value of δ, for instance if δ ≤ 0, even if the data is such that the model already uses too many short edges on average, this value of δ will push for more short edges.', 'By contrast the statements we can make in PR are of the form ""there should be more short edges than long edges"".', 'Such a statement is data-dependent in the sense that if the model satisfies the constraints then we do not need to change it; if it is far from satisfying it we might need to make very dramatic changes.']",0,"['In the context of word alignment , #AUTHOR_TAG use a state-duration HMM in order to model word-to-phrase translations .']"
CC658,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,europarl a parallel corpus for statistical machine translation,['Philipp Koehn'],,"We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.","results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En â Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .","['In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'We used the system proposed by Ganchev, Gillenwater, and Taskar (2009).', 'This system uses a word-aligned corpus and a parser for a resource-rich language (source language) in order to create a parser for a resource-poor language (target language).', 'We consider a parse tree on the source language as a set of dependency edges to be transferred.', 'For each such edge, if both end points are aligned to words in the target language, then the edge is transferred.', 'These edges are then used as weak supervision when training a generative or discriminative dependency parser.', 'In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised parse trees on the target side.', 'By changing the threshold in MBR decoding of alignments, we can trade off accuracy of the transferred edges vs. transferring more edges.', 'We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish.', 'Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es).', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .', 'We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM.']",5,"['In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.', 'This system uses a word-aligned corpus and a parser for a resource-rich language (source language) in order to create a parser for a resource-poor language (target language).', 'We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and Spanish.', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( #AUTHOR_TAG ) .']"
CC659,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,tailoring word alignments to syntactic machine translation,"['John DeNero', 'Dan Klein']",,"Extracting tree transducer rules for syntactic MT systems can be hindered by word alignment errors that violate syntactic correspondences. We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model. Our model's predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments.",This heuristic is called soft union ( #AUTHOR_TAG ) .,"['As discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'For MT the most commonly used heuristic is called grow diagonal final (Och and Ney 2003).', 'This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.', 'The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.', 'In syntax transfer the intersection heuristic is normally used, because one wants to have high precision links to transfer knowledge between languages.', 'One pitfall of these symmetrization heuristics is that they can obfuscate the link between the original alignment and the ones used for a specific task, making errors more difficult to analyze.', 'Because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance.', 'In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model.', 'We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.', 'This heuristic is called soft union ( #AUTHOR_TAG ) .', 'Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.', 'The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization.', 'This should not be very surprising, because the soft union symmetrization can be viewed as an approximation of our symmetry constraint applied only at decode time.', 'Applying the symmetrization to the model with symmetry constraints does not affect performance.']",5,"['As discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'For MT the most commonly used heuristic is called grow diagonal final (Och and Ney 2003).', 'The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.', 'In syntax transfer the intersection heuristic is normally used, because one wants to have high precision links to transfer knowledge between languages.', 'One pitfall of these symmetrization heuristics is that they can obfuscate the link between the original alignment and the ones used for a specific task, making errors more difficult to analyze.', 'Because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance.', 'We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.', 'This heuristic is called soft union ( #AUTHOR_TAG ) .', 'Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.', 'The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization.']"
CC660,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,bootstrapping parsers via syntactic projection across parallel texts natural language engineering,"['Rebecca Hwa', 'Philip Resnik', 'Amy Weinberg', 'Clara Cabezas', 'Okan Kolak']",introduction,"Broad coverage, high quality parsers are available for only a handful of languages. A prerequisite for developing broad coverage parsers for more languages is the annotation of text with the desired linguistic representations (also known as ""treebanking""). However, syntactic annotation is a labor intensive and time-consuming process, and it is difficult to find linguistically annotated text in sufficient quantities. In this article, we explore using parallel text to help solving the problem of creating syntactic annotation in more languages. The central idea is to annotate the English side of a parallel corpus, project the analysis to the second language, and then train a stochastic analyzer on the resulting noisy annotations. We discuss our background assumptions, describe an initial study on the ""projectability"" of syntactic relations, and then present two experiments in which stochastic parsers are developed with minimal human intervention via projection from English.","But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; #AUTHOR_TAG ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .","['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; #AUTHOR_TAG ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']",4,"['But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; #AUTHOR_TAG ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']"
CC661,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,a systematic comparison of various statistical alignment models,"['Franz Josef Och', 'Hermann Ney']",introduction,"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.","Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #AUTHOR_TAG ) .","['A word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a target language (Brown et al. 1993b).', 'There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one language but not the other (e.g., English He walked and French Il est allé), articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expressions translated indirectly.', 'Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #AUTHOR_TAG ) .', 'The top row of Figure 1 shows two word alignments between an English-French sentence pair.', 'We use the following notation: the alignment on the left (right) will be referenced as source-target (target-source) and contains source (target) words as rows and target (source) words as columns.', 'Each entry in the matrix corresponds to a source-target word pair, and is the candidate for an alignment link.', 'Sure links are represented as squares with borders, and possible links']",0,"['A word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a target language (Brown et al. 1993b).', 'Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( #AUTHOR_TAG ) .', 'We use the following notation: the alignment on the left (right) will be referenced as source-target (target-source) and contains source (target) words as rows and target (source) words as columns.', 'Each entry in the matrix corresponds to a source-target word pair, and is the candidate for an alignment link.']"
CC662,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,a systematic comparison of various statistical alignment models,"['Franz Josef Och', 'Hermann Ney']",,"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.",For MT the most commonly used heuristic is called grow diagonal final ( #AUTHOR_TAG ) .,"['As discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'For MT the most commonly used heuristic is called grow diagonal final ( #AUTHOR_TAG ) .', 'This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.', 'The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.', 'In syntax transfer the intersection heuristic is normally used, because one wants to have high precision links to transfer knowledge between languages.', 'One pitfall of these symmetrization heuristics is that they can obfuscate the link between the original alignment and the ones used for a specific task, making errors more difficult to analyze.', 'Because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance.', 'In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model.', 'We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.', 'This heuristic is called soft union (DeNero and Klein 2007).', 'Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.', 'The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization.', 'This should not be very surprising, because the soft union symmetrization can be viewed as an approximation of our symmetry constraint applied only at decode time.', 'Applying the symmetrization to the model with symmetry constraints does not affect performance.']",1,"['As discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'For MT the most commonly used heuristic is called grow diagonal final ( #AUTHOR_TAG ) .', 'This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.', 'The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.', 'One pitfall of these symmetrization heuristics is that they can obfuscate the link between the original alignment and the ones used for a specific task, making errors more difficult to analyze.', 'Because they are heuristics tuned for a particular phrasebased translation system, it is not clear when they will help and when they will hinder system performance.', 'In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model.', 'We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.', 'This heuristic is called soft union (DeNero and Klein 2007).', 'Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.', 'The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization.', 'Applying the symmetrization to the model with symmetry constraints does not affect performance.']"
CC663,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,simple robust scalable semisupervised learning via expectation regularization,"['G Mann', 'A McCallum']",related work,"Although semi-supervised learning has been an active area of research, its use in deployed applications is still relatively rare because the methods are often difficult to implement, fragile in tuning, or lacking in scalability. This paper presents expectation regularization, a semi-supervised learning method for exponential family parametric models that augments the traditional conditional label-likelihood objective function with an additional term that encourages model predictions on unlabeled data to match certain expectations---such as label priors. The method is extremely easy to implement, scales as well as logistic regression, and can handle non-independent features. We present experiments on five different data sets, showing accuracy improvements over other semi-supervised methods.","PR is closely related to the work of #AUTHOR_TAG , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning .","['PR is closely related to the work of #AUTHOR_TAG , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning .', 'They call their method generalized expectation (GE) constraints or alternatively expectation regularization.', 'In the original GE framework, the posteriors of the model on unlabeled data are regularized directly.', 'They train a discriminative model, using conditional likelihood on labeled data and an ""expectation regularization"" penalty term on the unlabeled data:']",0,"['PR is closely related to the work of #AUTHOR_TAG , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning .', 'They call their method generalized expectation (GE) constraints or alternatively expectation regularization.', 'In the original GE framework, the posteriors of the model on unlabeled data are regularized directly.', 'They train a discriminative model, using conditional likelihood on labeled data and an ""expectation regularization"" penalty term on the unlabeled data:']"
CC664,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora,"['David Yarowsky', 'Grace Ngai']",introduction,"This paper investigates the potential for projecting linguistic annotations including part-of-speech tags and base noun phrase bracketings from one language to another via automatically word-aligned parallel corpora. First, experiments assess the accuracy of unmodified direct transfer of tags and brackets from the source language English to the target languages French and Chinese, both for noisy machine-aligned sentences and for clean hand-aligned sentences. Performance is then substantially boosted over both of these baselines by using training techniques optimized for very noisy data, yielding 94-96% core French part-of-speech tag accuracy and 90% French bracketing F-measure for stand-alone monolingual tools trained without the need for any human-annotated data in the given language.","But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .","['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of ""word-byword"" alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al. 2004;Chiang et al. 2005]) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']",4,"['But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( #AUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']"
CC665,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,what’s in a translation rule,"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']",introduction,"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.","Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).","['The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1�5) for statistical machine translation and the concept of �word-by- word� alignment, the correspondence between words in source and target languages.', 'Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).']",0,"['Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #AUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).']"
CC666,J10-3007,Learning Tractable Word Alignment Models with Complex Constraints,annealing structural bias in multilingual weighted grammar induction,"['Noah A Smith', 'Jason Eisner']",related work,"We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004). Next, by annealing the free parameter that controls this bias, we achieve further improvements. We then describe an alternative kind of structural bias, toward ""broken "" hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement. We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1-17 % (absolute) over CE (and 8-30% over EM), achieving to our knowledge the best results on this task to date. Our method, structural annealing, is a general technique with broad applicability to hidden-structure discovery problems.","For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing .","['The idea of introducing constraints over a model to better guide the learning process has appeared before.', 'In the context of word alignment, Deng and Byrne (2005) use a state-duration HMM in order to model word-to-phrase translations.', 'The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant η > 1.', 'This encourages more transitions and hence shorter phrases.', ""For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing ."", ""They modify the model's distribution over trees p θ (y) by a penalty term as: p θ (y) ∝ p θ (y)e (δ e∈y length(e)) , where length(e) is the surface length of edge e."", 'The factor δ changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training.', 'These two approaches also have the goal of controlling unsupervised learning, and the form of the modified distributions is reminiscent of the form that the projected posteriors take.', 'However, the approaches differ substantially from PR. Smith and Eisner (2006) make a statement of the form ""scale the total length of edges"", which depending on the value of δ will prefer to have more shorter/longer edges.', 'Such statements are not data dependent.', 'Depending on the value of δ, for instance if δ ≤ 0, even if the data is such that the model already uses too many short edges on average, this value of δ will push for more short edges.', 'By contrast the statements we can make in PR are of the form ""there should be more short edges than long edges"".', 'Such a statement is data-dependent in the sense that if the model satisfies the constraints then we do not need to change it; if it is far from satisfying it we might need to make very dramatic changes.']",0,"[""For the task of unsupervised dependency parsing , #AUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing .""]"
CC667,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,a corpus for modeling morphosyntactic agreement in arabic gender number and rationality,"['Sarah Alkuhlani', 'Nizar Habash']",conclusion,"We present an enriched version of the Penn Arabic Treebank (Maamouri et al., 2004), where latent features necessary for modeling morpho-syntactic agreement in Arabic are manually annotated. We describe our process for efficient annotation, and present the first quantitative analysis of Arabic morpho-syntactic phenomena.","We use the agreement checker code developed by #AUTHOR_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference .","['Grammaticality of parse trees.', 'We now turn to our second type of error analysis, the evaluation of the grammaticality of the parse trees in terms of gender and number agreement patterns.', 'We use the agreement checker code developed by #AUTHOR_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference .', 'The agreement checker verifies, for all verb-nominal subject relations and noun-adjective relations found in the tree, whether the agreement conditions are met or not.', 'The accuracy number reflects the percentage of such relations found which meet the agreement criteria.', 'Note that we use the syntax given by the tree, not the gold syntax.', 'For all three trees, however, we used gold morphological features for this evaluation even when those features were not used in the parsing task.', 'This is because we want to see to what extent the predicted morphological features help find the correct syntactic relations, not whether the predicted trees are intrinsically coherent given possibly false predicted morphology.', 'The results can be found in Table 18.', 'We note that the grammaticality of the gold corpus is not 100%; this is approximately equally due to errors in the checking script and to annotation errors in the gold standard.', 'We take the given grammaticality of the gold corpus as a topline for this analysis.', 'Nominal modification has a smaller error band between baseline and gold compared with subject-verb agreement.', 'We assume this is because subject-verb agreement is more complex (it depends on their relative order), and because nominal modification can have multiple structural targets, only one of which is correct, although all, however, are plausible from the point of view of agreement.', 'The error reduction relative to the gold topline is 62% and 76% for nominal agreement and verb agreement, respectively.', 'Thus, we see that our second hypothesis-that the use of morphological features will reduce grammaticality errors in the resulting parse trees with respect to agreement phenomena-is borne out.']",5,"['Grammaticality of parse trees.', 'We now turn to our second type of error analysis, the evaluation of the grammaticality of the parse trees in terms of gender and number agreement patterns.', 'We use the agreement checker code developed by #AUTHOR_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference .', 'The agreement checker verifies, for all verb-nominal subject relations and noun-adjective relations found in the tree, whether the agreement conditions are met or not.', 'Note that we use the syntax given by the tree, not the gold syntax.', 'For all three trees, however, we used gold morphological features for this evaluation even when those features were not used in the parsing task.', 'This is because we want to see to what extent the predicted morphological features help find the correct syntactic relations, not whether the predicted trees are intrinsically coherent given possibly false predicted morphology.', 'We note that the grammaticality of the gold corpus is not 100%; this is approximately equally due to errors in the checking script and to annotation errors in the gold standard.', 'We take the given grammaticality of the gold corpus as a topline for this analysis.', 'Nominal modification has a smaller error band between baseline and gold compared with subject-verb agreement.', 'We assume this is because subject-verb agreement is more complex (it depends on their relative order), and because nominal modification can have multiple structural targets, only one of which is correct, although all, however, are plausible from the point of view of agreement.', 'The error reduction relative to the gold topline is 62% and 76% for nominal agreement and verb agreement, respectively.', 'Thus, we see that our second hypothesis-that the use of morphological features will reduce grammaticality errors in the resulting parse trees with respect to agreement phenomena-is borne out.']"
CC668,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,buckwalter arabic morphological analyzer version 20 linguistic data consortium,['Timothy A Buckwalter'],experiments,,"In comparison, the tag set of the Buckwalter Morphological Analyzer ( #AUTHOR_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8","['Linguistically, words have associated POS tags, e.g., �verb� or �noun,� which further abstract over morphologically and syntactically similar lexemes.', 'Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles.', 'In comparison, the tag set of the Buckwalter Morphological Analyzer ( #AUTHOR_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8', 'Cross-linguistically, a core set containing around 12 tags is often assumed as a �universal tag set� (Rambow et al. 2006; Petrov, Das, and McDonald 2012).', 'We have adapted the list from Rambow et al. (2006) for Arabic, and call it here CORE12.', 'It contains the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun (PN), pronoun (PRO), relative pronoun (REL), preposition (P), conjunction (C), particle (PRT), abbreviation (AB), and punctuation (PNX).', 'The CATIB6 tag set can be viewed as a further reduction, with the exception that CATIB6 contains a passive voice tag (a mor- phological feature); this tag constitutes only 0.5% of the tags in the training, however.']",1,"['Traditional Arabic grammars often describe a very general three-way distinction into verbs, nominals, and particles.', 'In comparison, the tag set of the Buckwalter Morphological Analyzer ( #AUTHOR_TAG ) used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8']"
CC669,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,better arabic parsing baselines evaluations and analysis,"['Spence Green', 'Christopher D Manning']",conclusion,"In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature. Second, we show that although the Penn Arabic Treebank is similar to other tree-banks in gross statistical terms, annotation consistency remains problematic. Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG. Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2--5% F1.","For better comparison with work of others , we adopt the suggestion made by #AUTHOR_TAG to evaluate the parsing quality on sentences up to 70 tokens long .","['For better comparison with work of others , we adopt the suggestion made by #AUTHOR_TAG to evaluate the parsing quality on sentences up to 70 tokens long .', 'We report these filtered results in Table 14.', 'Filtered results are consistently higher (as expected).', 'Results are about 0.9% absolute higher on the development set, and about 0.6% higher on the test set.', 'The contribution of the RAT feature across sets is negligible (or small and unstable), resulting in less than 0.1% absolute loss on the dev set, but about 0.15% gain on the test set.', 'For clarity and conciseness, we only show the best model (with RAT) in Table 14.']",5,"['For better comparison with work of others , we adopt the suggestion made by #AUTHOR_TAG to evaluate the parsing quality on sentences up to 70 tokens long .']"
CC670,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,algorithms for deterministic incremental dependency parsing,['Joakim Nivre'],related work,"Abstract Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.","As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; #AUTHOR_TAG ) and the CATiB ( Habash and Roth 2009 ) .","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; #AUTHOR_TAG ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']",0,"['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; #AUTHOR_TAG ) and the CATiB ( Habash and Roth 2009 ) .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']"
CC671,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,catib the columbia arabic treebank,"['Nizar Habash', 'Ryan Roth']",experiments,"The Columbia Arabic Treebank (CATiB) is a resource for Arabic parsing. CATiB contrasts with previous efforts on Arabic treebanking and treebanking of morphologically rich languages in that it encodes less linguistic information in the interest of speedier annotation of large amounts of text. This paper describes CATiB's representation and annotation procedure, and reports on achieved inter-annotator agreement and annotation speed",We use the Columbia Arabic Treebank ( CATiB ) ( #AUTHOR_TAG ) .,"['We use the Columbia Arabic Treebank ( CATiB ) ( #AUTHOR_TAG ) .', 'Specifically, we use the portion converted from Part 3 of the PATB to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information.', ""CATiB's dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations."", 'It has a reduced POS tag set consisting of six tags only (henceforth CATIB6).', 'The tags are: NOM (non-proper nominals including nouns, pronouns, adjectives, and adverbs), PROP (proper nouns), VRB (active-voice verbs), VRB-PASS (passive-voice verbs), PRT (particles such as prepositions or conjunctions), and PNX (punctuation).', 'CATiB uses a standard set of eight dependency relations: SBJ and OBJ for subject and (direct or indirect) object, respectively (whether they appear pre-or postverbally); IDF for the idafa (possessive) relation; MOD for most other modifications; and other less common relations that we will not discuss here.', 'For other PATB-based POS tag sets, see Sections 2.6 and 2.7.']",5,['We use the Columbia Arabic Treebank ( CATiB ) ( #AUTHOR_TAG ) .']
CC672,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,a statistical parser for czech,"['Michael Collins', 'Jan Hajic', 'Lance Ramshaw', 'Christoph Tillmann']",introduction,"This paper considers statistical parsing of Czech, which differs radically from English in at least two respects: (1) it is a highly inflected language, and (2) it has relatively free word order. These differences are likely to pose new problems for techniques that have been developed on English. We describe our experience in building on the parsing model of (Collins 97). Our final results- 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text.","For example , modeling CASE in Czech improves Czech parsing ( #AUTHOR_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .","['Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors.', 'In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'For example , modeling CASE in Czech improves Czech parsing ( #AUTHOR_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .', 'It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;.']",4,"['Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors.', 'In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'For example , modeling CASE in Czech improves Czech parsing ( #AUTHOR_TAG ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .']"
CC673,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,the penn arabic treebank building a largescale annotated arabic corpus,"['Mohamed Maamouri', 'Ann Bies', 'Timothy A Buckwalter', 'Wigdan Mekki']",related work,"From our three year experience of developing a large-scale corpus of annotated Arabic text, our paper will address the following: (a) review pertinent Arabic language issues as they relate to methodology choices, (b) explain our choice to use the Penn English Treebank style of guidelines, (requiring the Arabic-speaking annotators to deal with a new grammatical system) rather than doing the annotation in a more traditional Arabic grammar style (requiring NLP researchers to deal with a new system); (c) show several ways in which human annotation is important and automatic analysis difficult, including the handling of orthographic ambiguity by both the morphological analyzer and human annotators; (d) give an illustrative example of the Arabic Treebank methodology, focusing on a particular construction in both morphological analysis and tagging and syntactic analysis and following it in detail through the entire annotation process, and finally, (e) conclude with what has been achieved so far and what remains to be done.","For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 ( #AUTHOR_TAG ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 .","['For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 ( #AUTHOR_TAG ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 .', 'We used the same training / devtest split as in Zitouni, Sorensen, and Sarikaya (2006); and we further split the devtest into two equal parts: a development (dev) set and a blind test set.', 'For all experiments, unless specified otherwise, we used the dev set. 10 We kept the test unseen (""blind"") during training and model development.', 'Statistics about this split (after conversion to the CATiB dependency format) are given in Table 1.']",5,"['For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 ( #AUTHOR_TAG ) , converted to the CATiB Treebank format , as mentioned in Section 2.5 .']"
CC674,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,a corpus for modeling morphosyntactic agreement in arabic gender number and rationality,"['Sarah Alkuhlani', 'Nizar Habash']",related work,"We present an enriched version of the Penn Arabic Treebank (Maamouri et al., 2004), where latent features necessary for modeling morpho-syntactic agreement in Arabic are manually annotated. We describe our process for efficient annotation, and present the first quantitative analysis of Arabic morpho-syntactic phenomena.","18 In this article , we use a newer version of the corpus by #AUTHOR_TAG than the one we used in Marton , Habash , and Rambow ( 2011 ) .","['18 In this article , we use a newer version of the corpus by #AUTHOR_TAG than the one we used in Marton , Habash , and Rambow ( 2011 ) .']",5,"['18 In this article , we use a newer version of the corpus by #AUTHOR_TAG than the one we used in Marton , Habash , and Rambow ( 2011 ) .']"
CC675,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,identifying broken plurals irregular gender and rationality in arabic text,"['Sarah Alkuhlani', 'Nizar Habash']",experiments,"Arabic morphology is complex, partly because of its richness, and partly because of common irregular word forms, such as broken plurals (which resemble singular nouns), and nouns with irregular gender (feminine nouns that look masculine and vice versa). In addition, Arabic morpho-syntactic agreement interacts with the lexical semantic feature of rationality, which has no morphological realization. In this paper, we present a series of experiments on the automatic prediction of the latent linguistic features of functional gender and number, and rationality in Arabic. We compare two techniques, using simple maximum likelihood (MLE) with back-off and a support vector machine based sequence tagger (Yamcha). We study a number of orthographic, morphological and syntactic learning features. Our results show that the MLE technique is preferred for words seen in the training data, while the Yam-cha technique is optimal for unseen words, which are our real target. Furthermore, we show that for unseen words, morphological features help beyond orthographic features and that syntactic features help even more. A combination of the two techniques improves overall performance even further.","In this article , we use an in-house system which provides functional gender , number , and rationality features ( #AUTHOR_TAG ) .","['Some inflectional features, specifically gender and number, are expressed using different morphemes in different words (even within the same POS).', 'There are four sound gender-number suffixes in Arabic: 5 +φ (null morpheme) for masculine singular, + + for feminine singular, + +wn for masculine plural, and + +At for feminine plural.', 'Form-based GENDER and NUMBER feature values are set only according to these four morphemes (and a few others, ignored for simplicity).', 'There are exceptions and alternative ways to express GENDER and NUMBER, however, and functional feature values take them into account: Depending on the lexeme, plurality can be expressed using sound plural suffixes or using a pattern change together with singular suffixes.', ""A sound plural example is the word pair / Hafiyd+a /Hafiyd+At ('granddaughter/granddaughters.)"", ""On the other hand, the plural of the inflectionally and morphemically feminine singular word madras+a ('school') is the word madAris+φ ('schools'), which is feminine and plural inflectionally, but has a masculine singular suffix."", 'This irregular inflection, known as broken plural, is similar to the English mouse/mice, but is much more common in Arabic (over 50% of plurals in our training data).', ""A similar inconsistency appears in feminine nominals that are not inflected using sound gender suffixes, for example, the feminine form of the masculine singular adjective Âzraq+φ ('blue') is zarqA'+φ not * *Âzraq+a ."", 'To address this inconsistency in the correspondence between inflectional features and morphemes, and inspired by Smrž (2007), we distinguish between two types of inflectional features: formbased (a.k.a.', 'surface, or illusory) features and functional features. 6', 'ost available Arabic NLP tools and resources model morphology using formbased (""surface"") inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005;Habash, Rambow, and Roth 2012).', 'The Elixir-FM analyzer (Smrž 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality.', 'In this article , we use an in-house system which provides functional gender , number , and rationality features ( #AUTHOR_TAG ) .', 'See Section 5.2 for more details.']",5,"['Some inflectional features, specifically gender and number, are expressed using different morphemes in different words (even within the same POS).', 'There are four sound gender-number suffixes in Arabic: 5 +ph (null morpheme) for masculine singular, + + for feminine singular, + +wn for masculine plural, and + +At for feminine plural.', 'There are exceptions and alternative ways to express GENDER and NUMBER, however, and functional feature values take them into account: Depending on the lexeme, plurality can be expressed using sound plural suffixes or using a pattern change together with singular suffixes.', ""On the other hand, the plural of the inflectionally and morphemically feminine singular word madras+a ('school') is the word madAris+ph ('schools'), which is feminine and plural inflectionally, but has a masculine singular suffix."", 'This irregular inflection, known as broken plural, is similar to the English mouse/mice, but is much more common in Arabic (over 50% of plurals in our training data).', ""A similar inconsistency appears in feminine nominals that are not inflected using sound gender suffixes, for example, the feminine form of the masculine singular adjective Azraq+ph ('blue') is zarqA'+ph not * *Azraq+a ."", 'ost available Arabic NLP tools and resources model morphology using formbased (""surface"") inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005;Habash, Rambow, and Roth 2012).', 'In this article , we use an in-house system which provides functional gender , number , and rationality features ( #AUTHOR_TAG ) .']"
CC676,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,catib the columbia arabic treebank,"['Nizar Habash', 'Ryan Roth']",related work,"The Columbia Arabic Treebank (CATiB) is a resource for Arabic parsing. CATiB contrasts with previous efforts on Arabic treebanking and treebanking of morphologically rich languages in that it encodes less linguistic information in the interest of speedier annotation of large amounts of text. This paper describes CATiB's representation and annotation procedure, and reports on achieved inter-annotator agreement and annotation speed","As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( #AUTHOR_TAG ) .","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( #AUTHOR_TAG ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']",0,"['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( #AUTHOR_TAG ) .']"
CC677,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,an efficient algorithm for easyfirst nondirectional dependency parsing,"['Yoav Goldberg', 'Michael Elhadad']",related work,,"Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #AUTHOR_TAG ) ( Section 6 ) .","['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;).', 'Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.', 'Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #AUTHOR_TAG ) ( Section 6 ) .', 'Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise ""inverse"" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).']",5,"['Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser ( #AUTHOR_TAG ) ( Section 6 ) .', 'Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.']"
CC678,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,a corpus for modeling morphosyntactic agreement in arabic gender number and rationality,"['Sarah Alkuhlani', 'Nizar Habash']",related work,"We present an enriched version of the Penn Arabic Treebank (Maamouri et al., 2004), where latent features necessary for modeling morpho-syntactic agreement in Arabic are manually annotated. We describe our process for efficient annotation, and present the first quantitative analysis of Arabic morpho-syntactic phenomena.","To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #AUTHOR_TAG ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) .","['The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.', 'To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #AUTHOR_TAG ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) .', 'We conducted experiments with gold features to assess the potential of these features, and with predicted fea- tures, obtained from training a simple maximum likelihood estimation classifier on this resource (Alkuhlani and Habash 2012).', '19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input).', 'The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts.', 'The fourth part of the table shows the combination of these functional features with the other features that participated in the best combination so far (LMM, the extended DET2, and PERSON); without RAT, this combination is at least as useful as its form-based counterpart, in both gold and predicted input; adding RAT to this combination yields 0.4% (absolute) gain in gold, offering further support to the relevance of the rationality feature, but a slight decrease in predicted input, presumably due to insufficient accuracy again.', 'The last part of the table revalidates the gains achieved with the best controlled feature combination, using CATIBEX�the best performing tag set with predicted in- put.', 'Note, however, that the 1% (absolute) advantage of CATIBEX (without additional features) over the morphology-free CORE12 on machine-predicted input (Table 2) has shrunk with these functional feature combinations to 0.3%.', 'We take it as further support to the relevance of our functional morphology features, and their partial redundancy with the form-based morphological information embedded in the CATIBEX POS tags.']",5,"['The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.', 'To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality ( #AUTHOR_TAG ) .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) .', 'The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts.']"
CC679,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,arabic tokenization partofspeech tagging and morphological disambiguation in one fell swoop,"['Nizar Habash', 'Owen Rambow']",related work,"We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including part-of-speech tagging) Arabic words in one process. We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer. We obtain accuracy rates on all tasks in the high nineties.","Put differently, we are interested in the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit ( #AUTHOR_TAG ; Habash, Rambow, and Roth 2012)15 (see Table 2, columns 5�7).","['So far we discussed optimal (gold) conditions.', 'But in prac- tice, POS tags are annotated by automatic taggers, so parsers get predicted POS tags as input, as opposed to gold (human-annotated) tags.14', 'The more informative the tag set, the less accurate the tag prediction might be, so the effect on overall parsing quality is unclear.', 'Put differently, we are interested in the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit ( #AUTHOR_TAG ; Habash, Rambow, and Roth 2012)15 (see Table 2, columns 5�7).', 'It turned out that BW, the best gold performer but with lowest POS pre- diction accuracy (81.8%), suffered the biggest drop (11.4%) and was the worst performer with predicted tags.', 'The simplest tag set, CATIB6, and its extension, CATIBEX, benefited from the highest POS prediction accuracy (97.7%), and their performance suffered the least.', 'CATIBEX was the best performer with predicted POS tags.', 'Performance drop and POS prediction accuracy are given in columns 8 and 9.']",5,"['But in prac- tice, POS tags are annotated by automatic taggers, so parsers get predicted POS tags as input, as opposed to gold (human-annotated) tags.14', 'The more informative the tag set, the less accurate the tag prediction might be, so the effect on overall parsing quality is unclear.', 'Put differently, we are interested in the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit ( #AUTHOR_TAG ; Habash, Rambow, and Roth 2012)15 (see Table 2, columns 5�7).', 'CATIBEX was the best performer with predicted POS tags.']"
CC680,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,algorithms for deterministic incremental dependency parsing,['Joakim Nivre'],related work,"Abstract Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.","For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #AUTHOR_TAG .","['All results are reported mainly in terms of labeled attachment accuracy score (the parent word and the type of dependency relation to it, abbreviated as LAS), which is also used for greedy (hill-climbing) decisions for feature combination.', 'Unlabeled attachment accuracy score (UAS) and label accuracy (dependency relation regardless of parent, LS) are also given.', ""For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #AUTHOR_TAG ."", 'We denote p < 0.05 and p < 0.01 with + and ++ , respectively.']",5,"[""For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #AUTHOR_TAG .""]"
CC681,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,parsing indian languages with maltparser,['Joakim Nivre'],related work,"This paper describes the application of MaltParser, a transition-based dependency parser, to three Indian languages - Bangla, Hindi and Telugu - in the context of the NLP Tools Contest at ICON 2009. In the final evaluation, MaltParser was ranked second among the participating systems and achieved an unlabeled attachment score close to 90% for Bangla and Hindi, and over 85% for Telugu, while the labeled attachment score was 15-25 percentage points lower. It is likely that the high unlabeled accuracy is achieved thanks to a relatively low syntactic complexity in the data sets, while the low labeled accuracy is due to the limited amounts of training data.","For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; KÃ¼bler , McDonald , and #AUTHOR_TAG ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).","['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; KÃ¼bler , McDonald , and #AUTHOR_TAG ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).', 'We do not perform further weight optimization (which, if done, is done on a separate ""tuning set"").', 'to predict the next state in the parse derivation.', 'All experiments were done using the Nivre ""eager"" algorithm.', '11 There are five default attributes in the MaltParser terminology for each token in the text: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent).', 'There are default MaltParser features (in the machine learning sense), 12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers.', 'The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.).', 'Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on.', 'Kübler, McDonald, and Nivre (2009) describe a ""typical"" MaltParser model configuration of attributes and features.', '13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags.', 'For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0].', 'We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]), ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively, dependents of the specified argument).', 'This new MaltParser configuration resulted in gains of 0.3-1.1% in labeled attachment accuracy (depending on the POS tag set) over the default MaltParser configuration.', 'We also experimented with using normalized word-forms (Alif Maqsura conversion to Ya, and Hamza removal from each Alif ) as is common in parsing and statistical machine translation literature, but it resulted in a small decrease in performance, so we settled on using non-normalized word-forms.', 'All experiments reported here were conducted using this new configuration.', 'To recap, it has the following MaltParser attributes (machine learning features): 4 word-form attributes, POS tag attributes, and 5 deprel attributes (some of which are not useful for the Nivre ""eager"" algorithm), totaling 16 attributes and two more for every new feature described in Section 4.3 and on (e.g., CASE).']",5,"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; KÃ¼bler , McDonald , and #AUTHOR_TAG ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).']"
CC682,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,identifying broken plurals irregular gender and rationality in arabic text,"['Sarah Alkuhlani', 'Nizar Habash']",related work,"Arabic morphology is complex, partly because of its richness, and partly because of common irregular word forms, such as broken plurals (which resemble singular nouns), and nouns with irregular gender (feminine nouns that look masculine and vice versa). In addition, Arabic morpho-syntactic agreement interacts with the lexical semantic feature of rationality, which has no morphological realization. In this paper, we present a series of experiments on the automatic prediction of the latent linguistic features of functional gender and number, and rationality in Arabic. We compare two techniques, using simple maximum likelihood (MLE) with back-off and a support vector machine based sequence tagger (Yamcha). We study a number of orthographic, morphological and syntactic learning features. Our results show that the MLE technique is preferred for words seen in the training data, while the Yam-cha technique is optimal for unseen words, which are our real target. Furthermore, we show that for unseen words, morphological features help beyond orthographic features and that syntactic features help even more. A combination of the two techniques improves overall performance even further.","We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #AUTHOR_TAG ) .19","['The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.', 'To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (Alkuhlani and Habash 2011).18', 'This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender).', 'We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #AUTHOR_TAG ) .19', 'The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input).', 'The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts.', 'The fourth part of the table shows the combination of these functional features with the other features that participated in the best combination so far (LMM, the extended DET2, and PERSON); without RAT, this combination is at least as useful as its form-based counterpart, in both gold and predicted input; adding RAT to this combination yields 0.4% (absolute) gain in gold, offering further support to the relevance of the rationality feature, but a slight decrease in predicted input, presumably due to insufficient accuracy again.', 'The last part of the table revalidates the gains achieved with the best controlled feature combination, using CATIBEX�the best performing tag set with predicted in- put.', 'Note, however, that the 1% (absolute) advantage of CATIBEX (without additional features) over the morphology-free CORE12 on machine-predicted input (Table 2) has shrunk with these functional feature combinations to 0.3%.', 'We take it as further support to the relevance of our functional morphology features, and their partial redundancy with the form-based morphological information embedded in the CATIBEX POS tags.']",5,"['The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.', 'To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (Alkuhlani and Habash 2011).18', 'This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender).', 'We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( #AUTHOR_TAG ) .19', 'The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input).', 'The next two parts show the advantages of functional gender and number (denoted with a FN* prefix) over their surface-based counterparts.', 'We take it as further support to the relevance of our functional morphology features, and their partial redundancy with the form-based morphological information embedded in the CATIBEX POS tags.']"
CC683,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,the penn arabic treebank building a largescale annotated arabic corpus,"['Mohamed Maamouri', 'Ann Bies', 'Timothy A Buckwalter', 'Wigdan Mekki']",experiments,"From our three year experience of developing a large-scale corpus of annotated Arabic text, our paper will address the following: (a) review pertinent Arabic language issues as they relate to methodology choices, (b) explain our choice to use the Penn English Treebank style of guidelines, (requiring the Arabic-speaking annotators to deal with a new grammatical system) rather than doing the annotation in a more traditional Arabic grammar style (requiring NLP researchers to deal with a new system); (c) show several ways in which human annotation is important and automatic analysis difficult, including the handling of orthographic ambiguity by both the morphological analyzer and human annotators; (d) give an illustrative example of the Arabic Treebank methodology, focusing on a particular construction in both morphological analysis and tagging and syntactic analysis and following it in detail through the entire annotation process, and finally, (e) conclude with what has been achieved so far and what remains to be done.","Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) ( #AUTHOR_TAG ) , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005;  Habash, Rambow, and Roth 2012).","['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) ( #AUTHOR_TAG ) , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005;  Habash, Rambow, and Roth 2012).', 'The Elixir-FM analyzer (Smr_ 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality.', 'In this article, we use an in-house system which provides functional gender, number, and rationality features (Alkuhlani and Habash 2012).', 'See Section 5.2 for more details.']",1,"['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) ( #AUTHOR_TAG ) , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005;  Habash, Rambow, and Roth 2012).']"
CC684,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,arabic tokenization partofspeech tagging and morphological disambiguation in one fell swoop,"['Nizar Habash', 'Owen Rambow']",experiments,"We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including part-of-speech tagging) Arabic words in one process. We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer. We obtain accuracy rates on all tasks in the high nineties.","Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit ( #AUTHOR_TAG ;  Habash, Rambow, and Roth 2012).","['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit ( #AUTHOR_TAG ;  Habash, Rambow, and Roth 2012).', 'The Elixir-FM analyzer (Smr_ 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality.', 'In this article, we use an in-house system which provides functional gender, number, and rationality features (Alkuhlani and Habash 2012).', 'See Section 5.2 for more details.']",1,"['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit ( #AUTHOR_TAG ;  Habash, Rambow, and Roth 2012).']"
CC685,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,catib the columbia arabic treebank,"['Nizar Habash', 'Ryan Roth']",experiments,"The Columbia Arabic Treebank (CATiB) is a resource for Arabic parsing. CATiB contrasts with previous efforts on Arabic treebanking and treebanking of morphologically rich languages in that it encodes less linguistic information in the interest of speedier annotation of large amounts of text. This paper describes CATiB's representation and annotation procedure, and reports on achieved inter-annotator agreement and annotation speed","The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( #AUTHOR_TAG ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( Buckwalter 2004 ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both outperforming it : ( d ) Kulick , Gabbard , and Marcus ( 2006 ) 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .","[""The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( #AUTHOR_TAG ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( Buckwalter 2004 ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both outperforming it : ( d ) Kulick , Gabbard , and Marcus ( 2006 ) 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .""]",5,"[""The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( #AUTHOR_TAG ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( Buckwalter 2004 ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both outperforming it : ( d ) Kulick , Gabbard , and Marcus ( 2006 ) 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .""]"
CC686,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,buckwalter arabic morphological analyzer version 20 linguistic data consortium,['Timothy A Buckwalter'],experiments,,"Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer ( #AUTHOR_TAG ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).","['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer ( #AUTHOR_TAG ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).', 'The Elixir-FM analyzer (Smr_ 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality.', 'In this article, we use an in-house system which provides functional gender, number, and rationality features (Alkuhlani and Habash 2012).', 'See Section 5.2 for more details.']",1,"['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer ( #AUTHOR_TAG ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).']"
CC687,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,conllx shared task on multilingual dependency parsing,"['Sabine Buchholz', 'Erwin Marsi']",related work,"Each year the Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems. The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing. In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured. We also give an overview of the parsing approaches that participants took and the results that they achieved. Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser?","As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( #AUTHOR_TAG ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( #AUTHOR_TAG ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']",0,"['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( #AUTHOR_TAG ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']"
CC688,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,parsing indian languages with maltparser,['Joakim Nivre'],introduction,"This paper describes the application of MaltParser, a transition-based dependency parser, to three Indian languages - Bangla, Hindi and Telugu - in the context of the NLP Tools Contest at ICON 2009. In the final evaluation, MaltParser was ranked second among the participating systems and achieved an unlabeled attachment score close to 90% for Bangla and Hindi, and over 85% for Telugu, while the labeled attachment score was 15-25 percentage points lower. It is likely that the high unlabeled accuracy is achieved thanks to a relatively low syntactic complexity in the data sets, while the low labeled accuracy is due to the limited amounts of training data.","It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .","['Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors.', 'In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'For example, modeling CASE in Czech improves Czech parsing (Collins et al. 1999): CASE is relevant, not redundant, and can be predicted with sufficient accuracy.', 'It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .']",4,"['It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .']"
CC689,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,algorithms for deterministic incremental dependency parsing,['Joakim Nivre'],introduction,"Abstract Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.",The result holds for both the MaltParser ( #AUTHOR_TAG ) and the Easy-First Parser ( Goldberg and Elhadad 2010 ) .,"['results, assignment features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition.', 'In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions.', 'This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations.', 'The result holds for both the MaltParser ( #AUTHOR_TAG ) and the Easy-First Parser ( Goldberg and Elhadad 2010 ) .']",5,['The result holds for both the MaltParser ( #AUTHOR_TAG ) and the Easy-First Parser ( Goldberg and Elhadad 2010 ) .']
CC690,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,buckwalter arabic morphological analyzer version 20 linguistic data consortium,['Timothy A Buckwalter'],experiments,,"The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( Habash and Roth 2009 ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( #AUTHOR_TAG ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both outperforming it : ( d ) Kulick , Gabbard , and Marcus ( 2006 ) 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .","[""The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( Habash and Roth 2009 ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( #AUTHOR_TAG ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both outperforming it : ( d ) Kulick , Gabbard , and Marcus ( 2006 ) 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .""]",5,"[""The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) ( Habash and Roth 2009 ) and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) ( #AUTHOR_TAG ) ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both outperforming it : ( d ) Kulick , Gabbard , and Marcus ( 2006 ) 's tag set ( KULICK ) , size 43 , one of whose most important extensions is the marking of the definite article clitic , and ( e ) Diab and Benajiba 's ( in preparation ) EXTENDED RTS tag set ( ERTS ) , which marks gender , number , and definiteness , size 134 .""]"
CC691,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,a statistical parser for czech,"['Michael Collins', 'Jan Hajic', 'Lance Ramshaw', 'Christoph Tillmann']",related work,"This paper considers statistical parsing of Czech, which differs radically from English in at least two respects: (1) it is a highly inflected language, and (2) it has relatively free word order. These differences are likely to pose new problems for techniques that have been developed on English. We describe our experience in building on the parsing model of (Collins 97). Our final results- 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text.",#AUTHOR_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .,"['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', '#AUTHOR_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .', 'This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size ≈3000+).', 'They also report that the use of gender, number, and person features did not yield any improvements.', 'The results for Czech are the opposite of our results for Arabic, as we will see.', 'This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajič and Vidová-Hladká 1998) compared with Arabic (≈14.0%,', 'see Table 3).', 'Similarly, Cowan and Collins (2005) report that the use of a subset of Spanish morphological features (number for adjectives, determiners, nouns, pronouns, and verbs; and mode for verbs) outperforms other combinations.', 'Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', 'We also find that the number feature helps for Arabic.', ""Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima'an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing.""]",0,['#AUTHOR_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .']
CC692,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,getting more from morphology in multilingual dependency parsing,"['Matt Hohensee', 'Emily M Bender']",related work,"We propose a linguistically motivated set of features to capture morphological agreement and add them to the MSTParser dependency parser. Compared to the built-in morphological feature set, ours is both much smaller and more accurate across a sample of 20 morphologically annotated treebanks. We find increases in accuracy of up to 5.3% absolute. While some of this results from the feature set capturing information unrelated to morphology, there is still significant improvement, up to 4.6% absolute, due to the agreement model.","9 We do not relate to specific results in their study because it has been brought to our attention that #AUTHOR_TAG are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) .","['9 We do not relate to specific results in their study because it has been brought to our attention that #AUTHOR_TAG are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) .']",1,"['9 We do not relate to specific results in their study because it has been brought to our attention that #AUTHOR_TAG are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) .']"
CC693,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,an efficient algorithm for easyfirst nondirectional dependency parsing,"['Yoav Goldberg', 'Michael Elhadad']",introduction,,The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser ( #AUTHOR_TAG ) .,"['results, assignment features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition.', 'In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions.', 'This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations.', 'The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser ( #AUTHOR_TAG ) .']",5,['The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser ( #AUTHOR_TAG ) .']
CC694,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,introduction to arabic natural language processing,['Nizar Habash'],experiments,"Abstract This book provides system developers and researchers in natural language processing and computational linguistics with the necessary background information for working with the Arabic language. The goal is to introduce Arabic linguistic phenomena and review the state-of-the-art in Arabic processing. The book discusses Arabic script, phonology, orthography, morphology, syntax and semantics, with a final chapter on machine translation issues. The chapter sizes correspond more or less to what is linguistically distinctive about Arabic, with morphology getting the lion's share, followed by Arabic script. No previous knowledge of Arabic is needed. This book is designed for computer scientists and linguists alike. The focus of the book is on Modern Standard Arabic; however, notes on practical issues related to Arabic dialects and languages written in the Arabic script are presented in different chapters. Table of Contents: What is ""Arabic""? / Arabic Script / Arabic Phonology and Orthography / Arabic Mo...",A more detailed discussion of the various available Arabic tag sets can be found in #AUTHOR_TAG .,"['The notion of ""POS tag set"" in natural language processing usually does not refer to a core set.', 'Instead, the Penn English Treebank (PTB) uses a set of 46 tags, including not only the core POS, but also the complete set of morphological features (this tag set is still fairly small since English is morphologically impoverished).', 'In PATB-tokenized MSA, the corresponding type of tag set (core POS extended with a complete description of morphology) would contain upwards of 2,000 tags, many of which are extremely rare (in our training corpus of about 300,000 words, we encounter only POS tags with complete morphology).', 'Therefore, researchers have proposed tag sets for MSA whose size is similar to that of the English PTB tag set, as this has proven to be a useful size computationally.', 'These tag sets are hybrids in the sense that they are neither simply the core POS, nor the complete morphologically enriched tag set, but instead they selectively enrich the core POS tag set with only certain morphological features.', 'A more detailed discussion of the various available Arabic tag sets can be found in #AUTHOR_TAG .']",0,['A more detailed discussion of the various available Arabic tag sets can be found in #AUTHOR_TAG .']
CC695,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,catib the columbia arabic treebank,"['Nizar Habash', 'Ryan Roth']",experiments,"The Columbia Arabic Treebank (CATiB) is a resource for Arabic parsing. CATiB contrasts with previous efforts on Arabic treebanking and treebanking of morphologically rich languages in that it encodes less linguistic information in the interest of speedier annotation of large amounts of text. This paper describes CATiB's representation and annotation procedure, and reports on achieved inter-annotator agreement and annotation speed","For more information on CATiB , see #AUTHOR_TAG and Habash , Faraj , and Roth ( 2009 ) .","['The CATiB Treebank uses the word segmentation of the PATB.', ""It splits off several categories of orthographic clitics, but not the definite article + Al+ ('the')."", 'In all of the experiments reported in this article, we use the gold segmentation.', 'Tokenization involves further decisions on the segmented token forms, such as spelling normalization, which we only briefly touch on here (in Section 4.1).', 'An example CATiB dependency tree is shown in Figure 1.', 'For the corpus statistics, see Table 1.', 'For more information on CATiB , see #AUTHOR_TAG and Habash , Faraj , and Roth ( 2009 ) .']",0,"['The CATiB Treebank uses the word segmentation of the PATB.', 'An example CATiB dependency tree is shown in Figure 1.', 'For the corpus statistics, see Table 1.', 'For more information on CATiB , see #AUTHOR_TAG and Habash , Faraj , and Roth ( 2009 ) .']"
CC696,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,an efficient algorithm for easyfirst nondirectional dependency parsing,"['Yoav Goldberg', 'Michael Elhadad']",experiments,,"Some researchers , however , including #AUTHOR_TAG , train on predicted feature values instead .","['So far, we have only evaluated models trained on gold POS tag set and morphological feature values.', 'Some researchers , however , including #AUTHOR_TAG , train on predicted feature values instead .', 'It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test.', 'But we argue that it also makes sense that training on a combination of gold and predicted features (one copy of each) might do even better, because good predictions of feature values are reinforced (since they repeat the gold patterns), whereas noisy predicted feature values are still represented in training (in patterns that do not repeat the gold). 21', 'To test our hypothesis, we start this section by comparing three variations:']",1,"['So far, we have only evaluated models trained on gold POS tag set and morphological feature values.', 'Some researchers , however , including #AUTHOR_TAG , train on predicted feature values instead .', 'To test our hypothesis, we start this section by comparing three variations:']"
CC697,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,identifying broken plurals irregular gender and rationality in arabic text,"['Sarah Alkuhlani', 'Nizar Habash']",related work,"Arabic morphology is complex, partly because of its richness, and partly because of common irregular word forms, such as broken plurals (which resemble singular nouns), and nouns with irregular gender (feminine nouns that look masculine and vice versa). In addition, Arabic morpho-syntactic agreement interacts with the lexical semantic feature of rationality, which has no morphological realization. In this paper, we present a series of experiments on the automatic prediction of the latent linguistic features of functional gender and number, and rationality in Arabic. We compare two techniques, using simple maximum likelihood (MLE) with back-off and a support vector machine based sequence tagger (Yamcha). We study a number of orthographic, morphological and syntactic learning features. Our results show that the MLE technique is preferred for words seen in the training data, while the Yam-cha technique is optimal for unseen words, which are our real target. Furthermore, we show that for unseen words, morphological features help beyond orthographic features and that syntactic features help even more. A combination of the two techniques improves overall performance even further.","19 The paper by #AUTHOR_TAG presents additional , more sophisticated models that we do not use in this article .","['19 The paper by #AUTHOR_TAG presents additional , more sophisticated models that we do not use in this article .']",1,"['19 The paper by #AUTHOR_TAG presents additional , more sophisticated models that we do not use in this article .']"
CC698,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,towards an optimal pos tag set for modern standard arabic processing,['Mona Diab'],related work,,"As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; #AUTHOR_TAG ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; #AUTHOR_TAG ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']",0,"['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; #AUTHOR_TAG ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .']"
CC699,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,algorithms for deterministic incremental dependency parsing,['Joakim Nivre'],related work,"Abstract Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.","11 #AUTHOR_TAG reports that non-projective and pseudo-projective algorithms outperform the `` eager  projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies .","['11 #AUTHOR_TAG reports that non-projective and pseudo-projective algorithms outperform the `` eager  projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies .', 'The Nivre �standard� algorithm is also reported there to do better on Arabic, but in a preliminary experimentation, it did slightly worse than the �eager� one, perhaps due to the high percentage of right branching (left headed structures) in our Arabic training set�an observation already noted in Nivre (2008).']",1,"['11 #AUTHOR_TAG reports that non-projective and pseudo-projective algorithms outperform the `` eager  projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies .']"
CC700,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,parsing indian languages with maltparser,['Joakim Nivre'],related work,"This paper describes the application of MaltParser, a transition-based dependency parser, to three Indian languages - Bangla, Hindi and Telugu - in the context of the NLP Tools Contest at ICON 2009. In the final evaluation, MaltParser was ranked second among the participating systems and achieved an unlabeled attachment score close to 90% for Bangla and Hindi, and over 85% for Telugu, while the labeled attachment score was 15-25 percentage points lower. It is likely that the high unlabeled accuracy is achieved thanks to a relatively low syntactic complexity in the data sets, while the low labeled accuracy is due to the limited amounts of training data.","Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K�bler, McDonald, and #AUTHOR_TAG describe a �typical� MaltParser model configuration of attributes and features.13","['There are five default attributes in the MaltParser terminology for each token in the text: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent).', 'There are default MaltParser features (in the machine learning sense),12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers.', 'The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.).', 'Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K�bler, McDonald, and #AUTHOR_TAG describe a �typical� MaltParser model configuration of attributes and features.13', 'Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags.', 'For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0].', 'We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]), ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively, dependents of the specified argument).', 'This new MaltParser configuration resulted in gains of 0.3�1.1% in labeled attachment accuracy (depending on the POS tag set) over the default MaltParser configuration.', 'We also experimented with using normalized word-forms (Alif Maqsura conversion to Ya, and Hamza removal from each Alif ) as is common in parsing and statistical machine translation literature, but it resulted in a small decrease in performance, so we settled on using non-normalized word-forms.', 'All experiments reported here were conducted using this new configuration. To recap, it has the following MaltParser attributes (machine learning features): 4 word-form attributes, 7 POS tag attributes, and 5 deprel attributes (some of which are not useful for the Nivre �eager� algorithm), totaling 16 attributes and two more for every new feature described in Section 4.3 and on (e.g., CASE).']",5,"['Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on. K�bler, McDonald, and #AUTHOR_TAG describe a �typical� MaltParser model configuration of attributes and features.13', 'All experiments reported here were conducted using this new configuration. To recap, it has the following MaltParser attributes (machine learning features): 4 word-form attributes, 7 POS tag attributes, and 5 deprel attributes (some of which are not useful for the Nivre eager algorithm), totaling 16 attributes and two more for every new feature described in Section 4.3 and on (e.g., CASE).']"
CC701,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,better arabic parsing baselines evaluations and analysis,"['Spence Green', 'Christopher D Manning']",related work,"In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature. Second, we show that although the Penn Arabic Treebank is similar to other tree-banks in gross statistical terms, annotation consistency remains problematic. Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG. Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2--5% F1.","As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']",0,"['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #AUTHOR_TAG ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', 'Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.']"
CC702,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,an efficient algorithm for projective dependency parsing,['Joakim Nivre'],related work,"This paper presents a deterministic parsing algorithm for projective dependency grammar. The running time of the algorithm is linear in the length of the input string, and the dependency graph produced is guaranteed to be projective and acyclic. The algorithm has been experimentally evaluated in parsing unrestricted Swedish text, achieving an accuracy above 85% with a very simple grammar.","For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; KÃ¼bler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).","['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; KÃ¼bler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).', 'We do not perform further weight optimization (which, if done, is done on a separate ""tuning set"").', 'to predict the next state in the parse derivation.', 'All experiments were done using the Nivre ""eager"" algorithm.', '11 There are five default attributes in the MaltParser terminology for each token in the text: word ID (ordinal position in the sentence), word-form, POS tag, head (parent word ID), and deprel (the dependency relation between the current word and its parent).', 'There are default MaltParser features (in the machine learning sense), 12 which are the values of functions over these attributes, serving as input to the MaltParser internal classifiers.', 'The most commonly used feature functions are the top of the input buffer (next word to process, denoted buf[0]), or top of the stack (denoted stk[0]); following items on buffer or stack are also accessible (buf[1], buf[2], stk[1], etc.).', 'Hence MaltParser features are defined as POS tag at stk[0], word-form at buf[0], and so on.', 'Kübler, McDonald, and Nivre (2009) describe a ""typical"" MaltParser model configuration of attributes and features.', '13 Starting with it, in a series of initial controlled experiments, we settled on using buf[0-1] + stk[0-1] for word-forms, and buf[0-3] + stk[0-2] for POS tags.', 'For features of new MaltParser-attributes (discussed later), we used buf[0] + stk[0].', 'We did not change the features for deprel: stk[0], ldep(stk[0]), rdep(stk[0]), ldep(buf[0]), rdep(buf[0]) (where ldep and rdep are the left and right, respectively, dependents of the specified argument).', 'This new MaltParser configuration resulted in gains of 0.3-1.1% in labeled attachment accuracy (depending on the POS tag set) over the default MaltParser configuration.', 'We also experimented with using normalized word-forms (Alif Maqsura conversion to Ya, and Hamza removal from each Alif ) as is common in parsing and statistical machine translation literature, but it resulted in a small decrease in performance, so we settled on using non-normalized word-forms.', 'All experiments reported here were conducted using this new configuration.', 'To recap, it has the following MaltParser attributes (machine learning features): 4 word-form attributes, POS tag attributes, and 5 deprel attributes (some of which are not useful for the Nivre ""eager"" algorithm), totaling 16 attributes and two more for every new feature described in Section 4.3 and on (e.g., CASE).']",5,"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( #AUTHOR_TAG , 2008 ; KÃ¼bler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).']"
CC703,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,atanas chanev gulsen eryigit sandra kubler svetoslav marinov and erwin marsi,"['Joakim Nivre', 'Johan Hall', 'Jens Nilsson']",related work,,"Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .","['As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006;Diab 2007;Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006;Nivre 2008) and the CATiB .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']",0,"['As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006;Diab 2007;Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006;Nivre 2008) and the CATiB .', 'Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', 'Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( #AUTHOR_TAG ) , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']"
CC704,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,morphology and reranking for the statistical parsing of spanish,"['Brooke Cowan', 'Michael Collins']",related work,"We present two methods for incorporating detailed features in a Spanish parser, building on a baseline model that is a lexicalized PCFG. The first method exploits Spanish morphology, and achieves an F1 constituency score of 83.6%. This is an improvement over 81.2 % accuracy for the baseline, which makes little or no use of morphological information. The second model uses a reranking approach to add arbitrary global features of parse trees to the morphological model. The reranking model reaches 85.1 % F1 accuracy on the Spanish parsing task. The resulting model for Spanish parsing combines an approach that specifically targets morphological information with an approach that makes use of general structural features","Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .","['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', 'Collins et al. (1999) report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature (when applicable).', 'This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size �3000+).', 'They also report that the use of gender, number, and person features did not yield any improvements.', 'The results for Czech are the opposite of our results for Arabic, as we will see.', 'This may be due to CASE tagging having a lower error rate in Czech (5.0%) (Hajic� and Vidov�-Hladk� 1998) compared with Arabic (�14.0%, see Table 3).', 'Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .', 'Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', 'We also find that the number feature helps for Arabic.', 'Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima�an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing.']",0,"['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', 'Collins et al. (1999) report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature (when applicable).', 'Similarly , #AUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .']"
CC705,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,introduction to arabic natural language processing,['Nizar Habash'],experiments,"Abstract This book provides system developers and researchers in natural language processing and computational linguistics with the necessary background information for working with the Arabic language. The goal is to introduce Arabic linguistic phenomena and review the state-of-the-art in Arabic processing. The book discusses Arabic script, phonology, orthography, morphology, syntax and semantics, with a final chapter on machine translation issues. The chapter sizes correspond more or less to what is linguistically distinctive about Arabic, with morphology getting the lion's share, followed by Arabic script. No previous knowledge of Arabic is needed. This book is designed for computer scientists and linguists alike. The focus of the book is on Modern Standard Arabic; however, notes on practical issues related to Arabic dialects and languages written in the Arabic script are presented in different chapters. Table of Contents: What is ""Arabic""? / Arabic Script / Arabic Phonology and Orthography / Arabic Mo...","7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .","['7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .']",0,"['7 We ignore the rare ""false idafa"" construction ( #AUTHOR_TAG , p. 102 ) .']"
CC706,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,an efficient algorithm for easyfirst nondirectional dependency parsing,"['Yoav Goldberg', 'Michael Elhadad']",experiments,,"In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .","['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .', 'As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values.', 'The Easy-First Parser is a shift-reduce parser (as is MaltParser).', 'Unlike MaltParser, however, it does not attempt to attach arcs ""eagerly"" as early as possible (as in previous sections), or at the latest possible stage (an option we abandoned early on in preliminary experiments).', 'Instead, the Easy-First Parser keeps a stack of partially built treelets, and attaches them to one another in order of confidence (from high confidence, ""easy"" attachment, to low, as estimated by the classifier).', 'Labeling the relation arcs is done in a second pass, with a separate training step, after all attachments have been decided (the code for which was added after the publication of Goldberg and Elhadad (2010), which only included an unlabeled attachment version).']",5,"['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser ( #AUTHOR_TAG ) .', 'The Easy-First Parser is a shift-reduce parser (as is MaltParser).', 'Instead, the Easy-First Parser keeps a stack of partially built treelets, and attaches them to one another in order of confidence (from high confidence, ""easy"" attachment, to low, as estimated by the classifier).']"
CC707,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,parsing indian languages with maltparser,['Joakim Nivre'],related work,"This paper describes the application of MaltParser, a transition-based dependency parser, to three Indian languages - Bangla, Hindi and Telugu - in the context of the NLP Tools Contest at ICON 2009. In the final evaluation, MaltParser was ranked second among the participating systems and achieved an unlabeled attachment score close to 90% for Bangla and Hindi, and over 85% for Telugu, while the labeled attachment score was 15-25 percentage points lower. It is likely that the high unlabeled accuracy is achieved thanks to a relatively low syntactic complexity in the data sets, while the low labeled accuracy is due to the limited amounts of training data.","Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .","['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .', 'Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.', 'Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6).', 'Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise ""inverse"" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).']",1,"['Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #AUTHOR_TAG ) .', 'Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor.']"
CC708,J13-1008,Dependency Parsing of Modern Standard Arabic with Lexical and Inflectional Features,getting more from morphology in multilingual dependency parsing,"['Matt Hohensee', 'Emily M Bender']",related work,"We propose a linguistically motivated set of features to capture morphological agreement and add them to the MSTParser dependency parser. Compared to the built-in morphological feature set, ours is both much smaller and more accurate across a sample of 20 morphologically annotated treebanks. We find increases in accuracy of up to 5.3% absolute. While some of this results from the feature set capturing information unrelated to morphology, there is still significant improvement, up to 4.6% absolute, due to the agreement model.",#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .,"['Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing.', 'We go beyond previous work, however, and explore additional lexical and inflectional features.', 'Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;).', 'Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task.', 'Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6).', '#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-identity agreement patterns of irrational plurals, r one handles the loss of overt number agreement in constructions such as VS (where the verb precedes its subject), and r one adequately represents the otherwise ""inverse"" number agreement (a phenomenon common to other Semitic languages, such as Hebrew, too).']",0,['#AUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .']
CC709,J14-2004,Unsupervised Event Coreference Resolution,unsupervised event coreference resolution with rich linguistic features,"['Cosmin Adrian Bejan', 'Sanda Harabagiu']",related work,"This paper examines how a new class of nonparametric Bayesian models can be effectively applied to an open-domain event coreference task. Designed with the purpose of clustering complex linguistic objects, these models consider a potentially infinite number of features and categorical outcomes. The evaluation performed for solving both within- and cross-document event coreference shows significant improvements of the models when compared against two baselines for this task.",This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; #AUTHOR_TAG ) .,"['This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; #AUTHOR_TAG ) .', 'In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way.', 'As data sets, we consider three different resources, including our own corpus (which is the only corpus available that encodes event coreference annotations across and within documents).', 'In the next section, we provide additional information on how we performed the annotation of this corpus.', 'Another major contribution of this article is an extended description of the unsupervised models for solving event coreference.', 'In particular, we focused on providing further explanations about the implementation of the mIBP framework as well as its integration into the HDP and iHMM models.', 'Finally, in this work, we significantly extended the experimental results section, which also includes a novel set of experiments performed over the OntoNotes English corpus (LDC-ON 2007).']",2,['This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; #AUTHOR_TAG ) .']
CC710,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the harpy speech understanding system in lea,"['B Lowerre', 'R Reddy']",,,"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , #AUTHOR_TAG , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , #AUTHOR_TAG , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']",1,"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , #AUTHOR_TAG , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .']"
CC711,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,scripts plans goals and understanding lawrence erlbaum associates,"['R Schank', 'R Abelson']",experiments,,"Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in #AUTHOR_TAG .","['Though the implemented system is limited to matrix-oriented problems, the theoretical system is capable of learning a wide range of problem types.', 'The only requirement on the problem or situation is that it can be entered into the expectation system in the form of examples.', ""Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in #AUTHOR_TAG .""]",0,"[""Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in #AUTHOR_TAG .""]"
CC712,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,algorithmic program debugging,['E Shapiro'],,"The thesis lays a theoretical framework for program debugging, with the goal of partly mechanizing this activity. In particular, we formalize and develop algorithmic solutions to the following two questions: (1) How do we identify a bug in a program that behaves incorrectly? (2) How do we fix a bug, once one is identified?  We develop interactive diagnosis algorithms that identify a bug in a program that behaves incorrectly, and implement them in Prolog for the diagnosis of Prolog programs. Their performance suggests that they can be the backbone of debugging aids that go far beyond what is offered by current programming environments.  We develop an inductive inference algorithm that synthesizes logic programs from examples of their behavior. The algorithm incorporates the diagnosis algorithms as a component. It is incremental, and progresses by debugging a program with respect to the examples. The Model Inference System is a Prolog implementation of the algorithm. Its range of applications and efficiency is comparable to existing systems for program synthesis from examples and grammatical inference.  We develop an algorithm that can fix a bug that has been identified, and integrate it with the diagnosis algorithms to form an interactive debugging system. By restricting the class of bugs we attempt to correct, the system can debug programs that are too complex for the Model Inference System to synthesize.",There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. ( 1984 ) and the PROLOG synthesis method of #AUTHOR_TAG .,"['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984).', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert (1969), assertional statements as in Michalski (1980), or semantic nets as in Winston (1975).', 'That is, the current system learns procedures rather than data structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. ( 1984 ) and the PROLOG synthesis method of #AUTHOR_TAG .']",1,"['The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert (1969), assertional statements as in Michalski (1980), or semantic nets as in Winston (1975).', 'That is, the current system learns procedures rather than data structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. ( 1984 ) and the PROLOG synthesis method of #AUTHOR_TAG .']"
CC713,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,computer control via limited natural language,"['P Fink', 'A Sigmon', 'A Biermann']",conclusion,"A natural language processor is described for control of a machine in task-oriented situations. Particular emphasis is given to issues related to flow-of-control statements in dialogue. These include branching constructs, as in `if row 1 contains a positive entry, then . . .' and looping constructs, as in `repeat for all other rows'. Special problems are discussed concerning the processing of deeply nested control structures, pronoun resolution, and the handling of conjunctions. An experiment is described in which the robustness of the conditional feature was tested with a group of computer naive subjects. It was found that subjects could discover and use the feature effectively in solving problems even though the fact of its existence was systematically withheld during the training session.","â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by Biermann and Krishnaswamy ( 1976 ) , â¢ a `` conditioning '' facility as described by #AUTHOR_TAG , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns .","[""â\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by Biermann and Krishnaswamy ( 1976 ) , â\x80¢ a `` conditioning '' facility as described by #AUTHOR_TAG , â\x80¢ implementation of new types of paraphrasing , â\x80¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â\x80¢ examining inter-speaker dialogue patterns .""]",3,"[""â\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by Biermann and Krishnaswamy ( 1976 ) , â\x80¢ a `` conditioning '' facility as described by #AUTHOR_TAG , â\x80¢ implementation of new types of paraphrasing , â\x80¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â\x80¢ examining inter-speaker dialogue patterns .""]"
CC714,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the nomad system expectationbased detection and correction of errors during understanding of syntactically illformed text,['R Granger'],,,"The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , #AUTHOR_TAG , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .","['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , #AUTHOR_TAG , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']",1,"['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , #AUTHOR_TAG , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.']"
CC715,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,linguistic analysis of natural language communication with computers,['B Thompson'],,"Interaction with computers in natural  language requires a language that is flexible  and suited to the task. This study of natural  dialogue was undertaken to reveal those characteristics  which can make computer English more  natural. Experiments were made in three modes  of communication: face-to-face, terminal-to-terminal  and human-to-computer, involving over  80 subjects, over 80,000 words and over 50  hours. They showed some striking similarities,  especially in sentence length and proportion of  words in sentences. The three modes also share  the use of fragments, typical of dialogue.  Detailed statistical analysis and comparisons  are given. The nature and relative frequency of  fragments, which have been classified into  twelve categories, is shown in all modes. Special  characteristics of the face-to-face mode  are due largely to these fragments (which  include phatics employed to keep the channel of  communication open). Special characteristics of  the computational mode include other fragments,  namely definitions, which are absent from other  modes. Inclusion of fragments in computational  grammar is considered a major factor in improving  computer naturalness.    The majority of experiments involved a real  life task of loading Navy cargo ships. The  peculiarities of face-to-face mode were similar  in this task to results of earlier experiments  involving another task. It was found that in  task oriented situations the syntax of interactions  is influenced in all modes by this context  in the direction of simplification, resulting in  short sentences (about 7 words long). Users  seek to maximize efficiency In solving the problem.  When given a chance, in the computational  mode, to utilize special devices facilitating  the solution of the problem, they all resort to  them.    Analyses of the special characteristics of  the computational mode, including the analysis  of the subjects"" errors, provide guidance for  the improvement of the habitability of such systems.  The availability of the REL System, a  high performance natural language system, made  the experiments possible and meaningful. The  indicated improvements in habitability are now  being embodied in the POL (Problem Oriented  Language) System, a successor to REL.","The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , #AUTHOR_TAG , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .","['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , #AUTHOR_TAG , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']",1,"['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , #AUTHOR_TAG , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.']"
CC716,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,toward natural language computation,"['A Biermann', 'B Ballard']",experiments,"A computer programming system called the ""Natural Language Computer"" (NLC) is described which allows a user to type English commands while watching them executed on sample data appearing on a display screen. Direct visual feedback enables the user to detect most misinterpretation errors as they are made so that incorrect or ambiguous commands can be retyped or clarified immediately. A sequence of correctly executed commands may be given a name and used as a subroutine, thus extending the set of available operations and allowing larger English-language programs to be constructed hierarchically. In addition to discussing the transition network syntax and procedural semantics of the system, special attention is devoted to the following topics: the nature of imperative sentences in the matrix domain; the processing of non-trivial noun phrases; conjunction; pronominals; and programming constructs such as ""if"", ""repeat"", and procedure definition.","An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #AUTHOR_TAG ) .","['This section has given an overview of the approach to history-based expectation processing.', 'The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented.', 'The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve.', 'The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #AUTHOR_TAG ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '[The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.]']",0,"['An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #AUTHOR_TAG ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).']"
CC717,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,learning structural descriptions from examples in,['P Winston'],,Massachusetts Institute of Technology. Dept. of Electrical Engineering. Thesis. 1970. Ph.D.,"The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in Michalski ( 1980 ) , or semantic nets as in #AUTHOR_TAG .","['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984).', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in Michalski ( 1980 ) , or semantic nets as in #AUTHOR_TAG .', 'That is, the current system learns procedures rather than data structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. (1984) and the PROLOG synthesis method of Shapiro (1982).']",1,"['The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in Michalski ( 1980 ) , or semantic nets as in #AUTHOR_TAG .', 'That is, the current system learns procedures rather than data structures.']"
CC718,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the dialogue designing dialogue system dissertation,['T-P Ho'],,,Another dialogue acquisition system has been developed by #AUTHOR_TAG .,"['The problem of handling ill-formed input has been studied by Carbonell and Hayes (1983), Granger (1983), Jensen et al. (1983), Kwasny and Sondheimer (1981), Riesbeek and Schank (1976), Thompson (1980), Weischedel and Black (1980), and Weischedel and Sondheimer (1983.', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by #AUTHOR_TAG .', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']",1,['Another dialogue acquisition system has been developed by #AUTHOR_TAG .']
CC719,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the hwim speech understanding system in lea,"['J Wolf', 'W Woods']",,,"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and #AUTHOR_TAG ) .","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and #AUTHOR_TAG ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']",1,"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and #AUTHOR_TAG ) .']"
CC720,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,relaxation techniques for parsing grammatically illformed input in natural language understanding systems,"['S Kwasny', 'N Sondheimer']",,"This paper investigates several language phenomena either considered deviant by linguistic standards or insufficiently addressed by existing approaches. These include co-occurrence violations, some forms of ellipsis and extraneous forms, and conjunction. Relaxation techniques for their treatment in Natural Language Understanding Systems are discussed. These techniques, developed within the Augmented Transition Network (ATN) model, are shown to be adequate to handle many of these cases.","The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , #AUTHOR_TAG , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .","['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , #AUTHOR_TAG , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']",1,"['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , Jensen et al. ( 1983 ) , #AUTHOR_TAG , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.']"
CC721,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,pattern recognition as ruleguided inductive inference,['R Michalski'],,"The determination of pattern recognition rules is viewed as a problem of inductive inference, guided by generalization rules, which control the generalization process, and problem knowledge rules, which represent the underlying semantics relevant to the recognition problem under consideration. The paper formulates the theoretical framework and a method for inferring general and optimal (according to certain criteria) descriptions of object classes from examples of classification or partial descriptions. The language for expressing the class descriptions and the guidance rules is an extension of the first-order predicate calculus, called variable-valued logic calculus VL21. VL21 involves typed variables and contains several new operators especially suited for conducting inductive inference, such as selector, internal disjunction, internal conjunction, exception, and generalization. Important aspects of the theory include: 1) a formulation of several kinds of generalization rules; 2) an ability to uniformly and adequately handle descriptors (i.e., variables, functions, and predicates) of different type (nominal, linear, and structured) and of different arity (i.e., different number of arguments); 3) an ability to generate new descriptors, which are derived from the initial descriptors through a rule-based system (i.e., an ability to conduct the so called constructive induction); 4) an ability to use the semantics underlying the problem under consideration. An experimental computer implementation of the method is briefly described and illustrated by an example.","The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in #AUTHOR_TAG , or semantic nets as in Winston ( 1975 ) .","['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984).', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in #AUTHOR_TAG , or semantic nets as in Winston ( 1975 ) .', 'That is, the current system learns procedures rather than data structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. (1984) and the PROLOG synthesis method of Shapiro (1982).']",1,"['The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert ( 1969 ) , assertional statements as in #AUTHOR_TAG , or semantic nets as in Winston ( 1975 ) .', 'That is, the current system learns procedures rather than data structures.']"
CC722,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,construction of programs from example computations,"['A Biermann', 'R Krishnaswamy']",,"An autoprogrammer is an interactive computer programming system which automatically constructs computer programs from example computations executed by the user. The example calculations are done in a scratch pad fashion at a computer display using a light pen or other graphic input device, and the system stores a detailed history of all of the steps executed in the process. Then the system automatically synthesizes the shortest possible program which is capable of executing the observed examples. The paper describes the computational environment provided by the system, proves that the program synthesis technique is both ""sound"" and ""complete,"" describes the design of the system, and gives some programs it was used to create.",The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #AUTHOR_TAG where program flowcharts were constructed from traces of their behaviors .,"['The problem of handling ill-formed input has been studied by Carbonell and Hayes (1983), Granger (1983), Jensen et al. (1983), Kwasny and Sondheimer (1981), Riesbeek and Schank (1976), Thompson (1980), Weischedel and Black (1980), and Weischedel and Sondheimer (1983.', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #AUTHOR_TAG where program flowcharts were constructed from traces of their behaviors .', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']",1,"['The problem of handling ill-formed input has been studied by Carbonell and Hayes (1983), Granger (1983), Jensen et al. (1983), Kwasny and Sondheimer (1981), Riesbeek and Schank (1976), Thompson (1980), Weischedel and Black (1980), and Weischedel and Sondheimer (1983.', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #AUTHOR_TAG where program flowcharts were constructed from traces of their behaviors .', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']"
CC723,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,scripts plans goals and understanding lawrence erlbaum associates,"['R Schank', 'R Abelson']",experiments,,"The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph ( #AUTHOR_TAG ) , a deep parse of Si , or some other representation .","['We denote the meaning of each sentence Si with the notation M(Si).', 'The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph ( #AUTHOR_TAG ) , a deep parse of Si , or some other representation .', 'A user behavior is represented by a network, or directed graph, of such meanings.', 'At the beginning of a task, the state of the interaction is represented by the start state of the graph.']",0,"['We denote the meaning of each sentence Si with the notation M(Si).', 'The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph ( #AUTHOR_TAG ) , a deep parse of Si , or some other representation .', 'A user behavior is represented by a network, or directed graph, of such meanings.']"
CC724,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the sperry univac system for continuous speech recognition in,['M Medress'],,,"A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , #AUTHOR_TAG , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , #AUTHOR_TAG , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']",1,"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , #AUTHOR_TAG , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .']"
CC725,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the hearsayii speech understanding system integrating knowledge to resolve uncertainty,"['L Erman', 'F Hayes-Roth', 'V Lesser', 'D Reddy']",,"The Hearsay-II system, developed during the DARPA-sponsored five-year speech-understanding research program, represents both a specific solution to the speech-understanding problem and a general framework for coordinating independent processes to achieve cooperative problem-solving behavior. As a computational problem, speech understanding reflects a large number of intrinsically interesting issues. Spoken sounds are achieved by a long chain of successive transformations, from intentions, through semantic and syntactic structuring, to the eventually resulting audible acoustic waves. As a consequence, interpreting speech means effectively inverting these transformations to recover the speaker's intention from the sound. At each step in the interpretive process, ambiguity and uncertainty arise.    The Hearsay-II problem-solving framework reconstructs an intention from hypothetical interpretations formulated at various levels of abstraction. In addition, it allocates limited processing resources first to the most promising incremental actions. The final configuration of the Hearsay-II system comprises problem-solving components to generate and evaluate speech hypotheses, and a focus-of-control mechanism to identify potential actions of greatest value. Many of these specific procedures reveal novel approaches to speech problems. Most important, the system successfully integrates and coordinates all of these independent activities to resolve uncertainty and control combinatorics. Several adaptations of the Hearsay-II framework have already been undertaken in other problem domains, and it is anticipated that this trend will continue; many future systems necessarily will integrate diverse sources of knowledge to solve complex problems cooperatively.    Discussed in this paper are the characteristics of the speech problem in particular, the special kinds of problem-solving uncertainty in that domain, the structure of the Hearsay-II system developed to cope with that uncertainty, and the relationship between Hearsay-II's structure and those of other speech-understanding systems. The paper is intended for the general computer science audience and presupposes no speech or artificial intelligence background.","A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , #AUTHOR_TAG , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , #AUTHOR_TAG , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']",1,"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , #AUTHOR_TAG , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.']"
CC726,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,parse fitting and prose fixing getting a hold on illformedness,"['K Jensen', 'G Heidorn', 'L Miller', 'Y Ravin']",,"Processing syntactically ill-formed language is an important mission of the EPISTLE system, Ill-formed input is treated by this system in various ways. Misspellings are highlighted by a standard spelling checker; syntactic errors are detected and corrections are suggested; and stylistic infelicities are called to the user's attention.Central to the EPISTLE processing strategy is its technique of fitted parsing. When the rules of a conventional syntactic grammar are unable to produce a parse for an input string, this technique can be used to produce a reasonable approximate parse that can serve as input to the remaining stages of processing.This paper first describes the fitting process and gives examples of ill-formed language situations where it is called into play. We then show how a fitted parse allows EPISTLE to carry on its text-critiquing mission where conventional grammars would fail either because of input problems or because of limitations in the grammars themselves. Some inherent difficulties of the fitting technique are also discussed. In addition, we explore how style critiquing relates to the handling of ill-formed input, and how a fitted parse can be used in style checking.","The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , #AUTHOR_TAG , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .","['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , #AUTHOR_TAG , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']",1,"['The problem of handling ill-formed input has been studied by Carbonell and Hayes ( 1983 ) , Granger ( 1983 ) , #AUTHOR_TAG , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.']"
CC727,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,natural language with discrete speech as a mode for humantomachine communication,"['A Biermann', 'R Rodman', 'D Rubin', 'J Heidlage']",experiments,"A voice interactive natural language system, which allows users to solve problems with spoken English commands, has been constructed. The system utilizes a commercially available discrete speech recognizer which requires that each word be followed by approximately a 300 millisecond pause. In a test of the system, subjects were able to learn its use after about two hours of training. The system correctly processed about 77 percent of the over 6000 input sentences spoken in problem-solving sessions. Subjects spoke at the rate of about three sentences per minute and were able to effectively use the system to complete the given tasks. Subjects found the system relatively easy to learn and use, and gave a generally positive report of their experience.","[ The current system should be distinguished from an earlier voice system ( VNLC , #AUTHOR_TAG ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]","['This section has given an overview of the approach to history-based expectation processing.', 'The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented.', 'The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve.', 'The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.', 'An off-the-shelf speech recognition device, a Nippon Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (Ballard 1979, Biermann andBallard 1980).', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '[ The current system should be distinguished from an earlier voice system ( VNLC , #AUTHOR_TAG ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]']",1,"['The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.', 'An off-the-shelf speech recognition device, a Nippon Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (Ballard 1979, Biermann andBallard 1980).', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '[ The current system should be distinguished from an earlier voice system ( VNLC , #AUTHOR_TAG ) , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]']"
CC728,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,transition network grammars for natural language analysis,['W Woods'],experiments,"The use of augmented transition network grammars for the analysis of natural language sentences is described. Structure-building actions associated with the arcs of the grammar network allow for the reordering, restructuring, and copying of constituents necessary to produce deep-structure representations of the type normally obtained from a transformational analysis, and conditions on the arcs allow for a powerful selectivity which can rule out meaningless analyses and take advantage of semantic information to guide the parsing. The advantages of this model for natural language analysis are discussed in detail and illustrated by examples. An implementation of an experimental parsing system for transition network grammars is briefly described.",The expectation parser uses an ATN-like representation for its grammar ( #AUTHOR_TAG ) .,"['The expectation parser uses an ATN-like representation for its grammar ( #AUTHOR_TAG ) .', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar, imperative sentences with nested noun groups and conjunctions (Ballard 1979).', 'An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses.', 'Sentences have the same ""meaning"" if they ""result in identical tasks being performed.', 'The various sentence structures that']",5,"['The expectation parser uses an ATN-like representation for its grammar ( #AUTHOR_TAG ) .', 'The various sentence structures that']"
CC729,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the acquisition and use of dialogue expectation in speech recognition,['P Fink'],experiments,,"The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #AUTHOR_TAG ) .","['This section has given an overview of the approach to history-based expectation processing.', 'The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented.', 'The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve.', 'The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.', 'An off-the-shelf speech recognition device, a Nippon Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (Ballard 1979, Biermann andBallard 1980).', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #AUTHOR_TAG ) .', '[The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.]']",0,"['The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #AUTHOR_TAG ) .']"
CC730,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the sdc speech understanding system in lea,"['J Barnett', 'M Berstein', 'R Gillman', 'I Kameny']",,,"A number of speech understanding systems have been developed during the past fifteen years ( #AUTHOR_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .","['A number of speech understanding systems have been developed during the past fifteen years ( #AUTHOR_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']",1,"['A number of speech understanding systems have been developed during the past fifteen years ( #AUTHOR_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.']"
CC731,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the acquisition and use of dialogue expectation in speech recognition,['P Fink'],,,A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #AUTHOR_TAG .,"['A number of speech understanding systems have been developed during the past fifteen years (Barnett et al. 1980, Dixon and Martin 1979, Erman et al. 1980, Haton and Pierrel 1976, Lea 1980, Lowerre and Reddy 1980, Medress 1980, Reddy 1976, Walker 1978, and Wolf and Woods 1980.', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #AUTHOR_TAG .']",0,"['A number of speech understanding systems have been developed during the past fifteen years (Barnett et al. 1980, Dixon and Martin 1979, Erman et al. 1980, Haton and Pierrel 1976, Lea 1980, Lowerre and Reddy 1980, Medress 1980, Reddy 1976, Walker 1978, and Wolf and Woods 1980.', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #AUTHOR_TAG .']"
CC732,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,construction of programs from example computations,"['A Biermann', 'R Krishnaswamy']",conclusion,"An autoprogrammer is an interactive computer programming system which automatically constructs computer programs from example computations executed by the user. The example calculations are done in a scratch pad fashion at a computer display using a light pen or other graphic input device, and the system stores a detailed history of all of the steps executed in the process. Then the system automatically synthesizes the shortest possible program which is capable of executing the observed examples. The paper describes the computational environment provided by the system, proves that the program synthesis technique is both ""sound"" and ""complete,"" describes the design of the system, and gives some programs it was used to create.","â¢ use of low level knowledge from the speech recognition phase , â¢ use of high level knowledge about the domain in particular and the dialogue task in general , â¢ a `` continue '' facility and an `` auto-loop '' facility as described by #AUTHOR_TAG , â¢ a `` conditioning '' facility as described by Fink et al. ( 1985 ) , â¢ implementation of new types of paraphrasing , â¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â¢ examining inter-speaker dialogue patterns .","[""â\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by #AUTHOR_TAG , â\x80¢ a `` conditioning '' facility as described by Fink et al. ( 1985 ) , â\x80¢ implementation of new types of paraphrasing , â\x80¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â\x80¢ examining inter-speaker dialogue patterns .""]",3,"[""â\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by #AUTHOR_TAG , â\x80¢ a `` conditioning '' facility as described by Fink et al. ( 1985 ) , â\x80¢ implementation of new types of paraphrasing , â\x80¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â\x80¢ examining inter-speaker dialogue patterns .""]"
CC733,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,recovery strategies for parsing extragrammatical language,"['J Carbonell', 'P Hayes']",,"Practical natural language interfaces must exhibit robust behaviour in the presence of extragrammatical user input. This paper classifies different types of grammatical deviations and related phenomena at the lexical, sentential and dialogue levels and presents recovery strategies tailored to specific phenomena in the classification. Such strategies constitute a tool chest of computationally tractable methods for coping with extragrammatieality in restricted domain natural language. Some of the strategies have been tested and proven viable in existing parsers.  </p","The problem of handling ill-formed input has been studied by #AUTHOR_TAG , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .","['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'However, these methodologies have not used historical information at the dialogue level as described here.', 'In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'Thus, an error in this work has no pattern but occurs probabilistically.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by Ho (1984).', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']",1,"['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , Granger ( 1983 ) , Jensen et al. ( 1983 ) , Kwasny and Sondheimer ( 1981 ) , Riesbeck and Schank ( 1976 ) , Thompson ( 1980 ) , Weischedel and Black ( 1980 ) , and Weischedel and Sondheimer ( 1983 ) .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence.', 'A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc.', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.']"
CC734,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,perceptrons,"['M Minsky', 'S Papert']",,"Ashkin-Teller type perceptron models are introduced. Their maximal capacity per number of couplings is calculated within a first-step replica-symmetry-breaking Gardner approach. The results are compared with extensive numerical simulations using several algorithms.Comment: 8 pages in Latex with 2 eps figures, RSB1 calculations has been adde","The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #AUTHOR_TAG , assertional statements as in Michalski ( 1980 ) , or semantic nets as in Winston ( 1975 ) .","['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984).', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #AUTHOR_TAG , assertional statements as in Michalski ( 1980 ) , or semantic nets as in Winston ( 1975 ) .', 'That is, the current system learns procedures rather than data structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. (1984) and the PROLOG synthesis method of Shapiro (1982).']",1,"['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984).', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #AUTHOR_TAG , assertional statements as in Michalski ( 1980 ) , or semantic nets as in Winston ( 1975 ) .', 'That is, the current system learns procedures rather than data structures.']"
CC735,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,semantic processing for a natural language programming system,['B Ballard'],experiments,,"An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #AUTHOR_TAG , Biermann and Ballard 1980 ) .","['This section has given an overview of the approach to history-based expectation processing.', 'The details of the method are dependent on how the functions P, Predicts, Mergeable, and Merge are implemented.', 'The following sections describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve.', 'The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #AUTHOR_TAG , Biermann and Ballard 1980 ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '[The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.]']",0,"['An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( #AUTHOR_TAG , Biermann and Ballard 1980 ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).']"
CC736,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,speech recognition by machine a review,['D Reddy'],,"This paper presents a brief survey on Automatic Speech Recognition and discusses the major themes and advances made in the past 60 years of research, so as to provide a technological perspective and an appreciation of the fundamental progress that has been accomplished in this important area of speech communication. After years of research and development the accuracy of automatic speech recognition remains one of the important research challenges (e.g., variations of the context, speakers, and environment).The design of Speech Recognition system requires careful attentions to the following issues: Definition of various types of speech classes, speech representation, feature extraction techniques, speech classifiers, database and performance evaluation. The problems that are existing in ASR and the various techniques to solve these problems constructed by various research workers have been presented in a chronological order. Hence authors hope that this work shall be a contribution in the area of speech recognition. The objective of this review paper is to summarize and compare some of the well known methods used in various stages of speech recognition system and identify research topic and applications which are at the forefront of this exciting and challenging field.","A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , #AUTHOR_TAG , Walker 1978 , and Wolf and Woods 1980 ) .","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , #AUTHOR_TAG , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']",1,"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , #AUTHOR_TAG , Walker 1978 , and Wolf and Woods 1980 ) .']"
CC737,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,semantic processing for a natural language programming system,['B Ballard'],experiments,,"The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( #AUTHOR_TAG ) .","['The expectation parser uses an ATN-like representation for its grammar (Woods 1970).', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( #AUTHOR_TAG ) .', 'An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses.', 'Sentences have the same ""meaning"" if they ""result in identical tasks being performed.', 'The various sentence structures that']",0,"['The expectation parser uses an ATN-like representation for its grammar (Woods 1970).', 'The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions ( #AUTHOR_TAG ) .', 'The various sentence structures that']"
CC738,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,the acquisition and use of dialogue expectation in speech recognition,['P Fink'],experiments,,How it is done is beyond the scope of this paper but is explained in detail in #AUTHOR_TAG .,"[""In such a situation, the user's intentions may be reflected more correctly by the following expected sentence set: double (rARG) 1.0 which signifies that any row may be referred to."", 'However, though this simplified expected sentence set may be a good generalization of the pattern observed, it has ramifications for error correction.', 'Specifically, it will be unable to fill in a row number should that value be missing in the incoming sentence.', 'The first option also has its drawbacks.', 'In this case, should the row number be missing in the sentence, the expectation parser will error correct the sentence to the most probable value, or the first one in the set if the probabilities are equal, here the value one for row 1.', 'Thus, both options are imperfect in terms of the error correction capabilities that they can provide.', 'The comparison that must be made to determine which option is better in a given situation is how often the first will error correct incorrectly as opposed to how much error correcting power we will lose by using the second.', 'How it is done is beyond the scope of this paper but is explained in detail in #AUTHOR_TAG .']",0,"[""In such a situation, the user's intentions may be reflected more correctly by the following expected sentence set: double (rARG) 1.0 which signifies that any row may be referred to."", 'However, though this simplified expected sentence set may be a good generalization of the pattern observed, it has ramifications for error correction.', 'Specifically, it will be unable to fill in a row number should that value be missing in the incoming sentence.', 'The first option also has its drawbacks.', 'In this case, should the row number be missing in the sentence, the expectation parser will error correct the sentence to the most probable value, or the first one in the set if the probabilities are equal, here the value one for row 1.', 'The comparison that must be made to determine which option is better in a given situation is how often the first will error correct incorrectly as opposed to how much error correcting power we will lose by using the second.', 'How it is done is beyond the scope of this paper but is explained in detail in #AUTHOR_TAG .']"
CC739,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,automatic program construction techniques,"['A Biermann', 'G Guiho', 'Y Kodratoff', 'Eds']",,"The purpose of Avignon'86 is to provide a forum for presentat ion of new implementat ions of expert systems and basic tools and techniques for building expert systems. Aimed at developers and users of expert systems, the conference and exhibit ion will o f fer an assessment of available tools and techniques; will provide practical guidelines for making decisions concerning the application of expert system technology; and will help define, clarify, and make sense of the claims, promises, and realit ies of practical expert system applications.",There is some literature on procedure acquisition such as the LISP synthesis work described in #AUTHOR_TAG and the PROLOG synthesis method of Shapiro ( 1982 ) .,"['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984).', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert (1969), assertional statements as in Michalski (1980), or semantic nets as in Winston (1975).', 'That is, the current system learns procedures rather than data structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in #AUTHOR_TAG and the PROLOG synthesis method of Shapiro ( 1982 ) .']",1,"['That is, the current system learns procedures rather than data structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in #AUTHOR_TAG and the PROLOG synthesis method of Shapiro ( 1982 ) .']"
CC740,J86-1002,The Correction of Ill-Formed Input Using History-Based Expectation with Applications to Speech Understanding,organization and operation of a connected speech understanding system at lexical syntactic and semantic levels,"['J Haton', 'J Pierrel']",,"This paper describes a connected speech understanding system being implemented in Nancy, thanks to the work done in automatic speech recognition since 1968. This system is made up of four parts : an acoustic recognizer which gives a string of phoneme-like segments from a spoken sentence, a syntactic parser which controls the recognition process, a word recognizer working on words predicted by the parser and a dialog procedure which takes in account semantic constraints in order to avoid some of the errors and ambiguities. Some original features of the system are pointed out : modularily (e.g. the language used is considered as a parameter), possibility of processing slightly syntactically incorrect sentences, ... The application both in data management and in oral control of a telephone center has given very promising results. Work is in progress for generalizing our model : extension of the vocabulary and of the grammar, multi-speaker operation, etc.","A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983).']",1,"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #AUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', 'Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence.']"
CC741,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,processing dictionary definitions with phrasal pattern hierarchies in this issue,['Hiyan Alshawi'],introduction,"This paper shows how dictionary word sense definitions can be analysed by applying a hierarchy of phrasal patterns. An experimental system embodying this mechanism has been implemented for processing definitions from the Longman Dictionary of Contemporary English. A property of this dictionary, exploited by the system, is that it uses a restricted vocabulary in its word sense definitions. The structures generated by the experimental system are intended to be used for the classification of new word senses in terms of the senses of words in the restricted vocabulary. Examples illustrating the output generated are presented, and some qualitative performance results and problems that were encountered are discussed. The analysis process applies successively more specific phrasal analysis rules as determined by a hierarchy of patterns in which less specific patterns dominate more specific ones. This ensures that reasonable incomplete analyses of the definitions are produced when more complete analyses are not possible, resulting in a relatively robust analysis mechanism. Thus the work reported addresses two robustness problems faced by current experimental natural language processing systems: coping with an incomplete lexicon and with incomplete knowledge of phrasal constructions.",In this paper we focus on the exploitation of the LDOCE grammar coding system ; Alshawi et al. ( 1985 ) and #AUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE .,"['We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '(Michiels (1982) contains further description and discussion of LDOCE.)', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ; Alshawi et al. ( 1985 ) and #AUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE .']",0,"['We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", 'In this paper we focus on the exploitation of the LDOCE grammar coding system ; Alshawi et al. ( 1985 ) and #AUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE .']"
CC742,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,exploiting a large dictionary data base,['Archibal Michiels'],,,#AUTHOR_TAG and Akkerman et al. ( 1985 ) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .,"['The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, I5, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to I5 if the verb is felt to be semantically two place rather than one place, such as know versus appear.', 'On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic.', '#AUTHOR_TAG and Akkerman et al. ( 1985 ) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', 'Ingria (1984) comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated.']",0,['#AUTHOR_TAG and Akkerman et al. ( 1985 ) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .']
CC743,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a parser for generalised phrase structure grammars,"['John Phillips', 'Henry Thompson']",introduction,,"The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; Russell et al. , 1986 ; #AUTHOR_TAG ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .","['Recent developments in linguistics, and especially on grammatical theory --for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al., 1985), Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) --and on natural language parsing frameworks for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-II (Shieber, 1984) --make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; Russell et al. , 1986 ; #AUTHOR_TAG ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']",0,"['Recent developments in linguistics, and especially on grammatical theory --for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al., 1985), Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) --and on natural language parsing frameworks for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-II (Shieber, 1984) --make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; Russell et al. , 1986 ; #AUTHOR_TAG ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']"
CC744,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,machinereadable dictionaries lexical data bases and the lexical system,['Nicoletta Calzolari'],,,"However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #AUTHOR_TAG for further discussion ) .","['The lispified LDOCE file retains the broad structure of the typesetting tape and divides each entry into a number of fields --head word, pronunciation, grammar codes, definitions, examples, and so forth.', 'However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #AUTHOR_TAG for further discussion ) .', 'For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information.', 'For example, word senses are largely defined in terms of the 2000 word core vocabulary, however, in some cases other words (themselves defined elsewhere in terms of this vocabulary) are used.', 'These words always appear in small capitals and can therefore be recognised because they will be preceded by a font change control character.', 'In Figure 1 above the definition of rivet as verb includes the noun definition of ""RIVET 1\'\', as signalled by the font change and the numerical superscript which indicates that it is the first (i.e.', 'noun entry) homograph; additional notation exists for word senses within homographs.', 'On the typesetting tape, font control characters are indicated by hexadecimal numbers within curly brackets.', 'In addition, there is a further complication because this sense is used in the plural and the plural morpheme must be removed before RIVET can be associated with a dictionary entry.', 'However, the restructuring program can achieve this because such morphology is always italicised, so the program knows that, in the context of non-core vocabulary items, the italic font control character signals the Figure 3 occurrence of a morphological variant of a LDOCE head entry.']",0,"['The lispified LDOCE file retains the broad structure of the typesetting tape and divides each entry into a number of fields --head word, pronunciation, grammar codes, definitions, examples, and so forth.', 'However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #AUTHOR_TAG for further discussion ) .', 'For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information.', 'For example, word senses are largely defined in terms of the 2000 word core vocabulary, however, in some cases other words (themselves defined elsewhere in terms of this vocabulary) are used.', 'These words always appear in small capitals and can therefore be recognised because they will be preceded by a font change control character.', 'noun entry) homograph; additional notation exists for word senses within homographs.', 'On the typesetting tape, font control characters are indicated by hexadecimal numbers within curly brackets.', 'In addition, there is a further complication because this sense is used in the plural and the plural morpheme must be removed before RIVET can be associated with a dictionary entry.', 'However, the restructuring program can achieve this because such morphology is always italicised, so the program knows that, in the context of non-core vocabulary items, the italic font control character signals the Figure 3 occurrence of a morphological variant of a LDOCE head entry.']"
CC745,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,complement types in english,['Robert Ingria'],,"223 p.Thesis (Ph.D.)--University of Illinois at Urbana-Champaign, 1971.U of I OnlyRestricted to the U of I community idenfinitely during batch ingest of legacy ETD",#AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .,"['The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, I5, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to I5 if the verb is felt to be semantically two place rather than one place, such as know versus appear.', 'On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic.', 'Michiels (1982) and Akkerman et al. (1985) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description.', '#AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .']",1,['#AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .']
CC746,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,syntactic argumentation and the structure of english,"['D M Perlmutter', 'S Soames']",,"Syntactic Argumentation and the Structure of English (SASE) presents the major theoretical developments in generative syntax and the empirical arguments motivating them. Beautifully and lucidly written, it is an invaluable resource for working linguists as well as a pedagogical tool of unequaled depth and breadth. The chief focus of the book is syntactic argumentation. Beginning with the fundamentals of generative syntax, it proceeds by a series of gradually unfolding arguments to analyses of some of the most sophisticated proposals. It includes a wide variety of problems that guide the reader in constructing arguments deciding between alternative analyses of syntactic constructions and alternative theoretical formulations. Someone who has worked through the problems and arguments in this book will be able to apply the skills in argumentation it develops to novel issues in syntax. While teaching syntactic argumentation, SASE covers the major empirical results of generative syntax. Its contents include: Transformations in single-clause sentences; Complementation and multi-clause transformations; Universal principles governing rule interaction: the cycle and strict cyclicity; Movement rules; Ross' constraints; Pronominal reference and anaphora. SASE is an important book for several different audiences: for students, it is an introduction to syntax that teaches argumentation as well as a wide range of empirical results in the field; for linguists, it is a sourcebook of classical analyses and arguments, with some new arguments bearing on classical issues; and, for scholars, teachers, and students in related fields, it is a comprehensive guide to the major empirical and theoretical developments in generative syntax. SASE contains enough material for a two-semester or three-quarler sequence in syntax. Because it assumes no previous background, it can be used as the main text in an introduction to syntax. Since it covers a wide range of material not available in other texts, it is also suitable for intermediate and advanced syntax courses and as a supplementary source in more specialized courses and courses in other disciplines. A storehouse of classical and original arguments, SASE will prove to be of lasting value to the teacher, the student, and researchers in both linguistics and related fields.","Clearly, there are other syntactic and semantic tests for this distinction, (see eg. #AUTHOR_TAG:472 ), but these are the only ones which are explicit in the LDOCE coding system.","['Clearly, there are other syntactic and semantic tests for this distinction, (see eg. #AUTHOR_TAG:472 ), but these are the only ones which are explicit in the LDOCE coding system.']",0,"['Clearly, there are other syntactic and semantic tests for this distinction, (see eg. #AUTHOR_TAG:472 ), but these are the only ones which are explicit in the LDOCE coding system.']"
CC747,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,word formation in natural language processing systems,['Roy Byrd'],introduction,,"Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #AUTHOR_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .","['The idea of using the machine readable source of a published dictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name but a few applications.', 'Most of the work on automated dictionaries has concentrated on extracting lexical or other information, essentially by batch processing (eg. Amsler, 1981 ;Walker and Amsler, 1986), or Copyright 1987 by the Association for Computational Linguistics.', 'Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.', 'To copy otherwise, or to republish, requires a fee and/or specific permission.', '0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b).', 'Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', 'Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand.', 'Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #AUTHOR_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .', 'In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).', 'However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.', 'In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.']",1,"['Two exceptions to this generalisation are the Linguistic String Project ( Sager , 1981 ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; #AUTHOR_TAG ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .']"
CC748,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a dictionary and morphological analyser for english,"['Graham Russell', 'Steve Pulman', 'Graeme Ritchie', 'Alan Black']",,"This paper describes the current state of a three-year project aimed at the development of software for use in handling large quantit ies of dictionary information within natural language processing systems. 1 The project was accepted for funding by SERC/Alvey commencing ill June 1984, and is being carried out by Graeme Ritchie and Alan Black at the Universi ty of Edinburgh and Steve Puhnan and Graham Russell at the Universi ty of Cambridge. It is one of three closely related projects funded under the Alvey IKBS Programme (Natural Language Tlleme); a parser is under development at Edinburgh by Henry Thompson and John Phillips, and a sentence grammar is being devised by Ted Briscoe and Clare Grover at Lancaster and Bran Boguraev and John Carroll at Cambridge. It is intended tha t the software and rules produced by all three projects wil l be directly compatible and capable of functioning in an integrated system. Realistic and useful na tura l language processing systems such as database f ront-ends require large numbers of words, together wi th associated syntactic and semantic Information, to be efficiently stored in machine-readable form. Our system is Intended to provide the necessary facilities, being designed to store a large number (at least 10,000) of words and to perform morphological analysis on them, covering both Inflectional and derlvatlonal morphology. In pursuit of these objectives, the dictionary associates wi th each word information concerning its morphosyntactlc properties. Users are free to modify the system In a number of ways; they may add to the lexical entries Lisp functions tha t perform semantic manipulatlons, and tailor the dictionary to the particular subject mat ter they are interested in (different databases, for example). I t Is also hoped that the system is general enough to be of use to linguists wishing to Investigate the morphology of English and other languages. Contents of the basle data files may be altered or replaced: 1. A 'Word Grammar ' file contains rules assigning internal s t ructure to complex words, 2. A 'Lexicon' file holds the morpheme entries which include syntactic and other Information associated wi th stems and affixes. 3. A 'Spelling Rules' file contains rules governing permissible correspondences between the form of morphemes listed in the lextcon and complex words consisting of sequences of these morphemes. Once these data flies have been prepared, they are compiled using a number of pre-processtng functions tha t operate to produce a set of output files. These constitute a fu l ly expanded and cross-Indexed dictionary which can then be accessed from within LISP. The process of morphological analysis consists of parslng a sequence of Input morphemes wi th respect to the word grammar, It Is Implemented as an active chart parser (Thompson & Rltchle (1984)), and builds a s t ructure in the form of a tree in which each node has two","No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and #AUTHOR_TAG ) .","['The program which transforms the LDOCE grammar codes into lexical entries utilisable by a parser takes as input the decompacted codes and produces a relatively theory neutral representation of the lexical entry for a particular word, in the sense that this representation could be further transformed into a format suitable for most current parsing systems.', 'For example, if the input were the third sense of believe, as in Figure 4, the program would generate the (partial) entry shown in Figure 8  Figure 8 At the time of writing, rules for producing adequate entries to drive a parsing system have only been developed for verb codes.', 'In what follows we will describe the overall transformation strategy and the particular rules we have developed for the verb codes.', 'Extending the system to handle nouns, adjectives and adverbs would present no problems of principle.', 'However, the LDOCE coding of verbs is more comprehensive than elsewhere, so verbs are the obvious place to start in an evaluation of the usefulness of the coding system.', 'No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and #AUTHOR_TAG ) .']",0,"['No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and #AUTHOR_TAG ) .']"
CC749,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,towards a lexicon support environment for real time parsing,"['Hiyan Alshawi', 'Branimir Boguraev', 'Ted Briscoe']",introduction,,In this paper we focus on the exploitation of the LDOCE grammar coding system ; #AUTHOR_TAG and Alshawi ( 1987 ) describe further research in Cambridge utilising different types of information available in LDOCE .,"['We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '(Michiels (1982) contains further description and discussion of LDOCE.)', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ; #AUTHOR_TAG and Alshawi ( 1987 ) describe further research in Cambridge utilising different types of information available in LDOCE .']",0,"['We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", 'In this paper we focus on the exploitation of the LDOCE grammar coding system ; #AUTHOR_TAG and Alshawi ( 1987 ) describe further research in Cambridge utilising different types of information available in LDOCE .']"
CC750,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a critical assessment of the ldoce coding system to appear in,['Erik Akkerman'],,,"One approach to this problem is that taken by the ASCOT project ( Akkerman et al. , 1985 ; #AUTHOR_TAG ) .","['This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982, for further comment).', 'One approach to this problem is that taken by the ASCOT project ( Akkerman et al. , 1985 ; #AUTHOR_TAG ) .', 'In this project, a new lexicon is being manually derived from LDOCE.', 'The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power.', 'More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis.', 'In the medium term this approach, though time consuming, will be of some utility for producing more reliable lexicons for natural language processing.']",0,"['This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982, for further comment).', 'One approach to this problem is that taken by the ASCOT project ( Akkerman et al. , 1985 ; #AUTHOR_TAG ) .', 'The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power.', 'More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis.']"
CC751,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,the design of a computer language for linguistic information,['S Shieber'],,"A considerable body of accumulated knowledge about the design of languages for communicating information to computers has been derived from the subfields of programming language design and semantics. It has been the goal of the PATR group at SRI to utilize a relevant portion of this knowledge in implementing tools to facilitate communication of linguistic information to computers. The PATR-II formalism is our current computer language for encoding linguistic information. This paper, a brief overview of that formalism, attempts to explicate our design decisions in terms of a set of properties that effective computer languages should incorporate.Engineering and Applied Science",To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system ( #AUTHOR_TAG and references therein ) .,"['The output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms.', 'To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system ( #AUTHOR_TAG and references therein ) .', 'PATR-II was chosen because it has been reimplemented in Cambridge and was therefore, available; however, the task would be nearly identical if we were constructing entries for a system based on GPSG, FUG or LFG.', 'We The latter employs a grammatical formalism based on GPSG; the comparatively theory neutral lexical entries that we construct from LDOCE should translate straightforwardly into this framework as well.']",5,"['The output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms.', 'To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system ( #AUTHOR_TAG and references therein ) .', 'We The latter employs a grammatical formalism based on GPSG; the comparatively theory neutral lexical entries that we construct from LDOCE should translate straightforwardly into this framework as well.']"
CC752,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,designing a computerised lexicon for linguistic purposes,"['Erik Akkerman', 'Pieter Masereeuw', 'Willem Meijs']",,,"One approach to this problem is that taken by the ASCOT project ( #AUTHOR_TAG ; Akkerman , 1986 ) .","['This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982, for further comment).', 'One approach to this problem is that taken by the ASCOT project ( #AUTHOR_TAG ; Akkerman , 1986 ) .', 'In this project, a new lexicon is being manually derived from LDOCE.', 'The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power.', 'More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis.', 'In the medium term this approach, though time consuming, will be of some utility for producing more reliable lexicons for natural language processing.']",0,"['This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982, for further comment).', 'One approach to this problem is that taken by the ASCOT project ( #AUTHOR_TAG ; Akkerman , 1986 ) .', 'In this project, a new lexicon is being manually derived from LDOCE.', 'The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power.', 'More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis.', 'In the medium term this approach, though time consuming, will be of some utility for producing more reliable lexicons for natural language processing.']"
CC753,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,natural language information processing,['N Sager'],introduction,"Our aim is to extract information about literary characters in unstructured texts. We employ natural language processing and reasoning on domain ontologies. The first task is to identify the main characters and the parts of the story where these characters are described or act. We illustrate the system in a scenario in the folktale domain. The system relies on a folktale ontology that we have developed based on Propp's model for folktales morphology.Comment: IEEE 11 International Conference on Intelligent Computer   Communication and Processing (ICCP2015), Cluj-Napoca, Romania, 3-5 September   201","Two exceptions to this generalisation are the Linguistic String Project ( #AUTHOR_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .","['The idea of using the machine readable source of a published dictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name but a few applications.', 'Most of the work on automated dictionaries has concentrated on extracting lexical or other information, essentially by batch processing (eg. Amsler, 1981 ;Walker and Amsler, 1986), or Copyright 1987 by the Association for Computational Linguistics.', 'Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.', 'To copy otherwise, or to republish, requires a fee and/or specific permission.', '0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b).', 'Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', 'Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand.', 'Two exceptions to this generalisation are the Linguistic String Project ( #AUTHOR_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .', 'In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).', 'However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.', 'In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.']",1,"['Two exceptions to this generalisation are the Linguistic String Project ( #AUTHOR_TAG ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .']"
CC754,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,database design for a dictionary of the future,['Frank Tompa'],,,"Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #AUTHOR_TAG ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape .","['There is a well recognised problem with providing computational support for machine readable dictionaries, in particular where issues of access are concerned.', ""On the one hand, dictionaries exhibit far too much structure for conventional techniques for managing 'flat' text to apply to them."", 'On the other hand, the equally large amounts of free text in dictionary entries, as well as the implicitly marked relationships commonly used to encode linguistic information, makes a dictionary difficult to represent as a structured database of a standard, eg.', 'relational, type.', 'In addition, in order to link the machine readable version of LDOCE to our development environment, and eventually to our natural language processing systems, we need to provide fast access from Lisp to data held in secondary storage.', 'Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #AUTHOR_TAG ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape .']",0,"['There is a well recognised problem with providing computational support for machine readable dictionaries, in particular where issues of access are concerned.', ""On the one hand, dictionaries exhibit far too much structure for conventional techniques for managing 'flat' text to apply to them."", 'On the other hand, the equally large amounts of free text in dictionary entries, as well as the implicitly marked relationships commonly used to encode linguistic information, makes a dictionary difficult to represent as a structured database of a standard, eg.', 'relational, type.', 'In addition, in order to link the machine readable version of LDOCE to our development environment, and eventually to our natural language processing systems, we need to provide fast access from Lisp to data held in secondary storage.', 'Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #AUTHOR_TAG ) ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape .']"
CC755,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,computer exploitation of ldoces grammatical codes paper presented at a conference on survey of english language,"['A Moulin', 'J Jansen', 'A Michiels']",,,"In addition , #AUTHOR_TAG note that our Object Raising rule would assign mean to this category incorrectly .","['The four verbs which are misclassified as Object Equi and which do not have T5 codes anywhere in their entries are elect, love, represent and require.', 'None of these verbs take sentential complements and therefore they appear to be counterexamples to our Object Raising rule.', 'In addition , #AUTHOR_TAG note that our Object Raising rule would assign mean to this category incorrectly .', 'Mean is assigned both a V3 and a T5 category in the code field associated with sense 2 (i.e.', '""intend""), however, when it is used in this sense it must be treated as an Object Equi verb.']",1,"['In addition , #AUTHOR_TAG note that our Object Raising rule would assign mean to this category incorrectly .']"
CC756,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,and forthcoming machine readable dictionaries and research in computational linguistics,['Branimir Boguraev'],introduction,,"In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #AUTHOR_TAG for details ) .","['The idea of using the machine readable source of a published dictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name but a few applications.', 'Most of the work on automated dictionaries has concentrated on extracting lexical or other information, essentially by batch processing (eg. Amsler, 1981 ;Walker and Amsler, 1986), or Copyright 1987 by the Association for Computational Linguistics.', 'Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.', 'To copy otherwise, or to republish, requires a fee and/or specific permission.', '0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b).', 'Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', 'Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typically generated by hand.', 'Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources.', 'In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #AUTHOR_TAG for details ) .', 'However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.', 'In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.']",0,"['In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #AUTHOR_TAG for details ) .', 'However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.']"
CC757,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,lexicalfunctional grammar a formal system for grammatical representation in jbresnan ed the mental representation of grammatical relations,"['Ronald Kaplan', 'Joan Bresnan']",introduction,,"Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( #AUTHOR_TAG ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( Shieber , 1984 ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .","['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( #AUTHOR_TAG ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( Shieber , 1984 ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'The research described below is taking place in the context of three collaborative projects (Boguraev, 1987;Russell et al., 1986;Phillips and Thompson, 1986) to develop a general-purpose, wide coverage morphological and syntactic analyser for English.', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']",0,"['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( #AUTHOR_TAG ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( Shieber , 1984 ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .']"
CC758,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a key to the brandeis verb catalog unpublished mimeo under nsf grant ist8420073 quotinformation structure of a natural language lexiconquot,"['Ray Jackendoff', 'Jane Grimshaw']",,,This deficiency is rectified in the verb classification system employed by #AUTHOR_TAG in the Brandeis verb catalogue .,"['Prefer is misclassified as Object Raising, rather than as Object Equi, because the relevant code field contains a T5 code, as well as a V3 code.', 'The T5 code is marked as \'rare\', and the occurrence of prefer with a tensed sentential complement, as opposed to with an infinitive, is certainly marginal:  This example also highlights a deficiency in the LDOCE coding system since prefer occurs much more naturally with a sentential complement if it collocates with a modal such as ""would"".', 'This deficiency is rectified in the verb classification system employed by #AUTHOR_TAG in the Brandeis verb catalogue .']",1,['This deficiency is rectified in the verb classification system employed by #AUTHOR_TAG in the Brandeis verb catalogue .']
CC759,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a dictionary and morphological analyser for english,"['Graham Russell', 'Steve Pulman', 'Graeme Ritchie', 'Alan Black']",introduction,"This paper describes the current state of a three-year project aimed at the development of software for use in handling large quantit ies of dictionary information within natural language processing systems. 1 The project was accepted for funding by SERC/Alvey commencing ill June 1984, and is being carried out by Graeme Ritchie and Alan Black at the Universi ty of Edinburgh and Steve Puhnan and Graham Russell at the Universi ty of Cambridge. It is one of three closely related projects funded under the Alvey IKBS Programme (Natural Language Tlleme); a parser is under development at Edinburgh by Henry Thompson and John Phillips, and a sentence grammar is being devised by Ted Briscoe and Clare Grover at Lancaster and Bran Boguraev and John Carroll at Cambridge. It is intended tha t the software and rules produced by all three projects wil l be directly compatible and capable of functioning in an integrated system. Realistic and useful na tura l language processing systems such as database f ront-ends require large numbers of words, together wi th associated syntactic and semantic Information, to be efficiently stored in machine-readable form. Our system is Intended to provide the necessary facilities, being designed to store a large number (at least 10,000) of words and to perform morphological analysis on them, covering both Inflectional and derlvatlonal morphology. In pursuit of these objectives, the dictionary associates wi th each word information concerning its morphosyntactlc properties. Users are free to modify the system In a number of ways; they may add to the lexical entries Lisp functions tha t perform semantic manipulatlons, and tailor the dictionary to the particular subject mat ter they are interested in (different databases, for example). I t Is also hoped that the system is general enough to be of use to linguists wishing to Investigate the morphology of English and other languages. Contents of the basle data files may be altered or replaced: 1. A 'Word Grammar ' file contains rules assigning internal s t ructure to complex words, 2. A 'Lexicon' file holds the morpheme entries which include syntactic and other Information associated wi th stems and affixes. 3. A 'Spelling Rules' file contains rules governing permissible correspondences between the form of morphemes listed in the lextcon and complex words consisting of sequences of these morphemes. Once these data flies have been prepared, they are compiled using a number of pre-processtng functions tha t operate to produce a set of output files. These constitute a fu l ly expanded and cross-Indexed dictionary which can then be accessed from within LISP. The process of morphological analysis consists of parslng a sequence of Input morphemes wi th respect to the word grammar, It Is Implemented as an active chart parser (Thompson & Rltchle (1984)), and builds a s t ructure in the form of a tree in which each node has two","The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; #AUTHOR_TAG ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .","['Recent developments in linguistics, and especially on grammatical theory --for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al., 1985), Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) --and on natural language parsing frameworks for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-II (Shieber, 1984) --make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; #AUTHOR_TAG ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']",0,"['The research described below is taking place in the context of three collaborative projects ( Boguraev , 1987 ; #AUTHOR_TAG ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .']"
CC760,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,exploiting a large dictionary data base,['Archibal Michiels'],,,"There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see #AUTHOR_TAG , for further details ) .","['There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see #AUTHOR_TAG , for further details ) .', 'However, exploiting this information to the full would be a non-trivial task, because it would require accessing the relevant knowledge about the words contained in the qualifier fields from their LDOCE entries.']",0,"['There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see #AUTHOR_TAG , for further details ) .', 'However, exploiting this information to the full would be a non-trivial task, because it would require accessing the relevant knowledge about the words contained in the qualifier fields from their LDOCE entries.']"
CC761,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,the automatic grammatical tagging of the lob corpus,"['Geoffrey Leech', 'Roger Garside', 'Erik Atwell']",conclusion,"In collaboration with the English Department, University of Oslo, and the Nowegian Computing Centre for the Humanities, Bergen we have been engaged in the automatic grammatical tagging of the LOB (LancasterOslo/Bergen) Corpus of British English. The computer programs for this task are running at a success rate of approximately 96.7% and a substantial part of the 1,000,000-word corpus has already been tagged. The purpose of this paper is to give an account of the project, with special reference to the methods of tagging we have adopted.","In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #AUTHOR_TAG ) .","['In the longer term, it is clear that neither the contents nor form of any existing published dictionary meet all the requirements of a natural language processing system.', 'A substantial component of the research reported above has been devoted to restructuring LDOCE to make it more suitable for automatic analysis.', 'However, even after this process much of the information in LDOCE remains difficult to access, essentially because it is aimed at a human reader, as opposed to a computer system.', 'This suggests that the automatic construction of dictionaries from published sources intended for other purposes will have a limited life unless lexicography is heavily influenced by the requirements of automated natural language analysis.', 'In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #AUTHOR_TAG ) .', 'However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly.']",3,"['In the longer term, it is clear that neither the contents nor form of any existing published dictionary meet all the requirements of a natural language processing system.', 'In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #AUTHOR_TAG ) .']"
CC762,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,diagram a grammar for dialogues,['Jane Robinson'],introduction,,"#AUTHOR_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand .","['The idea of using the machine readable source of a published dictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name but a few applications.', 'Most of the work on automated dictionaries has concentrated on extracting lexical or other information, essentially by batch processing (eg. Amsler, 1981 ;Walker and Amsler, 1986), or Copyright 1987 by the Association for Computational Linguistics.', 'Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.', 'To copy otherwise, or to republish, requires a fee and/or specific permission.', '0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b).', 'Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', '#AUTHOR_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand .', 'Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources.', 'In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details).', 'However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons.', 'In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.']",1,"['Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', '#AUTHOR_TAG ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand .', 'Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982;Byrd, 1983); the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources.', 'In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.']"
CC763,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,predication linguistic inquiry,['E S Williams'],,,"Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see #AUTHOR_TAG , for further discussion ) .","['The solution we have adopted is to derive a semantic classification of the particular sense of the verb under consideration on the basis of the complete set of codes assigned to that sense.', 'In any subcategorisation frame which involves a predicate complement there will be a non-transparent relationship between the superficial syntactic form and the underlying logical relations in the sentence.', 'In these situations the parser can use the semantic type of the verb to compute this relationship.', 'Expanding on a suggestion of Michiels (1982), we classify verbs as Subject Equi, Object Equi, Subject Raising or Object Raising for each sense which has a predicate complement code associated with it.', 'These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation.', 'For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement.', 'Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see #AUTHOR_TAG , for further discussion ) .']",0,"['In any subcategorisation frame which involves a predicate complement there will be a non-transparent relationship between the superficial syntactic form and the underlying logical relations in the sentence.', 'In these situations the parser can use the semantic type of the verb to compute this relationship.', 'Expanding on a suggestion of Michiels (1982), we classify verbs as Subject Equi, Object Equi, Subject Raising or Object Raising for each sense which has a predicate complement code associated with it.', 'For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement.', 'Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see #AUTHOR_TAG , for further discussion ) .']"
CC764,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,exploiting a large dictionary data base,['Archibal Michiels'],introduction,,( #AUTHOR_TAG contains further description and discussion of LDOCE . ),"['We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '( #AUTHOR_TAG contains further description and discussion of LDOCE . )', 'In this paper we focus on the exploitation of the LDOCE grammar coding system; Alshawi et al. (1985) and Alshawi (1987) describe further research in Cambridge utilising different types of information available in LDOCE.']",0,"['We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '( #AUTHOR_TAG contains further description and discussion of LDOCE . )', 'In this paper we focus on the exploitation of the LDOCE grammar coding system; Alshawi et al. (1985) and Alshawi (1987) describe further research in Cambridge utilising different types of information available in LDOCE.']"
CC765,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,exploiting a large dictionary data base,['Archibal Michiels'],,,"Expanding on a suggestion of #AUTHOR_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .","['The solution we have adopted is to derive a semantic classification of the particular sense of the verb under consideration on the basis of the complete set of codes assigned to that sense.', 'In any subcategorisation frame which involves a predicate complement there will be a non-transparent relationship between the superficial syntactic form and the underlying logical relations in the sentence.', 'In these situations the parser can use the semantic type of the verb to compute this relationship.', 'Expanding on a suggestion of #AUTHOR_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .', 'These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation.', 'For example, labelling believe(3) (Type 20Raising) indicates that this is a two place predicate and that, if believe(3) occurs with a syntactic direct object, as in ( 1) John believes the Earth to be round it will function as the logical subject of the predicate complement.', 'Michiels proposed rules for doing this for infinitive complement codes; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP, AP and PP predication (see Williams (1980), for further discussion).']",2,"['The solution we have adopted is to derive a semantic classification of the particular sense of the verb under consideration on the basis of the complete set of codes assigned to that sense.', 'In these situations the parser can use the semantic type of the verb to compute this relationship.', 'Expanding on a suggestion of #AUTHOR_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .', 'These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction; the actual output of the program is a specification of the mapping from superficial syntactic form to an underlying logical representation.']"
CC766,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,an information theoretic analysis of phonetic dictionary access computer speech and language,['David Carter'],,,"In addition to headwords , dictionary search through the pronunciation field is available ; #AUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( Huttenlocher and Zue , 1983 ) .","['From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords , dictionary search through the pronunciation field is available ; #AUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( Huttenlocher and Zue , 1983 ) .', 'In addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information (Boguraev et al., 1987).', 'Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample.']",5,"['In addition to headwords , dictionary search through the pronunciation field is available ; #AUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( Huttenlocher and Zue , 1983 ) .']"
CC767,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a grammar of contemporary english longman group limited,"['Randolph Quirk', 'Sidney Greenbaum', 'Geoffrey Leech', 'Jan Svartvik']",,,"The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #AUTHOR_TAG , 1985 ) .","['Once the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing.', 'The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #AUTHOR_TAG , 1985 ) .', 'The codes are doubly articulated; capital letters represent the grammatical relations which hold between a verb and its arguments and numbers represent subcategorisation frames which a verb can appear in.', 'Most of the subcategorisation frames are specified by syntactic category, but some are very ill-specified; for instance, 9 is defined as ""needs a descriptive word or phrase"".', 'In practice many adverbial and predicative complements will satisfy this code, when attached to a verb; for example, put [xg] where the code marks a locative adverbial prepositional phrase vs. make under sense 14 (hereafter written make( 14)) is coded IX9] where it marks a predicative noun phrase or prepositional phrase.']",2,"['The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #AUTHOR_TAG , 1985 ) .']"
CC768,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,automatic analysis of texts in informatics 7,['Archibal Michiels'],,,"Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English ( #AUTHOR_TAG ) .","['Given that we were targeting all envisaged access routes from LDOCE to systems implemented in Lisp, and since the natural data structure for Lisp is the s-expression, we adopted the approach of converting the tape source into a set of list structures, one per entry.', 'Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English ( #AUTHOR_TAG ) .']",0,"['Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English ( #AUTHOR_TAG ) .']"
CC769,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,designing a computerised lexicon for linguistic purposes,"['Erik Akkerman', 'Pieter Masereeuw', 'Willem Meijs']",,,Michiels ( 1982 ) and #AUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .,"['The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, I5, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to I5 if the verb is felt to be semantically two place rather than one place, such as know versus appear.', 'On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic.', 'Michiels ( 1982 ) and #AUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', 'Ingria (1984) comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated.']",0,['Michiels ( 1982 ) and #AUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .']
CC770,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,the design of a computer language for linguistic information,['S Shieber'],introduction,"A considerable body of accumulated knowledge about the design of languages for communicating information to computers has been derived from the subfields of programming language design and semantics. It has been the goal of the PATR group at SRI to utilize a relevant portion of this knowledge in implementing tools to facilitate communication of linguistic information to computers. The PATR-II formalism is our current computer language for encoding linguistic information. This paper, a brief overview of that formalism, attempts to explicate our design decisions in terms of a set of properties that effective computer languages should incorporate.Engineering and Applied Science","Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( Kaplan and Bresnan , 1982 ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( #AUTHOR_TAG ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .","['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( Kaplan and Bresnan , 1982 ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( #AUTHOR_TAG ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'The research described below is taking place in the context of three collaborative projects (Boguraev, 1987;Russell et al., 1986;Phillips and Thompson, 1986) to develop a general-purpose, wide coverage morphological and syntactic analyser for English.', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']",0,"['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) ( Gazdar et al. , 1985 ) , Lexical Functional Grammar ( LFG ) ( Kaplan and Bresnan , 1982 ) -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) ( Kay , 1984a ) , PATR-II ( #AUTHOR_TAG ) -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.']"
CC771,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,phonotactic and lexical constraints in speech recognition,"['Daniel Huttenlocher', 'Victor Zue']",,"We demonstrate a method for partitioning a large lexicon into small equivalence classes, based on sequential phonetic and prosodic constraints. The representation is attractive for speech recognition systems because it allows all but a small number of word candidates to be excluded, using only gross phonetic and prosodic information. The approach is a robust one in that the representation is relatively insensitive to phonetic variability and recognition error.","In addition to headwords , dictionary search through the pronunciation field is available ; Carter ( 1987 ) has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #AUTHOR_TAG ) .","['From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords , dictionary search through the pronunciation field is available ; Carter ( 1987 ) has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #AUTHOR_TAG ) .', 'In addition, a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic, lexical, syntactic, and semantic information (Boguraev et al., 1987).', 'Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample.']",0,"['In addition to headwords , dictionary search through the pronunciation field is available ; Carter ( 1987 ) has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure ( #AUTHOR_TAG ) .']"
CC772,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a comprehensive grammar of english longman group limited,"['Randolph Quirk', 'Sidney Greenbaum', 'Geoffrey Leech', 'Jan Svartvik']",,,"The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #AUTHOR_TAG ) .","['In what follows we will discuss the format of the grammar codes in some detail as they are the focus of the current paper, however, the reader should bear in mind that they represent only one comparatively constrained field of an LDOCE entry and therefore, a small proportion of the overall restructuring task.', 'Figure 4 illustrates the grammar code field for the third word sense of the verb believe as it appears in the published dictionary, on the typesetting tape and after restructuring.', 'Figure 4 LDOCE provides considerably more syntactic information than a traditional dictionary.', 'The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #AUTHOR_TAG ) .', 'A grammar code describes a particular pattern of behaviour of a word.', 'Patterns are descriptive, and are used to convey a range of information: eg.', 'distinctions between count and mass nouns (dog vs. desire), predicative, postpositive and attributive adjectives (asleep vs. elect vs. jokular), noun complementation (fondness, fact) and, most importantly, verb complementation and valency.']",0,"['In what follows we will discuss the format of the grammar codes in some detail as they are the focus of the current paper, however, the reader should bear in mind that they represent only one comparatively constrained field of an LDOCE entry and therefore, a small proportion of the overall restructuring task.', 'Figure 4 LDOCE provides considerably more syntactic information than a traditional dictionary.', 'The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #AUTHOR_TAG ) .', 'A grammar code describes a particular pattern of behaviour of a word.']"
CC773,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,syntactic argumentation and the structure of english,"['D M Perlmutter', 'S Soames']",,"Syntactic Argumentation and the Structure of English (SASE) presents the major theoretical developments in generative syntax and the empirical arguments motivating them. Beautifully and lucidly written, it is an invaluable resource for working linguists as well as a pedagogical tool of unequaled depth and breadth. The chief focus of the book is syntactic argumentation. Beginning with the fundamentals of generative syntax, it proceeds by a series of gradually unfolding arguments to analyses of some of the most sophisticated proposals. It includes a wide variety of problems that guide the reader in constructing arguments deciding between alternative analyses of syntactic constructions and alternative theoretical formulations. Someone who has worked through the problems and arguments in this book will be able to apply the skills in argumentation it develops to novel issues in syntax. While teaching syntactic argumentation, SASE covers the major empirical results of generative syntax. Its contents include: Transformations in single-clause sentences; Complementation and multi-clause transformations; Universal principles governing rule interaction: the cycle and strict cyclicity; Movement rules; Ross' constraints; Pronominal reference and anaphora. SASE is an important book for several different audiences: for students, it is an introduction to syntax that teaches argumentation as well as a wide range of empirical results in the field; for linguists, it is a sourcebook of classical analyses and arguments, with some new arguments bearing on classical issues; and, for scholars, teachers, and students in related fields, it is a comprehensive guide to the major empirical and theoretical developments in generative syntax. SASE contains enough material for a two-semester or three-quarler sequence in syntax. Because it assumes no previous background, it can be used as the main text in an introduction to syntax. Since it covers a wide range of material not available in other texts, it is also suitable for intermediate and advanced syntax courses and as a supplementary source in more specialized courses and courses in other disciplines. A storehouse of classical and original arguments, SASE will prove to be of lasting value to the teacher, the student, and researchers in both linguistics and related fields.","Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( #AUTHOR_TAG:460 ff . )","['This small experiment demonstrates a number of points.', 'Firstly, it seems reasonable to conclude that the assignment of individual codes to verbs is on the whole relatively accurate in LDOCE.', 'Of the 139 verbs tested, we only found code omissions in 10 cases.', 'Secondly though, when we consider the interaction between the assignments of codes and word sense classification, LDOCE appears less reliable.', 'This is the primary source of error in the case of the Object Raising rule.', 'Thirdly, it seems clear that the Object Raising rule is straining the limits of what can be reliably extracted from the LDOCE coding system.', 'Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( #AUTHOR_TAG:460 ff . )', 'However, only two of these criteria are explicit in the coding system.']",3,"['Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( #AUTHOR_TAG:460 ff . )']"
CC774,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a multipurpose interface to an online dictionary,"['Branimir Boguraev', 'David Carter', 'Ted Briscoe']",,,"In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information ( #AUTHOR_TAG ) .","['From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords, dictionary search through the pronunciation field is available; Carter (1987) has merged information from the pronunciation and hyphenation fields, creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure (Huttenlocher and Zue, 1983).', 'In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information ( #AUTHOR_TAG ) .', 'Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample.']",0,"['In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information ( #AUTHOR_TAG ) .']"
CC775,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,the grammar of english predicate complement constructions,['P S Rosenbaum'],,"A set of phrase structure rules and a set of transformational rules are proposed for which the claim is made that these rules enumerate the underlying and derived sentential structures which exemplify two productive classes of sentential embedding in English. These are sentential embedding in noun phrases and sentential embedding in verb phrases. First, following a statement of the grammatical rules, the phrase structure rules are analyzed and defended. Second, the transformational rules which map the underlying structures generated by the phrase structure rules onto appropriate derived structures are justified with respect to noun phrase and verb phrase complementation. Finally, a brief treatment is offered for the extension of the proposed descriptive apparatus to noun phrase and verb phrase complementation in predicate adjectival constructions. Thesis Supervisor: Noam Chomsky Title: Professor of Modern Languages",We tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in #AUTHOR_TAG and Stockwell et al. ( 1973 ) .,"['We tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in #AUTHOR_TAG and Stockwell et al. ( 1973 ) .', 'Figure 16 gives the number of verbs classified under each category by these authors and the number successfully classified into the same categories by the system.']",5,['We tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in #AUTHOR_TAG and Stockwell et al. ( 1973 ) .']
CC776,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,processing dictionary definitions with phrasal pattern hierarchies in this issue,['Hiyan Alshawi'],,"This paper shows how dictionary word sense definitions can be analysed by applying a hierarchy of phrasal patterns. An experimental system embodying this mechanism has been implemented for processing definitions from the Longman Dictionary of Contemporary English. A property of this dictionary, exploited by the system, is that it uses a restricted vocabulary in its word sense definitions. The structures generated by the experimental system are intended to be used for the classification of new word senses in terms of the senses of words in the restricted vocabulary. Examples illustrating the output generated are presented, and some qualitative performance results and problems that were encountered are discussed. The analysis process applies successively more specific phrasal analysis rules as determined by a hierarchy of patterns in which less specific patterns dominate more specific ones. This ensures that reasonable incomplete analyses of the definitions are produced when more complete analyses are not possible, resulting in a relatively robust analysis mechanism. Thus the work reported addresses two robustness problems faced by current experimental natural language processing systems: coping with an incomplete lexicon and with incomplete knowledge of phrasal constructions.","As #AUTHOR_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.","['A series of systems in Cambridge are implemented in Lisp running under UnixTM.', 'They all make use of an efficient dictionary access system which services requests for s-expression entries made by client pro- grams.', 'A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved.', 'As #AUTHOR_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.', 'The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.', 'For reasons of efficiency and flexibility of customisation, namely the use of LDOCE by different client programs and from different Lisp and/or Prolog systems, the dictionary access system is implemented in the programming language C and makes use of the inter-process communication facilities provided by the Unix operating system.', 'To the Lisp programmer, the creation of a dictionary process and subsequent requests for information from the dictionary appear simply as Lisp function calls.']",0,"['A series of systems in Cambridge are implemented in Lisp running under UnixTM.', 'They all make use of an efficient dictionary access system which services requests for s-expression entries made by client pro- grams.', 'A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved.', 'As #AUTHOR_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.', 'The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.', 'For reasons of efficiency and flexibility of customisation, namely the use of LDOCE by different client programs and from different Lisp and/or Prolog systems, the dictionary access system is implemented in the programming language C and makes use of the inter-process communication facilities provided by the Unix operating system.', 'To the Lisp programmer, the creation of a dictionary process and subsequent requests for information from the dictionary appear simply as Lisp function calls.']"
CC777,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a natural language toolkit reconciling theory with practice,['Branimir Boguraev'],introduction,"Generalized Phrase Structure Grammar (GPSG) is a highly restrictive theory of natural language syntax, characterised by complex interaction between its various rule types and constraints. Motivated by desire for declarative semantics, the theory defines these as applying simultaneously in the process of licensing local trees. As a result, as far as practical implementations of GPSG are concerned, the theory loses its apparent efficient parsability and becomes computationally intractable. This paper describes one aspect of an UK collaborative effort to produce a general purpose morphological and syntactic analyser for English within the theoretical framework of Generalized Phrase Structure Grammar, namely the development of a tractable grammatical formalism with clear semantics, capable of supporting the task of writing a substantial grammar. The paper outlines the intellectual and pragmatic background of the development effort and traces the incremental evolution of this formalism, following discussions concerning the fundamental issues of rules interpretation, feature system, grammar organisation, parser strategy, environment for grammar writing and support, and the construction of a lexicon linked to the grammar. Particular emphasis is placed on the quesiton of how theoretical standpoints have been reconciled with practical constraints, and how the commitment to deliver a functional morphological and syntactic analyser of wide scope and coverage of English has influenced the current state of the grammatical formalism.","The research described below is taking place in the context of three collaborative projects ( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .","['Recent developments in linguistics, and especially on grammatical theory --for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al., 1985), Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) --and on natural language parsing frameworks for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-II (Shieber, 1984) --make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'The research described below is taking place in the context of three collaborative projects ( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']",0,"['Recent developments in linguistics, and especially on grammatical theory --for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al., 1985), Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) --and on natural language parsing frameworks for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-II (Shieber, 1984) --make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'The research described below is taking place in the context of three collaborative projects ( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']"
CC778,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,synthesis of speech from unrestricted text,['J Allen'],experiments,"E diatribe ekhei san stokho ten melete ton prosodikon kanonon tes ellenikes glossas. Ta apotelesmata mporoun na ensomatothoun se opoiodepote sustema suntheses omilias anexarteta apo ten epilegmene strategike suntheses. E prosodia parousiazetai san polumetrike sunartese tes themeliodous sukhnotetas tes entases kai tes diarkeias ton phonematon kai parousiazontai montela se epipedo lexes toso gia argo oso kai gia gregoro ruthmo ekphoras. Epises parousiazontai montela kai kanones se epipleon protaseis me tropo pou exantlei ola ta suntaktika phainomena tes ellenikes. Proteinetai oti e prosodia se epipedo protases mporei na suntethei apo montela epipedou lexes upertithemena pano se mia pherousa se epipedo protases e klise tes opoias exartatai apo ten uparxe phainomenon emphases.This thesis aims at the study of the prosodic rules of the Greek language for use in a text to speech synthesis from unrestricted text. Regardless of the underlying synthesis stratregie (diphones, phonemes, etc). Prosody is treated as a polymetric function of fundamental frequency, intensity and duration of the phonemes. Prosodic models are presented first for isolated intonation words for various tempos including slow and fast. Models for large sentences are also presented in a way that all syntactic phenomena of the language are included and respected. It is suggested that sentence level prosodic models can be derived and synthesized from word-level models that are superimposed on a carrier spanning the whole sentence. The trend of the carrier is dependent upon various emphatic phenomena such as local stress or sentence emphasis","Many investigators ( e.g. #AUTHOR_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .","['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators (e.g.', 'Many investigators ( e.g. #AUTHOR_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', ""And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O'Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem."", 'As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'We believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.', 'Correct phrase identification makes it possible to adopt an expanded pitch range that greatly enhances the naturalness of the final speech.', 'In constructing the system, we focused on two core questions: (i) what kind of parser is needed for the p:rosody rules?', 'and (ii) how should prosodic phrasing, i.e. boundary location and strength, be represented?']",4,"['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators ( e.g. #AUTHOR_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', ""And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O'Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem."", 'As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.']"
CC779,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,texttospeechan overview,"['S P Olive', 'M Y Liberman']",introduction,,"In previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .","['In previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .', 'The system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'This paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'Like the earlier version, it has been implemented in a text-to-speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries.']",0,"['In previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .', 'While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'This paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'Like the earlier version, it has been implemented in a text-to-speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries.']"
CC780,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,from sad to glad emotional computer voices,['J Cahn'],experiments,"Synthesized English speech is readily distinguished from human speech on the basis of inappropriate intonation and insu cient expressiveness. This is a drawback for conversational computer systems. Intonation is the carrier of emphasis or de-emphasis, serving to clarify meaning for the spoken word much as variations in typeface and punctuation do for the written word. Expressiveness is not tied to word or phrase meaning but is global in scope. It provides the context in which the intonation occurs, and reveals the speaker's intentions and general mental state. In synthesized speech, intonation makes the message easier to understand; enhanced expressiveness contributes to dramatic e ect, making the message easier to listen to.","Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; #AUTHOR_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .","['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators (e.g.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; #AUTHOR_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', ""And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O'Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem."", 'As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'We believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.', 'Correct phrase identification makes it possible to adopt an expanded pitch range that greatly enhances the naturalness of the final speech.', 'In constructing the system, we focused on two core questions: (i) what kind of parser is needed for the p:rosody rules?', 'and (ii) how should prosodic phrasing, i.e. boundary location and strength, be represented?']",4,"['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; #AUTHOR_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', 'As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'We believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.', 'Correct phrase identification makes it possible to adopt an expanded pitch range that greatly enhances the naturalness of the final speech.', 'and (ii) how should prosodic phrasing, i.e. boundary location and strength, be represented?']"
CC781,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,texttospeechan overview,"['S P Olive', 'M Y Liberman']",experiments,,We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .,"['We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .', 'Two concerns motivated our implementation.', 'First, we hoped the system would provide us with a research tool for testing our ideas about syntax and phrasing against a large unrestricted collection of sentences.', 'Second, we wished to investigate how well our approach would work for determining prosodic phrasing in a text-to-speech synthesizer.']",5,"['We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .', 'Second, we wished to investigate how well our approach would work for determining prosodic phrasing in a text-to-speech synthesizer.']"
CC782,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,now lets talk about now identifying cue phrases intonationally,"['J Hirschberg', 'D Litman']",introduction,"Cue phrases are words and phrases such as now and by the way which may be used to convey explicit information about the structure of a discourse. However, while cue phrases may convey discourse structure, each may also be used to different effect. The question of how speakers and hearers distinguish between such uses of cue phrases has not been addressed in discourse studies to date. Based on a study of now in natural recorded discourse, we propose that cue and non-cue usage can be distinguished intonationally, on the basis of phrasing and accent.",#AUTHOR_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing .,"['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure.', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.', '(The complement in 17a and the adjuncts in 17b-f are italicized.)', 'The relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', '#AUTHOR_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing .', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']",0,"['Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'The relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', '#AUTHOR_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing .', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']"
CC783,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,aspects of the theory of syntax,['N Chomsky'],introduction,Abstract : Contents: Methodological preliminaries: Generative grammars as theories of linguistic competence; theory of performance; organization of a generative grammar; justification of grammars; formal and substantive grammars; descriptive and explanatory theories; evaluation procedures; linguistic theory and language learning; generative capacity and its linguistic relevance Categories and relations in syntactic theory: Scope of the base; aspects of deep structure; illustrative fragment of the base component; types of base rules Deep structures and grammatical transformations Residual problems: Boundaries of syntax and semantics; structure of the lexicon,"Sentences like 12 , from #AUTHOR_TAG , are frequently cited .","['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers, e.g., Cooper and Paccia-Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra,;e.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12 , from #AUTHOR_TAG , are frequently cited .', '(Square brackets mark off the NP constituents that contain embed- ded sentences.)']",0,"['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers, e.g., Cooper and Paccia-Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra,;e.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12 , from #AUTHOR_TAG , are frequently cited .']"
CC784,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,the contribution of parsing to prosodic phrasing in an experimental texttospeech system,"['J Bachenko', 'E Fitzpatrick', 'C E Wright']",introduction,"While various aspects of syntactic structure have been shown to bear on the determination of phraselevel prosody, the text-to-speech field has lacked a robust working system to test the possible relations between syntax and prosody. We describe an implemented system which uses the deterministic parser Fidditch to create the input for a set of prosody rules. The prosody rules generate a prosody tree that specifies the location and relative strength of prosodic phrase boundaries. These specifications are converted to annotations for the Bell Labs text-to-speech system that dictate modulations in pitch and duration for the input sentence.We discuss the results of an experiment to determine the performance of our system. We are encouraged by an initial 5 percent error rate and we see the design of the parser and the modularity of the system allowing changes that will upgrade this rate.","In previous work ( #AUTHOR_TAG ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) .","['In previous work ( #AUTHOR_TAG ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) .', 'The system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'This paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'Like the earlier version, it has been implemented in a text-to-speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries.']",2,"['In previous work ( #AUTHOR_TAG ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) .', 'Like the earlier version, it has been implemented in a text-to-speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries.']"
CC785,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,finitestate parsing of phrasestructure languages and the status of readjustment rules in grammar,['D T Langendoen'],introduction,,"#AUTHOR_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model .","['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers, e.g., Cooper and Paccia-Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra,;e.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12, from Chomsky (1965) To account for such mismatches, ""readjustment rules"" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In Chomsky and Halle (1968), this flattening process is not part of the grammar.', 'Rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12]"" (p.', '372).', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', '#AUTHOR_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model .', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']",0,"['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In Chomsky and Halle (1968), this flattening process is not part of the grammar.', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', '#AUTHOR_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model .', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']"
CC786,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,performance structures a psycholinguistic and linguistic appraisal,"['J P Gee', 'F Grosjean']",introduction,,"The psycholinguistic studies of Martin ( 1970 ) , Allen ( 1975 ) , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and #AUTHOR_TAG , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .","['The psycholinguistic studies of Martin ( 1970 ) , Allen ( 1975 ) , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and #AUTHOR_TAG , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.', 'Discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths; this is the case in Chickens were eating I[ the remaining green vegetables, where the subject-predicate boundary finds no prosodic correspondent. 4']",0,"['The psycholinguistic studies of Martin ( 1970 ) , Allen ( 1975 ) , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and #AUTHOR_TAG , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .']"
CC787,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,on stress and linguistic rhythm,"['M Y Liberman', 'A Prince']",introduction,"JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.. The MIT Press is collaborating with JSTOR to digitize, preserve and extend access to Linguistic Inquiry.","3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #AUTHOR_TAG , which ""are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option"" (p. 251) .","['Our work on the prosodic phrase status of clause final prepositional phrases, which we discuss below, suggests the existence of a discourse-neutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase.', '3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #AUTHOR_TAG , which ""are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option"" (p. 251) .', 'This approach to prosodic phrase boundary determination brings us closer to a framework in which phonological, syntactic, and discourse features all contribute to prosodic phrasing.']",1,"['3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #AUTHOR_TAG , which ""are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option"" (p. 251) .']"
CC788,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,the sound pattern of english,"['N Chomsky', 'M Halle']",introduction,"Since this classic work in phonology was published in 1968, there has been no other book that gives as broad a view of the subject, combining generally applicable theoretical contributions with analysis of the details of a single language. The theoretical issues raised in The Sound Pattern of English continue to be critical to current phonology, and in many instances the solutions proposed by Chomsky and Halle have yet to be improved upon.Noam Chomsky and Morris Halle are Institute Professors of Linguistics and Philosophy at MIT.","In #AUTHOR_TAG , this flattening process is not part of the grammar .","['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers, e.g., Cooper and Paccia-Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra,;e.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12, from Chomsky (1965) To account for such mismatches, ""readjustment rules"" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In #AUTHOR_TAG , this flattening process is not part of the grammar .', 'Rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12]"" (p.', '372).', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', 'Langendoen (1975) proposes readjustment rules similar to those of Chomsky and Halle, but he claims that the readjustment of structure is part of the grammar, not part of the performance model.', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']",0,"['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers, e.g., Cooper and Paccia-Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra,;e.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In #AUTHOR_TAG , this flattening process is not part of the grammar .', 'Rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12]"" (p.', '372).', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', 'Langendoen (1975) proposes readjustment rules similar to those of Chomsky and Halle, but he claims that the readjustment of structure is part of the grammar, not part of the performance model.', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']"
CC789,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,on stress and linguistic rhythm,"['M Y Liberman', 'A Prince']",experiments,"JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.. The MIT Press is collaborating with JSTOR to digitize, preserve and extend access to Linguistic Inquiry.","An alternative representation based on #AUTHOR_TAG is presented in Selkirk ( 1984 ) , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree .","['Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'An alternative representation based on #AUTHOR_TAG is presented in Selkirk ( 1984 ) , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree .', 'Although a grid may be more descriptively suitable for some aspects of prosody (for example, Sproat and Liberman (1987) use the grid representation for their implementation of stress assignment in compound nominals), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing.']",1,"['Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'An alternative representation based on #AUTHOR_TAG is presented in Selkirk ( 1984 ) , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree .', 'Although a grid may be more descriptively suitable for some aspects of prosody (for example, Sproat and Liberman (1987) use the grid representation for their implementation of stress assignment in compound nominals), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing.']"
CC790,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,prosodic systems and intonation in english,['D Crystal'],introduction,Preface 1. Some preliminary considerations 2. Past work on prosodic features 3. Voice-quality and sound attributes in prosodic study 4. The prosodic features of English 5. The intonation system of English 6. The grammar of intonation 7. The semantics of intonation Bibliography Index of persons Index of subjects.,"#AUTHOR_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct .","['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', '#AUTHOR_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct .', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure.', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.', '(The complement in 17a and the adjuncts in 17b-f are italicized.)', 'The relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']",0,"['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', '#AUTHOR_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct .', 'The relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.']"
CC791,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,speech rhythm its relation to performance universals and articulatory timing,['G Allen'],introduction,,"The psycholinguistic studies of Martin ( 1970 ) , #AUTHOR_TAG , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and Gee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .","['The psycholinguistic studies of Martin ( 1970 ) , #AUTHOR_TAG , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and Gee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.', 'Discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths; this is the case in Chickens were eating I[ the remaining green vegetables, where the subject-predicate boundary finds no prosodic correspondent. 4']",0,"['The psycholinguistic studies of Martin ( 1970 ) , #AUTHOR_TAG , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and Gee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .']"
CC792,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,capacity demands in shortterm memory for synthetic and natural speech,"['P A Luce', 'T C Feustel', 'D B Pisoni']",experiments,"Three experiments were performed that compared recall for synthetic and natural lists of monosyllabic words. In the first experiment, presentation intervals of 1, 2, and 5 s per word were used. Although free recall was consistently poorer overall for the synthetic lists at all presentation rates, the decrement for synthetic stimuli did not increase differentially with faster rates. In a second experiment, zero, three, and six digits were presented visually for retention prior to free recall of each spoken word list in a preload paradigm. Fewer subjects were able to correctly recall all of the digits for the six-digit list than the three-digit list when the following word lists were synthetic. The third experiment required ordered recall of lists of natural and synthetic words. Differences in ordered recall between the synthetic and natural word lists were substantially larger for the primacy portion of the serial position curve than the recency portion. These results indicate that difficulties observed in the perception and comprehension of synthetic speech are due, in part, to increased processing demands in short-term memory.","Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; #AUTHOR_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .","['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators (e.g.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; #AUTHOR_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', ""And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O'Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem."", 'As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'We believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.', 'Correct phrase identification makes it possible to adopt an expanded pitch range that greatly enhances the naturalness of the final speech.', 'In constructing the system, we focused on two core questions: (i) what kind of parser is needed for the p:rosody rules?', 'and (ii) how should prosodic phrasing, i.e. boundary location and strength, be represented?']",4,"['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; #AUTHOR_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .']"
CC793,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,toward treating english nominals correctly,"['R W Sproat', 'M Y Liberman']",experiments,We describe a program for assigning correct stress contours to nominals in English. It makes use of idiosyncratic knowledge about the stress behavior of various nominal types and general knowledge about English stress rules. We have also investigated the related issue of parsing complex nominals in English. The importance of this work and related research to the problem of text-to-speech is &apos;discussed,"Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing .","['Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'An alternative representation based on Liberman and Prince (1977) is presented in Selkirk (1984), which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing .']",1,"['Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing .']"
CC794,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,the contribution of parsing to prosodic phrasing in an experimental texttospeech system,"['J Bachenko', 'E Fitzpatrick', 'C E Wright']",introduction,"While various aspects of syntactic structure have been shown to bear on the determination of phraselevel prosody, the text-to-speech field has lacked a robust working system to test the possible relations between syntax and prosody. We describe an implemented system which uses the deterministic parser Fidditch to create the input for a set of prosody rules. The prosody rules generate a prosody tree that specifies the location and relative strength of prosodic phrase boundaries. These specifications are converted to annotations for the Bell Labs text-to-speech system that dictate modulations in pitch and duration for the input sentence.We discuss the results of an experiment to determine the performance of our system. We are encouraged by an initial 5 percent error rate and we see the design of the parser and the modularity of the system allowing changes that will upgrade this rate.","Previous versions of our work , as described in #AUTHOR_TAG also assume that phrasing is dependent on predicate-argument structure .","['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'Previous versions of our work , as described in #AUTHOR_TAG also assume that phrasing is dependent on predicate-argument structure .', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.', '(The complement in 17a and the adjuncts in 17b-f are italicized.)', 'The relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']",2,"['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Previous versions of our work , as described in #AUTHOR_TAG also assume that phrasing is dependent on predicate-argument structure .', 'The relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.']"
CC795,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,syntax and speech,"['W Cooper', 'J Paccia-Cooper']",introduction,"Interactions between phonology and syntax are inspected in continuous speech samples from 30 speech-delayed children. Two types of interactions are examined: The co-occurrence of speech and language delay and the effects of phonological reduction on the realization of phonetically complex morphophonemes. Four possible patterns of association between the phonological and syntactic systems are outlined, and subjects are assigned to these patterns based on their phonological and syntactic performance. Results indicate that two-thirds of the subjects display evidence of overall syntactic delay, whereas half show some limitation in the use of phonetically complex morphophonemes, their performance in that area being below the level of their syntactic production. Implications of these findings for a theory of speech delay and for management programming are discussed","This observation has led some researchers , e.g. , #AUTHOR_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase .","['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers , e.g. , #AUTHOR_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase .', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12, from Chomsky (1965) To account for such mismatches, ""readjustment rules"" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In Chomsky and Halle (1968), this flattening process is not part of the grammar.', 'Rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12]"" (p.', '372).', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', 'Langendoen (1975) proposes readjustment rules similar to those of Chomsky and Halle, but he claims that the readjustment of structure is part of the grammar, not part of the performance model.', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']",0,"['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers , e.g. , #AUTHOR_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase .', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']"
CC796,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,performance structures a psycholinguistic and linguistic appraisal,"['J P Gee', 'F Grosjean']",introduction,,"Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .","['The psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.', 'Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']",1,"['The psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.', 'Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']"
CC797,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,aspects of prosody,['J Bing'],introduction,,"The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .","['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure.', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.', '(The complement in 17a and the adjuncts in 17b-f are italicized.)', 'The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']",0,"['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).']"
CC798,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,prosodic structure and spoken word recognition,"['F Grosjean', 'J P Gee']",introduction,,"Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .","['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .', 'Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree.', 'If the terminal is a content word, i.e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own.', 'Otherwise the word combines with one or more orthographically distinct words to form a single phonological word that has no internal word or phrase boundaries.', 'This is accomplished by adjoining a word to its left or right neighbor depending on its lexical category and its position in the tree.', 'Function words, e.g.', 'auxiliary verbs, articles, prepositions, pronouns, and conjunctions, are all eligible for adjunction in certain syntactic contexts.', 'Content words, copular verbs, demonstratives, quantifiers and elements in the complementizer node can serve as hosts for the adjoined material or stand alone.']",5,"['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .', 'Otherwise the word combines with one or more orthographically distinct words to form a single phonological word that has no internal word or phrase boundaries.']"
CC799,J91-2003,On compositional semantics,logic and conversationquot,['H P Grice'],introduction,,"Secondly , the cooperative principle of #AUTHOR_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader .","['However, there are at least three arguments against iterating PT.', 'First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time.', 'Secondly , the cooperative principle of #AUTHOR_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader .', 'Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.', 'Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions.']",4,"['Secondly , the cooperative principle of #AUTHOR_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader .', 'Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.']"
CC800,J91-2003,On compositional semantics,coherence and coreferencequot,['J R Hobbs'],introduction,,"We are going to make such a comparison with the theories proposed by J. #AUTHOR_TAG , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W. Kintch ( 1983 ) , who are more interested in addressing psychological and cognitive aspects of discourse coherence .","['At this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others.', 'We are going to make such a comparison with the theories proposed by J. #AUTHOR_TAG , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W. Kintch ( 1983 ) , who are more interested in addressing psychological and cognitive aspects of discourse coherence .', 'The quoted works seem to be good representatives for each of the directions; they also point to related literature.']",1,"['We are going to make such a comparison with the theories proposed by J. #AUTHOR_TAG , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W. Kintch ( 1983 ) , who are more interested in addressing psychological and cognitive aspects of discourse coherence .']"
CC801,J91-2003,On compositional semantics,domain circumscription a reevaluationquot,"['D W Etherington', 'R E Mercer']",introduction,,"Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure as- sumption"" (ibid.), ""domain circumscription"" (cf. #AUTHOR_TAG), and their kin.","['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure as- sumption"" (ibid.), ""domain circumscription"" (cf. #AUTHOR_TAG), and their kin.', 'Similarly, the notion of R+ M-abduction is spiritually related to the ""abduc- tive inference"" of Reggia (1985), the ""diagnosis from first principles"" of Reiter (1987), ""explainability"" of Poole (1988), and the subset principle of Berwick (1986).', 'But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'These connections are being examined elsewhere (Zadrozny forthcoming).']",1,"['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure as- sumption"" (ibid.), ""domain circumscription"" (cf. #AUTHOR_TAG), and their kin.']"
CC802,J91-2003,On compositional semantics,the boundaries of words and their meaningsquot,['W Labov'],introduction,,W. #AUTHOR_TAG discussed sentences of the form * This is a chair but you can sit on it .,['W. #AUTHOR_TAG discussed sentences of the form * This is a chair but you can sit on it .'],0,['W. #AUTHOR_TAG discussed sentences of the form * This is a chair but you can sit on it .']
CC803,J91-2003,On compositional semantics,a grammar of contemporary english,"['R Quirk', 'S Greenbaum', 'G Leech', 'J Svartvik']",introduction,"The publication of this important volume fills the need for an up-to-date survey of the entire scope of English syntax. Though it falls short of a perfectly balanced treatment of the whole system, it touches upon all the essential topics and treats in depth a number of crucial problems of current interest such as case, ellipsis, and information focus. Even the publishers' claims are vindicated to a surprising degree. The statement that it ""constitutes a standard reference grammar"" is reasonably well justified. Recent investigations, including the authors' own research, are integrated into the ""accumulated grammatical tradition"" quite effectively. But whether it is ""the fullest and most comprehensive synchronic description of English grammar ever written"" is arguable. No one acquainted with Poutsma's work would agree with that. Very advanced foreign students o r native speakers of English who want to learn about basic grammar will find some of thel sections suitable for their needs, such as the lesson about restrictive and nonrestrictive relative clauses, though even here some of the explanations require very intensive study. Most of the chapters are rather like an advanced textbook for teachers or linguists. The organization and viewpoint give the impression of a carefully planned university lecture supplemented by diagrams, charts, and lists. A good example is the lesson on auxiliaries and verb phrases, which starts with a set of sample sentences demonstrating that ""should see"" and ""happen to see"" behave differently under various transformations and expansions. After the essential concepts are explained and exemplified-lexical verb, semi-auxiliary, operator, and the like-lists and paradigms are given as in the usual reference work. A particularly useful feature of this chapter is the outline of modal auxiliaries with examples of their divergent meanings.","Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also #AUTHOR_TAG , p. 672 ) .","['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also #AUTHOR_TAG , p. 672 ) .']",0,"['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also #AUTHOR_TAG , p. 672 ) .']"
CC804,J91-2003,On compositional semantics,cohesion in english,"['M A K Halliday', 'R Hasan']",introduction,"Despite all the dire predictions, Germany continues to exhibit a high level of social cohesion. Even the country's growing cultural and religious diversity is not at odds with its degree of togetherness. Yet, there are clear indications of potential threats: for example, the generally perceived lack of social justice and the gaping cleavage in togetherness between east and west as well as between structurally weak regions and such that are flourishing. To that end, cohesion is much weaker in Germany's eastern federal states than its western ones. The federal states with the highest levels of cohesion are Saarland, Baden-Wurttemberg, and Bavaria. These are the core findings from the Social Cohesion Radar (SCR), for which Bertelsmann Stiftung collected fresh data in 2017 by surveying more than 5,000 people throughout the country. The goal was to examine cohesion in terms of its strengths, weaknesses, causes, and effects. For this empirical study, social cohesion is defined as the quality of communal life and is viewed as a multidimensional phenomenon. High levels of cohesion result from strong social relations, a positive feeling of connectedness to the community, and a strong focus on the common good","Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , #AUTHOR_TAG , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .","['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , #AUTHOR_TAG , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']",1,"['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , #AUTHOR_TAG , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.']"
CC805,J91-2003,On compositional semantics,a dictionary of modern english usage,['H W Fowler'],introduction,,"This problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e.g. , `` colorless green ideas ... '' ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG , p. 546 ) gives 10 definitions of a sentence .","['Still, our definition of coherence may not be restrictive enough: two collections of sentences, one referring to ""black"" (about black pencils, black pullovers, and black poodles), the other one about ""death"" (war, cancer, etc.), connected by a sentence referring to both of these, could be interpreted as one paragraph about the new, broader topic ""black + death.""', ""This problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e.g. , `` colorless green ideas ... '' ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG , p. 546 ) gives 10 definitions of a sentence .""]",0,"[""This problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e.g. , `` colorless green ideas ... '' ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG , p. 546 ) gives 10 definitions of a sentence .""]"
CC806,J91-2003,On compositional semantics,mental models,['P N Johnson-Laird'],introduction,"The complexity of conceptualizing mental models has made Virtual Reality an interesting way to enhance communication and understanding between individuals working together on a project or idea. Here, the authors discuss practical applications of using VR for this purpose","This Principle of Finitism is also assumed by #AUTHOR_TAG , Jackendoff ( 1983 ) , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .","['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by #AUTHOR_TAG , Jackendoff ( 1983 ) , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']",1,"['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by #AUTHOR_TAG , Jackendoff ( 1983 ) , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']"
CC807,J91-2003,On compositional semantics,a theory of truth and semantic representationquot,['H Kamp'],introduction,,"But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of #AUTHOR_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora .","['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of #AUTHOR_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora ."", 'The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", 'Jackendoff (1983, p. 14) writes ""it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.""', 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']",1,"['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of #AUTHOR_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora ."", 'The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", 'Jackendoff (1983, p. 14) writes ""it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.""', 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']"
CC808,J91-2003,On compositional semantics,so what can we talk about nowquot,['B Webber'],introduction,Impure water is made suitable for drinking in an apparatus comprising a pressurizable holding tank attached to a purification cartridge containing an impurities adsorbent and a fine filter. A gas-containing cartridge is pierced to provide a bactericidal gas for killing pathogenic microorganisms and for pressurizing the holding tank to force the water through the purification cartridge.,"Other factors , such as the role of focus ( Grosz 1977 , 1978 ; Sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG ) must play a role , too .","['We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ; Sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']",0,"['They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ; Sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']"
CC809,J91-2003,On compositional semantics,coherence and coreferencequot,['J R Hobbs'],introduction,,"According to #AUTHOR_TAG , p. 67 ) , these two sentences are incoherent .","['According to #AUTHOR_TAG , p. 67 ) , these two sentences are incoherent .', 'However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country... (ibid.)', 'suddenly (for Hobbs) becomes coherent.', ""It seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change; after all, the first two sentences didn't change when the third one was added."", 'On the other hand, this change is easily explained when we treat the first two sentences as a paragraph: if the third sentence is not a part of the background knowledge, the paragraph is incoherent.', 'And the paragraph obtained by adding the third sentence is coherent.', 'Moreover, coherence here is clearly the result of the existence of the topic ""John likes spinach.""']",1,"['According to #AUTHOR_TAG , p. 67 ) , these two sentences are incoherent .', ""It seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change; after all, the first two sentences didn't change when the third one was added."", 'And the paragraph obtained by adding the third sentence is coherent.', 'Moreover, coherence here is clearly the result of the existence of the topic ""John likes spinach.""']"
CC810,J91-2003,On compositional semantics,artificial intelligence the very idea,['J Haugeland'],introduction,"The idea that human thinking and machine computing are ""radically the same"" provides the central theme for this marvelously lucid and witty book on what artificial intelligence is all about. Although presented entirely in nontechnical","For instance , relating ""they"" to ""apples"" in the sentence ( cfXXX #AUTHOR_TAG p. 195 ; Zadrozny 1987a ) : We bought the boys apples because they were so cheap","['We adopt the three-level semantics as a formal tool for the analysis of paragraphs.', 'This semantics was constructed (Zadrozny 1987a, 1987b) as a formal framework for default and commonsense reasoning.', 'It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'For instance , relating ""they"" to ""apples"" in the sentence ( cfXXX #AUTHOR_TAG p. 195 ; Zadrozny 1987a ) : We bought the boys apples because they were so cheap']",0,"['This semantics was constructed (Zadrozny 1987a, 1987b) as a formal framework for default and commonsense reasoning.', 'It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'For instance , relating ""they"" to ""apples"" in the sentence ( cfXXX #AUTHOR_TAG p. 195 ; Zadrozny 1987a ) : We bought the boys apples because they were so cheap']"
CC811,J91-2003,On compositional semantics,the episode schema in story processingquot,"['K Haberlandt', 'C Berian', 'J Sandson']",introduction,,Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #AUTHOR_TAG .,"['An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #AUTHOR_TAG .']",0,"['These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #AUTHOR_TAG .']"
CC812,J91-2003,On compositional semantics,paragraph structure inference,['E J Crothers'],introduction,,"Finally , it has been shown by Groesser ( 1981 ) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph .","['However, there are at least three arguments against iterating PT.', 'First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time.', 'Secondly, the cooperative principle of Grice (1975Grice ( , 1978, under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader.', 'Finally , it has been shown by Groesser ( 1981 ) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph .', 'Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions.']",4,"['Finally , it has been shown by Groesser ( 1981 ) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph .']"
CC813,J91-2003,On compositional semantics,krypton a functional approach to knowledge representationquot,"['R J Brachman', 'R E Fikes', 'H J Levesque']",introduction,,"The last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX #AUTHOR_TAG ) .","['The last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX #AUTHOR_TAG ) .', 'Brachman et al. 1985).', ""KRYPTON's A-box, encoding the object theory as a set of assertions, uses standard first order logic; the T-box contains information expressed in a frame-based language equivalent to a fragment of FOL."", ""However, the distinction between the two parts is purely functional--that is, characterized in terms of the system's behavior."", 'From the logical point of view, the knowledge base is the union of the two boxes, i.e. a theory, and the entailment is standard.', 'In our system, we also distinguish between the ""definitional"" and factual information, but the ""definitional"" part contains collections of mutually excluding theories, not just of formulas describing a semantic network.', 'Moreover, in addition to proposing this structure of R, we have described the two mechanisms for exploiting it, ""coherence"" and ""dominance,"" which are not variants of the standard first order entailment, but abduction.']",1,"['The last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX #AUTHOR_TAG ) .']"
CC814,J91-2003,On compositional semantics,abductive inferencequot,['J A Reggia'],introduction,,"Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #AUTHOR_TAG , the `` diagnosis from first principles '' of Reiter ( 1987 ) , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) .","['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', '""domain circumscription"" (cf.', 'Etherington and Mercer 1987), and their kin.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #AUTHOR_TAG , the `` diagnosis from first principles '' of Reiter ( 1987 ) , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) ."", 'But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'These connections are being examined elsewhere (Zadrozny forthcoming).']",1,"[""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #AUTHOR_TAG , the `` diagnosis from first principles '' of Reiter ( 1987 ) , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) .""]"
CC815,J91-2003,On compositional semantics,the interpretation of tense in discoursequot,['B Webber'],introduction,,The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #AUTHOR_TAG ) to see what a formal interpretation of events in time might look like .,"['The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event compo- nents.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #AUTHOR_TAG ) to see what a formal interpretation of events in time might look like .', 'Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""Extending and revising Jackendoff's (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid.).""]",0,"['The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event compo- nents.', 'The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #AUTHOR_TAG ) to see what a formal interpretation of events in time might look like .', 'Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.']"
CC816,J91-2003,On compositional semantics,organizational patterns in discoursequot,['J Hinds'],introduction,,"According to #AUTHOR_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases .","['The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'He lists, classifies, and discusses various types of inference, by which he means, generally, ""the linguistic-logical notions of consequent and presupposition"" Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences).', 'These chunks are sometimes called ""episodes,"" and sometimes ""paragraphs.""', 'According to #AUTHOR_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases .', 'Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types.', 'The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation).', 'For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined).', 'Segments themselves are composed of clauses and regulated by ""switching"" patterns, such as the question-answer pattern and the remark-reply pattern.']",0,"['The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'He lists, classifies, and discusses various types of inference, by which he means, generally, ""the linguistic-logical notions of consequent and presupposition"" Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences).', 'These chunks are sometimes called ""episodes,"" and sometimes ""paragraphs.""', 'According to #AUTHOR_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases .', 'Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types.', 'For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined).', 'Segments themselves are composed of clauses and regulated by ""switching"" patterns, such as the question-answer pattern and the remark-reply pattern.']"
CC817,J91-2003,On compositional semantics,a theory of diagnosis from first principlesquot,['R Reiter'],introduction,,"Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of Reggia ( 1985 ) , the `` diagnosis from first principles '' of #AUTHOR_TAG , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) .","['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', '""domain circumscription"" (cf.', 'Etherington and Mercer 1987), and their kin.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of Reggia ( 1985 ) , the `` diagnosis from first principles '' of #AUTHOR_TAG , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) ."", 'But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'These connections are being examined elsewhere (Zadrozny forthcoming).']",1,"['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', 'Etherington and Mercer 1987), and their kin.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of Reggia ( 1985 ) , the `` diagnosis from first principles '' of #AUTHOR_TAG , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) .""]"
CC818,J91-2003,On compositional semantics,inference without chainingquot,['A Frisch'],introduction,,"Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX Levesque 1984 ; #AUTHOR_TAG ; Patel-Schneider 1985 ) .","['Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX Levesque 1984 ; #AUTHOR_TAG ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', ""But we won't pursue this topic further here.""]",1,"['Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX Levesque 1984 ; #AUTHOR_TAG ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', ""But we won't pursue this topic further here.""]"
CC819,J91-2003,On compositional semantics,the game of language,['J Hintikka'],introduction,,"This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX #AUTHOR_TAG .","['Unless explicitly stated otherwise, we assume that formulas are expressed in a certain (formal) language L without equality; the extension L(=) of L is going to be used only in Section 5 for dealing with noun phrase references.', ""This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX #AUTHOR_TAG ."", 'Hintikka (1985).']",1,"[""This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX #AUTHOR_TAG .""]"
CC820,J91-2003,On compositional semantics,intended models circumscription and commonsense reasoningquot,['W Zadrozny'],introduction,"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations.","Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing ""there exists"" by ""all"" in the above definitions (cf. #AUTHOR_TAGb ).","['Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing ""there exists"" by ""all"" in the above definitions (cf. #AUTHOR_TAGb ).', 'We will have, however, no need for ""strong"" notions in this paper.', 'Also, in a practical system, ""satisfies"" should be probably replaced by ""violates fewest.""']",1,"['Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing ""there exists"" by ""all"" in the above definitions (cf. #AUTHOR_TAGb ).', 'We will have, however, no need for ""strong"" notions in this paper.']"
CC821,J91-2003,On compositional semantics,38 examples of elusive antecedents from published texts,['J R Hobbs'],introduction,,"Adding selectional restrictions ( semantic feature information , #AUTHOR_TAG ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation .","['Adding selectional restrictions ( semantic feature information , #AUTHOR_TAG ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation .', 'Later, Hobbs (1979Hobbs ( , 1982 proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using ""salience"" in choosing facts from this knowledge base.']",0,"['Adding selectional restrictions ( semantic feature information , #AUTHOR_TAG ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation .']"
CC822,J91-2003,On compositional semantics,temporal ontology in natural languagequot,"['M Moens', 'M Steedman']",introduction,,The reader may consult recent papers on this subject ( e.g. #AUTHOR_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like .,"['The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event components.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject ( e.g. #AUTHOR_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like .', 'Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""Extending and revising Jackendoff's (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid.).""]",0,"['The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event components.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject ( e.g. #AUTHOR_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like .', 'Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.']"
CC823,J91-2003,On compositional semantics,intended models circumscription and commonsense reasoningquot,['W Zadrozny'],introduction,"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations.","Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX #AUTHOR_TAGa , 1987b ) .","['Since it is the ""highest"" path, fint is the most plausible (relative to R) interpretation of the words that appear in the sentence.', 'Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX #AUTHOR_TAGa , 1987b ) .', 'Zadrozny 1987aZadrozny , 1987b.', 'Another theory, consisting of f~ = {el, sh2, pl, b2~ dl} and S, saying that A space vehicle came into the harbor and caused a disease~illness is less plausible according to that ordering.', 'As it turns out, f~ is never constructed in the process of building an interpretation of a paragraph containing the sentence S, unless assuming fint would lead to a contradiction, for instance within the higher level context of a science fiction story.']",0,"['Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX #AUTHOR_TAGa , 1987b ) .']"
CC824,J91-2003,On compositional semantics,the paragraph as a grammatical unit,['R E Longacre'],introduction,,"Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , #AUTHOR_TAG , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .","['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , #AUTHOR_TAG , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']",1,"['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , #AUTHOR_TAG , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.']"
CC825,J91-2003,On compositional semantics,analysis without actual infinityquot,['J Mycielski'],introduction,,"As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #AUTHOR_TAG ) .","['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1983, Kamp (1981), and implicitly or explicitly by almost all researchers in computational linguistics.', 'As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #AUTHOR_TAG ) .', 'Mycielski 1981).']",0,"['For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #AUTHOR_TAG ) .', 'Mycielski 1981).']"
CC826,J91-2003,On compositional semantics,disambiguating prepositional phrase attachments by using online dictionary definitionsquot computational linguistics 1334251260 special issue on the lexicon,"['K Jensen', 'J-L Binot']",introduction,,"We have shown elsewhere ( #AUTHOR_TAG ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .","['We have shown elsewhere ( #AUTHOR_TAG ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']",2,"['We have shown elsewhere ( #AUTHOR_TAG ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .']"
CC827,J91-2003,On compositional semantics,introduction to artificial intelligence,"['E Charniak', 'D McDermott']",introduction,"This book is an introduction on artificial intelligence. Topics include reasoning under uncertainty, robot plans, language understanding, and learning. The history of the field as well as intellectual ties to related disciplines are presented.","The necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG call it abductive unification/matching , Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .","['The necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG call it abductive unification/matching , Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']",0,"['The necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG call it abductive unification/matching , Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']"
CC828,J91-2003,On compositional semantics,universal grammarquot,['R Montague'],introduction,,"The logical notation of #AUTHOR_TAG is more sophisticated , and may be considered another possibility .","['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora."", 'The logical notation of #AUTHOR_TAG is more sophisticated , and may be considered another possibility .', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", 'Jackendoff (1983, p. 14) writes ""it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.""', 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']",1,"['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora."", 'The logical notation of #AUTHOR_TAG is more sophisticated , and may be considered another possibility .', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", 'Jackendoff (1983, p. 14) writes ""it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.""', 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']"
CC829,J91-2003,On compositional semantics,semantics and cognition,['R Jackendoff'],introduction,"A new perspective on cognition views cortical cell assemblies linking together knowledge about actions and perceptions not only as the vehicles of integrated action and perception processing but, furthermore, as a brain basis for a wide range of higher cortical functions, including attention, meaning and concepts, sequences, goals and intentions, and even communicative social interaction. This article explains mechanisms relevant to mechanistic action perception theory, points to concrete neuronal circuits in brains along with artificial neuronal network simulations, and summarizes recent brain imaging and other experimental data documenting the role of action perception circuits in cognition, language and communication","This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , #AUTHOR_TAG , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .","['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , #AUTHOR_TAG , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']",1,"['For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , #AUTHOR_TAG , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .']"
CC830,J91-2003,On compositional semantics,paragraph structure inference,['E J Crothers'],introduction,,"He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #AUTHOR_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) .","['The textualist approach to paragraph analysis is exemplified by E. J. Crothers.', 'His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #AUTHOR_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) .', 'These chunks are sometimes called ""episodes,"" and sometimes ""paragraphs.""', 'According to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.', 'Paragraphs therefore give hierarchical structure to sentences.', 'Hinds discusses three major types of paragraphs, and their corresponding segment types.', 'The three types are procedural (how-to), expository (essay), and narrative (in this case, spontaneous conversation).', 'For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined).', 'Segments themselves are composed of clauses and regulated by ""switching"" patterns, such as the question-answer pattern and the remark-reply pattern.']",0,"['The textualist approach to paragraph analysis is exemplified by E. J. Crothers.', 'His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #AUTHOR_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) .', 'These chunks are sometimes called ""episodes,"" and sometimes ""paragraphs.""', 'According to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.', 'Paragraphs therefore give hierarchical structure to sentences.', 'Hinds discusses three major types of paragraphs, and their corresponding segment types.', 'For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined).']"
CC831,J91-2003,On compositional semantics,dont blame the toolquot,['W Woods'],introduction,,"However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #AUTHOR_TAG ) .","['Once the word ""up"" is given its meaning relative to our experience with gravity, it is not free to ""slip"" into its opposite.', '""Up"" means up and not down ....', 'We have a model that men and women couple to produce offspring who are similar to their parents, and this model is grounded in genetics, and the semantics of kinship metaphor is grounded in this model.', 'Mothers have a different role than fathers in this model, and thus there is a reason why ""Death is the father of beauty"" fails poetically while ""Death is the mother of beauty"" succeeds ....', 'It is precisely this ""grounding"" of logical predicates in other conceptual structures that we would like to capture.', 'We investigate here only the ""grounding"" in logical theories.', 'However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #AUTHOR_TAG ) .', 'Woods 1987).']",0,"['Once the word ""up"" is given its meaning relative to our experience with gravity, it is not free to ""slip"" into its opposite.', '""Up"" means up and not down ....', 'We investigate here only the ""grounding"" in logical theories.', 'However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #AUTHOR_TAG ) .', 'Woods 1987).']"
CC832,J91-2003,On compositional semantics,semantics and cognition,['R Jackendoff'],introduction,"A new perspective on cognition views cortical cell assemblies linking together knowledge about actions and perceptions not only as the vehicles of integrated action and perception processing but, furthermore, as a brain basis for a wide range of higher cortical functions, including attention, meaning and concepts, sequences, goals and intentions, and even communicative social interaction. This article explains mechanisms relevant to mechanistic action perception theory, points to concrete neuronal circuits in brains along with artificial neuronal network simulations, and summarizes recent brain imaging and other experimental data documenting the role of action perception circuits in cognition, language and communication","#AUTHOR_TAG , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . ''","['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora."", 'The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", ""#AUTHOR_TAG , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . ''"", 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']",1,"['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora."", 'The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", ""#AUTHOR_TAG , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . ''"", 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']"
CC833,J91-2003,On compositional semantics,cues people use to paragraph text,"['S J Bond', 'J R Hayes']",introduction,"This paper reports the results of three studies on the paragraph. In Study 1, subjects were asked to paragraph a text from which paragraph indentations had been removed. Results indicate that readers can consistently paragraph unparagraphed text, thus supporting Young and Becker's (1966) assertion that the paragraph is a psychologically real unit of discourse. Results also reveal that readers rely heavily on breaks in text cohesion (e.g., topic shift) and on paragraph length as paragraphing cues. In Study 2, four new subjects were asked to paragraph the same text used in Study 1, and to ""think aloud,"" giving their reasons for paragraphing, as they did so. Analysis of these thinking aloud protocols reveal that, in the absence of a strong paragraphing cue, readers will read ahead in a text, sometimes flagging weaker paragraphing cues as they go. If they feel the unparagraphed text is too long, they will go back and paragraph at these weaker cues until all paragraphs in the text are an acceptable length. Based on the results of Studies 1 and 2, a model of how readers paragraph was devised. The model was tested in Study 3 on new subjects who were asked to think aloud as they paragraphed the same text used in Study 1 , and another, longer text. The model predicted the new data quite accurately. Deciding whether paragraph boundaries are psychologically real or arbitrary is very much like deciding whether geographical boundaries are psychologically real or arbitrary. Typically, state boundaries are not psychologically real because travelers cannot find them without the help of signs. Coast lines, on the other hand, are very real. People who miss them fall into the ocean. In the same way, we would consider paragraph boundaries artificial if people could find them only with the help of paragraphing marks, and real if people could consistently find them in texts from which paragraphing marks had been removed. Some linguists such as Hodges (1941) have viewed the paragraph as an arbitrary device used by the writer to ""give the reader a breathing spell"" (p. 311). Similarly, Rodgers (1967) has suggested that a section of text ""becomes a paragraph not by virtue of its structure, but because the writer elects to indent"" (p. 182). However, Young and Becker (1966) and Koen, Becker, and Young (1969) have provided strong evidence that paragraphs are indeed psychologically real. They asked readers to paragraph text from which all paragraphing markers had been removed and found that their readers Research in the Teaching of English, Vol. 18, No. 2, May 1984",An example of psycholinguistically oriented research work can be found in #AUTHOR_TAG .,"['An example of psycholinguistically oriented research work can be found in #AUTHOR_TAG .', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholinguistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al. (1980).']",0,"['An example of psycholinguistically oriented research work can be found in #AUTHOR_TAG .', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholinguistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al. (1980).']"
CC834,J91-2003,On compositional semantics,paragraph structure inference,['E J Crothers'],introduction,,"#AUTHOR_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , ' which could be invoked to generate inferences ... '' .","['This demonstrates that information needed to identify and resolve anaphoric references can be found, to an interesting extent, in standard dictionaries and thesauri.', '(Other reference works could be treated as additional sources of world knowledge.)', 'This type of consultation uses existing natural language texts as a referential level for processing purposes.', 'It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit.', ""#AUTHOR_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , ' which could be invoked to generate inferences ... '' ."", 'With respect to that independent source of knowledge, our main contributions are two.', 'First, we identify its possible structure (a collection of partially ordered theories) and make formal the choice of a most plausible interpretation.', 'In other words, we recognize it as a separate logical level--the referential level.', 'Second, we suggest that natural language reference works, like dictionaries and thesauri, can quite often fill the role of the referential level.']",0,"['(Other reference works could be treated as additional sources of world knowledge.)', ""#AUTHOR_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , ' which could be invoked to generate inferences ... '' ."", 'Second, we suggest that natural language reference works, like dictionaries and thesauri, can quite often fill the role of the referential level.']"
CC835,J91-2003,On compositional semantics,coherence and coreferencequot,['J R Hobbs'],introduction,,"Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only ""petty conversational implicature"" ( #AUTHOR_TAG ) , or the metarules of Section 5.2?","['6.1.1 Was the Use of a Gricean Maxim Necessary?', 'Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only ""petty conversational implicature"" ( #AUTHOR_TAG ) , or the metarules of Section 5.2?', 'It seems to us that the answer is no.']",0,"['Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only ""petty conversational implicature"" ( #AUTHOR_TAG ) , or the metarules of Section 5.2?']"
CC836,J91-2003,On compositional semantics,intended models circumscription and commonsense reasoningquot,['W Zadrozny'],introduction,"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations.","This semantics was constructed ( #AUTHOR_TAGa , 1987b ) as a formal framework for default and commonsense reasoning .","['We adopt the three-level semantics as a formal tool for the analysis of paragraphs.', 'This semantics was constructed ( #AUTHOR_TAGa , 1987b ) as a formal framework for default and commonsense reasoning .', 'It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'For instance, relating ""they"" to ""apples"" in the sentence (cf.', 'Haugeland 1985 p. 195;Zadrozny 1987a):']",5,"['This semantics was constructed ( #AUTHOR_TAGa , 1987b ) as a formal framework for default and commonsense reasoning .']"
CC837,J91-2003,On compositional semantics,learning from positiveonly examples the subset principle and three case studiesquot,['R C Berwick'],introduction,,"Similarly, the notion of R + M-abduction is spiritually related to the ""abductive inference"" of Reggia (1985), the ""diagnosis from first principles"" of Reiter (1987), ""explainability"" of Poole (1988), and the subset principle of #AUTHOR_TAG .","['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', '""domain circumscription"" (cf.', 'Etherington and Mercer 1987), and their kin.', 'Similarly, the notion of R + M-abduction is spiritually related to the ""abductive inference"" of Reggia (1985), the ""diagnosis from first principles"" of Reiter (1987), ""explainability"" of Poole (1988), and the subset principle of #AUTHOR_TAG .', 'But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'These connections are being examined elsewhere (Zadrozny forthcoming).']",1,"['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', 'Etherington and Mercer 1987), and their kin.', 'Similarly, the notion of R + M-abduction is spiritually related to the ""abductive inference"" of Reggia (1985), the ""diagnosis from first principles"" of Reiter (1987), ""explainability"" of Poole (1988), and the subset principle of #AUTHOR_TAG .', 'These connections are being examined elsewhere (Zadrozny forthcoming).']"
CC838,J91-2003,On compositional semantics,the episode schema in story processingquot,"['K Haberlandt', 'C Berian', 'J Sandson']",introduction,,"Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 , #AUTHOR_TAG ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .","['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 , #AUTHOR_TAG ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']",1,"['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 , #AUTHOR_TAG ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']"
CC839,J91-2003,On compositional semantics,focusing in the comprehension of definite anaphoraquot,['C Sidner'],introduction,,"Other factors , such as the role of focus ( Grosz 1977 , 1978 ; #AUTHOR_TAG ) or quantifier scoping ( Webber 1983 ) must play a role , too .","['We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ; #AUTHOR_TAG ) or quantifier scoping ( Webber 1983 ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']",0,"['We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ; #AUTHOR_TAG ) or quantifier scoping ( Webber 1983 ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']"
CC840,J91-2003,On compositional semantics,death is the mother of beauty,['M Turner'],introduction,"Let's read! We will often find out this sentence everywhere. When still being a kid, mom used to order us to always read, so did the teacher. Some books are fully read in a week and we need the obligation to support reading. What about now? Do you still love reading? Is reading only for you who have obligation? Absolutely not! We here offer you a new book enPDFd death is the mother of beauty to read.","Some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG , pp. 7-8 ) : ... Semantics is constrained by our models of ourselves and our worlds .","['The referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'Some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG , pp. 7-8 ) : ... Semantics is constrained by our models of ourselves and our worlds .', 'We have models of up and down that are based by the way our bodies actually function.']",0,"['The referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'Some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG , pp. 7-8 ) : ... Semantics is constrained by our models of ourselves and our worlds .', 'We have models of up and down that are based by the way our bodies actually function.']"
CC841,J91-2003,On compositional semantics,passing markers a theory of contextual influence in language comprehensionquot,['E Charniak'],introduction,,"`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( Hirst 1987 ; #AUTHOR_TAG ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .","['The idea of using preferences among theories is new, hence it was described in more detail.', ""`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( Hirst 1987 ; #AUTHOR_TAG ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .""]",1,"[""`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( Hirst 1987 ; #AUTHOR_TAG ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .""]"
CC842,J91-2003,On compositional semantics,a logic of implicit and explicit beliefsquot,['H J Levesque'],introduction,,"Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #AUTHOR_TAG ; Frisch 1987 ; Patel-Schneider 1985 ) .","['Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #AUTHOR_TAG ; Frisch 1987 ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', ""But we won't pursue this topic further here.""]",1,"['Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #AUTHOR_TAG ; Frisch 1987 ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', ""But we won't pursue this topic further here.""]"
CC843,J91-2003,On compositional semantics,the representation and use of focus in a system for understanding dialogsquot,['B J Grosz'],introduction,"As a dialog progresses the objects and actions that are most relevant to the conversation, and hence in the focus of attention of the dialog participants, change. This paper describes a representation of focus for language understanding systems, emphasizing its use in understanding task-oriented dialogs. The representation highlights that part of the knowledge base relevant at a given point in a dialog. A model of the task is used both to structure the focus representation and to provide an index into potentially relevant concepts in the knowledge base The use of the focus representation to make retrieval of items from the knowledge base more efficient is described.","Other factors , such as the role of focus ( #AUTHOR_TAG , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too .","['We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( #AUTHOR_TAG , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']",0,"['Other factors , such as the role of focus ( #AUTHOR_TAG , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too .']"
CC844,J91-2003,On compositional semantics,cohesion in english,"['M A K Halliday', 'R Hasan']",introduction,"Despite all the dire predictions, Germany continues to exhibit a high level of social cohesion. Even the country's growing cultural and religious diversity is not at odds with its degree of togetherness. Yet, there are clear indications of potential threats: for example, the generally perceived lack of social justice and the gaping cleavage in togetherness between east and west as well as between structurally weak regions and such that are flourishing. To that end, cohesion is much weaker in Germany's eastern federal states than its western ones. The federal states with the highest levels of cohesion are Saarland, Baden-Wurttemberg, and Bavaria. These are the core findings from the Social Cohesion Radar (SCR), for which Bertelsmann Stiftung collected fresh data in 2017 by surveying more than 5,000 people throughout the country. The goal was to examine cohesion in terms of its strengths, weaknesses, causes, and effects. For this empirical study, social cohesion is defined as the quality of communal life and is viewed as a multidimensional phenomenon. High levels of cohesion result from strong social relations, a positive feeling of connectedness to the community, and a strong focus on the common good","This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #AUTHOR_TAG , p. 329 ) .","['Note: In our translation from English to logic we are assuming that ""it"" is anaphoric (with the pronoun following the element that it refers to), not cataphoric (the other way around).', 'This means that the ""it"" that brought the disease in P1 will not be considered to refer to the infection ""i"" or the death ""d"" in P3.', 'This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #AUTHOR_TAG , p. 329 ) .']",4,"['This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #AUTHOR_TAG , p. 329 ) .']"
CC845,J91-2003,On compositional semantics,parsing strategies in a broadcoverage grammar of english research report rc 12147 ibm tj watson research center,['K Jensen'],introduction,," #AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language .","['We can also hope for some fine-tuning of the notion of topic, which would prevent many offensive examples.', 'This approach is taken in computational syntactic grammars (e.g.', ' #AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language .']",0,"['This approach is taken in computational syntactic grammars (e.g.', ' #AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language .']"
CC846,J91-2003,On compositional semantics,semantic interpretation and the resolution of ambiguity,['G Hirst'],introduction,"Preface 1. Introduction 2. Semantic interpretation 3. The Absity semantic interpreter 4. Lexical disambiguation 5. Polaroid words 6. Structural disambiguation 7. The semantic enquiry desk 8. Conclusion 9. Speculations, partially baked ideas, and exercises for the reader References Index of names Index of subjects.","`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG ; Charniak 1983 ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .","['The idea of using preferences among theories is new, hence it was described in more detail.', ""`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG ; Charniak 1983 ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .""]",1,"[""`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG ; Charniak 1983 ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .""]"
CC847,J91-2003,On compositional semantics,coherence and coreferencequot,['J R Hobbs'],introduction,,"We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #AUTHOR_TAG .","['We do not claim that Gla is the best or unique way of expressing the rule ""assume that the writer did not say too much.""', 'Rather, we stress the possibility that one can axiomatize and productively use such a rule.', 'We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #AUTHOR_TAG .']",2,"['We do not claim that Gla is the best or unique way of expressing the rule ""assume that the writer did not say too much.""', 'We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #AUTHOR_TAG .']"
CC848,J91-2003,On compositional semantics,cohesion in english,"['M A K Halliday', 'R Hasan']",introduction,"Despite all the dire predictions, Germany continues to exhibit a high level of social cohesion. Even the country's growing cultural and religious diversity is not at odds with its degree of togetherness. Yet, there are clear indications of potential threats: for example, the generally perceived lack of social justice and the gaping cleavage in togetherness between east and west as well as between structurally weak regions and such that are flourishing. To that end, cohesion is much weaker in Germany's eastern federal states than its western ones. The federal states with the highest levels of cohesion are Saarland, Baden-Wurttemberg, and Bavaria. These are the core findings from the Social Cohesion Radar (SCR), for which Bertelsmann Stiftung collected fresh data in 2017 by surveying more than 5,000 people throughout the country. The goal was to examine cohesion in terms of its strengths, weaknesses, causes, and effects. For this empirical study, social cohesion is defined as the quality of communal life and is viewed as a multidimensional phenomenon. High levels of cohesion result from strong social relations, a positive feeling of connectedness to the community, and a strong focus on the common good","Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .","['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .']",0,"['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .']"
CC849,J91-2003,On compositional semantics,the flow of thought and the flow of languagequot,['W L Chafe'],introduction,,"#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .","['Although there are other discussions of the paragraph as a central element of discourse (e.g.', '#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']",1,"['#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Our interest, however, lies precisely in that area.']"
CC850,J91-2003,On compositional semantics,a theory of truth and semantic representationquot,['H Kamp'],introduction,,"This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .","['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']",1,"['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .', 'Mycielski 1981).']"
CC851,J91-2003,On compositional semantics,episodes as chunks in narrative memoryquot,"['J B Black', 'G H Bower']",introduction,,Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .,"['An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']",0,"['These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']"
CC852,J91-2003,On compositional semantics,intended models circumscription and commonsense reasoningquot,['W Zadrozny'],introduction,"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations.","We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .","['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']",2,"['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']"
CC853,J91-2003,On compositional semantics,intended models circumscription and commonsense reasoningquot,['W Zadrozny'],introduction,"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations.",It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .,"['Building an interpretation of a paragraph does not mean finding all of its possible meanings; the implausible ones should not be computed at all.', 'This viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates.', 'Now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence.', 'We can then later (Section 5.2) define p-models--a category of models corresponding to paragraphs--as models of coherent theories that satisfy all metalevel conditions.', 'The partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'This immediate information may be insufficient to decide the truth of certain predicates.', 'It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .', 'Zadrozny 1987b).']",1,"['Building an interpretation of a paragraph does not mean finding all of its possible meanings; the implausible ones should not be computed at all.', 'This viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates.', 'Now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence.', 'We can then later (Section 5.2) define p-models--a category of models corresponding to paragraphs--as models of coherent theories that satisfy all metalevel conditions.', 'The partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'This immediate information may be insufficient to decide the truth of certain predicates.', 'It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .', 'Zadrozny 1987b).']"
CC854,J91-2003,On compositional semantics,coherence and coreferencequot,['J R Hobbs'],introduction,,"Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .","['Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]",0,"['Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]"
CC855,J91-2003,On compositional semantics,resolving pronoun referencesquot,['J R Hobbs'],introduction,,"The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .","['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']",0,"['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']"
CC856,J91-2003,On compositional semantics,languages with self reference i foundationsquot,['D Perlis'],introduction,,"Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .","['5.1.1', 'Translation to Logic.', 'The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event components.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject (e.g.', 'Moens and Steedman 1987;Webber 1987) to see what a formal interpretation of events in time might look like.', 'Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .', 'Extending and revising Jackendoff\'s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (""that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon""---ibid.).', 'However, as noted before, we will use a simplified version of such a logical notation; we will have only time, event, result, and property as primitives.', 'After these remarks we can begin constructing the model of the example paragraph.', 'We assume that constants are introduced by NPs.', 'We have then (i) Constants s, m, d, i, b, 1347 satisfying: ship(s), Messina(m), disease(d), infection(i), death(b), year(1347).']",0,"['5.1.1', 'Translation to Logic.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject (e.g.', 'Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .']"
CC857,J91-2003,On compositional semantics,lectures on contemporary syntactic theories csli lecture notes,['P Sells'],introduction,,"For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''","['A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''"", 'The pairing of these two sentences may be said to create a small paragraph.', 'Our point is that an acceptable structure can be assigned to the utterance ""Reagan thinks bananas"" only within the paragraph in which this utterance occurs.', 'We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task.', ""That is, we can ask in the first sentence of a paragraph about Kissinger's favorite fruit, elaborate the question and the circumstances in the next few sentences, and give the above answer at the end."", 'We do not claim that a paragraph is necessarily described by a set of grammar rules in some grammar formalism (although it may be); rather, it has the grammatical role of providing functional structures that can be assigned to strings.']",4,"['A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''"", 'The pairing of these two sentences may be said to create a small paragraph.', 'Our point is that an acceptable structure can be assigned to the utterance ""Reagan thinks bananas"" only within the paragraph in which this utterance occurs.', 'We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task.', 'We do not claim that a paragraph is necessarily described by a set of grammar rules in some grammar formalism (although it may be); rather, it has the grammatical role of providing functional structures that can be assigned to strings.']"
CC858,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the interaction of word recognition and linguistic processing in speech understandingquot,['H Niemann'],introduction,"This contribution describes an approach to integrate a speech understanding and dialog system into a homogeneous architecture based on semantic networks. The definition of the network as well as its use in speech understanding is described briefly. A scoring function for word hypotheses meeting the requirements of a graph search algorithm is presented. The main steps of the linguistic analysis, i.e. syntax, semantics, and pragmatics, are described and their realization in the semantic network is shown. The processing steps alternating between data- and model-driven phases are outlined using an example sentence which demonstrates a tight interaction between word recognition and linguistic processing.","Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , #AUTHOR_TAG , and Young ( 1989 ) .","['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , #AUTHOR_TAG , and Young ( 1989 ) .']",0,"['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , #AUTHOR_TAG , and Young ( 1989 ) .']"
CC859,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,automatic speech recognition the development of the sphinx system appendix i,['K F Lee'],,,A formula for the test set perplexity ( #AUTHOR_TAG ) is :13,['A formula for the test set perplexity ( #AUTHOR_TAG ) is :13'],0,['A formula for the test set perplexity ( #AUTHOR_TAG ) is :13']
CC860,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the voyager speech understanding system preliminary development and evaluationquot,"['V Zue', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff']",,"Early experience with the development of the MIT VOYAGER spoken language system is described, and its current performance is documented. The three components of VOYAGER, the speech recognition component, the natural language component, and the application back-end, are described.>",The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #AUTHOR_TAG ) .,"['To date, four distinct domain-specific versions of TINA have been implemented.', 'The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al. 1986).', 'The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years.', 'The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #AUTHOR_TAG ) .', 'The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community.']",5,"['The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al. 1986).', 'The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #AUTHOR_TAG ) .']"
CC861,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the collection and preliminary analysis of a spontaneous speech databasequot darpa speech and natural language workshop,"['V Zue', 'N Daly', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff', 'M Soclof']",,,Subsequent processing by the natural language and response generation components was done automatically by the computer ( #AUTHOR_TAG ) .,"['To obtain training and test data for this task, we had a number of naive subjects use the system as if they were trying to obtain actual information.', 'Their speech was recorded in a simulation mode in which the speech recognition component was excluded.', 'Instead, an experimenter in a separate room typed in the utterances as spoken by the subject.', 'Subsequent processing by the natural language and response generation components was done automatically by the computer ( #AUTHOR_TAG ) .']",5,['Subsequent processing by the natural language and response generation components was done automatically by the computer ( #AUTHOR_TAG ) .']
CC862,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,on whmovementquot in formal syntax edited by,['Noam Chomsky'],,,( #AUTHOR_TAG ) .,['( #AUTHOR_TAG ) .'],0,['( #AUTHOR_TAG ) .']
CC863,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,grammaticallybased automatic word class formationquot,"['L Hirschman', 'R Grishman', 'N Sager']",,,This approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions .,"['Thus far, we have added all semantic filters by hand, and they are implemented in a hard-fail mode, i.e., if the semantic restrictions fail, the node dies.', 'This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains.', 'In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'This approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', 'There is obviously a great deal more work to be done in this important area.']",1,"['In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'This approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.']"
CC864,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,error bounds for convolutional codes and an asymptotically optimal decoding algorithmquot,['A Viterbi'],,"The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms.","The search algorithm is the standard Viterbi search ( #AUTHOR_TAG ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence .","['At present, we have available at MIT two systems, VOYAGER and ATIS, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input.', 'In both of these systems, TINA provides the interface between the recognizer and the application back-end.', 'In this section, I will describe our current interfaces between TINA and the recognizer and our future plans in this area.', 'In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.', 'This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'The recognizer for these systems is the SUMMIT system (Zue et al. 1989), which uses a segmental-based framework and includes an auditory model in the front-end processing.', 'The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'The search algorithm is the standard Viterbi search ( #AUTHOR_TAG ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence .']",5,"['This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'The search algorithm is the standard Viterbi search ( #AUTHOR_TAG ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence .']"
CC865,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the case for casequot in universals in linguistic theory,['C J Fillmore'],,,Semantic filters can also be used to prevent multiple versions of the same case frame ( #AUTHOR_TAG ) showing up as complements .,"['Semantic filters can also be used to prevent multiple versions of the same case frame ( #AUTHOR_TAG ) showing up as complements .', 'For instance, the set of complements [from-place], [to-place], and [at-time] are freely ordered following a movement verb such as ""leave.""', 'Thus a flight can ""leave for Chicago from Boston at nine,"" or, equivalently, ""leave at nine for Chicago from Boston.""', 'If these complements are each allowed to follow the other, then in TINA an infinite sequence of [from-place]s, [to-place]s and [at-time]s is possible.', 'This is of course unacceptable, but it is straightforward to have each node, as it occurs, or in a semantic bit specifying its case frame, and, in turn, fail if that bit has already been set.', 'We have found that this strategy, in conjunction with the capability of erasing all semantic bits whenever a new clause is entered (through the meta level ""detach"" operation mentioned previously) serves the desired goal of eliminating the unwanted redundancies.']",5,['Semantic filters can also be used to prevent multiple versions of the same case frame ( #AUTHOR_TAG ) showing up as complements .']
CC866,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the bbn spoken language systemquot,"['S Boisen', 'Y-L Chow', 'A Haas', 'R Ingria', 'S Roukos', 'D Stallard']",introduction,,"Representative systems are described in #AUTHOR_TAG , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and Young ( 1989 ) .","['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in #AUTHOR_TAG , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and Young ( 1989 ) .']",0,"['Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in #AUTHOR_TAG , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and Young ( 1989 ) .']"
CC867,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,full integration of speech and language understanding in the mit spoken language systemquot,"['D Goodine', 'S Seneff', 'L Hirschman', 'M Phillips']",,,"We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( #AUTHOR_TAG ) .","['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions (Zue et al. 1991), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( #AUTHOR_TAG ) ."", ""Ultimately we want to incorporate TINA'S probabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.""]",5,"[""We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( #AUTHOR_TAG ) .""]"
CC868,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,speech database development design and analysis of the acousticphonetic corpusquot,"['L Lamel', 'R H Kassel', 'S Seneff']",,,The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #AUTHOR_TAG ) .,"['To date, four distinct domain-specific versions of TINA have been implemented.', 'The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #AUTHOR_TAG ) .', 'The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years.', 'The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990).', 'The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community.']",5,['The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #AUTHOR_TAG ) .']
CC869,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,development and preliminary evaluation of the mit atis systemquot,"['S Seneff', 'J Glass', 'D Goddeau', 'D Goodine', 'L Hirschman', 'H Leung', 'M Phillips', 'J Polifroni', 'V Zue']",,,"However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .","['Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .', 'We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).', 'The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.', 'Each node receives a frame in both a top-down and a bottom-up cycle, and modifies the frame according to specifications based on its broad-class identity (as one of noun, noun-phrase, predicate, quantifier, etc.).', 'For example, a [subject] is a noun-phrase node with the label ""topic.""', 'During the top-down cycle, it creates a blank frame and inserts it into a ""topic"" slot in the frame that was handed to it.', 'It passes the blank frame to its children, who will then fill it appropriately, labeling it as a QSET or as a REFERENCE.', 'It then passes along to the right sibling the same frame that was handed to it from above, with the completed topic slot filled with the information delivered by the children.']",3,"['However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .', 'We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).', 'The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.', 'Each node receives a frame in both a top-down and a bottom-up cycle, and modifies the frame according to specifications based on its broad-class identity (as one of noun, noun-phrase, predicate, quantifier, etc.).', 'For example, a [subject] is a noun-phrase node with the label ""topic.""', 'During the top-down cycle, it creates a blank frame and inserts it into a ""topic"" slot in the frame that was handed to it.', 'It passes the blank frame to its children, who will then fill it appropriately, labeling it as a QSET or as a REFERENCE.']"
CC870,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,development and preliminary evaluation of the mit atis systemquot,"['S Seneff', 'J Glass', 'D Goddeau', 'D Goodine', 'L Hirschman', 'H Leung', 'M Phillips', 'J Polifroni', 'V Zue']",,,"The second one, ATIS ( #AUTHOR_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights.","['We currently have two application domains that can carry on a spoken dialog with a user.', 'One, the VOYAGER domain (Zue et al. 1990), answers questions about places of interest in an urban area, in our case, the vicinity of MIT and Harvard University.', 'The second one, ATIS ( #AUTHOR_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights.', 'Work continues on improving all aspects of these domains. Our current research is directed at a number of different remaining issues.']",5,"['We currently have two application domains that can carry on a spoken dialog with a user.', 'The second one, ATIS ( #AUTHOR_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights.', 'Work continues on improving all aspects of these domains. Our current research is directed at a number of different remaining issues.']"
CC871,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,discovery procedures for sublanguage selectional patterns initial experimentsquot,"['R Grishman', 'L Hirschman', 'N T Nhan']",,"Selectional constraints specify, for a particular domain, the combinations of semantic classes acceptable in subject-verb-object relationships and other syntactic structures. These constraints are important in blocking incorrect analyses in natural language processing systems. However, these constraints are domain-specific and hence must be developed anew when a system is ported to a new domain. A discovery procedure for selectional constraints is therefore essential in enhancing the portability of such systems.This paper describes a semi-automated procedure for collecting the co-occurrence patterns from a sample of texts in a domain, and then using these patterns as the basis for selectional constraints in analyzing further texts. We discuss some of the difficulties in automating the collection process, and describe two experiments that measure the completeness of these patterns and their effectiveness compared with manually-prepared patterns. We then describe and evaluate a procedure for selectional constraint relaxation, intended to compensate for gaps in the set of patterns. Finally, we suggest how these procedures could be combined with a system that queries a domain expert, in order to produce a more efficient discovery procedure.",This approach resembles the work by #AUTHOR_TAG and Hirschman et al. ( 1975 ) on selectional restrictions .,"['Thus far, we have added all semantic filters by hand, and they are implemented in a hard-fail mode, i.e., if the semantic restrictions fail, the node dies.', 'This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains.', 'In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'This approach resembles the work by #AUTHOR_TAG and Hirschman et al. ( 1975 ) on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', 'There is obviously a great deal more work to be done in this important area.']",1,['This approach resembles the work by #AUTHOR_TAG and Hirschman et al. ( 1975 ) on selectional restrictions .']
CC872,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,estimation of probabilities from sparse data for the language model component of a speech recognizerquot assp35,['S M Katz'],,"The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises.","Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model ( #AUTHOR_TAG ) .","['This appendix walks through a pedagogical example to parse spoken digit sequences up to three long, as in ""three hundred and sixteen.""', 'Included is a set of initial contextfree rules, a set of training sentences, an illustration of how to compute the path probabilities from the training sentences, and an illustration of both parsing and perplexity computation for a test sentence.', 'Since there are only five training sentences, a number of the arcs of the original grammar are lost after training.', 'This is a problem to be aware of in building grammars from example sentences.', 'In the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out.', 'Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model ( #AUTHOR_TAG ) .']",0,"['This appendix walks through a pedagogical example to parse spoken digit sequences up to three long, as in ""three hundred and sixteen.""', 'Included is a set of initial contextfree rules, a set of training sentences, an illustration of how to compute the path probabilities from the training sentences, and an illustration of both parsing and perplexity computation for a test sentence.', 'Since there are only five training sentences, a number of the arcs of the original grammar are lost after training.', 'This is a problem to be aware of in building grammars from example sentences.', 'In the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out.', 'Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model ( #AUTHOR_TAG ) .']"
CC873,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,on whmovementquot in formal syntax edited by,['Noam Chomsky'],,,"To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded ( #AUTHOR_TAG ) by its generator .","['2.5.1 Gaps.', 'The mechanism to deal with gaps resembles in certain respects the Hold register idea of ATNs, but with an important difference, reflecting the design philoso-phy that no node can have access to information outside of its immediate domain.', 'The mechanism involves two slots that are available in the feature vector of each parse node.', 'These are called the CURRENT-FOCUS and the FLOAT-OBJECT, respectively.', 'The CURRENT-FOCUS slot contains, at any given time, a pointer to the most recently mentioned content phrase in the sentence.', 'If the FLOAT-OBJECT slot is occupied, it means that there is a gap somewhere in the future that will ultimately be filled by the partial parse contained in the FLOAT-OBJECT.', 'The process of getting into the FLOAT-OBJECT slot (which is analogous to the Hold register) requires two steps, executed independently by two different nodes.', 'The first node, the generator, fills the CURRENT-FOCUS slot with the subparse returned to it by its children.', 'The second node, the activator, moves the CURRENT-FOCUS into the FLOAT-OBJECT position, for its children, during the top-down cycle.', 'It also requires that the FLOAT-OBJECT be absorbed somewhere among its descendants by a designated absorber node, a condition that is checked during the bottom-up cycle.', 'The CURRENT-FOCUS only gets passed along to siblings and their descendants, and hence is unavailable to activators at higher levels of the parse tree.', 'That is to say, the CURRENT-FOCUS is a feature, like verb-mode, that is blocked when an [end] node is encountered.', 'To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded ( #AUTHOR_TAG ) by its generator .', 'Finally, certain blocker nodes block the transfer of the FLOAT-OBJECT to their children.']",0,"['2.5.1 Gaps.', 'The mechanism involves two slots that are available in the feature vector of each parse node.', 'The CURRENT-FOCUS slot contains, at any given time, a pointer to the most recently mentioned content phrase in the sentence.', 'If the FLOAT-OBJECT slot is occupied, it means that there is a gap somewhere in the future that will ultimately be filled by the partial parse contained in the FLOAT-OBJECT.', 'The first node, the generator, fills the CURRENT-FOCUS slot with the subparse returned to it by its children.', 'The CURRENT-FOCUS only gets passed along to siblings and their descendants, and hence is unavailable to activators at higher levels of the parse tree.', 'That is to say, the CURRENT-FOCUS is a feature, like verb-mode, that is blocked when an [end] node is encountered.', 'To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded ( #AUTHOR_TAG ) by its generator .']"
CC874,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the voyager speech understanding system preliminary development and evaluationquot,"['V Zue', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff']",conclusion,"Early experience with the development of the MIT VOYAGER spoken language system is described, and its current performance is documented. The three components of VOYAGER, the speech recognition component, the natural language component, and the application back-end, are described.>","One , the VOYAGER domain ( #AUTHOR_TAG ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University .","['We_currently have two application domains that can carry on a spoken dialog with a user.', 'One , the VOYAGER domain ( #AUTHOR_TAG ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University .', 'The second one, ATIS (Seneff et al. 1991), is a system for accessing data in the Official Airline Guide and booking flights.', 'Work continues on improving all aspects of these domains.']",5,"['We_currently have two application domains that can carry on a spoken dialog with a user.', 'One , the VOYAGER domain ( #AUTHOR_TAG ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University .', 'The second one, ATIS (Seneff et al. 1991), is a system for accessing data in the Official Airline Guide and booking flights.', 'Work continues on improving all aspects of these domains.']"
CC875,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,semantics and quantification in natural language question answeringquot,['W A Woods'],,,"The example used to illustrate the power of ATNs ( #AUTHOR_TAG ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator .","[""The example used to illustrate the power of ATNs ( #AUTHOR_TAG ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator ."", 'Cases of crossed traces are automatically blocked because the second CURRENT-FOCUS gets moved into the FLOAT-OBJECT position at the time of the second activator, overriding the preexisting FLOAT-OBJECT set up by the earlier activator.', 'The wrong FLOAT-OBJECT is available at the position of the first trace, and the parse dies: *(Which books)/did you ask John (where)j Bill bought (ti) (tj)?', 'The CURRENT-FOCUS slot is not restricted to nodes that represent nouns.', 'Some of the generators are adverbial or adjectival parts of speech (pos).', 'An absorber checks for agreement in POS before it can accept the FLOAT-OBJECT as its subparse.', 'As an example, the question, ""(How oily)/do you like your salad dressing (ti)?"" contains a [q-subject] ""how oily"" that is an adjective.', 'The absorber [pred-adjective] accepts the available float-object as its subparse, but only after confirming that POS is ADJECTIVE.']",1,"[""The example used to illustrate the power of ATNs ( #AUTHOR_TAG ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator ."", 'The wrong FLOAT-OBJECT is available at the position of the first trace, and the parse dies: *(Which books)/did you ask John (where)j Bill bought (ti) (tj)?', 'The CURRENT-FOCUS slot is not restricted to nodes that represent nouns.', 'Some of the generators are adverbial or adjectival parts of speech (pos).', 'As an example, the question, ""(How oily)/do you like your salad dressing (ti)?"" contains a [q-subject] ""how oily"" that is an adjective.']"
CC876,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the use of a semantic network in speech dialoguequot,['G Th Niedermair'],introduction,,"Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , #AUTHOR_TAG , Niemann ( 1990 ) , and Young ( 1989 ) .","['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , #AUTHOR_TAG , Niemann ( 1990 ) , and Young ( 1989 ) .']",0,"['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , #AUTHOR_TAG , Niemann ( 1990 ) , and Young ( 1989 ) .']"
CC877,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,benchmark tests for darpa resource management database performance evaluationsquot,['D Pallett'],,"A nominally 1000-word resource management database for continuous speech recognition was developed for use in the DARPA Speech Research Program. This database has now been used at several sites for benchmark tests, and the database is expected to be made available to a wider community in the near future. The author documents the structure of the benchmark tests, including the selection of test material and details of studies of scoring algorithms.>",The second version ( RM ) concerns the Resource Management task ( #AUTHOR_TAG ) that has been popular within the DARPA community in recent years .,"['To date, four distinct domain-specific versions of TINA have been implemented.', 'The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al. 1986).', 'The second version ( RM ) concerns the Resource Management task ( #AUTHOR_TAG ) that has been popular within the DARPA community in recent years .', 'The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990).', 'The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community.']",5,['The second version ( RM ) concerns the Resource Management task ( #AUTHOR_TAG ) that has been popular within the DARPA community in recent years .']
CC878,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,a formal basis for the heuristic determination of minimum cost pathsquot,"['P Hart', 'N J Nilsson', 'B Raphael']",,"Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies.","For the A * algorithm ( #AUTHOR_TAG ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion .","['Some modification of this scheme is necessary when the input stream is not deterministic.', 'For the A * algorithm ( #AUTHOR_TAG ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion .', 'Unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.', 'With a deterministic word sequence it seems reasonable to assume probability 1.0 for what has been found.']",5,"['For the A * algorithm ( #AUTHOR_TAG ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion .', 'Unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.', 'With a deterministic word sequence it seems reasonable to assume probability 1.0 for what has been found.']"
CC879,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the collection and preliminary analysis of a spontaneous speech databasequot darpa speech and natural language workshop,"['V Zue', 'N Daly', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff', 'M Soclof']",,,"The recognizer for these systems is the SUMMIT system ( #AUTHOR_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .","['At present, we have available at MIT two systems, VOYAGER and ATIS, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input.', 'In both of these systems, TINA provides the interface between the recognizer and the application back-end.', 'In this section, I will describe our current interfaces between TINA and the recognizer and our future plans in this area.', 'In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.', 'This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'The recognizer for these systems is the SUMMIT system ( #AUTHOR_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .', 'The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'The search algorithm is the standard Viterbi search (Viterbi 1967), except that the match involves a network-to-network alignment problem rather than sequence-to-sequence.']",5,"['The recognizer for these systems is the SUMMIT system ( #AUTHOR_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .']"
CC880,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,transition network grammars for natural language analysisquot,['W A Woods'],,"The use of augmented transition network grammars for the analysis of natural language sentences is described. Structure-building actions associated with the arcs of the grammar network allow for the reordering, restructuring, and copying of constituents necessary to produce deep-structure representations of the type normally obtained from a transformational analysis, and conditions on the arcs allow for a powerful selectivity which can rule out meaningless analyses and take advantage of semantic information to guide the parsing. The advantages of this model for natural language analysis are discussed in detail and illustrated by examples. An implementation of an experimental parsing system for transition network grammars is briefly described.","The gap mechanism resembles the Hold register idea of ATNs ( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff . ), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes.","['This section describes how TINA handles several issues that are often considered to be part of the task of a parser.', 'These include agreement constraints, semantic restrictions, subject-tagging for verbs, and long distance movement (often referred to as gaps, or the trace, as in ""(which article)/do you think I should read (ti)?"") (Chomsky 1977).', 'The gap mechanism resembles the Hold register idea of ATNs ( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff . ), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes.']",1,"['This section describes how TINA handles several issues that are often considered to be part of the task of a parser.', 'The gap mechanism resembles the Hold register idea of ATNs ( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff . ), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes.']"
CC881,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the minds system using context and dialog to enhance speech recognitionquot,['S R Young'],introduction,,"Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .","['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']",0,"['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']"
CC882,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,integration of speech recognition and natural language processing in the mit voyager systemquot,"['V Zue', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff']",,"The MIT VOYAGER speech understanding system is an urban exploration and navigation system that interacts with the user through spoken dialogue, text, and graphics. The authors describe recent attempts at improving the integration between the speech recognition and natural language components. They used the generation capability of the natural language component to produce a word-pair language model to constrain the recognizer's search space, thus improving the coverage of the overall system. They also implemented a strategy in which the recognizer generates the top N word strings and passes them along to the natural language component for filtering. Results on performance evaluation are presented.>","We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) .","['When we first integrated this recognizer with TINA, we used a ""wire"" connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing.', 'A simple word-pair grammar constrained the search space.', 'If the parse failed, then the sentence was rejected.', ""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) ."", 'Both the A* and the Viterbi search are left-to-right search algorithms.', 'However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time.', 'That is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold.']",5,"[""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) .""]"
CC883,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,integration of speech recognition and natural language processing in the mit voyager systemquot,"['V Zue', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff']",,"The MIT VOYAGER speech understanding system is an urban exploration and navigation system that interacts with the user through spoken dialogue, text, and graphics. The authors describe recent attempts at improving the integration between the speech recognition and natural language components. They used the generation capability of the natural language component to produce a word-pair language model to constrain the recognizer's search space, thus improving the coverage of the overall system. They also implemented a strategy in which the recognizer generates the top N word strings and passes them along to the natural language component for filtering. Results on performance evaluation are presented.>","Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .","['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (Goodine et al. 1991)."", ""Ultimately we want to incorporate TINA'Sprobabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.""]",5,"['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (Goodine et al. 1991)."", ""Ultimately we want to incorporate TINA'Sprobabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.""]"
CC884,J97-4003,On Expressing Lexical Generalizations in HPSG,towards a semantics for lexical rules as used in hpsg,['Detmar Meurers'],introduction,,The formalization of DLRs provided by #AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 .,"['One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided by #AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 .', 'This description can then be given the standard set-theoretical interpretation of King (1989, 1994).']",0,"['One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided by #AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 .']"
CC885,J97-4003,On Expressing Lexical Generalizations in HPSG,offline constraint propagation for efficient hpsg processing,"['Detmar Meurers', 'Guido Minnen']",introduction,We investigate the use of a technique developed in the constraint programming community called constraint propagation to automatically make a HPSG theory more specific at those places where linguistically motivated underspecification would lead to inefficient processing. We discuss two concrete HPSG examples showing how off-line constraint propagation helps improve processing efficiency.,The reader is referred to #AUTHOR_TAG for a more detailed discussion of our use of constraint propagation.,"['The most specific generalization does not necessarily provide additional constrain- ing information.', 'However, usually it is the case that lexical entries resulting from lexical rule application differ in very few specifications compared to the number of specifica- tions in a base lexical entry.', 'Most of the specifications of a lexical entry are assumed to be passed unchanged via the automatically generated frame specification.', 'Therefore, after lifting the common information into the extended lexical entry, the out-argument in many cases contains enough information to permit a postponed execution of the interaction predicate.', 'When C is the common information, and D1, ..., Dk are the definitions of the interaction predicate called, we use distributivity to factor out C in (C A D1) V -.. V (C A Dk): We compute C A (D1 V ... V Dk), where the r) are assumed to contain no further common factors.', 'Once we have computed c, we use it to make the extended lexical entry more specific.', 'This technique closely resembles the off-line constraint propagation technique described by Marriott, Naish, and Lassez (1988).', 'The reader is referred to #AUTHOR_TAG for a more detailed discussion of our use of constraint propagation.']",0,"['This technique closely resembles the off-line constraint propagation technique described by Marriott, Naish, and Lassez (1988).', 'The reader is referred to #AUTHOR_TAG for a more detailed discussion of our use of constraint propagation.']"
CC886,J97-4003,On Expressing Lexical Generalizations in HPSG,the compleat lkb,['Ann Copestake'],related work,,"This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG , 31 ) .","['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']",1,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG , 31 ) .']"
CC887,J97-4003,On Expressing Lexical Generalizations in HPSG,controlling the application of lexical rules,"['Ted Briscoe', 'Ann Copestake']",introduction,"In this paper, we describe an item-familiarity account of the semi-productivity of morphological and lexical rules, and illustrate how it can be applied to practical issues which arise when building large scale lexical knowledge bases which utilize lexical rules. Our approach assumes that attested uses of derived words and senses are explicitly recorded, but that productive use of lexical rules is also possible, though controlled by probabilities associated with rule application. We discuss how the necessary probabilities and estimates of lexical rule productivity may be acquired from corpora.","27 #AUTHOR_TAG argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry .","['The way these predicates interconnect is represented in Figure 19.', '27 #AUTHOR_TAG argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry .', '28 In order to distinguish the different interaction predicates for the different classes of lexical entries, the compiler indexes the names of the interaction predicates.', 'Since for expository reasons we will only discuss one kind of lexical entry in this paper, we will not show those indices in the examples given.']",0,"['27 #AUTHOR_TAG argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry .', 'Since for expository reasons we will only discuss one kind of lexical entry in this paper, we will not show those indices in the examples given.']"
CC888,J97-4003,On Expressing Lexical Generalizations in HPSG,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan Sag']",introduction,,"/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements.","['• The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', '/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']",0,"['* The authors are listed alphabetically.', '/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']"
CC889,J97-4003,On Expressing Lexical Generalizations in HPSG,the representation of lexical semantic information cognitive science research paper csrp 280,['Ann Copestake'],introduction,,"This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #AUTHOR_TAG ) .","['This disjunction thus constitutes the base lexicon.', 'The disjuncts in the constraint on derived-word, on the other hand, encode the lexical rules.', 'The in-specification of a lexical rule specifies the IN feature, the out-specification, the derived word itself.', 'Note that the value of the IN feature is of type word and thus also has to satisfy either a base lexical entry or an out-specification of a lexical rule.', 'While this introduces the recursion necessary to permit successive lexical rule application, it also grounds the recursion in a word described by a base lexical entry.', 'Contrary to the MLR setup, the DLR formalization therefore requires all words feeding lexical rules to be grammatical with respect to the theory.', 'Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata.', 'This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #AUTHOR_TAG ) .', 'Both the input and output of a lexical rule, i.e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process.', 'As a result, in addition to the information present in the lexical entry, syntactic information can be accessed to execute the constraints on the input of a lexical rule.', 'The computational treatment of lexical rules that we propose in this paper is essentially a domain-specific refinement of such an approach to lexical rules.', '9']",0,"['This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #AUTHOR_TAG ) .', 'The computational treatment of lexical rules that we propose in this paper is essentially a domain-specific refinement of such an approach to lexical rules.']"
CC890,J97-4003,On Expressing Lexical Generalizations in HPSG,transformations of logic programs foundations and techniques,"['Alberto Pettorossi', 'Maurizio Proietti']",introduction,,"As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG ) .","['The automata resulting from word class specialization group the lexical entries into natural classes.', 'In case the automata corresponding to two lexical entries are identical, the entries belong to the same natural class.', 'However, each lexical rule application, i.e., each transition in an automaton, calls a frame predicate that can have a large number of defining clauses.', 'Intuitively understood, each defining clause of a frame predicate corresponds to a subclass of the class of lexical entries to which a lexical rule can be applied.', 'During word class specialization, though, when the finite-state automaton representing global lexical rule application is pruned with respect to a particular base lexical entry, we know which subclass we are dealing with.', 'For each interaction definition we can therefore check which of the flame clauses are applicable and discard the non-applicable ones.', 'We thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates.', 'The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques (Tamaki and Sato 1984).', '29 The unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', 'Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.', 'As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG ) .', 'Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section.', 'To eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates.', '3° The successive unfolding steps are schematically represented in Figure 20.']",1,"['We thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates.', 'The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques (Tamaki and Sato 1984).', '29 The unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', 'As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG ) .', 'To eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates.']"
CC891,J97-4003,On Expressing Lexical Generalizations in HPSG,modularizing contexted constraints,['John Griffith'],introduction,"This paper describes a method for compiling a constraint-based grammar into a potentially more efficient form for processing. This method takes dependent disjunctions within a constraint formula and factors them into non-interacting groups whenever possible by determining their independence. When a group of dependent disjunctions is split into smaller groups, an exponential amount of redundant information is reduced. At runtime, this means that an exponential amount of processing can be saved as well. Since the performance of an algorithm for processing constraints with dependent disjunctions is highly determined by its input, the transformation presented in this paper should prove beneficial for all such algorithms. 1 Introduction  There are two facts that conspire to make the treatment of disjunction an important consideration when building a natural language processing (NLP) system. The first fact is that natural languages are full of ambiguities, and in a grammar many of these ambi..",32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( Maxwell and Kaplan 1989 ; Eisele and Dorre 1990 ; #AUTHOR_TAG ) can be used to circumvent constraint propagation .,"['32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( Maxwell and Kaplan 1989 ; Eisele and Dorre 1990 ; #AUTHOR_TAG ) can be used to circumvent constraint propagation .', 'Encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup.', 'For analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficient processing.']",0,['32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( Maxwell and Kaplan 1989 ; Eisele and Dorre 1990 ; #AUTHOR_TAG ) can be used to circumvent constraint propagation .']
CC892,J97-4003,On Expressing Lexical Generalizations in HPSG,towards a semantics for lexical rules as used in hpsg,['Detmar Meurers'],introduction,,"12 In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG .","['The translation of the lexical rule into a predicate is trivial.', 'The result is displayed description language.', '12 In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG .', 'The reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 A more detailed presentation can be found in Minnen (in preparation).', '14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15 Hinrichs and Nakazawa (1996) show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention.', 'We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994).', 'Computationally, a subsumption test could equally well be used in our compiler.']",0,"['12 In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG .', '14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.']"
CC893,J97-4003,On Expressing Lexical Generalizations in HPSG,partialvp and splitnp topicalization in german an hpsg analysis,"['Erhard Hinrichs', 'Tsuneko Nakazawa']",introduction,,"6 The Partial-VP Topicalization Lexical Rule proposed by #AUTHOR_TAG , 10 ) is a linguistic example .","['The terminology used in the literature varies.', 'Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world (Gerdemann and King 1994;Gerdemann 1995).', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6 The Partial-VP Topicalization Lexical Rule proposed by #AUTHOR_TAG , 10 ) is a linguistic example .', 'The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.']",0,"['6 The Partial-VP Topicalization Lexical Rule proposed by #AUTHOR_TAG , 10 ) is a linguistic example .']"
CC894,J97-4003,On Expressing Lexical Generalizations in HPSG,open and closed world types in nlp systems,['Dale Gerdemann'],introduction,,4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #AUTHOR_TAG ) .,"['The terminology used in the literature varies.', 'Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #AUTHOR_TAG ) .', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.', 'The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.']",0,"['The terminology used in the literature varies.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #AUTHOR_TAG ) .']"
CC895,J97-4003,On Expressing Lexical Generalizations in HPSG,on implementing an hpsg theory aspects of the logical architecture the formalization and the implementation of headdriven phrase structure grammars,['Detmar Meurers'],introduction,"The paper presents some aspects involved in the formalization and implementation of HPSG theories. As basis, the logical setups of Carpenter (1992) and King (1989, 1994) are briefly compared regarding their usefulness as basis for HPSGII (Pollard and Sag 1994). The possibilities for expressing HPSG theories in the HPSGII architecture and in various computational systems (ALE, Troll, CUF, and TFS) are discussed. Beside a formal characterization of the possibilities, the paper investigates the specific choices for constraints with certain linguistic motivations, i.e. the lexicon, structure licencing, and grammatical principles. An ALE implementation of a theory for German proposed by Hinrichs and Nakazawa (1994) is used as example and the ALE grammar is included in the appendix.","Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG ) .","['Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'This is so since the lexical rule in Figure 2 ""(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.""', '(Pollard and Sag [1994, 314], following Flickinger [1987]).', 'This idea of preserving properties can be considered an instance of the well-known frame problem in AI (McCarthy and Hayes 1969), and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule.', 'Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG ) .']",0,"['Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'This is so since the lexical rule in Figure 2 ""(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.""', 'Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG ) .']"
CC896,J97-4003,On Expressing Lexical Generalizations in HPSG,on implementing an hpsg theory aspects of the logical architecture the formalization and the implementation of headdriven phrase structure grammars,['Detmar Meurers'],introduction,"The paper presents some aspects involved in the formalization and implementation of HPSG theories. As basis, the logical setups of Carpenter (1992) and King (1989, 1994) are briefly compared regarding their usefulness as basis for HPSGII (Pollard and Sag 1994). The possibilities for expressing HPSG theories in the HPSGII architecture and in various computational systems (ALE, Troll, CUF, and TFS) are discussed. Beside a formal characterization of the possibilities, the paper investigates the specific choices for constraints with certain linguistic motivations, i.e. the lexicon, structure licencing, and grammatical principles. An ALE implementation of a theory for German proposed by Hinrichs and Nakazawa (1994) is used as example and the ALE grammar is included in the appendix.",As shown in #AUTHOR_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .,"['A computational treatment expanding out the lexicon cannot be used for the increasing number of HPSG analyses that propose lexical rules that would result in an infinite lexicon.', 'Most current HPSG analyses of Dutch, German, Italian, and French fall into that category.', '1 Furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run-time.', 'Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry.', 'This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned.', 'As shown in #AUTHOR_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .']",4,"['A computational treatment expanding out the lexicon cannot be used for the increasing number of HPSG analyses that propose lexical rules that would result in an infinite lexicon.', '1 Furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run-time.', 'Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry.', 'This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned.', 'As shown in #AUTHOR_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .']"
CC897,J97-4003,On Expressing Lexical Generalizations in HPSG,unfoldfold transformation of logic programs,"['Hisao Tamaki', 'Taisuke Sato']",introduction,,The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #AUTHOR_TAG ) .,"['The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #AUTHOR_TAG ) .', '29 The unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', 'Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.', 'As a result, the literal can be removed from the body of the clause.', 'Whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation.']",5,"['The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #AUTHOR_TAG ) .', 'Whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation.']"
CC898,J97-4003,On Expressing Lexical Generalizations in HPSG,statische programmtransformationen zur effizienten verarbeitung constraintbasierter grammatiken diplomarbeit,['Annette Opalka'],related work,,"In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994; #AUTHOR_TAG ; Sanfilippo 1995).","['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994; #AUTHOR_TAG ; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994; #AUTHOR_TAG ; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']"
CC899,J97-4003,On Expressing Lexical Generalizations in HPSG,the formalism and implementation of patr ii,"['Stuart Shieber', 'Hans Uszkoreit', 'Fernando Pereira', 'Jane Robinson', 'Mabry Tyson']",related work,,A similar method is included in PATR-II ( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) .,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']",1,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']"
CC900,J97-4003,On Expressing Lexical Generalizations in HPSG,the typed feature structure representation formalism,['Martin Emele'],related work,,A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; #AUTHOR_TAG ) .,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; #AUTHOR_TAG ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']",1,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; #AUTHOR_TAG ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']"
CC901,J97-4003,On Expressing Lexical Generalizations in HPSG,lexical rules in hpsg what are they,"['Mike Calcagno', 'Carl Pollard']",introduction,,"Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ; #AUTHOR_TAG ) and the .","['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ; #AUTHOR_TAG ) and the .', 'lexical rules (DLRs; Meurers 1995).', '5']",0,"['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ; #AUTHOR_TAG ) and the .', 'lexical rules (DLRs; Meurers 1995).']"
CC902,J97-4003,On Expressing Lexical Generalizations in HPSG,an expanded logical formalism for headdriven phrase structure grammar,['Paul King'],,"Though Pollard and Sag 1994] assumes that an unspeciied variant of the formal logic of Carpenter 1992] will provide a formalism for HPSG, a precise formulation of the envisaged formalism is not immediately obvious, primarily because a principal tenet of Carpenter 1992], that feature structures represent partial information, seems to connict with a principal tenet of Pollard and Sag 1994], that feature structures represent abstract linguistic entities. This has caused many HPSGians to be mistakenly concerned with partial-information speciic notions, such as subsumption, that are appropriate for the Carpenter 1992] logic but inappropriate for the formalism Pollard and Sag 1994] envisages. This paper hopes to allay this concern and the confusion it engenders by substituting King 1989] for Carpenter 1992] as the basis of the envisaged formalism. It demonstrates that the formal logic of King 1989] provides a formalism for HPSG that meets all Pollard and Sag 1994] asks of the envisaged formalism. It further shows that the most credible variant of the Carpenter 1992] logic consistent with the aims of Pollard and Sag 1994] is not only incompatible with the tenet that feature structures represent partial information, but also an instance of the King 1989] logic.",The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #AUTHOR_TAG ; Gotz and Meurers 1997a ) .,"['The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #AUTHOR_TAG ; Gotz and Meurers 1997a ) .', 'We tested the covariation approach with a complex grammar implementing an HPSG analysis covering the so-called aux-flip phenomenon, and partial-VP topicalization in the three clause types of German (Hinrichs, Meurers, and Nakazawa 1994).', 'This test grammar includes eight lexical rules; some serve syntactic purposes, like the Partial-VP Topicalization Lexical Rule, others are of morphological nature as, for example, an inflectional lexical rule that relates nonfinite verbs to their finite form.', 'Our compiler distinguished seven word classes.', 'Some nouns and most verbal lexical entries fed lexical rules, and a single base lexical entry resulted in up to 12 derivations.']",5,"['The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #AUTHOR_TAG ; Gotz and Meurers 1997a ) .', 'We tested the covariation approach with a complex grammar implementing an HPSG analysis covering the so-called aux-flip phenomenon, and partial-VP topicalization in the three clause types of German (Hinrichs, Meurers, and Nakazawa 1994).', 'This test grammar includes eight lexical rules; some serve syntactic purposes, like the Partial-VP Topicalization Lexical Rule, others are of morphological nature as, for example, an inflectional lexical rule that relates nonfinite verbs to their finite form.', 'Some nouns and most verbal lexical entries fed lexical rules, and a single base lexical entry resulted in up to 12 derivations.']"
CC903,J97-4003,On Expressing Lexical Generalizations in HPSG,interpreting lexical rules,['Mike Calcagno'],introduction,,"Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #AUTHOR_TAG ; Calcagno and Pollard 1995 ) and the .","['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #AUTHOR_TAG ; Calcagno and Pollard 1995 ) and the .', 'lexical rules (DLRs; Meurers 1995).', '5']",0,"['Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #AUTHOR_TAG ; Calcagno and Pollard 1995 ) and the .', 'lexical rules (DLRs; Meurers 1995).']"
CC904,J97-4003,On Expressing Lexical Generalizations in HPSG,flipped out aux in german,"['Erhard Hinrichs', 'Tsuneko Nakazawa']",introduction,,"/b4home.html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.","['• The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', '/b4home.html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']",0,"['* The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', '/b4home.html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']"
CC905,J97-4003,On Expressing Lexical Generalizations in HPSG,ale—the attribute logic engine users guide version 201,"['Bob Carpenter', 'Gerald Penn']",related work,"ale 3.0 is completely compatible with ale 2.0 grammars, and adds the following new features: * A semantic-head-driven generator, based on the algorithm presented in Shieber et al. (1990). The generator was adapted to the logic of typed feature structures by Octav Popescu in his Carnegie Mellon Master's Thesis, Popescu (1996). Octav also wrote most of the generation code for this release. Grammars can be compiled for parsing only, generation only, or both. Some glue-code is also available from the ale homepage, to parse and generate with different grammars through a unix pipe. * A source-level debugger with a graphical XEmacs interface. This debugger works only with SICStus Prolog 3.0.6 and higher. A debugger with reduced functionality will be made available to SWI Prolog users in a later release. This debugger builds on, and incorporates the functionality of the code for the SICStus source-level debugger, written by Per Mildner at Uppsala University. * a /1 atoms. There are now an infinite number of atoms (types with no appropriate features), implicitly declared in every signature. These atoms can be arbitrary Prolog terms, including unbound variables, and can be used wherever normal ale types can, e.g., f:(a p(3.7)). a /1 atoms are extensional as Prolog terms, i.e., are taken to be identical according to the Prolog predicate , ==/2. In particular, this means that ground atoms behave exactly as ale extensional types. * Optional edge subsumption checking. For completeness of parsing, one only needs to ensure that, for every pair of nodes in the chart, the most general feature structure spanning those nodes is stored in the chart. This can reduce the number of edges in many domains. * An autonomous intro/2 operator. Features can now be declared on their own in a separate part of the grammar. * Default specifications for types. These are NOT default types. If a type appears on the right-hand side of a sub/2 or intro/2 specification, but not on iii iv CONTENTS the left-hand side of one, ale will assume this type is maximal, i.e., assume the specification, T ype sub []. Similarly, if it occurs on a left-hand side, but not on a right-hand side, ale will assume the type is immediately subsumed by bot, the most general type. In both cases, ale will announce these assumptions during compilation. * Several bug corrections and more compile-time warning and error messages. * An SWI Prolog 2.9.7 ...","A common computational treatment of lexical rules adopted , for example , in the ALE system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .","['A common computational treatment of lexical rules adopted , for example , in the ALE system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .', 'While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.', 'We mentioned in Section 2.2 that eliminating lexical rules in a precompilation step makes it impossible to process lexical rules or lexical entries that impose constraints that can only be properly executed once information from syntactic processing is available.', 'A related problem is that for analyses resulting in infinite lexica, the number of lexical rule applications needs to be limited.', 'In the ALE system, for example, a depth bound can be specified for this purpose.', 'Finally, as shown in Section 6, using an expanded out lexicon can be less time and space efficient than using a lexicon encoding that makes computational use of generalizations over lexical information, as, for example, the covariation encoding.']",1,"['A common computational treatment of lexical rules adopted , for example , in the ALE system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .', 'While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.', 'A related problem is that for analyses resulting in infinite lexica, the number of lexical rule applications needs to be limited.', 'In the ALE system, for example, a depth bound can be specified for this purpose.', 'Finally, as shown in Section 6, using an expanded out lexicon can be less time and space efficient than using a lexicon encoding that makes computational use of generalizations over lexical information, as, for example, the covariation encoding.']"
CC906,J97-4003,On Expressing Lexical Generalizations in HPSG,a logical formalism for headdriven phrase structure grammar,['Paul King'],introduction,,"A logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #AUTHOR_TAG , 1994 ) .","['A logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #AUTHOR_TAG , 1994 ) .', 'The formal language of King allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the (token) identity of objects.', 'These atomic expressions can be combined using conjunction, disjunction, and negation.', 'The expressions are interpreted by a set-theoretical semantics.']",0,"['A logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #AUTHOR_TAG , 1994 ) .', 'The expressions are interpreted by a set-theoretical semantics.']"
CC907,J97-4003,On Expressing Lexical Generalizations in HPSG,prolog and natural language analysis csli lecture notes center for the study of language and information,"['Fernando Pereira', 'Stuart Shieber']",introduction,,"The unfolding transformation is also referred to as partial execution , for example , by #AUTHOR_TAG .","['The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( Tamaki and Sato 1984 ) .29', 'The unfolding transformation is also referred to as partial execution , for example , by #AUTHOR_TAG .', 'Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.', 'As a result, the literal can be removed from the body of the clause.', 'Whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation.']",0,"['The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( Tamaki and Sato 1984 ) .29', 'The unfolding transformation is also referred to as partial execution , for example , by #AUTHOR_TAG .', 'Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.', 'Whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation.']"
CC908,J97-4003,On Expressing Lexical Generalizations in HPSG,typed unification grammars,"['Martin Emele', 'Remi Zajac']",related work,"This paper defines unification based ID/LP grammars based on typed feature structures as nonterminals and proposes a variant of Earley's algorithm to decide whether a given input sentence is a member of the language generated by a particular typed unification ID/LP grammar. A solution to the problem of the nonlocal flow of information in unification ID/LP grammars as discussed in Seiffert (1991) is incorporated into the algorithm. At the same time, it tries to connect this technical work with linguistics by presenting an example of the problem resulting from HPSG approaches to linguistics (Hinrichs and Nakasawa 1994, Richter and Sailer 1995) and with computational linguistics by drawing connections from this approach to systems implementing HPSG, especially the TROLL system, Gerdemann et al. (forthcoming).Comment: paper (81 pages), appendix (17 pages, Prolog code), format: .ps   compressed and uuencode",A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( #AUTHOR_TAG ; Emele 1994 ) .,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( #AUTHOR_TAG ; Emele 1994 ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']",1,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( #AUTHOR_TAG ; Emele 1994 ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']"
CC909,J97-4003,On Expressing Lexical Generalizations in HPSG,a logical formalism for headdriven phrase structure grammar,['Paul King'],introduction,,"This description can then be given the standard set-theoretical interpretation of #AUTHOR_TAG , 1994 ) . '","['One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided by Meurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1.', ""This description can then be given the standard set-theoretical interpretation of #AUTHOR_TAG , 1994 ) . '"", '11 10 Note that the passivization lexical rule in Figure 2 is only intended to illustrate the mechanism.', 'We do not make the linguistic claim that passives should be analyzed using such a lexical rule.', 'For space reasons, the SYNSEM feature is abbreviated by its first letter.', 'The traditional (First I Rest) list notation is used, and the operator • stands for the append relation in the usual way.', '1l Manandhar (1995) proposes to unify these two steps by including an update operator in the The computational treatment we discuss in the rest of the paper follows this setup in that it automatically computes, for each lexical rule specification, the frames necessary to preserve the properties not changed by it.', '12 We will show that the detection and specification of frames and the use of program transformation to advance their integration into the lexicon encoding is one of the key ingredients of the covariation approach to HPSG lexical rules.']",0,"['One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided by Meurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1.', ""This description can then be given the standard set-theoretical interpretation of #AUTHOR_TAG , 1994 ) . '"", 'For space reasons, the SYNSEM feature is abbreviated by its first letter.', '12 We will show that the detection and specification of frames and the use of program transformation to advance their integration into the lexicon encoding is one of the key ingredients of the covariation approach to HPSG lexical rules.']"
CC910,J97-4003,On Expressing Lexical Generalizations in HPSG,hpsg lexicon without lexical rules,['Karel Oliva'],related work,"this paper, I shall try  (i) to show that ueglecting standtu:d insights of tile orgauization of lexicon is detrimental both to the linguistic adequacy and to the practical useful- hess of the lexicon,  (ii) to make a proposal of an alternative reconcil ing the needs of HPSG with the usual lexicographic practic","In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; #AUTHOR_TAG ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .","['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; #AUTHOR_TAG ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; #AUTHOR_TAG ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']"
CC911,J97-4003,On Expressing Lexical Generalizations in HPSG,the update operation in feature logic,['Suresh Manandhar'],introduction,,11 #AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language.,['11 #AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language.'],0,['11 #AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language.']
CC912,J97-4003,On Expressing Lexical Generalizations in HPSG,an overview of disjunctive constraint satisfaction,"['John Maxwell', 'Ronald Kaplan']",introduction,This paper presents a new algorithm for solving disjunctive systems of constraints. The algorithm determines whether a system is satisfiable and produces the models if the system is satisfiable. There are three main steps for determining whether or not the system is satisfiable: 1 ) turn the disjunctive system into an equi-satisfiable conjunctive system in polynomial time 2) convert the conjunctive system into canonical form using extensions of standard techniques #3) extract and solve a propositional 'disjunctive residue',32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #AUTHOR_TAG ; Eisele and Dorre 1990 ; Griffith 1996 ) can be used to circumvent constraint propagation .,"['32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #AUTHOR_TAG ; Eisele and Dorre 1990 ; Griffith 1996 ) can be used to circumvent constraint propagation .', 'Encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup.', 'For analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficient processing.']",0,['32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #AUTHOR_TAG ; Eisele and Dorre 1990 ; Griffith 1996 ) can be used to circumvent constraint propagation .']
CC913,J97-4003,On Expressing Lexical Generalizations in HPSG,the generative power of categorial grammars and headdriven phrase structure grammars with lexical rules,['Bob Carpenter'],related work,"In this paper, it is shown that the addition of simple and linguistically motivated forms of lexical rules to grammatical theories based on subcategorization lists, such as categorial grammars (CG) or head-driven phrase structure grammars (HPSG), results in a system that can generate all and only the recursively enumerable languages. The proof of this result is carried out by means of a reduction of generalized rewriting systems. Two restrictions are considered, each of which constrains the generative power of the resulting system to context-free languages.",The powerful mechanism of lexical rules ( #AUTHOR_TAG ) has been used in many natural language processing systems .,"['The powerful mechanism of lexical rules ( #AUTHOR_TAG ) has been used in many natural language processing systems .', 'In this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper.']",0,['The powerful mechanism of lexical rules ( #AUTHOR_TAG ) has been used in many natural language processing systems .']
CC914,J97-4003,On Expressing Lexical Generalizations in HPSG,some philosophical problems from the standpoint of artificial intelligence,"['John McCarthy', 'Patrick Hayes']",introduction,,"This idea of preserving properties can be considered an instance of the well-known frame problem in AT ( #AUTHOR_TAG ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .","['Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'This is so since the lexical rule in Figure 2 ""(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.""', '(Pollard and Sag [1994, 314], following Flickinger [1987]).', 'This idea of preserving properties can be considered an instance of the well-known frame problem in AT ( #AUTHOR_TAG ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .', 'Not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up (Meurers 1994).']",1,"['Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'This is so since the lexical rule in Figure 2 ""(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.""', '(Pollard and Sag [1994, 314], following Flickinger [1987]).', 'This idea of preserving properties can be considered an instance of the well-known frame problem in AT ( #AUTHOR_TAG ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .', 'Not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up (Meurers 1994).']"
CC915,J97-4003,On Expressing Lexical Generalizations in HPSG,the craft of prolog,"[""Richard O'Keefe""]",introduction,"Hacking your program is no substitute for understanding your problem. Prolog is different, but not that different. Elegance is not optional. These are the themes that unify Richard O'Keefe's very personal statement on how Prolog programs should be written. The emphasis in ""The Craft of Prolog"" is on using Prolog effectively. It presents a loose collection of topics that build on and elaborate concepts learning in a first course. These may be read in any order following the first chapter, ""Basic Topics in Prolog, "" which provides a basis for the rest of the material in the book.","Using an accumulator passing technique ( #AUTHOR_TAG ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .","['Encoding a finite-state automaton as definite relations is rather straightforward.', 'In fact, one can view the representations as notational variants of one another.', 'Each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause.', 'Using an accumulator passing technique ( #AUTHOR_TAG ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .', 'Because of the word class specialization step discussed in Section 3.3, the execution avoids trying out many lexical rule applications that are guaranteed to fail.']",5,"['Encoding a finite-state automaton as definite relations is rather straightforward.', 'In fact, one can view the representations as notational variants of one another.', 'Using an accumulator passing technique ( #AUTHOR_TAG ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .']"
CC916,J97-4003,On Expressing Lexical Generalizations in HPSG,applying lexical rules under subsumption,"['Erhard Hinrichs', 'Tsuneko Nakazawa']",introduction,"Lexical rules are used in constraint based grammar formalisms such as Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag 1994) to express generalizations among lexical entries. This paper discusses a number of lexical rules from recent HPSG analyses of German (Hinrichs and Nakazawa 1994) and shows that the grammar in some cases vastly overgenerates and in other cases introduces massive spurious structural ambiguity, if lexical rules apply under unification. Such problems of overgeneration or spurious ambiguity do not arise, if a lexical rule applies to a given lexical entry iff the lexical entry is subsumed by the left-hand side of the lexical rule. Finally, the paper discusses computational consequences of applying lexical rules under subsumption.",15 #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention .,"['The translation of the lexical rule into a predicate is trivial.', 'The result is displayed description language.', '12 In order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in Meurers (1995).', 'The reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 A more detailed presentation can be found in Minnen (in preparation).', '14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15 #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention .', 'We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994).', 'Computationally, a subsumption test could equally well be used in our compiler.']",0,"['14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15 #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention .']"
CC917,J97-4003,On Expressing Lexical Generalizations in HPSG,word formation in lexical type hierarchies a case study of baradjectives in german masters thesis,['Susanne Riehemann'],related work,,"In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; #AUTHOR_TAG ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .","['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; #AUTHOR_TAG ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; #AUTHOR_TAG ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']"
CC918,J97-4003,On Expressing Lexical Generalizations in HPSG,verb second by underspecification,['Annette Frank'],related work,,"In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; Oliva 1994 ; #AUTHOR_TAG ; Opalka 1995 ; Sanfilippo 1995 ) .","['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; Oliva 1994 ; #AUTHOR_TAG ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; Oliva 1994 ; #AUTHOR_TAG ; Opalka 1995 ; Sanfilippo 1995 ) .']"
CC919,J97-4003,On Expressing Lexical Generalizations in HPSG,an expanded logical formalism for headdriven phrase structure grammar,['Paul King'],introduction,"Though Pollard and Sag 1994] assumes that an unspeciied variant of the formal logic of Carpenter 1992] will provide a formalism for HPSG, a precise formulation of the envisaged formalism is not immediately obvious, primarily because a principal tenet of Carpenter 1992], that feature structures represent partial information, seems to connict with a principal tenet of Pollard and Sag 1994], that feature structures represent abstract linguistic entities. This has caused many HPSGians to be mistakenly concerned with partial-information speciic notions, such as subsumption, that are appropriate for the Carpenter 1992] logic but inappropriate for the formalism Pollard and Sag 1994] envisages. This paper hopes to allay this concern and the confusion it engenders by substituting King 1989] for Carpenter 1992] as the basis of the envisaged formalism. It demonstrates that the formal logic of King 1989] provides a formalism for HPSG that meets all Pollard and Sag 1994] asks of the envisaged formalism. It further shows that the most credible variant of the Carpenter 1992] logic consistent with the aims of Pollard and Sag 1994] is not only incompatible with the tenet that feature structures represent partial information, but also an instance of the King 1989] logic.",4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #AUTHOR_TAG ; Gerdemann 1995 ) .,"['The terminology used in the literature varies.', 'Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #AUTHOR_TAG ; Gerdemann 1995 ) .', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.', 'The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.']",0,"['The terminology used in the literature varies.', 'Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #AUTHOR_TAG ; Gerdemann 1995 ) .', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.', 'The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.']"
CC920,J97-4003,On Expressing Lexical Generalizations in HPSG,french clitic climbing without clitics or climbing,"['Philip Miller', 'Ivan Sag']",introduction,,"de URL : http://www.sfs.nphil.uni-tuebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( Hinrichs and Nakazawa 1989 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements .","['• The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', 'de URL : http://www.sfs.nphil.uni-tuebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( Hinrichs and Nakazawa 1989 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements .', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']",0,"['* The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', 'de URL : http://www.sfs.nphil.uni-tuebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( Hinrichs and Nakazawa 1989 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements .', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']"
CC921,J97-4003,On Expressing Lexical Generalizations in HPSG,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan Sag']",introduction,,"16 A linguistic example based on the signature given by #AUTHOR_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given by Pollard and Sag (1994, p. 360, fn. 20).","['16 A linguistic example based on the signature given by #AUTHOR_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given by Pollard and Sag (1994, p. 360, fn. 20).', 'In such a Predicative Lexical Rule (which we only note as an example and not as a linguistic proposal) the subtype of the head object undergoing the rule as well as the value of the features only appropriate for the subtypes of substantive either is lost or must be specified by a separate rule for each of the subtypes.']",0,"['16 A linguistic example based on the signature given by #AUTHOR_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given by Pollard and Sag (1994, p. 360, fn. 20).']"
CC922,J97-4003,On Expressing Lexical Generalizations in HPSG,towards a semantics for lexical rules as used in hpsg,['Detmar Meurers'],introduction,,"The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and #AUTHOR_TAG , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program .","['Definite relations are a convenient way of encoding the interaction of lexical rules, as they readily support various program transformations to improve the encoding: We show that the definite relations produced by the compiler can be refined by program transformation techniques to increase efficiency.', 'The resulting encoding allows the execution of lexical rules on-the-fly, i.e., coroutined with other constraints at some time after lexical lookup.', 'The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and #AUTHOR_TAG , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program .']",2,"['The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and #AUTHOR_TAG , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program .']"
CC923,J97-4003,On Expressing Lexical Generalizations in HPSG,passive without lexical rules in,['Andreas Kathol'],related work,,"In a number of proposals , lexical generalizations are captured using lexical underspecification ( #AUTHOR_TAG ; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; Sanfilippo 1995 ) .","['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( #AUTHOR_TAG ; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( #AUTHOR_TAG ; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; Sanfilippo 1995 ) .']"
CC924,J97-4003,On Expressing Lexical Generalizations in HPSG,lexical polymorphism and word disambiguation,['Antonio Sanfilippo'],related work,We present an approach to lexical ambiguity where regularities about sense/u~ge extensibillty are represented by underepecifying word entries through lexic~d polymorphism. Word diumbiguation is carried out using contextual information gathered during language processing to ground polymorphic lexical entries.,"In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; #AUTHOR_TAG ) .","['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; #AUTHOR_TAG ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; #AUTHOR_TAG ) .']"
CC925,J97-4003,On Expressing Lexical Generalizations in HPSG,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan Sag']",introduction,,"/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.","['• The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', '/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']",0,"['* The authors are listed alphabetically.', '/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']"
CC926,J97-4003,On Expressing Lexical Generalizations in HPSG,a computational treatment of hpsg lexical rules as covariation in lexical entries,"['Detmar Meurers', 'Guido Minnen']",introduction,We describe a compiler which translates a set of HPSG lexical rules and their interaction into definite relations used to constrain lexical entries. The compiler ensures automatic transfer of properties unchanged by a lexical rule. Thus an operational semantics for the full lexical rule mechanism as used in HPSG linguistics is provided. Program transformation techniques are used to advance the resulting encoding. The final output constitutes a computational counterpart of the linguistic generalizations captured by lexical rules and allows ``on the fly'' application.,"Based on the research results reported in #AUTHOR_TAG , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG .","['Based on the research results reported in #AUTHOR_TAG , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG .', 'We developed a compiler that takes as its input a set of lexical rules, deduces the nec- essary transfer of properties not changed by the individual lexical rules, and encodes the set of lexical rules and their interaction into definite relations constraining lexical entries.', 'Each lexical entry is automatically extended with a definite clause encoding of the lexical rule applications which the entry can undergo.', 'The definite clauses thereby introduce what we refer to as systematic covariationinlexicalentries.']",4,"['Based on the research results reported in #AUTHOR_TAG , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG .']"
CC927,J97-4003,On Expressing Lexical Generalizations in HPSG,the representation of lexical semantic information cognitive science research paper csrp 280,['Ann Copestake'],related work,,"This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .","['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']",1,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']"
CC928,J97-4003,On Expressing Lexical Generalizations in HPSG,on implementing an hpsg theory aspects of the logical architecture the formalization and the implementation of headdriven phrase structure grammars,['Detmar Meurers'],introduction,"The paper presents some aspects involved in the formalization and implementation of HPSG theories. As basis, the logical setups of Carpenter (1992) and King (1989, 1994) are briefly compared regarding their usefulness as basis for HPSGII (Pollard and Sag 1994). The possibilities for expressing HPSG theories in the HPSGII architecture and in various computational systems (ALE, Troll, CUF, and TFS) are discussed. Beside a formal characterization of the possibilities, the paper investigates the specific choices for constraints with certain linguistic motivations, i.e. the lexicon, structure licencing, and grammatical principles. An ALE implementation of a theory for German proposed by Hinrichs and Nakazawa (1994) is used as example and the ALE grammar is included in the appendix.","However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .","['To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value.', 'In the latter case, we can also take care of transferring the value of z.', 'However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .', 'Instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'This is accomplished by having each lexical rule predicate call a so-called framepredicate,which can have multiple defining clauses.', 'So for the lexical rule 1, the frame specification is taken care of by extending the predicate in Figure 6 with a call to a frame predicate, as shown in Figure 8.17']",4,"['To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value.', 'However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .', 'Instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'This is accomplished by having each lexical rule predicate call a so-called framepredicate,which can have multiple defining clauses.', 'So for the lexical rule 1, the frame specification is taken care of by extending the predicate in Figure 6 with a call to a frame predicate, as shown in Figure 8.17']"
CC929,J97-4003,On Expressing Lexical Generalizations in HPSG,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan Sag']",introduction,,"Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .","['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 ° The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'The index that the subject bore in the input is assigned to an optional prepositional complement in the output.']",0,"['Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .']"
CC930,J97-4003,On Expressing Lexical Generalizations in HPSG,featurebased inheritance networks for computational lexicons,"['Hans-Ulrich Krieger', 'John Nerbonne']",related work,"The virtues of viewing the lexicon as an inheritance network are its succinctness and its tendency to highlight significant clusters of linguistic properties. From its succinctness follow two practical advantages, namely its ease of maintenance and modification. In this paper we present a feature-based foundation for lexical inheritance. We argue that the feature-based foundation is both more economical and expressively more powerful than non-feature-based systems. It is more economical because it employs only mechanisms already assumed to be present elsewhere in the grammar (viz., in the feature system), and it is more expressive because feature systems are more expressive than other mechanisms used in expressing lexical inheritance (cf. DATR). The lexicon furthermore allows the use of default unification, based on the ideas of default unification, defined by Bouma. These claims are buttressed in sections sketching the opportunities for lexical description in feature-based lexicons in two central lexical topics, inflection and derivation. Briefly, we argue that the central notion of paradigm may be defined in feature structures, and that it may be more satisfactorily (in fact, immediately) linked to the syntactic information in this fashion. Our discussion of derivation is more programmatic; but here, too, we argue that feature structures of a suitably rich sort provide a foundation for the definition of lexical rules. We illustrate theoretical claims in application to German lexis. This work is currently under implementation in a natural language understanding effort (DISCO) at the German Artiffical Intelligence Center (Deutsches Forschungszentrum fur Kunstliche Intelligenz).","In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).","['Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']"
CC931,J97-4003,On Expressing Lexical Generalizations in HPSG,of csli lecture notes center for the study of language and information,"['Carl Pollard', 'Ivan Sag']",introduction,,"Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .","['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 ° The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'The index that the subject bore in the input is assigned to an optional prepositional complement in the output.']",1,"['Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .']"
CC932,J97-4003,On Expressing Lexical Generalizations in HPSG,towards a semantics for lexical rules as used in hpsg,['Detmar Meurers'],introduction,,"Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )","['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )']",0,"['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )']"
CC933,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,design challenges and misconceptions in named entity recognition,"['L Ratinov', 'D Roth']",related work,"We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.","#AUTHOR_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .","['The task of mention detection is closely related to Named Entity Recognition (NER).', 'Punyakanok and Roth (2001) thoroughly study phrase identification in sentences and propose three different general approaches.', 'They aim to learn several different local classifiers and combine them to optimally satisfy some global constraints.', 'Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases.', 'However, the phrases detected are not necessarily mentions that we need to discover.', '#AUTHOR_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .', 'NER can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e.g.', 'Hidden Markov Model (Rabiner, 1989) or Conditional Random Fields (Sarawagi and Cohen, 2004).', 'The typical BIO representation was introduced in Ramshaw and Marcus (1995); OC representations were introduced in Church (1988), while Finkel and Manning (2009) further study nested named entity recognition, which employs a tree structure as a representation of identifying named entities within other named entities.']",0,"['#AUTHOR_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .']"
CC934,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,understanding the value of features for coreference resolution,"['E Bengtson', 'D Roth']",,"In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques. These works often show that complex models improve over a weak pairwise baseline. However, less attention has been given to the importance of selecting strong features to support learning a coreference model.    This paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features. We show that this produces a state-of-the-art system that outperforms systems built with complex models. We suggest that our system can be used as a baseline for the development of more complex models -- which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features.","For this mention-pair coreference model Ï ( u , v ) , we use the same set of features used in #AUTHOR_TAG .","['Here, y u,v = 1 iff mentions u, v are directly linked.', 'Thus, we can construct a forest and the mentions in the same connected component (i.e., in the same tree) are co-referred.', 'For this mention-pair coreference model Ï\x86 ( u , v ) , we use the same set of features used in #AUTHOR_TAG .']",5,"['For this mention-pair coreference model Ï\x86 ( u , v ) , we use the same set of features used in #AUTHOR_TAG .']"
CC935,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,understanding the value of features for coreference resolution,"['E Bengtson', 'D Roth']",introduction,"In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques. These works often show that complex models improve over a weak pairwise baseline. However, less attention has been given to the importance of selecting strong features to support learning a coreference model.    This paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features. We show that this produces a state-of-the-art system that outperforms systems built with complex models. We suggest that our system can be used as a baseline for the development of more complex models -- which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features.","In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( #AUTHOR_TAG ) .","['Here, phrases in the brackets are mentions and the underlined simple phrases are mention heads.', 'Moreover, mention boundaries can be nested (the boundary of a mention is inside the boundary of another mention), but mention heads never overlap.', 'This property also simplifies the problem of mention head candidate generation.', 'In the example above, the first ""they"" refers to ""Multinational companies investing in China"" and the second ""They"" refers to ""Domestic manufacturers, who are also suffering"".', 'In both cases, the mention heads are sufficient to support the decisions: ""they"" refers to ""companies"", and ""They"" refers to ""manufacturers"".', 'In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( #AUTHOR_TAG ) .']",0,"['In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( #AUTHOR_TAG ) .']"
CC936,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a joint model for entity analysis coreference typing and linking,"['G Durrett', 'D Klein']",experiments,"We present a joint model of three core tasks in the entity analysis stack: coreference res-olution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia en-tities). Our model is formally a structured con-ditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.","For Berkeley system , we use the reported results from #AUTHOR_TAG .","['The latest scorer is version v8.01, but MUC, B , CEAF e and CoNLL average scores are not changed.', 'For evaluation on ACE-2004, we convert the system output and gold annotations into CoNLL format. 12', 'We do not provide results from Berkeley and HOTCoref on ACE-2004 dataset as they do not directly support ACE input.', 'Results for HOTCoref are slightly different from the results reported in Björkelund and Kuhn (2014).', 'For Berkeley system , we use the reported results from #AUTHOR_TAG .']",1,"['The latest scorer is version v8.01, but MUC, B , CEAF e and CoNLL average scores are not changed.', 'For evaluation on ACE-2004, we convert the system output and gold annotations into CoNLL format. 12', 'We do not provide results from Berkeley and HOTCoref on ACE-2004 dataset as they do not directly support ACE input.', 'Results for HOTCoref are slightly different from the results reported in Bjorkelund and Kuhn (2014).', 'For Berkeley system , we use the reported results from #AUTHOR_TAG .']"
CC937,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a joint model for entity analysis coreference typing and linking,"['G Durrett', 'D Klein']",experiments,"We present a joint model of three core tasks in the entity analysis stack: coreference res-olution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia en-tities). Our model is formally a structured con-ditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.","Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #AUTHOR_TAG ) and HOTCoref system ( Bj Â¨ orkelund and Kuhn , 2014 ) .","['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'dataset only has mention annotations.', 'Therefore, we preprocess Ontonote-5.0 to derive mention heads using Collins head rules (Collins, 1999) with gold constituency parsing information and gold named entity information.', 'The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec.', '3.1.1.', 'The nonoverlapping mention head assumption in Sec.', '3.1.1', 'can be verified empirically on both ACE-2004 and OntoNotes-5.0', 'datasets.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #AUTHOR_TAG ) and HOTCoref system ( Bj Â¨ orkelund and Kuhn , 2014 ) .', 'Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 .', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.', 'The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.', 'We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10', 'We use Gurobi v5.0.1 as our ILP solver.', '3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset.', 'Subscripts ( M , H ) indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.9, λ 1 = 0.25 and λ 2 = 0.2.', 'erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref.', 'We name our new proposed end-to-end coreference resolution system incorporating both the mention head candidate generation module and the joint framework as H-Joint-M.', 'Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005).', 'We use the average F1 scores (AVG) of these three metrics as the main metric for comparison.', 'We use the v7.0 scorer provided by CoNLL-2012 Shared Task 11 .', 'We also evaluate the mention detection performance based on precision, recall and F1 score.', 'As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads.']",1,"['Therefore, we preprocess Ontonote-5.0 to derive mention heads using Collins head rules (Collins, 1999) with gold constituency parsing information and gold named entity information.', 'The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'The nonoverlapping mention head assumption in Sec.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #AUTHOR_TAG ) and HOTCoref system ( Bj Â¨ orkelund and Kuhn , 2014 ) .', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.', '3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref.', 'Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005).', 'As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads.']"
CC938,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,understanding the value of features for coreference resolution,"['E Bengtson', 'D Roth']",experiments,"In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques. These works often show that complex models improve over a weak pairwise baseline. However, less attention has been given to the importance of selecting strong features to support learning a coreference model.    This paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features. We show that this produces a state-of-the-art system that outperforms systems built with complex models. We suggest that our system can be used as a baseline for the development of more complex models -- which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features.","We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ) .","['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ) .', 'The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'We report results on the test documents for both datasets.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3.']",5,"['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ) .', 'The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'We report results on the test documents for both datasets.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Parameters of our proposed system are tuned as a = 0.9, b = 0.8, l 1 = 0.2 and l 2 = 0.3.']"
CC939,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,headdriven statistical models for natural language parsing,['M Collins'],,"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.","We then use Illinois Chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules ( #AUTHOR_TAG ) to identify their heads .","['Most of the head mentions proposed by the algorithms described in Sec. 3 are positive examples.', 'We ensure a balanced training of the mention head detection model by adding sub-sampled invalid mention head candidates as negative examples.', 'Specifically, after mention head candidate generation (described in Sec.', '3), we train on a set of candidates with precision larger than 50%.', 'We then use Illinois Chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules ( #AUTHOR_TAG ) to identify their heads .', 'When these extracted heads do not overlap with gold mention heads, we treat them as negative examples.']",5,"['Specifically, after mention head candidate generation (described in Sec.', 'We then use Illinois Chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules ( #AUTHOR_TAG ) to identify their heads .']"
CC940,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a constrained latent variable model for coreference resolution,"['K-W Chang', 'R Samdani', 'D Roth']",introduction,"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.","Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj Â¨ orkelund and Kuhn , 2014 ) .","['Mention detection is rarely studied as a stand-alone research problem (Recasens et al. (2013) is one key exception).', 'Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj Â¨ orkelund and Kuhn , 2014 ) .', 'However, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty.', 'Indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in Table 1.', 'Current state-of-the-art systems show a very significant drop in performance when running on system generated mentions.', 'These performance gaps are worrisome, since the real goal of NLP systems is to process raw data.', '1: Performance gaps between using gold mentions and predicted mentions for three state-of-the-art coreference resolution systems.', 'Performance gaps are always larger than 10%.', ""Illinois's system (Chang et al., 2013) is evaluated on CoNLL (2012CoNLL ( , 2011) Shared Task and ACE-2004 datasets."", 'It reports an average F1 score of MUC, B and CEAF e metrics using CoNLL v7.0 scorer.', ""Berkeley's system (Durrett and Klein, 2013) reports the same average score on the CoNLL-2011 Shared Task dataset."", ""Results of Stanford's system (Lee et al., 2011) are for B 3 metric on ACE-2004 dataset."", 'This paper focuses on improving end-to-end coreference performance.', 'We do this by: 1) Developing a new ILP-based joint learning and inference formulation for coreference and mention head detection.', '2) Developing a better mention head candidate generation algorithm.', 'Importantly, we focus on heads rather than mention boundaries since those can be identified more robustly and used effectively in an end-to-end system.', 'As we show, this results in a dramatic improvement in the quality of the MD component and, consequently, a significant reduction in the performance gap between coreference on gold mentions and coreference on raw data.']",0,"['Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj Â¨ orkelund and Kuhn , 2014 ) .']"
CC941,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,design challenges and misconceptions in named entity recognition,"['L Ratinov', 'D Roth']",,"We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.","Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in #AUTHOR_TAG .","['Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in #AUTHOR_TAG .', 'The BILOUrepresentation suggests learning classifiers that identify the Beginning, Inside and Last tokens of multi-token chunks as well as Unit-length chunks.', 'The problem is then transformed into a simple, but constrained, 5-class classification problem.']",4,"['Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in #AUTHOR_TAG .', 'The BILOUrepresentation suggests learning classifiers that identify the Beginning, Inside and Last tokens of multi-token chunks as well as Unit-length chunks.']"
CC942,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,conll2012 shared task modeling multilingual unrestricted coreference in ontonotes,"['S Pradhan', 'A Moschitti', 'N Xue', 'O Uryupina', 'Y Zhang']",experiments,"The CoNLL-2012 shared task involved predicting coreference in three languages -- English, Chinese and Arabic -- using OntoNotes data. It was a follow-on to the English-only task organized in 2011. Until the creation of the OntoNotes corpus, resources in this subfield of language processing have tended to be limited to noun phrase coreference, often on a restricted set of entities, such as ACE entities. OntoNotes provides a large-scale corpus of general anaphoric coreference not restricted to noun phrases or to a specified set of entity types and covering multiple languages. OntoNotes also provides additional layers of integrated annotation, capturing additional shallow semantic structure. This paper briefly describes the OntoNotes annotation (coreference and other layers) and then describes the parameters of the shared task including the format, pre-processing information, evaluation criteria, and presents and discusses the results achieved by the participating systems. Being a task that has a complex evaluation history, and multiple evalation conditions, it has, in the past, been difficult to judge the improvement in new algorithms over previously reported results. Having a standard test set and evaluation parameters, all based on a resource that provides multiple integrated annotation layers (parses, semantic roles, word senses, named entities and coreference) that could support joint models, should help to energize ongoing research in the task of entity and event coreference.","The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #AUTHOR_TAG ) , contains 3,145 annotated documents .","['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007;Bengtson and Roth, 2008).', 'The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #AUTHOR_TAG ) , contains 3,145 annotated documents .', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'We report results on the test documents for both datasets.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3.']",5,"['The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #AUTHOR_TAG ) , contains 3,145 annotated documents .', 'For gold mentions and mention heads, they yield the same performance for coreference.']"
CC943,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a constrained latent variable model for coreference resolution,"['K-W Chang', 'R Samdani', 'D Roth']",experiments,"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.","Developed Systems Our developed system is built on the work by #AUTHOR_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 .","['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'dataset only has mention annotations.', 'Therefore, we preprocess Ontonote-5.0 to derive mention heads using Collins head rules (Collins, 1999) with gold constituency parsing information and gold named entity information.', 'The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec.', '3.1.1.', 'The nonoverlapping mention head assumption in Sec.', '3.1.1', 'can be verified empirically on both ACE-2004 and OntoNotes-5.0', 'datasets.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Björkelund and Kuhn, 2014).', 'Developed Systems Our developed system is built on the work by #AUTHOR_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 .', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.', 'The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.', 'We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10', 'We use Gurobi v5.0.1 as our ILP solver.', '3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset.', 'Subscripts ( M , H ) indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.9, λ 1 = 0.25 and λ 2 = 0.2.', 'erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref.', 'We name our new proposed end-to-end coreference resolution system incorporating both the mention head candidate generation module and the joint framework as H-Joint-M.', 'Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005).', 'We use the average F1 scores (AVG) of these three metrics as the main metric for comparison.', 'We use the v7.0 scorer provided by CoNLL-2012 Shared Task 11 .', 'We also evaluate the mention detection performance based on precision, recall and F1 score.', 'As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads.']",5,"['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'dataset only has mention annotations.', 'The nonoverlapping mention head assumption in Sec.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bjorkelund and Kuhn, 2014).', 'Developed Systems Our developed system is built on the work by #AUTHOR_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 .', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.', '3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref.', 'We name our new proposed end-to-end coreference resolution system incorporating both the mention head candidate generation module and the joint framework as H-Joint-M.', 'Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005).', 'As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads.']"
CC944,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a constrained latent variable model for coreference resolution,"['K-W Chang', 'R Samdani', 'D Roth']",related work,"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.","In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in #AUTHOR_TAG in our experiments .","['Coreference resolution has been extensively studied, with several state-of-the-art approaches addressing this task (Lee et al., 2011;Durrett and Klein, 2013;Björkelund and Kuhn, 2014;Song et al., 2012).', 'Many of the early rule-based systems like Hobbs (1978) and Lappin and Leass (1994) gained considerable popularity.', 'The early designs were easy to understand and the rules were designed manually.', 'Machine learning approaches were introduced in many works (Connolly et al., 1997;Ng and Cardie, 2002;Bengtson and Roth, 2008;Soon et al., 2001).', 'The introduction of ILP methods has influenced the coreference area too Denis and Baldridge, 2007).', 'In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in #AUTHOR_TAG in our experiments .']",5,"['Coreference resolution has been extensively studied, with several state-of-the-art approaches addressing this task (Lee et al., 2011;Durrett and Klein, 2013;Bjorkelund and Kuhn, 2014;Song et al., 2012).', 'The introduction of ILP methods has influenced the coreference area too Denis and Baldridge, 2007).', 'In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in #AUTHOR_TAG in our experiments .']"
CC945,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,firstorder probabilistic models for coreference resolution,"['A Culotta', 'M Wick', 'A McCallum']",experiments,"Traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases. In this paper, we propose a machine learning method that enables features over sets of noun phrases, resulting in a first-order probabilistic model for coreference. We outline a set of approximations that make this approach practical, and apply our method to the ACE coreference dataset, achieving a 45% error reduction over a comparable method that only considers features of pairs of noun phrases. This result demonstrates an example of how a firstorder logic representation can be incorporated into a probabilistic model and scaled efficiently.","We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ) .","['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ) .', 'The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'We report results on the test documents for both datasets.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3.']",5,"['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ) .', 'The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.']"
CC946,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a constrained latent variable model for coreference resolution,"['K-W Chang', 'R Samdani', 'D Roth']",,"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.",More details can be found in #AUTHOR_TAG et al. (2013).,"['More details can be found in #AUTHOR_TAG et al. (2013).', 'The difference here is that we also consider the validity of mention heads using �(u),�(m)']",0,['More details can be found in #AUTHOR_TAG et al. (2013).']
CC947,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a joint model for entity analysis coreference typing and linking,"['G Durrett', 'D Klein']",related work,"We present a joint model of three core tasks in the entity analysis stack: coreference res-olution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia en-tities). Our model is formally a structured con-ditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.",Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #AUTHOR_TAG consider joint coreference and entity-linking .,"['Several recent works suggest studying coreference jointly with other tasks.', 'Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #AUTHOR_TAG consider joint coreference and entity-linking .', 'The work closest to ours is that of Lassalle and Denis (2015), which studies a joint anaphoricity detection and coreference resolution framework.', 'While their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different.']",0,['Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #AUTHOR_TAG consider joint coreference and entity-linking .']
CC948,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,headdriven statistical models for natural language parsing,['M Collins'],experiments,"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.","Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information .","['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'dataset only has mention annotations.', 'Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information .', 'The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec.', '3.1.1.', 'The nonoverlapping mention head assumption in Sec.', '3.1.1', 'can be verified empirically on both ACE-2004 and OntoNotes-5.0', 'datasets.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Björkelund and Kuhn, 2014).', 'Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 .', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.', 'The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.', 'We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10', 'We use Gurobi v5.0.1 as our ILP solver.', '3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset.', 'Subscripts ( M , H ) indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.9, λ 1 = 0.25 and λ 2 = 0.2.', 'erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref.', 'We name our new proposed end-to-end coreference resolution system incorporating both the mention head candidate generation module and the joint framework as H-Joint-M.', 'Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005).', 'We use the average F1 scores (AVG) of these three metrics as the main metric for comparison.', 'We use the v7.0 scorer provided by CoNLL-2012 Shared Task 11 .', 'We also evaluate the mention detection performance based on precision, recall and F1 score.', 'As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads.']",5,"['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information .', 'The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'The nonoverlapping mention head assumption in Sec.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bjorkelund and Kuhn, 2014).', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005).', 'As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads.']"
CC949,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a constrained latent variable model for coreference resolution,"['K-W Chang', 'R Samdani', 'D Roth']",,"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.",Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .,"['This section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .', 'The joint learning and inference model takes as input mention head candidates (Sec.', '3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions.', 'This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier.', 'The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones.', 'By learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from Recasens et al. (2013).', 'This joint framework aims to improve performance on both mention head detection and on coreference.']",5,"['This section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .', 'The joint learning and inference model takes as input mention head candidates (Sec.', '3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions.', 'This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier.', 'By learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from Recasens et al. (2013).', 'This joint framework aims to improve performance on both mention head detection and on coreference.']"
CC950,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,ontonotes the 90 solution,"['E Hovy', 'M Marcus', 'M Palmer', 'L Ramshaw', 'R Weischedel']",experiments,"We describe the OntoNotes methodology and its result, a large multilingual richly-annotated corpus constructed at 90% interannotator agreement. An initial portion (300K words of English newswire and 250K words of Chinese newswire) will be made available to the community during 2007.","We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .","['We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .', '(Hovy et al., 2006).', 'Our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets.']",5,"['We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .', '(Hovy et al., 2006).']"
CC951,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,weaklysupervised bayesian learning of a ccg supertagger,"['Dan Garrette', 'Chris Dyer', 'Jason Baldridge', 'Noah A Smith']",method,,We can define PCAT using a probabilistic grammar ( #AUTHOR_TAG ) .,"['For the root, binary, and unary parameters, we want to choose prior means that encode our bias toward cross-linguistically-plausible categories.', 'To formalize the notion of what it means for a category to be more ""plausible"", we extend the category generator of our previous work, which we will call P CAT .', 'We can define PCAT using a probabilistic grammar ( #AUTHOR_TAG ) .', 'The grammar may first generate a start or end category ( S , E ) with probability p se or a special tokendeletion category ( D ; explained in §5) with probability p del , or a standard CCG category C:']",0,"['We can define PCAT using a probabilistic grammar ( #AUTHOR_TAG ) .', 'The grammar may first generate a start or end category ( S , E ) with probability p se or a special tokendeletion category ( D ; explained in SS5) with probability p del , or a standard CCG category C:']"
CC952,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,a generative constituentcontext model for improved grammar induction,"['Dan Klein', 'Christopher D Manning']",related work,,#AUTHOR_TAG 's CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) .,"[""#AUTHOR_TAG 's CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) ."", 'They found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents.', 'While CCM is designed to learn which tag pairs make for likely contexts, without regard for the constituents themselves, our model attempts to learn the relationships between context categories and the types of the constituents, allowing us to take advantage of the natural a priori knowledge about which contexts fit with which constituent labels.']",0,"[""#AUTHOR_TAG 's CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) ."", 'They found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents.']"
CC953,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,bayesian inference for pcfgs via markov chain monte carlo,"['Mark Johnson', 'Thomas Griffiths', 'Sharon Goldwater']",introduction,"This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar.","In order to estimate the parameters of our model , we develop a blocked sampler based on that of #AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .","['In this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags.', 'In order to estimate the parameters of our model , we develop a blocked sampler based on that of #AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .', 'However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior.', 'Our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability-preferring priors on those parameters yields further gains in many scenarios.']",5,"['In order to estimate the parameters of our model , we develop a blocked sampler based on that of #AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .', 'However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior.']"
CC954,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,building a large annotated corpus of english the penn treebank,"['Mitchell P Marcus', 'Beatrice Santorini', 'Mary Ann Marcinkiewicz']",experiments,"Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant.","We evaluated on the English CCGBank ( Hockenmaier and Steedman , 2007 ) , which is a transformation of the Penn Treebank ( #AUTHOR_TAG ) ; the CTBCCG ( Tse and Curran , 2010 ) transformation of the Penn Chinese Treebank ( Xue et al. , 2005 ) ; and the CCG-TUT corpus ( Bos et al. , 2009 ) , built from the TUT corpus of Italian text ( Bosco et al. , 2000 ) .","['In our evaluation we compared our supertagcontext approach to (our reimplementation of) the best-performing model of our previous work (Garrette et al., 2015), which SCM extends.', 'We evaluated on the English CCGBank ( Hockenmaier and Steedman , 2007 ) , which is a transformation of the Penn Treebank ( #AUTHOR_TAG ) ; the CTBCCG ( Tse and Curran , 2010 ) transformation of the Penn Chinese Treebank ( Xue et al. , 2005 ) ; and the CCG-TUT corpus ( Bos et al. , 2009 ) , built from the TUT corpus of Italian text ( Bosco et al. , 2000 ) .']",5,"['We evaluated on the English CCGBank ( Hockenmaier and Steedman , 2007 ) , which is a transformation of the Penn Treebank ( #AUTHOR_TAG ) ; the CTBCCG ( Tse and Curran , 2010 ) transformation of the Penn Chinese Treebank ( Xue et al. , 2005 ) ; and the CCG-TUT corpus ( Bos et al. , 2009 ) , built from the TUT corpus of Italian text ( Bosco et al. , 2000 ) .']"
CC955,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,weaklysupervised bayesian learning of a ccg supertagger,"['Dan Garrette', 'Chris Dyer', 'Jason Baldridge', 'Noah A Smith']",method,,The right-side context of a non-terminal category -- the probability of generating a category to the right of the current constituent 's category -- corresponds directly to the category transitions used for the HMM supertagger of #AUTHOR_TAG .,"[""The right-side context of a non-terminal category -- the probability of generating a category to the right of the current constituent 's category -- corresponds directly to the category transitions used for the HMM supertagger of #AUTHOR_TAG ."", ""Thus, the right-side context prior mean θ RCTX-0 t can be biased in exactly the same way as the HMM supertagger's transitions: toward context supertags that connect to the constituent label.""]",1,"[""The right-side context of a non-terminal category -- the probability of generating a category to the right of the current constituent 's category -- corresponds directly to the category transitions used for the HMM supertagger of #AUTHOR_TAG ."", ""Thus, the right-side context prior mean th RCTX-0 t can be biased in exactly the same way as the HMM supertagger's transitions: toward context supertags that connect to the constituent label.""]"
CC956,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,weaklysupervised bayesian learning of a ccg supertagger,"['Dan Garrette', 'Chris Dyer', 'Jason Baldridge', 'Noah A Smith']",experiments,,We use the same splits as #AUTHOR_TAG .,"['Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set.', 'We use the same splits as #AUTHOR_TAG .', 'Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\X)/X rather than introducing special conjunction rules.', 'In order to increase the amount of raw data available to the sampler, we supplemented the English data with raw, unannotated newswire sentences from the NYT Giga- word 5 corpus (Parker et al., 2011) and supplemented Italian with the out-of-domain WaCky corpus (Baroni et al., 1999).', 'For English and Italian, this allowed us to use 100k raw tokens for training (Chinese uses 62k).', 'For Chinese and Italian, for training efficiency, we used only raw sentences that were 50 words or fewer (note that we did not drop tag dictionary set or test set sentences).']",5,"['Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set.', 'We use the same splits as #AUTHOR_TAG .', 'Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\X)/X rather than introducing special conjunction rules.']"
CC957,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,online learning of relaxed ccg grammars for parsing to logical form,"['Luke S Zettlemoyer', 'Michael Collins']",experiments,"We consider the problem of learning to parse sentences to lambda-calculus representations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG). A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar--for example allowing flexible word order, or insertion of lexical items-- with learned costs. We also present a new, online algorithm for inducing a weighted CCG. Results for the approach on ATIS data show 86% F-measure in recovering fully correct semantic analyses and 95.9% F-measure by a partial-match criterion, a more than 5% improvement over the 90.3% partial-match figure reported by He and Young (2006).","This is similar to the ""deletion"" strategy employed by #AUTHOR_TAG , but we do it directly in the grammar .","['When evaluating on test set sentences, if the model is unable to find a parse given the constraints of the tag dictionary, then we would have to take a score of zero for that sentence: every dependency would be ""wrong"".', 'Thus, it is important that we make a best effort to find a parse.', 'To accomplish this, we implemented a parsing backoff strategy.', 'The parser first tries to find a valid parse that has either s dcl or np at its root.', 'If that fails, then it searches for a parse with any root.', 'If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without).', 'This is similar to the ""deletion"" strategy employed by #AUTHOR_TAG , but we do it directly in the grammar .', 'We add unary rules of the form D →u for every potential supertag u in the tree.', 'Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t→ D , v and t→ v, D .', 'Recall that in §3.1, we stated that D is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary.', 'Additionally, since u will still remain as the preterminal, it will be the category examined as the context by adjacent constituents.']",1,"['When evaluating on test set sentences, if the model is unable to find a parse given the constraints of the tag dictionary, then we would have to take a score of zero for that sentence: every dependency would be ""wrong"".', 'Thus, it is important that we make a best effort to find a parse.', 'The parser first tries to find a valid parse that has either s dcl or np at its root.', 'If that fails, then it searches for a parse with any root.', 'If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without).', 'This is similar to the ""deletion"" strategy employed by #AUTHOR_TAG , but we do it directly in the grammar .', 'We add unary rules of the form D -u for every potential supertag u in the tree.', 'Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t- D , v and t- v, D .', 'Recall that in SS3.1, we stated that D is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary.']"
CC958,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,weaklysupervised bayesian learning of a ccg supertagger,"['Dan Garrette', 'Chris Dyer', 'Jason Baldridge', 'Noah A Smith']",introduction,,We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #AUTHOR_TAG ) .,"['Baldridge observed is that, cross-linguistically, grammars prefer simpler syntactic structures when possible, and that due to the natural correspondence of categories and syntactic structure, biasing toward simpler categories encourages simpler structures.', 'In previous work, we were able to incorporate this preference into a Bayesian parsing model, biasing PCFG productions toward sim-pler categories by encoding a notion of category simplicity into a prior (Garrette et al., 2015).', 'Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable.', 'We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #AUTHOR_TAG ) .']",2,['We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #AUTHOR_TAG ) .']
CC959,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,typesupervised hidden markov models for partofspeech tagging with incomplete tag dictionaries,"['Dan Garrette', 'Jason Baldridge']",method,"Past work on learning part-of-speech taggers from tag dictionaries and raw data has reported good results, but the assumptions made about those dictionaries are often unrealistic: due to historical precedents, they assume access to information about labels in the raw and test sets. Here, we demonstrate ways to learn hidden Markov model taggers from incomplete tag dictionaries. Taking the min-greedy algorithm (Ravi et al., 2010) as a starting point, we improve it with several intuitive heuristics. We also define a simple HMM emission initialization that takes advantage of the tag dictionary and raw data to capture both the openness of a given tag and its estimated prevalence in the raw data. Altogether, our augmentations produce improvements to performance over the original min-greedy algorithm for both English and Italian data.","We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ) .4","['We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ) .4', 'This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of raw-corpus occurrences of each word in the dictionary evenly across all of its associated tags.', 'These counts are then combined with estimates of the �openness� of each tag in order to assess its likelihood of appearing with new words.']",5,"['We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ) .4']"
CC960,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,a generative constituentcontext model for improved grammar induction,"['Dan Klein', 'Christopher D Manning']",method,,"Since we are not generating from the model , this does not introduce difficulties ( #AUTHOR_TAG ) .","['Like CCM, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid.', 'Since we are not generating from the model , this does not introduce difficulties ( #AUTHOR_TAG ) .']",4,"['Since we are not generating from the model , this does not introduce difficulties ( #AUTHOR_TAG ) .']"
CC961,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,widecoverage efficient statistical parsing with ccg and loglinear models,"['Stephen Clark', 'James R Curran']",,,We further add rules for combining with punctuation to the left and right and allow for the merge rule X â X X of #AUTHOR_TAG .,"['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'The direction of the slash operator gives the behavior of the function.', 'A category (s\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\) with a noun phrase (np) that serves as its subject.', 'We follow Lewis and Steedman (2014) in allowing a small set of generic, linguistically-plausible unary and binary grammar rules.', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X â\x86\x92 X X of #AUTHOR_TAG .']",5,"['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'The direction of the slash operator gives the behavior of the function.', 'We follow Lewis and Steedman (2014) in allowing a small set of generic, linguistically-plausible unary and binary grammar rules.', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X â\x86\x92 X X of #AUTHOR_TAG .']"
CC962,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,bayesian inference for pcfgs via markov chain monte carlo,"['Mark Johnson', 'Thomas Griffiths', 'Sharon Goldwater']",,"This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar.","To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by #AUTHOR_TAG that samples entire parse trees .","['To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by #AUTHOR_TAG that samples entire parse trees .', 'For a sentence w, the strategy is to use the Inside algorithm (Lari and Young, 1990) to inductively compute, for each potential non-terminal position spanning words w i through w j−1 and category t, going ""up"" the tree, the probability of generating w i , . . .', ', w j−1 via any arrangement of productions that is rooted by y ij = t.']",5,"['To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by #AUTHOR_TAG that samples entire parse trees .']"
CC963,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,bayesian inference for pcfgs via markov chain monte carlo,"['Mark Johnson', 'Thomas Griffiths', 'Sharon Goldwater']",,"This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar.",Our strategy is based on the approach presented by #AUTHOR_TAG .,"['We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences.', 'Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution.', 'Our strategy is based on the approach presented by #AUTHOR_TAG .', 'At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', 'To efficiently sample new model parameters, we exploit Dirichlet-multinomial conjugacy.', 'By repeating these alternating steps and accumu- lating the productions, we obtain an approximation of the required posterior quantities.']",5,"['We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences.', 'Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution.', 'Our strategy is based on the approach presented by #AUTHOR_TAG .', 'At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', 'To efficiently sample new model parameters, we exploit Dirichlet-multinomial conjugacy.', 'By repeating these alternating steps and accumu- lating the productions, we obtain an approximation of the required posterior quantities.']"
CC964,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,a generative constituentcontext model for improved grammar induction,"['Dan Klein', 'Christopher D Manning']",introduction,,"One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .","['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .', 'This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.', 'For example, the part-of-speech (POS) sequence ADJ NOUN frequently occurs between the tags DET and VERB.', 'This DET-VERB context also frequently applies to the single-word sequence NOUN and to ADJ ADJ NOUN.', 'From this, we might deduce that DET-VERB is a likely context for a noun phrase.', 'CCM is able to learn which POS contexts are likely, and does so via a probabilistic generative model, providing a statistical, data-driven take on substitutability.', 'However, since there is nothing intrinsic about the POS pair DET-VERB that indicates a priori that it is a likely constituent context, this fact must be inferred entirely from the data.', 'Baldridge (2008) observed that unlike opaque, atomic POS labels, the rich structures of Combinatory Categorial Grammar (CCG) (Steedman, 2000;Steedman and Baldridge, 2011) categories reflect universal grammatical properties.', 'CCG is a lexicalized grammar formalism in which every constituent in a sentence is associated with a structured category that specifies its syntactic relationship to other constituents.', 'For example, a category might encode that ""this constituent can combine with a noun phrase to the right (an object) and then a noun phrase to the left (a subject) to produce a sentence"" instead of simply VERB.', 'CCG has proven useful as a framework for grammar induction due to its ability to incorporate linguistic knowledge to guide parser learning by, for example, specifying rules in lexical-expansion algorithms (Bisk and Hockenmaier, 2012;2013) or encoding that information as priors within a Bayesian framework (Garrette et al., 2015).']",0,"['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .', 'CCG is a lexicalized grammar formalism in which every constituent in a sentence is associated with a structured category that specifies its syntactic relationship to other constituents.', 'CCG has proven useful as a framework for grammar induction due to its ability to incorporate linguistic knowledge to guide parser learning by, for example, specifying rules in lexical-expansion algorithms (Bisk and Hockenmaier, 2012;2013) or encoding that information as priors within a Bayesian framework (Garrette et al., 2015).']"
CC965,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,a ccg parsing with a supertagfactored model,"['Mike Lewis', 'Mark Steedman']",,,"We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .","['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'The direction of the slash operator gives the behavior of the function.', 'A category (s\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\) with a noun phrase (np) that serves as its subject.', 'We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X → X X of Clark and Curran (2007).']",5,"['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .']"
CC966,N01-1003,SPoT,discriminative reranking for natural language parsing,['Michael Collins'],,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #AUTHOR_TAG .","['Rankboost, like other machine learning programs of the boosting family, can handle a very large number of features.', 'Therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let Rank-Boost choose the relevant ones.', 'In total, we used 3,291 features in training the SPR.', 'Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #AUTHOR_TAG .', 'The motivation for the features was to capture declaratively decisions made by the randomized SPG.', 'We avoided features specific to particular text plans by discarding those that occurred fewer than 10 times.']",1,"['Rankboost, like other machine learning programs of the boosting family, can handle a very large number of features.', 'Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #AUTHOR_TAG .']"
CC967,N01-1003,SPoT,clause aggregation using linguistic knowledge,['James Shaw'],related work,"By combining multiple clauses into one single sentence, a text generation system can express the same amount of information in fewer words and at the same time, produce a great variety of complex constructions. In this paper, we describe hypotactic and paratactic operators for generating complex sentences from clause-sized semantic representations. These two types of operators are portable and reusable because they are based on general resources such as the lexicon and the grammar.",Previous work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG ) for a recent example with further references ) .,"['Previous work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG ) for a recent example with further references ) .', 'This approach is difficult to scale due to the nonrobustness of rules and unexpected interactions (Hovy and Wanner, 1996), and it is difficult to develop new applications quickly.', 'Presumably, this is the reason why dialog systems to date have not used this kind of sentence planning.']",0,"['Previous work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG ) for a recent example with further references ) .', 'This approach is difficult to scale due to the nonrobustness of rules and unexpected interactions (Hovy and Wanner, 1996), and it is difficult to develop new applications quickly.']"
CC968,N01-1003,SPoT,discriminative reranking for natural language parsing,['Michael Collins'],introduction,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","The SPR uses rules automatically learned from training data , using techniques similar to ( #AUTHOR_TAG ; Freund et al. , 1998 ) .","['Sentence planning is a set of inter-related but distinct tasks, one of which is sentence scoping, i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into sentences. 1', 'For example, consider the required capabilities of a sentence planner for a mixed-initiative spoken dialog system for travel planning: (D1) System1: Welcome.... What airport would you like to fly out of?', 'User2: I need to go to Dallas.', 'System3: Flying to Dallas.', 'What departure airport was that?', 'User4: from Newark on September the 1st.', 'System5: What time would you like to travel on September the 1st to Dallas from Newark?', ""Utterance System1 requests information about the caller's departure airport, but in User2, the caller takes the initiative to provide information about her destination."", ""In System3, the system's goal is to implicitly confirm the destination (because of the possibility of error in the speech recognition component), and request information (for the second time) of the caller's departure airport."", 'In User4, the caller provides this information but also provides the month and day of travel.', ""Given the system's dialog strategy, the communicative goals for its next turn are to implicitly confirm all the information that the user has provided so far, i.e. the departure and destination cities and the month and day information, as well as to request information about the time of travel."", ""The system's representation of its communicative goals for utterance System5 is in Figure 1."", 'The job of the sentence planner is to decide among the large number of potential realizations of these communicative goals.', 'Some example alternative realizations are in Figure 2. 2 implicit-confirm(orig-city:NEWARK) implicit-confirm(dest-city:DALLAS) implicit-confirm(month:9) implicit-confirm(day-number:1) request(depart-time) In this paper, we present SPoT, for ""Sentence Planner, Trainable"".', 'We also present a new methodology for automatically training SPoT on the basis of feedback provided by human judges.', 'In order to train SPoT, we reconceptualize its task as consisting of two distinct phases.', 'In the first phase, the sentence-plan-generator (SPG) generates a potentially large sample of possible sentence plans for a given text-plan input.', 'In the second phase, the sentence-plan-ranker (SPR) ranks the sample sentence plans, and then selects the top-ranked output to input to the surface realizer.', 'Our primary contribution is a method for training the SPR.', 'The SPR uses rules automatically learned from training data , using techniques similar to ( #AUTHOR_TAG ; Freund et al. , 1998 ) .']",1,"['The SPR uses rules automatically learned from training data , using techniques similar to ( #AUTHOR_TAG ; Freund et al. , 1998 ) .']"
CC969,N01-1003,SPoT,clause aggregation using linguistic knowledge,['James Shaw'],,"By combining multiple clauses into one single sentence, a text generation system can express the same amount of information in fewer words and at the same time, produce a great variety of complex constructions. In this paper, we describe hypotactic and paratactic operators for generating complex sentences from clause-sized semantic representations. These two types of operators are portable and reusable because they are based on general resources such as the lexicon and the grammar.","These operations are not domain-specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ) , although the various MERGE operations are , to our knowledge , novel in this form .","['As already mentioned, we divide the sentence planning task into two phases.', 'In the first phase, the sentenceplan-generator (SPG) generates 12-20 possible sentence plans for a given input text plan.', ""Each speech act is assigned a canonical lexico-structural representation (called a DSyntS -Deep Syntactic Structure (Mel'čuk, 1988))."", 'The sentence plan is a tree recording how these elementary DSyntS are combined into larger DSyntSs; the DSyntS for the entire input text plan is associated with the root node of the tree.', 'In the second phase, the sentence plan ranker (SPR) ranks sentence plans generated by the SPG, and then selects the top-ranked output as input to the surface realizer, RealPro (Lavoie and Rambow, 1997)  The research presented here is primarily concerned with creating a trainable SPR.', 'A strength of our approach is the ability to use a very simple SPG, as we explain below.', 'The basis of our SPG is a set of clausecombining operations that incrementally transform a list of elementary predicate-argument representations (the DSyntSs corresponding to elementary speech acts, in our case) into a single lexico-structural representation, by combining these representations using the following combining operations.', 'Examples can be found in Figure  ADJECTIVE.', 'This transforms a predicative use of an adjective into an adnominal construction.', 'PERIOD.', 'Joins two complete clauses with a period.', 'These operations are not domain-specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ) , although the various MERGE operations are , to our knowledge , novel in this form .']",0,"['The basis of our SPG is a set of clausecombining operations that incrementally transform a list of elementary predicate-argument representations (the DSyntSs corresponding to elementary speech acts, in our case) into a single lexico-structural representation, by combining these representations using the following combining operations.', 'These operations are not domain-specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ) , although the various MERGE operations are , to our knowledge , novel in this form .']"
CC970,N01-1003,SPoT,sentence planning as description using tree adjoining grammar,"['Matthew Stone', 'Christine Doran']",,"We present an algorithm for simultaneously constructing both the syntax and semantics of a sentence using a Lexicalized Tree Adjoining Grammar (LTAG). This approach captures naturally and elegantly the interaction between pragmatic and syntactic constraints on descriptions in a sentence, and the inferential interactions between multiple descriptions in a sentence. At the same time, it exploits linguistically motivated, declarative specifications of the discourse functions of syntactic constructions to make contextually appropriate syntactic choices.","The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .","['The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts  from the input text plan, and with its interior nodes labeled with clause-combining operations 3 .', 'Each node is also associated with a DSyntS: the leaves (which correspond to elementary speech acts from the input text plan) are linked to a canonical DSyntS for that speech act (by lookup in a hand-crafted dictionary).', 'The interior nodes are associated with DSyntSs by executing their clausecombing operation on their two daughter nodes.', '(A PE-RIOD node results in a DSyntS headed by a period and whose daughters are the two daughter DSyntSs.)', 'If a clause combination fails, the sp-tree is discarded (for example, if we try to create a relative clause of a structure which already contains a period).', 'As a result, the DSyntS for the entire turn is associated with the root node.', 'This DSyntS can be sent to RealPro, which returns a sentence (or several sentences, if the DSyntS contains period nodes).', 'The SPG is designed in such a way that if a DSyntS is associated with the root node, it is a valid structure which can be realized.', 'Figure 2 shows some of the realizations of alternative sentence plans generated by our SPG for utterance Sys- 3 The sp-tree is inspired by (Lavoie and Rambow, 1998).', 'The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .', '4 shows the result of applying the soft-merge operation when Args 1 and 2 are implicit confirmations of the origin and destination cities. Figure 8 illustrates the relationship between the sp-tree and the DSyntS for alternative 8.', 'The labels and arrows show the DSyntSs associated with each node in the sp-tree (in Figure 7), and the diagram also shows how structures are composed into larger structures by the clause combining operations.', 'The complexity of most sentence planners arises from the attempt to encode constraints on the application of, and ordering of, the operations, in order to generate a single high quality sentence plan.', 'In our approach, we do not need to encode such constraints.', 'Rather, we generate a random sample of possible sentence plans for each text plan, up to a pre-specified maximum number of sentence plans, by randomly selecting among the operations according to some probability distribution. 4']",0,"['The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts  from the input text plan, and with its interior nodes labeled with clause-combining operations 3 .', 'Each node is also associated with a DSyntS: the leaves (which correspond to elementary speech acts from the input text plan) are linked to a canonical DSyntS for that speech act (by lookup in a hand-crafted dictionary).', 'This DSyntS can be sent to RealPro, which returns a sentence (or several sentences, if the DSyntS contains period nodes).', 'Figure 2 shows some of the realizations of alternative sentence plans generated by our SPG for utterance Sys- 3 The sp-tree is inspired by (Lavoie and Rambow, 1998).', 'The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .', 'The complexity of most sentence planners arises from the attempt to encode constraints on the application of, and ordering of, the operations, in order to generate a single high quality sentence plan.']"
CC971,N01-1006,Transformation Based Learning in the Fast Lane,independence and commitment assumptions for rapid training and execution of rulebased pos taggers,['M Hepple'],method,"This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method.","2The algorithm was implemented by the the authors , following the description in #AUTHOR_TAG .","['2The algorithm was implemented by the the authors , following the description in #AUTHOR_TAG .']",5,"['2The algorithm was implemented by the the authors , following the description in #AUTHOR_TAG .']"
CC972,N01-1006,Transformation Based Learning in the Fast Lane,independence and commitment assumptions for rapid training and execution of rulebased pos taggers,['M Hepple'],,"This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method.","The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system ( #AUTHOR_TAG ) .","['The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system ( #AUTHOR_TAG ) .']",1,"['The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system ( #AUTHOR_TAG ) .']"
CC973,N01-1006,Transformation Based Learning in the Fast Lane,classifier combination for improved lexical disambiguation,"['E Brill', 'J Wu']",experiments,"One of the most exciting recent directions in machine learning is the discovery that the combination of multiple classifiers often results in significantly better performance than what can be achieved with a single classifier. In this paper, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary. Next, we show how this complementary behavior can be used to our advantage. By using contextual cues to guide tagger combination, we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers. Introduction Part of speech tagging has been a central problem in natural language processing for many years. Since the advent of manually tagged corpora such as the Brown Corpus and the Penn Treebank (Francis(1982), Marcus(1993)), the efficacy of machine learning for training a tagger has been demonstrated using a wide array of techniques, including: Markov models, decision trees..","The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by #AUTHOR_TAG .","['The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by #AUTHOR_TAG .']",5,"['The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by #AUTHOR_TAG .']"
CC974,N01-1006,Transformation Based Learning in the Fast Lane,independence and commitment assumptions for rapid training and execution of rulebased pos taggers,['M Hepple'],experiments,"This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method.","â¢ The regular TBL , as described in section 2 ; â¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â¢ The FastTBL algorithm ; â¢ The ICA algorithm ( #AUTHOR_TAG ) .","[""â\x80¢ The regular TBL , as described in section 2 ; â\x80¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â\x80¢ The FastTBL algorithm ; â\x80¢ The ICA algorithm ( #AUTHOR_TAG ) .""]",1,"[""â\x80¢ The regular TBL , as described in section 2 ; â\x80¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â\x80¢ The FastTBL algorithm ; â\x80¢ The ICA algorithm ( #AUTHOR_TAG ) .""]"
CC975,N01-1006,Transformation Based Learning in the Fast Lane,independence and commitment assumptions for rapid training and execution of rulebased pos taggers,['M Hepple'],,"This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method.",The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .,['The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .'],0,['The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .']
CC976,N01-1010,Tree-cut and a lexicon based on systematic polysemy,automatic extraction of systematic polysemy using treecut,['N Tomuro'],experiments,"This paper describes an automatic method for extracting systematic polysemy from a hierarchically organized semantic lexicon (WordNet). Systematic polysemy is a set of word senses that are related in systematic and predictable ways. Our method uses a modification of a tree generalization technique used in (Li and Abe, 1998), and generates a tree-cut, which is a list of clusters that partition a tree. We compare the systematic relations extracted by our automatic method to manually extracted WordNet cousins.","Using the tree-cut technique described above , our previous work ( #AUTHOR_TAG ) extracted systematic polysemy from WordNet .","['Using the tree-cut technique described above , our previous work ( #AUTHOR_TAG ) extracted systematic polysemy from WordNet .', 'In this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method.']",2,"['Using the tree-cut technique described above , our previous work ( #AUTHOR_TAG ) extracted systematic polysemy from WordNet .']"
CC977,N01-1010,Tree-cut and a lexicon based on systematic polysemy,automatic extraction of systematic polysemy using treecut,['N Tomuro'],introduction,"This paper describes an automatic method for extracting systematic polysemy from a hierarchically organized semantic lexicon (WordNet). Systematic polysemy is a set of word senses that are related in systematic and predictable ways. Our method uses a modification of a tree generalization technique used in (Li and Abe, 1998), and generates a tree-cut, which is a list of clusters that partition a tree. We compare the systematic relations extracted by our automatic method to manually extracted WordNet cousins.","In our previous work ( #AUTHOR_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .","['In this paper, we describes a lexicon organized around systematic polysemy.', 'The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998).', 'In our previous work ( #AUTHOR_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .', 'In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy.', 'We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996).', 'The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partitions in our lexicon yielded better values (Carletta, 1996) than arbitrary sense groupings on the agreement data.']",2,"['In this paper, we describes a lexicon organized around systematic polysemy.', 'The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998).', 'In our previous work ( #AUTHOR_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .', 'In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy.', 'The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partitions in our lexicon yielded better values (Carletta, 1996) than arbitrary sense groupings on the agreement data.']"
CC978,N01-1024,Knowledge-free induction of inflectional morphologies,knowledgefree induction of morphology using latent semantic analysis,"['P Schone', 'D Jurafsky']",method,"Morphology induction is a subproblem of important tasks like automatic learning of machine-readable dictionaries and grammar induction. Previous morphology induction approaches have relied solely on statistics of hypothesized stems and affixes to choose which affixes to consider legitimate. Relying on stem-and-affix statistics rather than semantic knowledge leads to a number of problems, such as the inappropriate use of valid affixes (""ally"" stemming to ""all""). We introduce a semantic-based algorithm for learning morphology which only proposes affixes when the stem and stem-plus-affix are sufficiently similar semantically. We implement our approach using Latent Semantic Analysis and show that our semantics-only approach provides morphology induction results that rival a current state-of-the-art system.","In order to obtain semantic representations of each word , we apply our previous strategy ( #AUTHOR_TAG ) .","['In order to obtain semantic representations of each word , we apply our previous strategy ( #AUTHOR_TAG ) .', 'Rather than using a termdocument matrix, we had followed an approach akin to that of Schütze (1993), who performed SVD on a Nx2N term-term matrix.', 'The N here represents the N-1 most-frequent words as well as a glob position to account for all other words not in the top N-1.', ""The matrix is structured such that for a given word w's row, the first N columns denote words that -NCS (µ,1)""]",2,"['In order to obtain semantic representations of each word , we apply our previous strategy ( #AUTHOR_TAG ) .']"
CC979,N12-1010,A user modeling-based performance analysis of a wizarded uncertainty-adaptive dialogue system corpus,examining the impacts of dialogue content and system automation on affect models in a spoken tutorial dialogue system,"['J Drummond', 'D Litman']",,"Many dialogue system developers use data gathered from previous versions of the dialogue system to build models which enable the system to detect and respond to users ' affect. Previous work in the dialogue systems community for domain adaptation has shown that large differences between versions of dialogue systems affect performance of ported models. Thus, we wish to investigate how more minor differences, like small dialogue content changes and switching from a wizarded system to a fully automated system, influence the performance of our affect detection models. We perform a post-hoc experiment where we use various data sets to train multiple models, and compare against a test set from the most recent version of our dialogue system. Analyzing these results strongly suggests that these differences do impact these models ' performance","Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #AUTHOR_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) .","['To train our DISE model, we first extracted the set of speech and dialogue features shown in Figure 2 from the user turns in our corpus.', 'As shown, the acoustic-prosodic features represent duration, pausing, pitch, and energy, and were normalized by the first user turn, as well as totaled and averaged over each dialogue.', 'The lexical and dialogue features consist of the current dialogue name (i.e., one of the six physics problems) and turn number, the current ITSPOKE question\'s name (e.g.,T 3 in Figure 1 has a unique identifier) and depth in the discourse structure (e.g., an ITSPOKE remediation question after an incorrect user answer would be at one greater depth than the prior question), a word occurrence vector for the automatically recognized text of the user turn, an automatic (in)correctness label, and lastly, the number of user turns since the last correct turn (""incorrect runs"").', 'We also included two user-based features, gender and pretest score.', 'Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #AUTHOR_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) .', 'To date, however, these features have only decreased the crossvalidation performance of our models.', '8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of ""response appropriateness"" in other domains, while pretest score corresponds to the general notion of domain expertise).', 'Moreover, all of our features are fully automatic and available in real-time, so that the model can be directly implemented and deployed.', 'To that end, we now describe the results of our intrinsic and extrinsic evaluations of our DISE model, aimed at determining whether it is ready to be evaluated with real users.']",2,"['To train our DISE model, we first extracted the set of speech and dialogue features shown in Figure 2 from the user turns in our corpus.', 'As shown, the acoustic-prosodic features represent duration, pausing, pitch, and energy, and were normalized by the first user turn, as well as totaled and averaged over each dialogue.', 'The lexical and dialogue features consist of the current dialogue name (i.e., one of the six physics problems) and turn number, the current ITSPOKE question\'s name (e.g.,T 3 in Figure 1 has a unique identifier) and depth in the discourse structure (e.g., an ITSPOKE remediation question after an incorrect user answer would be at one greater depth than the prior question), a word occurrence vector for the automatically recognized text of the user turn, an automatic (in)correctness label, and lastly, the number of user turns since the last correct turn (""incorrect runs"").', 'We also included two user-based features, gender and pretest score.', 'Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #AUTHOR_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) .', '8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of ""response appropriateness"" in other domains, while pretest score corresponds to the general notion of domain expertise).', 'To that end, we now describe the results of our intrinsic and extrinsic evaluations of our DISE model, aimed at determining whether it is ready to be evaluated with real users.']"
CC980,P00-1004,Translation with Cascaded Finite State Transducers,a polynomialtime algorithm for statistical machine translation,['D Wu'],introduction,,"Finite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation ( Amengual et al. , 2000 ) , as have been bilingual stochastic grammars ( #AUTHOR_TAG ) .","['Finite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation ( Amengual et al. , 2000 ) , as have been bilingual stochastic grammars ( #AUTHOR_TAG ) .']",0,"['Finite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation ( Amengual et al. , 2000 ) , as have been bilingual stochastic grammars ( #AUTHOR_TAG ) .']"
CC981,P00-1004,Translation with Cascaded Finite State Transducers,a polynomialtime algorithm for statistical machine translation,['D Wu'],,,It also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG ) .,['It also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG ) .'],5,['It also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG ) .']
CC982,P00-1006,A Maximum Entropy/Minimum Divergence Translation Model,a maximum entropy approach to natural language processing,"['Adam L Berger', 'Stephen A Della Pietra', 'Vincent J Della Pietra']",method,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.",It can be shown ( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events .,['It can be shown ( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events .'],4,['It can be shown ( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events .']
CC983,P00-1006,A Maximum Entropy/Minimum Divergence Translation Model,a maximum entropy approach to natural language processing,"['Adam L Berger', 'Stephen A Della Pietra', 'Vincent J Della Pietra']",,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.",#AUTHOR_TAG describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t .,['#AUTHOR_TAG describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t .'],0,['#AUTHOR_TAG describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t .']
CC984,P00-1006,A Maximum Entropy/Minimum Divergence Translation Model,a maximum entropy approach to natural language processing,"['Adam L Berger', 'Stephen A Della Pietra', 'Vincent J Della Pietra']",introduction,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.",A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #AUTHOR_TAG ) .,['A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #AUTHOR_TAG ) .'],5,['A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #AUTHOR_TAG ) .']
CC985,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,memorybased shallow parsing,"['Walter Daelemans', 'Sabine Buchholz', 'Jorn Veenstra']",conclusion,,"As ( #AUTHOR_TAG ) show , lexical information improves on NP and VP chunking as well .","['As ( #AUTHOR_TAG ) show , lexical information improves on NP and VP chunking as well .']",3,"['As ( #AUTHOR_TAG ) show , lexical information improves on NP and VP chunking as well .']"
CC986,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,statistical decisiontree models for parsing,['David M Magerman'],,,"The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #AUTHOR_TAG , Collins ( 1997 ) , and Ratnaparkhi ( 1997 ) , and became a common testbed .","['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #AUTHOR_TAG , Collins ( 1997 ) , and Ratnaparkhi ( 1997 ) , and became a common testbed .']",1,"['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #AUTHOR_TAG , Collins ( 1997 ) , and Ratnaparkhi ( 1997 ) , and became a common testbed .']"
CC987,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,three generative lexicalised models for statistical parsing,['M Collins'],introduction,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , #AUTHOR_TAG , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .","['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , #AUTHOR_TAG , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .']",0,"['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , #AUTHOR_TAG , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .']"
CC988,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,structural ambiguity and lexical relations,"['D Hindle', 'M Rooth']",conclusion,"We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus. This suggests that a distributional approach can provide an approximate solution to parsing problems that, in the worst case, call for complex reasoning.","It is not aimed at handling dependencies , which require heavy use of lexical information ( #AUTHOR_TAG , for PP attachment ) .","['It is not aimed at handling dependencies , which require heavy use of lexical information ( #AUTHOR_TAG , for PP attachment ) .']",1,"['It is not aimed at handling dependencies , which require heavy use of lexical information ( #AUTHOR_TAG , for PP attachment ) .']"
CC989,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,a maximumentropy partial parser for unrestricted text,"['W Skut', 'T Brants']",introduction,,Another approach for partial parsing was presented by #AUTHOR_TAG .,['Another approach for partial parsing was presented by #AUTHOR_TAG .'],0,['Another approach for partial parsing was presented by #AUTHOR_TAG .']
CC990,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,a linear observed time statistical parser based on maximum entropy models,['A Ratnaparkhi'],introduction,"This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.","A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , Collins ( 1997 ) , #AUTHOR_TAG , and Sekine ( 1998 ) ) .","['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , Collins ( 1997 ) , #AUTHOR_TAG , and Sekine ( 1998 ) ) .']",0,"['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , Collins ( 1997 ) , #AUTHOR_TAG , and Sekine ( 1998 ) ) .']"
CC991,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,three generative lexicalised models for statistical parsing,['M Collins'],,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","Our results are lower than those of full parsers , e.g. , #AUTHOR_TAG as might be expected since much less structural data , and no lexical data are being used .","['Our results are lower than those of full parsers , e.g. , #AUTHOR_TAG as might be expected since much less structural data , and no lexical data are being used .']",1,"['Our results are lower than those of full parsers , e.g. , #AUTHOR_TAG as might be expected since much less structural data , and no lexical data are being used .']"
CC992,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,cascaded grammatical relation assignment,"['S Buchholz', 'J Veenstra', 'W Daelemans']",conclusion,"In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.","In a similar vain to Skut and Brants ( 1998 ) and #AUTHOR_TAG , the method extends an existing flat shallow-parsing method to handle composite structures .","['In a similar vain to Skut and Brants ( 1998 ) and #AUTHOR_TAG , the method extends an existing flat shallow-parsing method to handle composite structures .']",3,"['In a similar vain to Skut and Brants ( 1998 ) and #AUTHOR_TAG , the method extends an existing flat shallow-parsing method to handle composite structures .']"
CC993,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,statistical decisiontree models for parsing,['David M Magerman'],introduction,,"A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , #AUTHOR_TAG , Collins ( 1997 ) , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .","['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , #AUTHOR_TAG , Collins ( 1997 ) , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .']",0,"['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , #AUTHOR_TAG , Collins ( 1997 ) , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .']"
CC994,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,a linear observed time statistical parser based on maximum entropy models,['A Ratnaparkhi'],,"This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.","The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , Collins ( 1997 ) , and #AUTHOR_TAG , and became a common testbed .","['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , Collins ( 1997 ) , and #AUTHOR_TAG , and became a common testbed .']",1,"['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , Collins ( 1997 ) , and #AUTHOR_TAG , and became a common testbed .']"
CC995,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,three generative lexicalised models for statistical parsing,['M Collins'],,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , #AUTHOR_TAG , and Ratnaparkhi ( 1997 ) , and became a common testbed .","['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , #AUTHOR_TAG , and Ratnaparkhi ( 1997 ) , and became a common testbed .']",1,"['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , #AUTHOR_TAG , and Ratnaparkhi ( 1997 ) , and became a common testbed .']"
CC996,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,cascaded grammatical relation assignment,"['S Buchholz', 'J Veenstra', 'W Daelemans']",introduction,"In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.","One approach to partial parsing was presented by #AUTHOR_TAG , who extended a shallow-parsing technique to partial parsing .","['One approach to partial parsing was presented by #AUTHOR_TAG , who extended a shallow-parsing technique to partial parsing .']",0,"['One approach to partial parsing was presented by #AUTHOR_TAG , who extended a shallow-parsing technique to partial parsing .']"
CC997,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,a maximumentropy partial parser for unrestricted text,"['W Skut', 'T Brants']",conclusion,,"In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .","['In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .']",3,"['In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .']"
CC998,P00-1012,The order of prenominal adjectives in natural language generation,ordering among premodifiers,"['James Shaw', 'Vasileios Hatzivassiloglou']",experiments,"We present a corpus-based study of the sequential ordering among premodifiers in noun phrases. This information is important for the fluency of generated text in practical applications. We propose and evaluate three approaches to identify sequential order among premodifiers: direct evidence, transitive closure, and clustering. Our implemented system can make over 94 % of such ordering decisions correctly, as evaluated on a large, previously unseen test corpus.","To quantify the relative strengths of these transitive inferences , #AUTHOR_TAG propose to assign a weight to each link .","['Intuitively, the evidence for the first order is quite a bit stronger than the evidence for the second.', 'The first ordered pairs are more frequent, as are the individual adjectives involved.', 'To quantify the relative strengths of these transitive inferences , #AUTHOR_TAG propose to assign a weight to each link .', 'Say the order a, b occurs m times and the pair {a, b} occurs n times in total.', 'Then the weight of the pair a → b is:']",0,"['Intuitively, the evidence for the first order is quite a bit stronger than the evidence for the second.', 'The first ordered pairs are more frequent, as are the individual adjectives involved.', 'To quantify the relative strengths of these transitive inferences , #AUTHOR_TAG propose to assign a weight to each link .', 'Say the order a, b occurs m times and the pair {a, b} occurs n times in total.', 'Then the weight of the pair a - b is:']"
CC999,P00-1012,The order of prenominal adjectives in natural language generation,distributional clustering of english words,"['Fernando Pereira', 'Naftali Tishby', 'Lilian Lee']",conclusion,"We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical ""soft"" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.","More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself .","['While the combined MBL method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement.', 'Future work will pursue at least two directions for improving the results.', 'First, while semantic information is not available for all adjectives, it is clearly available for some.', 'Furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available.', 'More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself .', 'Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results.']",3,"['More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself .']"
CC1000,P00-1012,The order of prenominal adjectives in natural language generation,ordering among premodifiers,"['James Shaw', 'Vasileios Hatzivassiloglou']",experiments,"We present a corpus-based study of the sequential ordering among premodifiers in noun phrases. This information is important for the fluency of generated text in practical applications. We propose and evaluate three approaches to identify sequential order among premodifiers: direct evidence, transitive closure, and clustering. Our implemented system can make over 94 % of such ordering decisions correctly, as evaluated on a large, previously unseen test corpus.",The simplest strategy for ordering adjectives is what #AUTHOR_TAG call the direct evidence method .,"['The simplest strategy for ordering adjectives is what #AUTHOR_TAG call the direct evidence method .', 'To order the pair {a, b}, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often.']",0,"['The simplest strategy for ordering adjectives is what #AUTHOR_TAG call the direct evidence method .', 'To order the pair {a, b}, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often.']"
CC1001,P00-1012,The order of prenominal adjectives in natural language generation,boosting applied to tagging and pp attachment,"['Steven Abney', 'Robert E Schapire', 'Yoram Singer']",conclusion,Boosting is a machine learning algorithm that is not well known in computational linguistics. We apply it to part-of-speech tagging and prepositional phrase attachment. Performance is very encouraging. We also show how to improve data quality by using boosting to identify annotation errors.,"In particular , boosting ( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly .","['The second area where the methods described here could be improved is in the way that multiple information sources are integrated.', 'The technique method described in section 3.7 is a fairly crude method for combining frequency information with symbolic data.', 'It would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (Dietterich, 1997).', 'In particular , boosting ( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly .']",3,"['In particular , boosting ( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly .']"
CC1002,P00-1012,The order of prenominal adjectives in natural language generation,ordering among premodifiers,"['James Shaw', 'Vasileios Hatzivassiloglou']",experiments,"We present a corpus-based study of the sequential ordering among premodifiers in noun phrases. This information is important for the fluency of generated text in practical applications. We propose and evaluate three approaches to identify sequential order among premodifiers: direct evidence, transitive closure, and clustering. Our implemented system can make over 94 % of such ordering decisions correctly, as evaluated on a large, previously unseen test corpus.",#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation .,"['One way to think of the direct evidence method is to see that it defines a relation ≺ on the set of English adjectives.', 'Given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a , then a ≺ b.', 'If the re-verse is true, and b, a is found more often than a, b , then b ≺ a.', 'If neither order appears in the training data, then neither a ≺ b nor b ≺ a and an order must be randomly assigned.', '#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation .', 'That is, if a ≺ c and c ≺ b, we can conclude that a ≺ b.', 'To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method.', 'However, the pairs large, new and new, green occur fairly frequently.', 'Therefore, in the face of this evidence we can assign this pair the order large, green , which not coincidently is the correct English word order.']",0,"['If the re-verse is true, and b, a is found more often than a, b , then b  a.', 'If neither order appears in the training data, then neither a  b nor b  a and an order must be randomly assigned.', '#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation .', 'That is, if a  c and c  b, we can conclude that a  b.', 'To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method.', 'However, the pairs large, new and new, green occur fairly frequently.', 'Therefore, in the face of this evidence we can assign this pair the order large, green , which not coincidently is the correct English word order.']"
CC1003,P00-1012,The order of prenominal adjectives in natural language generation,generation that exploits corpusbased statistical knowledge,"['Irene Langkilde', 'Kevin Knight']",method,"We describe novel aspects of a new natural language generator called Nitrogen. This generator has a highly flexible input representation that allows a spectrum of input from syntactic to semantic depth, and shifts the burden of many linguistic decisions to the statistical post-processor. The generation algorithm is compositional, making it efficient, yet it also handles non-compositional aspects of language. Nitrogen's design makes it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities.","One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .","['The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model ."", 'Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word.', 'It also should be straightforwardly applicable to the more specific problem we are addressing here.', 'To determine the correct order for a sequence of prenominal adjectives, we can simply generate all possible orderings and choose the one with the highest probability.', 'This has the advantage of reducing the problem of adjective ordering to the problem of estimating n-gram probabilities, something which is relatively well understood.']",5,"['The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .""]"
CC1004,P02-1001,Parameter estimation for probabilistic finite-state transducers,translation with finitestate devices,"['Kevin Knight', 'Yaser Al-Onaizan']",introduction,,"Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #AUTHOR_TAG ) .","['The availability of toolkits for this weighted case (Mohri et al., 1998;van Noord and Gerdemann, 2001) promises to unify much of statistical NLP.', ""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #AUTHOR_TAG ) ."", 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']",0,"[""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #AUTHOR_TAG ) ."", 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']"
CC1005,P02-1001,Parameter estimation for probabilistic finite-state transducers,a rational design for a weighted finitestate transducer library,"['M Mohri', 'F Pereira', 'M Riley']",,,"fÎ¸ on demand ( #AUTHOR_TAG ) can pay off here , since only part of fÎ¸ may be needed subsequently . )","['10 Traditionally log(strength) values are called weights, but this paper uses ""weight"" to mean something else.', 'fÎ¸ on demand ( #AUTHOR_TAG ) can pay off here , since only part of fÎ¸ may be needed subsequently . )']",0,"['10 Traditionally log(strength) values are called weights, but this paper uses ""weight"" to mean something else.', 'fÎ¸ on demand ( #AUTHOR_TAG ) can pay off here , since only part of fÎ¸ may be needed subsequently . )']"
CC1006,P02-1001,Parameter estimation for probabilistic finite-state transducers,rational series and their languages,"['Jean Berstel', 'Christophe Reutenauer']",,"I. Rational Series.- 1 Semirings.- 2 Formal Series.- 3 The Topology of K""X"".- 4 Rational Series.- 5 Recognizable Series.- 6 The Fundamental Theorem.- Exercises for Chapter I.- Notes to Chapter I.- II. Minimization.- 1 Syntactic Ideals.- 2 Reduced Linear Representations.- 3 The Reduction Algorithm.- Exercises for Chapter II.- Notes to Chapter II.- III. Series and Languages.- 1 The Theorem of Kleene.- 2 Series and Rational Languages.- 3 Supports.- 4 Iteration.- 5 Complementation.- Exercises for Chapter III.- Notes to Chapter III.- IV. Rational Series in One Variable.- 1 Rational Functions.- 2 The Exponential Polynomial.- 3 A Theorem of Polya.- 4 A Theorem of Skolem, Mahler and Lech.- Notes to Chapter IV.- V. Changing the Semiring.- 1 Rational Series over a Principal Ring.- 2 Positive Rational Series.- 3 Fatou Extensions.- Exercises for Chapter V.- Notes to Chapter V.- VI. Decidability.- 1 Problems of Supports.- 2 Growth.- Exercises for Chapter VI.- Notes to Chapter VI.- VII. Noncommutative Polynomials.- 1 The Weak Algorithm.- 2 Continuant Polynomials.- 3 Inertia.- 4 Gauss's Lemma.- Exercises for Chapter VII.- Notes to Chapter VII.- VIII. Codes and Formal Series.- 1 Codes.- 2 Completeness.- 3 The Degree of a Code.- 4 Factorization.- Exercises for Chapter VIII.- Notes to Chapter VIII.- References.",#AUTHOR_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) .,"['The denominator of equation ( 1) is the total probability of all accepting paths in x i • f • y i .', 'But while computing this, we will also compute the numerator.', 'The idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability.', 'We will enforce an invariant: the weight of any pathset Π must be ( π∈Π P (π), π∈Π P (π) val(π)) ∈ R ≥0 × V , from which (1) is trivial to compute.', '#AUTHOR_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) .', 'Multiplication and addition are replaced by binary operations ⊗ and ⊕ on K. Thus ⊗ is used to combine arc weights into a path weight and ⊕ is used to combine the weights of alternative paths.', 'To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * = ∞ i=0 k i .', 'The usual finite-state algorithms work if (K, ⊕, ⊗, * ) has the structure of a closed semiring. 15', 'rdinary probabilities fall in the semiring (R ≥0 , +, ×, * ). 16', 'Our novel weights fall in a novel 14 Formal derivation of (1):']",5,"['But while computing this, we will also compute the numerator.', '#AUTHOR_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) .', 'To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * =  i=0 k i .', 'The usual finite-state algorithms work if (K, , , * ) has the structure of a closed semiring. 15', 'Our novel weights fall in a novel 14 Formal derivation of (1):']"
CC1007,P02-1001,Parameter estimation for probabilistic finite-state transducers,an inequality and associated maximization technique in statistical estimation of probabilistic functions of a markov process,['L E Baum'],,,"For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG ) .","['• In many cases of interest, T i is an acyclic graph. 20', ""hen Tarjan's method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of ⊕ and ⊗ operations."", 'For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG ) .', 'But notice that it has no backward pass.', 'In place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs (more generally, values in V ) forward to the probabilities.', 'This is slower because our ⊕ and ⊗ are vector operations, and the vectors rapidly lose sparsity as they are added together.']",0,"['* In many cases of interest, T i is an acyclic graph. 20', ""hen Tarjan's method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of  and  operations."", 'For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG ) .', 'But notice that it has no backward pass.', 'In place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs (more generally, values in V ) forward to the probabilities.', 'This is slower because our  and  are vector operations, and the vectors rapidly lose sparsity as they are added together.']"
CC1008,P02-1001,Parameter estimation for probabilistic finite-state transducers,a rational design for a weighted finitestate transducer library,"['M Mohri', 'F Pereira', 'M Riley']",introduction,,"The availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical NLP .","['The availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical NLP .', 'Such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy-channel decoding, 1 including classic models for speech recognition (Pereira and Riley, 1997) and machine translation (Knight and Al-Onaizan, 1998).', 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']",0,"['The availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical NLP .', 'Such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy-channel decoding, 1 including classic models for speech recognition (Pereira and Riley, 1997) and machine translation (Knight and Al-Onaizan, 1998).', 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']"
CC1009,P02-1001,Parameter estimation for probabilistic finite-state transducers,learning string edit distance,"['E Ristad', 'P Yianilos']",introduction,"In many applications, it is necessary to determine the similarity of two strings. A widely-used notion of string similarity is the edit distance: the minimum number of insertions, deletions, and substitutions required to transform one string into the other. In this report, we provide a stochastic model for string-edit distance. Our stochastic model allows us to learn a string-edit distance function from a corpus of examples. We illustrate the utility of our approach by applying it to the difficult problem of learning the pronunciation of words in conversational speech. In this application, we learn a string-edit distance with nearly one-fifth the error rate of the untrained Levenshtein distance. Our approach is applicable to any string classification problem that may be solved using a similarity function against a database of labeled prototypes.","For example , the forward-backward algorithm ( Baum , 1972 ) trains only Hidden Markov Models , while ( #AUTHOR_TAG ) trains only stochastic edit distance .","['Unfortunately, there is a stumbling block: Where do the weights come from?', 'After all, statistical models require supervised or unsupervised training.', 'Currently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'For example , the forward-backward algorithm ( Baum , 1972 ) trains only Hidden Markov Models , while ( #AUTHOR_TAG ) trains only stochastic edit distance .']",0,"['For example , the forward-backward algorithm ( Baum , 1972 ) trains only Hidden Markov Models , while ( #AUTHOR_TAG ) trains only stochastic edit distance .']"
CC1010,P02-1001,Parameter estimation for probabilistic finite-state transducers,iterative methods for solving linear systems,['Anne Greenbaum'],,"bthkhady`  withiithamchamsamhrabhaaphlechlyrabbechingesnmii`yuudwykan 2 aebbaihy + khuue` withiithamcham`yaangningkabwithiipriphuumiy`yaikhrl`f bthkhwaamwichaakaarnii`phipraayaenwkhidthawaipaelaethkhnikhphuuenthaankh`ngwithiithamcham`yaangning aidaek withiicchaaokhbii withiiekaas-aichedl aelawithiiph`nprnekinsuuebenuue`ng n`kcchaaknanyangphicchaarnaawithiithamchamthiiphathnaat`y`dcchaakwithiidangklaaw aidaek withiiph`nprnekinaebberng withiithamchamthiimiithaancchaakekrediiynt eaelawithiithamchamkamlangs`ngn`ysud samhrabwithiipriphuumiy`yaikhrl`fnanmiitnaebbmaacchaakwithiikh`ncchuuektekrediiynt withiidangklaawcchasraangthaanhlakechingtangchaakkh`ngpriphuumiaebbyukhlidcchaakemthrikchsamprasiththiodyphicchaarnaacchaakekrediiyntkh`ngfangkchankamlangs`ngthiis`dkhl`ng thaanhlakdangklaawprak`bdwyewket`rthiimiithisthaangthiithamaihphlechlykhaapramaanekhaaaiklphlechlycchringaiderwthiisud klaawodysrupaidwaa withiithamcham`yaangning 4 withiiaerkthiiklaawmaanancchakaarantiikaarluuekhaakh`nglamdabkh`ngphlechlyodypramaansuuphlechlycchringemuue`aichkabrabbthiimiiemthrikchsamprasiththi`yuuainruupaebbechphaaa echn emthrikchaenwthaeyngmumkhmaeth emthrikchldth`naimaid aelaemthrikchaebbae`l odyt`ngkamhndtawaepresrimthiiehmaaasm swnwithiithamchamthiimiithaancchaakekrediiyntaelawithiithamchamkamlangs`ngn`ysudaichaidkabrabbthiimiiemthrikchsamprasiththimiikhaalamdabchanetm samhrabwithiikh`ncchuuektekrediiyntaichaidkabrabbthiiemthrikchsamprasiththiepnemthrikchsmmaatrthiiepnbwkaenn`n   khamsamkhay: rabbechingesn withiiph`nprnekinsuuebenuue`ng  withiiph`nprnekinaebberng withiithamchamthiimiithaancchaakekrediiynt  withiikh`ncchuuektekrediiynt     ABSTRACT There are two major types of iterative methods for solving linear systems, namely, stationary iterative methods and Krylov subspace methods. This survey article discusses general ideas and elementary techniques for stationary iterative methods such as Jacobi method, Gauss-Seidel method, and the successive over-relaxation method. Moreover, we investigate further developed methods, namely, the accelerated over-relaxation method, the gradient based iterative method, and the least squares iterative method. On the other hand, Krylov subspace methods have prototypes from the conjugate gradient method. The latter method constructs an orthogonal basis for the Euclidean space from the gradient of the associated quadratic function. Such basis consists of vectors in directions so that the approximated solutions fastest approach to the exact solution. In conclusions, all 1st-4th mentioned stationary iterative methods guarantee the convergence of the sequence of approximated solutions to the exact solution when applying to the system with specific coefficient matrices such as strictly diagonally dominant matrices, irreducible matrices, and L-matrices. Here, the parameters in the methods must be appropriate. The gradient based iterative method and the least squares iterative method can be applied to systems with full-column rank coefficient matrices. The conjugate gradient method is applicable for the system whose coefficient matrix is a positive definite symmetric matrix.Keywords: linear system, successive over-relaxation method, accelerated over-relaxation method, gradient based iterative method, conjugate gradient metho","The forward and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , â ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations ( #AUTHOR_TAG ) .","['We therefore reintroduce a backward pass that lets us avoid ⊕ and ⊗ when computing t i (so they are needed only to construct T i ).', 'This speedup also works for cyclic graphs and for any V .', 'Write w jk as (p jk , v jk ), and let w 1 jk = (p 1 jk , v 1 jk ) denote the weight of the edge from j to k. 19 Then it can be shown that w 0n = (p 0n , j,k p 0j v 1 jk p kn ).', 'The forward and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , â\x88\x97 ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations ( #AUTHOR_TAG ) .']",0,"['The forward and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , â\x88\x97 ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations ( #AUTHOR_TAG ) .']"
CC1011,P02-1001,Parameter estimation for probabilistic finite-state transducers,rational series and their languages,"['Jean Berstel', 'Christophe Reutenauer']",introduction,"I. Rational Series.- 1 Semirings.- 2 Formal Series.- 3 The Topology of K""X"".- 4 Rational Series.- 5 Recognizable Series.- 6 The Fundamental Theorem.- Exercises for Chapter I.- Notes to Chapter I.- II. Minimization.- 1 Syntactic Ideals.- 2 Reduced Linear Representations.- 3 The Reduction Algorithm.- Exercises for Chapter II.- Notes to Chapter II.- III. Series and Languages.- 1 The Theorem of Kleene.- 2 Series and Rational Languages.- 3 Supports.- 4 Iteration.- 5 Complementation.- Exercises for Chapter III.- Notes to Chapter III.- IV. Rational Series in One Variable.- 1 Rational Functions.- 2 The Exponential Polynomial.- 3 A Theorem of Polya.- 4 A Theorem of Skolem, Mahler and Lech.- Notes to Chapter IV.- V. Changing the Semiring.- 1 Rational Series over a Principal Ring.- 2 Positive Rational Series.- 3 Fatou Extensions.- Exercises for Chapter V.- Notes to Chapter V.- VI. Decidability.- 1 Problems of Supports.- 2 Growth.- Exercises for Chapter VI.- Notes to Chapter VI.- VII. Noncommutative Polynomials.- 1 The Weak Algorithm.- 2 Continuant Polynomials.- 3 Inertia.- 4 Gauss's Lemma.- Exercises for Chapter VII.- Notes to Chapter VII.- VIII. Codes and Formal Series.- 1 Codes.- 2 Completeness.- 3 The Degree of a Code.- 4 Factorization.- Exercises for Chapter VIII.- Notes to Chapter VIII.- References.","4To prove ( 1 ) â ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction ( #AUTHOR_TAG ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp .","['4To prove ( 1 ) â\x87\x92 ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction ( #AUTHOR_TAG ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp .', 'A full proof is straightforward, as are proofs of (3)_(2), (2)_(1).']",5,"['4To prove ( 1 ) â\x87\x92 ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction ( #AUTHOR_TAG ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp .']"
CC1012,P02-1001,Parameter estimation for probabilistic finite-state transducers,a systolic array algorithm for the algebraic path problem shortest paths matrix inversion,['G¨unter Rote'],,"It is shown how the Gauss-Jordan Elimination algorithm for the Algebraic Path Problem can be implemented on a hexagonal systolic array of a quadratic number of simple processors in linear time. Special instances of this general algorithm include parallelizations of the Warshall-Floyd Algorithm, which computes the shortest distances in a graph or the transitive closure of a relation, and of the Gauss-Jordan Elimination algorithm for computing the inverse of a real matrix.ZusammenfassungEs wird dargestellt, wie man den gaus-Jordanschen Eliminationsalgorithmus fur das algebraische Wegproblem auf einem hexagonalen systolischen Feld (systolic array) mit einer quadratischen Anzahl einfacher Prozessoren in linearer Zeit ausfuhren kann. Zu den Anwendungsbeispielen dieses allgemeinen Algorithmus gehort der Warshall-Floyd-Algorithmus zur Berechnung der kurzesten Wegen in einem Graphen oder zur Bestimmung der transitiven Hulle einer Relation sowie der Gauss-Jordansche Eliminationsalgorithmus zur Inversion reeller Matrizen.",Efficient hardware implementation is also possible via chip-level parallelism ( #AUTHOR_TAG ) .,"['Division and subtraction are also possible: −(p, v) = (−p, −v) and (p, v) −1 = (p −1 , −p −1 vp −1 ).', 'Division is commonly used in defining f θ (for normalization). 19', 'Multiple edges from j to k are summed into a single edge.', '(Mohri, 2002).', 'Efficient hardware implementation is also possible via chip-level parallelism ( #AUTHOR_TAG ) .']",3,"['Division and subtraction are also possible: -(p, v) = (-p, -v) and (p, v) -1 = (p -1 , -p -1 vp -1 ).', 'Multiple edges from j to k are summed into a single edge.', 'Efficient hardware implementation is also possible via chip-level parallelism ( #AUTHOR_TAG ) .']"
CC1013,P02-1001,Parameter estimation for probabilistic finite-state transducers,expectation semirings flexible em for finitestate transducers,['Jason Eisner'],,,"Per-state joint normalization ( #AUTHOR_TAGb , Â§ 8.2 ) is similar but drops the dependence on a .","['13 For per-state conditional normalization, let Dj,a be the set of arcs from state j with input symbol a ∈ Σ; their weights are normalized to sum to 1.', 'Besides computing c, the E step must count the expected number dj,a of traversals of arcs in each Dj,a.', 'Then the predicted vector given θ is j,a dj,a • (expected feature counts on a randomly chosen arc in Dj,a).', 'Per-state joint normalization ( #AUTHOR_TAGb , Â§ 8.2 ) is similar but drops the dependence on a .', 'The difficult case is global conditional normalization.', 'It arises, for example, when training a joint model of the form']",1,"['13 For per-state conditional normalization, let Dj,a be the set of arcs from state j with input symbol a  S; their weights are normalized to sum to 1.', 'Per-state joint normalization ( #AUTHOR_TAGb , Â§ 8.2 ) is similar but drops the dependence on a .', 'The difficult case is global conditional normalization.']"
CC1014,P02-1001,Parameter estimation for probabilistic finite-state transducers,maximum likelihood from incomplete data via the em algorithm,"['A P Dempster', 'N M Laird', 'D B Rubin']",,,The EM algorithm ( #AUTHOR_TAG ) can maximize these functions .,"['The EM algorithm ( #AUTHOR_TAG ) can maximize these functions .', 'Roughly, the E step guesses hidden information: if (x i , y i ) was generated from the current f θ , which FST paths stand a chance of having been the path used?', '(Guessing the path also guesses the exact input and output.)', 'The M step updates θ to make those paths more likely.', 'EM alternates these steps and converges to a local optimum.', ""The M step's form depends on the parameterization and the E step serves the M step's needs.""]",5,"['The EM algorithm ( #AUTHOR_TAG ) can maximize these functions .', 'Roughly, the E step guesses hidden information: if (x i , y i ) was generated from the current f th , which FST paths stand a chance of having been the path used?', '(Guessing the path also guesses the exact input and output.)', 'The M step updates th to make those paths more likely.', 'EM alternates these steps and converges to a local optimum.', ""The M step's form depends on the parameterization and the E step serves the M step's needs.""]"
CC1015,P02-1001,Parameter estimation for probabilistic finite-state transducers,compilation of weighted finitestate transducers from decision trees,"['Richard Sproat', 'Michael Riley']",introduction,"We report on a method for compiling decision trees into weighted finite-state transducers. The key assumptions are that the tree predictions specify how to rewrite symbols from an input string, and the decision at each tree node is stateable in terms of regular expressions on the input string. Each leaf node can then be treated as a separate rule where the left and right contexts are constructable from the decisions made traversing the tree from the root to the leaf. These rules are compiled into transducers using the weighted rewite-rule rule-compilation algorithm described in (Mohri and Sproat, 1996).","For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 ) , ( 3 ) by compilation of decision trees ( #AUTHOR_TAG ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .","['For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 ) , ( 3 ) by compilation of decision trees ( #AUTHOR_TAG ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .']",0,"['For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 ) , ( 3 ) by compilation of decision trees ( #AUTHOR_TAG ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .']"
CC1016,P02-1001,Parameter estimation for probabilistic finite-state transducers,algebraic structures for transitive closure,['D J Lehmann'],,"AbstractClosed semi-rings and the closure of matrices over closed semi-rings are defined and studied. Closed semi-rings are structures weaker than the structures studied by Conway [3] and Aho, Hopcroft and Ullman [1]. Examples of closed semi-rings and closure operations are given, including the case of semi-rings on which the closure of an element is not always defined. Two algorithms are proved to compute the closure of a matrix over any closed semi-ring; the first one based on Gauss-Jordan elimination is a generalization of algorithms by Warshall, Floyd and Kleene; the second one based on Gauss elimination has been studied by Tarjan [11, 12], from the complexity point of view in a slightly different framework. Simple semi-rings, where the closure operation for elements is trivial, are defined and it is shown that the closure of an n x n-matrix over a simple semi-ring is the sum of its powers of degree less than n. Dijkstra semi-rings are defined and it is shown that the rows of the closure of a matrix over a Dijkstra semi-ring, can be computed by a generalized version of Dijkstra's algorithm","Now for some important remarks on efficiency : â¢ Computing ti is an instance of the well-known algebraic path problem ( #AUTHOR_TAG ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted).","['Now for some important remarks on efficiency : â\x80¢ Computing ti is an instance of the well-known algebraic path problem ( #AUTHOR_TAG ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted).', 'It is wasteful to compute ti as suggested earlier, by minimizing (__xi)_f_(yi__), since then the real work is done by an _-closure step (Mohri, 2002) that implements the all-pairs version of algebraic path, whereas all we need is the single-source version.', 'If n and m are the number of states and edges,19 then both problems are O(n3) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b).', 'For a general graph Ti, Tarjan (1981b) shows how to partition into �hard� subgraphs that localize the cyclicity or irreducibility, then run the O(n3) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results.', 'The overhead of partitioning and recombining is essentially only O(m).']",0,"['Now for some important remarks on efficiency : â\x80¢ Computing ti is an instance of the well-known algebraic path problem ( #AUTHOR_TAG ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted).', 'It is wasteful to compute ti as suggested earlier, by minimizing (__xi)_f_(yi__), since then the real work is done by an _-closure step (Mohri, 2002) that implements the all-pairs version of algebraic path, whereas all we need is the single-source version.', 'If n and m are the number of states and edges,19 then both problems are O(n3) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b).', 'For a general graph Ti, Tarjan (1981b) shows how to partition into hard subgraphs that localize the cyclicity or irreducibility, then run the O(n3) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results.']"
CC1017,P02-1001,Parameter estimation for probabilistic finite-state transducers,conditional random fields probabilistic models for segmenting and labeling sequence data,"['J Lafferty', 'A McCallum', 'F Pereira']",introduction,"We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.","Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001b ; #AUTHOR_TAG ) .","['o:e −→, and a:ae −→ share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001b ; #AUTHOR_TAG ) .']",0,"['Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001b ; #AUTHOR_TAG ) .']"
CC1018,P02-1001,Parameter estimation for probabilistic finite-state transducers,a gaussian prior for smoothing maximum entropy models,"['Stanley F Chen', 'Ronald Rosenfeld']",,"Abstract : In certain contexts, maximum entropy (ME) modeling can be viewed as maximum likelihood training for exponential models, and like other maximum likelihood methods is prone to overfitting of training data. Several smoothing methods for maximum entropy models have been proposed to address this problem, but previous results do not make it clear how these smoothing methods compare with smoothing methods for other types of related models. In this work, we survey previous work in maximum entropy smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing n-gram language models. Because of the mature body of research in n-gram model smoothing and the close connection between maximum entropy and conventional n-gram models, this domain is well-suited to gauge the performance of maximum entropy smoothing methods. Over a large number of data sets, we find that an ME smoothing method proposed to us by Lafferty performs as well as or better than all other algorithms under consideration. This general and efficient method involves using a Gaussian prior on the parameters of the model and selecting maximum a posteriori instead of maximum likelihood parameter values. We contrast this method with previous n-gram smoothing methods to explain its superior performance.","In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #AUTHOR_TAG ) .","['Maximum-posterior estimation tries to maximize P (θ) • i f θ (x i , y i ) where P (θ) is a prior probability.', 'In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #AUTHOR_TAG ) .']",5,"['Maximum-posterior estimation tries to maximize P (th) * i f th (x i , y i ) where P (th) is a prior probability.', 'In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #AUTHOR_TAG ) .']"
CC1019,P02-1001,Parameter estimation for probabilistic finite-state transducers,hidden markov models with finite state supervision in,['E Ristad'],,"In this chapter we provide a supervised training paradigm for hidden Markov models (HMMs). Unlike popular ad-hoc approaches, our paradigm is completely general, need not make any simplifying assumptions about independence, and can take better advantage of the information contained in the training corpus.","For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #AUTHOR_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11","['As training data we are given a set of observed (input, output) pairs, (xi, yi).', 'These are assumed to be independent random samples from a joint dis- tribution of the form f_�(x, y); the goal is to recover the true _�.', 'Samples need not be fully observed (partly supervised training): thus xi _ __, yi _ �_ may be given as regular sets in which input and output were observed to fall.', 'For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #AUTHOR_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11']",0,"['For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #AUTHOR_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11']"
CC1020,P02-1001,Parameter estimation for probabilistic finite-state transducers,speech recognition by composition of weighted finite automata,"['Fernando C N Pereira', 'Michael Riley']",introduction,"We present a general framework based on weighted finite automata and weighted finite-state transducers for describing and implementing speech recognizers. The framework allows us to represent uniformly the information sources and data structures used in recognition, including context-dependent units, pronunciation dictionaries, language models and lattices. Furthermore, general but efficient algorithms can used for combining information sources in actual recognizers and for optimizing their application. In particular, a single composition algorithm is used both to combine in advance information sources such as language models and dictionaries, and to combine acoustic observations and information sources dynamically during recognition.","Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( Knight and Al-Onaizan , 1998 ) .","['The availability of toolkits for this weighted case (Mohri et al., 1998;van Noord and Gerdemann, 2001) promises to unify much of statistical NLP.', ""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( Knight and Al-Onaizan , 1998 ) ."", 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']",0,"[""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( Knight and Al-Onaizan , 1998 ) ."", 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']"
CC1021,P02-1001,Parameter estimation for probabilistic finite-state transducers,generic epsilonremoval and input epsilonnormalization algorithms for weighted transducers,['M Mohri'],,,"It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #AUTHOR_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version .","['• Computing t i is an instance of the well-known algebraic path problem (Lehmann, 1977;Tarjan, 1981a).', 'Let T i = x i •f •y i .', 'Then t i is the total semiring weight w 0n of paths in T i from initial state 0 to final state n (assumed WLOG to be unique and unweighted).', 'It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #AUTHOR_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version .', 'If n and m are the number of states and edges, 19 then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b).', 'For a general graph T i , Tarjan (1981b) shows how to partition into ""hard"" subgraphs that localize the cyclicity or irreducibility, then run the O(n 3 ) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results.', 'The overhead of partitioning and recombining is essentially only O(m).']",0,"['* Computing t i is an instance of the well-known algebraic path problem (Lehmann, 1977;Tarjan, 1981a).', 'Then t i is the total semiring weight w 0n of paths in T i from initial state 0 to final state n (assumed WLOG to be unique and unweighted).', 'It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #AUTHOR_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version .', 'If n and m are the number of states and edges, 19 then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b).']"
CC1022,P02-1001,Parameter estimation for probabilistic finite-state transducers,conditional random fields probabilistic models for segmenting and labeling sequence data,"['J Lafferty', 'A McCallum', 'F Pereira']",introduction,"We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.","Undesirable consequences of this fact have been termed ""label bias"" ( #AUTHOR_TAG ) .","['• An easy approach is to normalize the options at each state to make the FST Markovian.', 'Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation.', 'Undesirable consequences of this fact have been termed ""label bias"" ( #AUTHOR_TAG ) .', 'Also, in the conditional case such per-state normalization is only correct if all states accept all input suffixes (since ""dead ends"" leak probability mass). 8', ' A better-founded approach is global normalization, which simply divides each f (x, y) by']",0,"['* An easy approach is to normalize the options at each state to make the FST Markovian.', 'Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation.', 'Undesirable consequences of this fact have been termed ""label bias"" ( #AUTHOR_TAG ) .', ' A better-founded approach is global normalization, which simply divides each f (x, y) by']"
CC1023,P02-1001,Parameter estimation for probabilistic finite-state transducers,expectation semirings flexible em for finitestate transducers,['Jason Eisner'],introduction,,"A brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAGa ) .","['A brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAGa ) .', 'A leisurely journal-length version with more details has been prepared and is available.']",2,"['A brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAGa ) .']"
CC1024,P02-1001,Parameter estimation for probabilistic finite-state transducers,regular approximation of contextfree grammars through transformation,"['M Mohri', 'M-J Nederhof']",introduction,We present an algorithm for approximating context-free languages with regular languages. The algorithm is based on a simple transformation that applies to any context-free grammar and guarantees that the result can be compiled into a finite automaton. The resulting grammar contains at most one new nonterminal for any nonterminal symbol of the input grammar. The result thus remains readable and if necessary modifiable. We extend the approximation algorithm to the case of weighted context-free grammars. We also report experiments with several grammars showing that the size of the minimal deterministic automata accepting the resulting approximations is of practical use for applications such as speech recognition.,"A more subtle example is weighted FSAs that approximate PCFGs ( Nederhof , 2000 ; #AUTHOR_TAG ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .","['w,x,y P (v|w)P (w, x)P (y|x)P (z|y), implemented by composing 4 machines. 6,7', 'here are also procedures for defining weighted FSTs that are not probabilistic (Berstel and Reutenauer, 1988).', 'Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 −→ arcs).', 'A more subtle example is weighted FSAs that approximate PCFGs ( Nederhof , 2000 ; #AUTHOR_TAG ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .', ""These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution.""]",0,"['A more subtle example is weighted FSAs that approximate PCFGs ( Nederhof , 2000 ; #AUTHOR_TAG ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .']"
CC1025,P02-1001,Parameter estimation for probabilistic finite-state transducers,an efficient compiler for weighted rewrite rules,"['Mehryar Mohri', 'Richard Sproat']",introduction,"Context-dependent rewrite rules are used in many areas of natural language and speech processing. Work in computational phonology has demonstrated that, given certain conditions, such rewrite rules can be represented as finite-state transducers (FSTs). We describe a new algorithm for compiling rewrite rules into FSTs. We show the algorithm to be simpler and more efficient than existing algorithms. Further, many of our applications demand the ability to compile weighted rules into weighted FSTs, transducers generalized by providing transitions with weights. We have extended the algorithm to allow for this.","For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( #AUTHOR_TAG ) , ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .","['For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( #AUTHOR_TAG ) , ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .']",0,"['For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( #AUTHOR_TAG ) , ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .']"
CC1026,P02-1001,Parameter estimation for probabilistic finite-state transducers,maximum entropy markov models for information extraction and segmentation,"['A McCallum', 'D Freitag', 'F Pereira']",introduction,"Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ's.","Such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001b ; Lafferty et al. , 2001 ) .","['o:e −→, and a:ae −→ share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001b ; Lafferty et al. , 2001 ) .']",0,"['The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001b ; Lafferty et al. , 2001 ) .']"
CC1027,P02-1001,Parameter estimation for probabilistic finite-state transducers,practical experiments with regular approximation of contextfree languages,['Mark-Jan Nederhof'],introduction,,"A more subtle example is weighted FSAs that approximate PCFGs ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .","['w,x,y P (v|w)P (w, x)P (y|x)P (z|y), implemented by composing 4 machines. 6,7', 'here are also procedures for defining weighted FSTs that are not probabilistic (Berstel and Reutenauer, 1988).', 'Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 −→ arcs).', 'A more subtle example is weighted FSAs that approximate PCFGs ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .', ""These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution.""]",0,"['A more subtle example is weighted FSAs that approximate PCFGs ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .']"
CC1028,P02-1001,Parameter estimation for probabilistic finite-state transducers,an inequality and associated maximization technique in statistical estimation of probabilistic functions of a markov process,['L E Baum'],introduction,,"For example , the forward-backward algorithm ( #AUTHOR_TAG ) trains only Hidden Markov Models , while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance .","['Unfortunately, there is a stumbling block: Where do the weights come from?', 'After all, statistical models require supervised or unsupervised training.', 'Currently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'For example , the forward-backward algorithm ( #AUTHOR_TAG ) trains only Hidden Markov Models , while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance .']",0,"['Unfortunately, there is a stumbling block: Where do the weights come from?', 'After all, statistical models require supervised or unsupervised training.', 'For example , the forward-backward algorithm ( #AUTHOR_TAG ) trains only Hidden Markov Models , while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance .']"
CC1029,P02-1001,Parameter estimation for probabilistic finite-state transducers,expectation semirings flexible em for finitestate transducers,['Jason Eisner'],introduction,,"Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .","['o:e −→, and a:ae −→ share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']",0,"['The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']"
CC1030,P02-1001,Parameter estimation for probabilistic finite-state transducers,inducing features of random fields,"['S Della Pietra', 'V Della Pietra', 'J Lafferty']",,"We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing.","The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and","['• If arc probabilities (or even λ, ν, µ, ρ) have loglinear parameterization, then the E step must compute c = i ec f (x i , y i ), where ec(x, y) denotes the expected vector of total feature counts along a random path in f θ whose (input, output) matches (x, y).', 'The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f (Σ * , ∆ * ).', 'If the log-linear probabilities are conditioned on the state and/or the input, the predicted vector is harder to describe (though usually much easier to compute). 13']",5,"['The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f (S * ,  * ).']"
CC1031,P02-1001,Parameter estimation for probabilistic finite-state transducers,speech recognition by composition of weighted finite automata,"['Fernando C N Pereira', 'Michael Riley']",introduction,"We present a general framework based on weighted finite automata and weighted finite-state transducers for describing and implementing speech recognizers. The framework allows us to represent uniformly the information sources and data structures used in recognition, including context-dependent units, pronunciation dictionaries, language models and lattices. Furthermore, general but efficient algorithms can used for combining information sources in actual recognizers and for optimizing their application. In particular, a single composition algorithm is used both to combine in advance information sources such as language models and dictionaries, and to combine acoustic observations and information sources dynamically during recognition.","A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .","['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .', 'The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (λ); reading another b (ν); transducing b to p rather than q (µ); starting to transduce p to rather than x (ρ). 4 To prove (1)⇒(3), express f as an FST and apply the well-known Kleene-Schützenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'A full proof is straightforward, as are proofs of (3)⇒( 2), ( 2)⇒(1).']",0,"['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .']"
CC1032,P05-3005,Dynamically generating a protein entity dictionary using online resources,dr introducing refseq and locuslink curated human genome resources at the ncbi trends genet,"['Pruitt KD', 'Katz KS', 'H Sicotte', 'Maglott']",,,"The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq #AUTHOR_TAG , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and","['The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq #AUTHOR_TAG , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and', 'Additionally, several model organism databases or nomenclature databases were used.', 'Correspondences among records from these databases are identified using the rich cross-reference information provided by the iProClass database of PIR [14].', 'The following provides a brief description of each of the database.']",5,"['The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq #AUTHOR_TAG , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and']"
CC1033,P05-3005,Dynamically generating a protein entity dictionary using online resources,sgd saccharomyces genome database nucleic acids res,"['Cherry JM', 'C Adler', 'C Ball', 'Chervitz SA', 'Dwight SS', 'Hester ET', 'Y Jia', 'G Juvik', 'T Roe', 'M Schroeder']",,,"Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) #AUTHOR_TAG , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) #AUTHOR_TAG , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) #AUTHOR_TAG , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"
CC1034,P05-3005,Dynamically generating a protein entity dictionary using online resources,boddy wj et al the mouse genome database mgd integrating biology with the genome nucleic acids res,"['Bult CJ', 'Blake JA', 'Richardson JE', 'Kadin JA', 'Eppig JT', 'Baldarelli RM', 'K Barsanti', 'M Baya', 'Beal JS']",,,"Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) #AUTHOR_TAG , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) #AUTHOR_TAG , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) #AUTHOR_TAG , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"
CC1035,P05-3005,Dynamically generating a protein entity dictionary using online resources,al wormbase a multispecies resource for nematode biology and genomics nucleic acids res,"['Harris TW', 'N Chen', 'F Cunningham', 'M TelloRuiz', 'I Antoshechkin', 'C Bastiani', 'T Bieri', 'D Blasiar', 'K Bradnam', 'Chan J et']",,,"Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase #AUTHOR_TAG , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase #AUTHOR_TAG , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase #AUTHOR_TAG , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"
CC1036,P05-3005,Dynamically generating a protein entity dictionary using online resources,the unified medical language system umls integrating biomedical terminology,['O Bodenreider'],,"The Unified Medical Language System (http://umlsks.nlm.nih.gov) is a repository of biomedical vocabularies developed by the US National Library of Medicine. The UMLS integrates over 2 million names for some 900,000 concepts from more than 60 families of biomedical vocabularies, as well as 12 million relations among these concepts. Vocabularies integrated in the UMLS Metathesaurus include the NCBI taxonomy, Gene Ontology, the Medical Subject Headings (MeSH), OMIM and the Digital Anatomist Symbolic Knowledge Base. UMLS concepts are not only inter-related, but may also be linked to external resources such as GenBank. In addition to data, the UMLS includes tools for customizing the Metathesaurus (MetamorphoSys), for generating lexical variants of concept names (lvg) and for extracting UMLS concepts from text (MetaMap). The UMLS knowledge sources are updated quarterly. All vocabularies are available at no fee for research purposes within an institution, but UMLS users are required to sign a license agreement. The UMLS knowledge sources are distributed on CD-ROM and by FTP.",The UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM ) #AUTHOR_TAG .,"['The UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM ) #AUTHOR_TAG .', 'It contains three knowledge sources: the Metathesaurus (META), the SPECIALIST lexicon, and the Semantic Network.', 'The META provides a uniform, integrated platform for over 60 biomedical vocabularies and classifications, and group different names for the same concept.', 'The SPECIALIST lexicon contains syntactic information for many terms, component words, and English words, including verbs, which do not appear in the META.', 'The Semantic Network contains information about the types or categories (e.g., ""Disease or Syndrome"", ""Virus"") to which all META concepts have been assigned.']",0,['The UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM ) #AUTHOR_TAG .']
CC1037,P05-3005,Dynamically generating a protein entity dictionary using online resources,mining the biomedical literature in the genomic era an overview,"['H Shatkay', 'R Feldman']",introduction,"The past decade has seen a tremendous growth in the amount of experimental and computational biomedical data, specifically in the areas of genomics and proteomics. This growth is accompanied by an accelerated increase in the number of biomedical publications discussing the findings. In the last few years, there has been a lot of interest within the scientific community in literature-mining tools to help sort through this abundance of literature and find the nuggets of information most relevant and useful for specific analysis tasks. This paper provides a road map to the various literature-mining methods, both in general and within bioinformatics. It surveys the disciplines involved in unstructured-text analysis, categorizes current work in biomedical literature mining with respect to these disciplines, and provides examples of text analysis methods applied towards meeting some of the current challenges in bioinformatics.","With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG .","['With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG .', 'One requirement for NLP is the ability to accurately recognize terms that represent biological entities in free text.', 'Another requirement is the ability to associate these terms with corresponding biological entities (i.e., records in biological databases) in order to be used by other automated systems for literature mining.', 'Such task is called biological entity tagging.', 'Biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely: synonymy (i.e., different terms refer to the same entity), ambiguity (i.e., one term is associated with different entities), and coverage (i.e., entity terms or entities are not present in databases or knowledge bases).']",0,"['With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG .', 'One requirement for NLP is the ability to accurately recognize terms that represent biological entities in free text.', 'Another requirement is the ability to associate these terms with corresponding biological entities (i.e., records in biological databases) in order to be used by other automated systems for literature mining.', 'Such task is called biological entity tagging.', 'Biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely: synonymy (i.e., different terms refer to the same entity), ambiguity (i.e., one term is associated with different entities), and coverage (i.e., entity terms or entities are not present in databases or knowledge bases).']"
CC1038,P05-3005,Dynamically generating a protein entity dictionary using online resources,online mendelian inheritance in man omim a knowledgebase of human genes and genetic disorders nucleic acids res,"['A Hamosh', 'Scott AF', 'Amberger JS', 'Bocchini CA', 'McKusick VA']",,"Online Mendelian Inheritance in Man (OMIM) is a comprehensive, authoritative and timely knowledgebase of human genes and genetic disorders compiled to support human genetics research and education and the practice of clinical genetics. Started by Dr Victor A. McKusick as the definitive reference Mendelian Inheritance in Man, OMIM (http://www.ncbi.nlm.nih.gov/omim/) is now distributed electronically by the National Center for Biotechnology Information, where it is integrated with the Entrez suite of databases. Derived from the biomedical literature, OMIM is written and edited at Johns Hopkins University with input from scientists and physicians around the world. Each OMIM entry has a full-text summary of a genetically determined phenotype and/or gene and has numerous links to other genetic databases such as DNA and protein sequence, PubMed references, general and locus-specific mutation databases, HUGO nomenclature, MapViewer, GeneTests, patient support groups and many others. OMIM is an easy and straightforward portal to the burgeoning information in human genetics.","Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) #AUTHOR_TAG , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) #AUTHOR_TAG , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) #AUTHOR_TAG , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"
CC1039,P05-3005,Dynamically generating a protein entity dictionary using online resources,godzik a clustering of highly homologous sequences to reduce the size of large protein databases bioinformatics,"['W Li', 'L Jaroszewski']",,"We present a fast and flexible program for clustering large protein databases at different sequence identity levels. It takes less than 2 h for the all-against-all sequence comparison and clustering of the non-redundant protein database of over 560,000 sequences on a high-end PC. The output database, including only the representative sequences, can be used for more efficient and sensitive database searches.","Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #AUTHOR_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence .","['PIR Resources -There are three databases in PIR: the Protein Sequence Database (PSD), iProClass, and PIR-NREF.', 'PSD database includes functionally annotated protein sequences.', 'The iProClass database is a central point for exploration of protein information, which provides summary descriptions of protein family, function and structure for all protein sequences from PIR, Swiss-Prot, and TrEMBL (now UniProt).', 'Additionally, it links to over 70 biological databases in the world.', 'The PIR-NREF database is a comprehensive database for sequence searching and protein identification.', 'It contains non-redundant protein sequences from PSD, Swiss-Prot, TrEMBL, RefSeq, GenPept, and PDB.', 'Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #AUTHOR_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence .', 'NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE.', 'GenPept entries are those translated from the GenBanknucleotide sequence database.', 'RefSeq is a comprehensive, integrated, non-redundant set of sequences, including genomic DNA, transcript (RNA), and protein products, for major research organisms.', ""Entrez GENE provides a unified query environment for genes defined by sequence and/or in NCBI's Map Viewer."", 'It records gene names, symbols, and many other attributes associated with genes and the products they encode.']",5,"['The PIR-NREF database is a comprehensive database for sequence searching and protein identification.', 'Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #AUTHOR_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence .']"
CC1040,P05-3005,Dynamically generating a protein entity dictionary using online resources,kwitek a et al rat genome database rgd mapping disease onto the genome nucleic acids res,"['S Twigger', 'J Lu', 'M Shimoyama', 'D Chen', 'D Pasko', 'H Long', 'J Ginster', 'Chen CF', 'R Nigam']",,,"Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) #AUTHOR_TAG , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) #AUTHOR_TAG , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) #AUTHOR_TAG , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"
CC1041,P05-3005,Dynamically generating a protein entity dictionary using online resources,suzek be et al the protein information resource nucleic acids res,"['Wu CH', 'Yeh LS', 'H Huang', 'L Arminski', 'J Castro-Alvear', 'Y Chen', 'Z Hu', 'P Kourtesis', 'Ledley RS']",,,"The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq [ 12 ] , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) #AUTHOR_TAG , and","['The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq [ 12 ] , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) #AUTHOR_TAG , and', 'Additionally, several model organism databases or nomenclature databases were used.', 'Correspondences among records from these databases are identified using the rich cross-reference information provided by the iProClass database of PIR [14].', 'The following provides a brief description of each of the database.']",5,"['The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq [ 12 ] , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) #AUTHOR_TAG , and']"
CC1042,P05-3005,Dynamically generating a protein entity dictionary using online resources,the flybase database of the drosophila genome projects and community literature nucleic acids res,['F Consortium'],,"FlyBase (http://flybase.bio.indiana.edu/) provides an integrated view of the fundamental genomic and genetic data on the major genetic model Drosophila melanogaster and related species. FlyBase has primary responsibility for the continual reannotation of the D. melanogaster genome. The ultimate goal of the reannotation effort is to decorate the euchromatic sequence of the genome with as much biological information as is available from the community and from the major genome project centers. A complete revision of the annotations of the now-finished euchromatic genomic sequence has been completed. There are many points of entry to the genome within FlyBase, most notably through maps, gene products and ontologies, structured phenotypic and gene expression data, and anatomy.","Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase #AUTHOR_TAG , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase #AUTHOR_TAG , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase #AUTHOR_TAG , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"
CC1043,P05-3005,Dynamically generating a protein entity dictionary using online resources,enzyme nomenclature functional or structural rna,['P Gegenheimer'],,"Altman and colleagues (this issue) call attention to the inability of current standardized enzyme nomenclature to distinguish between enzymatic activities that reside in nonhomologous macromolecules+ This issue is highlighted by the fact that the pre-tRNA 59-maturation activities of bacteria and plant chloroplasts present the first instance (of which I am aware) of two naturally occurring enzymes that cannot be evolutionarily related, but which catalyze an identical reaction+ (In the classic example of convergent evolution between the trypsin family and subtilisin, the enzymes do not have an identical substrate specificity+) Altman and colleagues propose that a single trivial name be used only for members of a family of homologous macromolecules; in other words, that different trivial names be given to enzymes that catalyze the same precursor-product conversion but do so with different catalytic mechanisms, or which are not members of a single family of homologous macromolecules+ I am not convinced that there is a problem needing solution+ The current proposal seems to run counter to the rationale behind current EC nomenclature, and could create more confusion than it would alleviate+ One can distinguish between a function-based nomenclature based on the biochemical reaction catalyzed--the substrate-product conversion--and a structure-based nomenclature based on the physical nature of the catalyst+ For a classical enzymologist, the reaction type being catalyzed is paramount: It is the reaction that one uses to purify the enzyme+ One identifies the enzyme based on its activity,whereas its physical structure may initially be of secondary importance+ The value of function-based nomenclature is precisely that it allows the biochemical reaction (the substrate- product conversion) to be described, specified, and studied concomitant with continuing purification and analysis of the corresponding enzyme+ Further, as more is learned about the enzyme's structure and catalytic mechanism, it is not necessary to rename it+ Indeed, the utility of function-based nomenclature is exemplified by the history of bacterial RNase P purification and characterization+","Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) #AUTHOR_TAG .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) #AUTHOR_TAG .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) #AUTHOR_TAG .']"
CC1044,P07-1007,Estimating class priors in domain adaptation for word sense disambiguation,an empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation,"['Y K Lee', 'H T Ng']",,"In this paper, we evaluate a variety  of knowledge sources and supervised  learning algorithms for word sense  disambiguation on SENSEVAL-2 and  SENSEVAL-1 data. Our knowledge  sources include the part-of-speech of  neighboring words, single words in the  surrounding context, local collocations,  and syntactic relations. The learning algorithms  evaluated include Support Vector  Machines (SVM), Naive Bayes, AdaBoost,  and decision tree algorithms. We  present empirical results showing the relative  contribution of the component knowledge  sources and the different learning  algorithms. In particular, using all of  these knowledge sources and SVM (i.e.,  a single learning algorithm) achieves accuracy  higher than the best official scores  on both SENSEVAL-2 and SENSEVAL-1  test data",These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( #AUTHOR_TAG ) .,"['For our experiments, we use naive Bayes as the learning algorithm.', 'The knowledge sources we use include parts-of-speech, local collocations, and surrounding words.', 'These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( #AUTHOR_TAG ) .']",2,"['The knowledge sources we use include parts-of-speech, local collocations, and surrounding words.', 'These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( #AUTHOR_TAG ) .']"
CC1045,P07-1068,Advanced Machine Learning Models for Coreference Resolution,the nonutility of predicateargument frequencies for pronoun interpretation,"['A Kehler', 'D Appelt', 'L Taylor', 'A Simma']",introduction,,"While these approaches have been reasonably successful ( see Mitkov ( 2002 ) ) , #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .","['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful ( see Mitkov ( 2002 ) ) , #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).']",0,"['While these approaches have been reasonably successful ( see Mitkov ( 2002 ) ) , #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .']"
CC1046,P07-1068,Advanced Machine Learning Models for Coreference Resolution,a mentionsynchronous coreference resolution algorithm based on the bell tree,"['X Luo', 'A Ittycheriah', 'H Jing', 'N Kambhatla', 'S Roukos']",introduction,This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.,"More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #AUTHOR_TAG ); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.","['Many ACE participants have also adopted a corpus-based approach to SC deter- mination that is investigated as part of the mention detection (MD) task (e.g., Florian et al. (2006)).', 'Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).', 'Un- like them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowl- edge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC clas- sifier; instead, we use the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and an- notated with their SCs.', 'This provides us with a train- ing set that is approximately five times bigger than that of ACE.', 'More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #AUTHOR_TAG ); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.']",1,"['More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #AUTHOR_TAG ); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.']"
CC1047,P07-1068,Advanced Machine Learning Models for Coreference Resolution,a corpusbased evaluation of centering and pronoun resolution,['J Tetreault'],introduction,,"In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , Mitkov ( 1998 ) , #AUTHOR_TAG ) .","['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , Mitkov ( 1998 ) , #AUTHOR_TAG ) .', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).']",0,"['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , Mitkov ( 1998 ) , #AUTHOR_TAG ) .', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).']"
CC1048,P07-1068,Advanced Machine Learning Models for Coreference Resolution,bbn pronoun coreference and entity type corpus linguistica data consortium,"['R Weischedel', 'A Brunstein']",introduction,,"Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #AUTHOR_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.","['Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task (e.g., Florian et al. (2006)).', 'Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).', 'Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #AUTHOR_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.', 'This provides us with a training set that is approximately five times bigger than that of ACE.', 'More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., Luo et al. (2004)); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.']",5,"['Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #AUTHOR_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.']"
CC1049,P07-1068,Advanced Machine Learning Models for Coreference Resolution,anaphora resolution,['R Mitkov'],introduction,"In anaphora resolution for English, animacy identification can play an integral role in the application of agreement restrictions between pronouns and candidates, and as a result, can improve the accuracy of anaphora resolution systems. In this paper, two methods for animacy identification are proposed and evaluated using intrinsic and extrinsic measures. The first method is a rule-based one which uses information about the unique beginners in WordNet to classify NPs on the basis of their animacy. The second method relies on a machine learning algorithm which exploits a WordNet enriched with animacy information for each sense. The effect of word sense disambiguation on the two methods is also assessed. The intrinsic evaluation reveals that the machine learning method reaches human levels of performance. The extrinsic evaluation demonstrates that animacy identification can be beneficial in anaphora resolution, especially in the cases where animate entities are identified with high precision","While these approaches have been reasonably successful ( see #AUTHOR_TAG ) , Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .","['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful ( see #AUTHOR_TAG ) , Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).']",0,"['While these approaches have been reasonably successful ( see #AUTHOR_TAG ) , Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .']"
CC1050,P07-1068,Advanced Machine Learning Models for Coreference Resolution,libsvm a library for support vector machines software available at httpwwwcsientuedutw∼cjlinlibsvm,"['C-C Chang', 'C-J Lin']",introduction,,"Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.�s method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #AUTHOR_TAG )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features.","['In addition, we train an SVM classifier for SC determination by combining the output of five clas- sification methods: DL, 1-NN, ME, NB, and Soon et al.�s method as described in the introduction,8 with the goal of examining whether SC classifica- tion accuracy can be improved by combining the output of individual classifiers in a supervised man- ner.', 'Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.�s method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #AUTHOR_TAG )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features.', 'More specifically, let L = i li\x0e ; li2 ; li3 ; li4 ; li5 be the set of predictions that we obtained for i in step (2).', 'To represent i, we generate one feature from each non-empty subset of Li.']",5,"['In addition, we train an SVM classifier for SC determination by combining the output of five clas- sification methods: DL, 1-NN, ME, NB, and Soon et al.s method as described in the introduction,8 with the goal of examining whether SC classifica- tion accuracy can be improved by combining the output of individual classifiers in a supervised man- ner.', 'Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.�s method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #AUTHOR_TAG )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features.', 'More specifically, let L = i li\x0e ; li2 ; li3 ; li4 ; li5 be the set of predictions that we obtained for i in step (2).', 'To represent i, we generate one feature from each non-empty subset of Li.']"
CC1051,P07-1068,Advanced Machine Learning Models for Coreference Resolution,an algorithm that learns what’s in a name,"['D M Bikel', 'R Schwartz', 'R M Weischedel']",introduction,"In this paper, we present IdentiFinderTM, a hidden Markov model that learns to recognize and classify names, dates, times, and numerical quantities. We have evaluated the model in English (based on data from the Sixth and Seventh Message Understanding Conferences [MUC-6, MUC-7] and broadcast news) and in Spanish (based on data distributed through the First Multilingual Entity Task [MET-1]), and on speech input (based on broadcast news). We report results here on standard materials only to quantify performance on data available to the community, namely, MUC-6 and MET-1. Results have been consistently better than reported by any other learning algorithm. IdentiFinder's performance is competitive with approaches based on handcrafted rules on mixed case text and superior on text where case information is not available. We also present a controlled experiment showing the effect of training set size on performance, demonstrating that as little as 100,000 words of training data is adequate to get performance around 90% on newswire. Although we present our understanding of why this algorithm performs so well on this class of problems, we believe that significant improvement in performance may still be possible.","( 4 ) NE : We use BBN 's IdentiFinder ( #AUTHOR_TAG ) , a MUC-style NE recognizer to determine the NE type of NPZ .","[""( 4 ) NE : We use BBN 's IdentiFinder ( #AUTHOR_TAG ) , a MUC-style NE recognizer to determine the NE type of NPZ ."", 'If NPi is determined to be a PERSON or ORGANIZATION, we create an NE feature whose value is simply its MUC NE type.', 'However, if NPi is determined to be a LOCATION, we create a feature with value GPE (because most of the MUC LOCA- TION NEs are ACE GPE NEs).', 'Otherwise, no NE feature will be created (because we are not interested in the other MUC NE types).']",5,"[""( 4 ) NE : We use BBN 's IdentiFinder ( #AUTHOR_TAG ) , a MUC-style NE recognizer to determine the NE type of NPZ ."", 'If NPi is determined to be a PERSON or ORGANIZATION, we create an NE feature whose value is simply its MUC NE type.', 'Otherwise, no NE feature will be created (because we are not interested in the other MUC NE types).']"
CC1052,P07-1068,Advanced Machine Learning Models for Coreference Resolution,comparing knowledge sources for nominal anaphora resolution,"['K Markert', 'M Nissim']",introduction,"We compare two ways of obtaining lexical knowledge for antecedent selection in other-anaphora  and definite noun phrase coreference. Specifically, we compare an algorithm that relies on links  encoded in the manually created lexical hierarchy WordNet and an algorithm that mines corpora  by means of shallow lexico-semantic patterns. As corpora we use the British National  Corpus (BNC), as well as the Web, which has not been previously used for this task. Our  results show that (a) the knowledge encoded in WordNet is often insufficient, especially for  anaphor-antecedent relations that exploit subjective or context-dependent knowledge; (b) for  other-anaphora, the Web-based method outperforms the WordNet-based method; (c) for definite  NP coreference, the Web-based method yields results comparable to those obtained using  WordNet over the whole dataset and outperforms the WordNet-based method on subsets of the  dataset; (d) in both case studies, the BNC-based method is worse than the other methods because  of data sparseness. Thus, in our studies, the Web-based method alleviated the lexical knowledge  gap often encountered in anaphora resolution, and handled examples with context-dependent relations  between anaphor and antecedent. Because it is inexpensive and needs no hand-modelling  of lexical knowledge, it is a promising knowledge source to integrate in anaphora resolution systems","However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , Soon et al. ( 2001 ) , #AUTHOR_TAG ) .","['Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs.', 'However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , Soon et al. ( 2001 ) , #AUTHOR_TAG ) .', ""It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al.'s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic.""]",0,"['However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , Soon et al. ( 2001 ) , #AUTHOR_TAG ) .']"
CC1053,P07-1068,Advanced Machine Learning Models for Coreference Resolution,unsupervised models for named entity classification,"['M Collins', 'Y Singer']",introduction,"This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of la-beled examples should be required to train a classi-fier. However, we show that the use of unlabeled data can reduce the requirements for supervision to just 7 simple ""seed "" rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type. We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98).","We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #AUTHOR_TAG , motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and NE classification ( Collins and Singer , 1999 ) .","['Learning algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #AUTHOR_TAG , motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and NE classification ( Collins and Singer , 1999 ) .', 'We apply add-one smoothing to smooth the class posteriors.']",5,"['We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #AUTHOR_TAG , motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and NE classification ( Collins and Singer , 1999 ) .']"
CC1054,P07-1068,Advanced Machine Learning Models for Coreference Resolution,a modeltheoretic coreference scoring scheme,"['M Vilain', 'J Burger', 'J Aberdeen', 'D Connolly', 'L Hirschman']",,,"We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #AUTHOR_TAG ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved .","['As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #AUTHOR_TAG ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved .', 'Following Ponzetto and Strube (2006), we consider an anaphoric reference, NP i , correctly resolved if NP i and its closest antecedent are in the same coreference chain in the resulting partition.', 'In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder.']",5,"['As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #AUTHOR_TAG ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved .']"
CC1055,P07-1068,Advanced Machine Learning Models for Coreference Resolution,a machine learning approach to coreference resolution of noun phrases,"['W M Soon', 'H T Ng', 'D Lim']",,"this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data set","Following #AUTHOR_TAG , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj.","['After training, the decision tree classifier is used to select an antecedent for each NP in a test text.', 'Following #AUTHOR_TAG , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj.', 'If no such NP exists, no antecedent is selected for NPj.']",4,"['After training, the decision tree classifier is used to select an antecedent for each NP in a test text.', 'Following #AUTHOR_TAG , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj.', 'If no such NP exists, no antecedent is selected for NPj.']"
CC1056,P07-1068,Advanced Machine Learning Models for Coreference Resolution,factorizing complex models a case study in mention detection,"['R Florian', 'H Jing', 'N Kambhatla', 'I Zitouni']",introduction,"As natural language understanding research advances towards deeper knowledge modeling, the tasks become more and more complex: we are interested in more nuanced word characteristics, more linguistic properties, deeper semantic and syntactic features. One such example, explored in this article, is the mention detection and recognition task in the Automatic Content Extraction project, with the goal of identifying named, nominal or pronominal references to real-world entities---mentions---and labeling them with three types of information: entity type, entity subtype and mention type. In this article, we investigate three methods of assigning these related tags and compare them on several data sets. A system based on the methods presented in this article participated and ranked very competitively in the ACE'04 evaluation.","Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #AUTHOR_TAG ) .","['Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #AUTHOR_TAG ) .', 'Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).', 'Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.', 'This provides us with a training set that is approximately five times bigger than that of ACE.', 'More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., Luo et al. (2004)); and for those that do report performance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.']",1,"['Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #AUTHOR_TAG ) .']"
CC1057,P07-1068,Advanced Machine Learning Models for Coreference Resolution,coreference resolution using competitive learning approach,"['X Yang', 'G Zhou', 'J Su', 'C L Tan']",,"In this paper we propose a competition learning approach to coreference resolution. Traditionally, supervised machine learning approaches adopt the single-candidate model. Nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model. By contrast, our approach adopts a twin-candidate learning model. Such a model can present the competition criterion for antecedent candidates reliably, and ensure that the most preferred candidate is selected. Furthermore, our approach applies a candidate filter to reduce the computational cost and data noises during training and resolution. The experimental results on MUC-6 and MUC-7 data set show that our approach can outperform those based on the single-candidate model.","Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\x0e, NPi+2, � � �, NPj positional features that have been employed by high- performing resolvers such as Ng and Cardie (2002) and #AUTHOR_TAG , as described below .","['Our baseline coreference system uses the C4.5 deci- sion tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\x0e, NPi+2, � � �, NPj positional features that have been employed by high- performing resolvers such as Ng and Cardie (2002) and #AUTHOR_TAG , as described below .']",1,"['Our baseline coreference system uses the C4.5 deci- sion tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\\x0e, NPi+2, � � �, NPj positional features that have been employed by high- performing resolvers such as Ng and Cardie (2002) and #AUTHOR_TAG , as described below .', 'Our baseline coreference system uses the C4.5 deci- sion tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.']"
CC1058,P07-1068,Advanced Machine Learning Models for Coreference Resolution,c45 programs for machine learning,['J R Quinlan'],,,Our baseline coreference system uses the C4 .5 decision tree learner ( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two NPs are coreferent .,"['Our baseline coreference system uses the C4 .5 decision tree learner ( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two NPs are coreferent .', 'Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NP j , and its closest antecedent, NP i ; and a negative instance is created for NP j paired with each of the intervening NPs, NP i+1 , NP i+2 , . .', '., NP j−1 .', 'Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below.']",5,"['Our baseline coreference system uses the C4 .5 decision tree learner ( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two NPs are coreferent .', '., NP j-1 .', 'Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below.']"
CC1059,P07-1068,Advanced Machine Learning Models for Coreference Resolution,improving machine learning approaches to coreference resolution,"['V Ng', 'C Cardie']",,"We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC-6 and MUC-7 coreference resolution data sets --- F-measures of 70.4 and 63.4, respectively. Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge.","Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as #AUTHOR_TAG and Yang et al. (2003), as described below.","['Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\x0e, NPi+2, � � �, NPj.', 'Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as #AUTHOR_TAG and Yang et al. (2003), as described below.']",1,"['Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as #AUTHOR_TAG and Yang et al. (2003), as described below.']"
CC1060,P07-1068,Advanced Machine Learning Models for Coreference Resolution,exploiting semantic role labeling wordnet and wikipedia for coreference resolution,"['S P Ponzetto', 'M Strube']",,"In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.","Following #AUTHOR_TAG , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .","['As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonly-used MUC scorer (Vilain et al., 1995), and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'Following #AUTHOR_TAG , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .', 'In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder.']",5,"['Following #AUTHOR_TAG , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .']"
CC1061,P07-1068,Advanced Machine Learning Models for Coreference Resolution,unsupervised learning of contextual role knowledge for coreference resolution,"['D Bean', 'E Riloff']",introduction,"We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a Dempster-Shafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns.","As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see #AUTHOR_TAG ) .","['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see #AUTHOR_TAG ) .']",0,"['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see #AUTHOR_TAG ) .']"
CC1062,P07-1068,Advanced Machine Learning Models for Coreference Resolution,a machine learning approach to coreference resolution of noun phrases,"['W M Soon', 'H T Ng', 'D Lim']",introduction,"this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data set","However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #AUTHOR_TAG , Markert and Nissim ( 2005 ) ) .","['Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs.', 'However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #AUTHOR_TAG , Markert and Nissim ( 2005 ) ) .', ""It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al.'s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic.""]",0,"['Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs.', 'However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #AUTHOR_TAG , Markert and Nissim ( 2005 ) ) .', ""It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al.'s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic.""]"
CC1063,P07-1068,Advanced Machine Learning Models for Coreference Resolution,a machine learning approach to coreference resolution of noun phrases,"['W M Soon', 'H T Ng', 'D Lim']",,"this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data set","Following previous work ( e.g. , #AUTHOR_TAG and Ponzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 .","['Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work ( e.g. , #AUTHOR_TAG and Ponzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 .', '., NP j−1 .', 'Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below.']",5,"['Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work ( e.g. , #AUTHOR_TAG and Ponzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 .', 'Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below.']"
CC1064,P07-1068,Advanced Machine Learning Models for Coreference Resolution,unsupervised word sense disambiguation rivaling supervised methods,['D Yarowsky'],introduction,"This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints -- that words tend to have one sense per discourse and one sense per collocation -- exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%","We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .","['Learning algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .', 'We apply add-one smoothing to smooth the class posteriors.']",4,"['We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .']"
CC1065,P07-1068,Advanced Machine Learning Models for Coreference Resolution,exploiting semantic role labeling wordnet and wikipedia for coreference resolution,"['S P Ponzetto', 'M Strube']",introduction,"In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.","As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .","['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']",0,"['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']"
CC1066,P07-1068,Advanced Machine Learning Models for Coreference Resolution,using semantic relations to refine coreference decisions,"['H Ji', 'D Westbrook', 'R Grishman']",introduction,We present a novel mechanism for improving reference resolution by using the output of a relation tagger to rescore coreference hypotheses. Experiments show that this new framework can improve performance on two quite different languages - English and Chinese.,"As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .","['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']",0,"['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']"
CC1067,P07-1068,Advanced Machine Learning Models for Coreference Resolution,automatic retrieval and clustering of similar words,['D Lin'],introduction,Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.,( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .,"['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'Again, this represents our attempt to coarsely model subcategorization.', ""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1, while others appear to be correlated with these ACE SCs. 2 (6) INDUCED CLASS: Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP, we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics (e.g., Hearst (1992)).', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.', 'An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.', 'We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.', '( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]",4,"['( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .', ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]"
CC1068,P07-1068,Advanced Machine Learning Models for Coreference Resolution,automatic acquisition of hyponyms from large text corpora,['M Hearst'],introduction,"We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested. 1 Introduction  Currently there is much interest in the automatic acquisition of lexical syntax and semantics, with the goal of building up large lexicons for natural language processing. Projects..","These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .","['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'Again, this represents our attempt to coarsely model subcategorization.', ""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.', 'An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.', 'We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.', '(7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a)).', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]",4,"['Again, this represents our attempt to coarsely model subcategorization.', ""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.', 'We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.', '(7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a)).', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]"
CC1069,P10-2059,Classification of Feedback Expressions in Multimodal Data,contextual recognition of head gestures,"['Louis-Philippe Morency', 'Candace Sidner', 'Christopher Lee', 'Trevor Darrell']",introduction,"Head pose and gesture offer several key conversational grounding cues and are used extensively in face-to-face interaction among people. We investigate how dialog context from an embodied conversational agent (ECA) can improve visual recognition of user gestures. We present a recogntion framework which (1) extracts contextual features from an ECA's dialog manager, (2) computes a predicition of head nod and head shakes, and (3) integrates the contextual predictions with the visual observation of a vision-based head gesture recognizer. We found a subset of lexical, punctuation and timing features that are easily available in most ECA architectures and can be used to learn how to predict user feedback. Using a discriminative approach to contextual prediction and multi-modal integration, we were able to improve the performancae of head gesture detection even when the topic of the test set was significantly different than the training set","Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ) .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ) .', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",0,"['Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ) .', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']"
CC1070,P10-2059,Classification of Feedback Expressions in Multimodal Data,the mumin coding scheme for the annotation of feedback turn management and sequencing multimodal corpora for modelling human multimodal behaviour,"['Jens Allwood', 'Loredana Cerrato', 'Kristiina Jokinen', 'Costanza Navarretta', 'Patrizia Paggio']",,,All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme ( #AUTHOR_TAG ) .,"['All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme ( #AUTHOR_TAG ) .', 'The MU-MIN scheme is a general framework for the study of gestures in interpersonal communication.', 'In this study, we do not deal with functional classification of the gestures in themselves, but rather with how gestures contribute to the semantic interpretations of linguistic expressions.', 'Therefore, only a subset of the MUMIN attributes has been used, i.e.', 'Smile, Laughter, Scowl, FaceOther for facial expressions, and Nod, Jerk, Tilt, SideTurn, Shake, Waggle, Other for head movements.']",5,"['All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme ( #AUTHOR_TAG ) .', 'The MU-MIN scheme is a general framework for the study of gestures in interpersonal communication.']"
CC1071,P10-2059,Classification of Feedback Expressions in Multimodal Data,turnyielding cues in taskoriented dialogue,"['Agustin Gravano', 'Julia Hirschberg']",introduction,,"Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",0,"['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.']"
CC1072,P10-2059,Classification of Feedback Expressions in Multimodal Data,a conversation robot using head gesture recognition as paralinguistic information,"['Shinya Fujie', 'Y Ejiri', 'K Nakajima', 'Y Matsusaka', 'Tetsunor Kobayashi']",introduction,"A conversation robot that recognizes user's head gestures and uses its results as para-linguistic information is developed. In the conversation, humans exchange linguistic information, which can be obtained by transcription of the utterance, and para-linguistic information, which helps the transmission of linguistic information. Para-linguistic information brings a nuance that cannot be transmitted by linguistic information, and the natural and effective conversation is realized. We recognize user's head gestures as the para-linguistic information in the visual channel. We use the optical flow over the head region as the feature and model them using HMM for the recognition. In actual conversation, while the user performs a gesture, the robot may perform a gesture, too. In this situation, the image sequence captured by the camera mounted on the eyes of the robot includes sways caused by the movement of the camera. To solve this problem, we introduced two artifices. One is for the feature extraction: the optical flow of the body area is used to compensate the swayed images. The other is for the probability models: mode-dependent models are prepared by the MLLR model adaptation technique, and the models are switched according to the motion mode of the robot. Experimental results show the effectiveness of these techniques.","Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) .', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",0,"['Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) .']"
CC1073,P10-2059,Classification of Feedback Expressions in Multimodal Data,linguistic functions of head movements in the context of speech,['Evelyn McClave'],introduction,,Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG for an overview ) .,"['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG for an overview ) .', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus.']",0,['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG for an overview ) .']
CC1074,P10-2059,Classification of Feedback Expressions in Multimodal Data,coefficient kappa some uses misuses and alternatives,"['Robert L Brennan', 'Dale J Prediger']",introduction,"This paper considers some appropriate and inappropriate uses of coefficient kappa and alternative kappa-like statistics. Discussion is restricted to the descriptive characteristics of these statistics for measuring agreement with categorical data in studies of reliability and validity. Special consideration is given to assumptions about whether marginals are fixed a priori, or free to vary. In reliability studies, when marginals are fixed, coefficient kappa is found to be appropriate. When either or both of the marginals are free to vary, however, it is suggested that the ""chance"" term in kappa be replaced by 1/n, where n is the number of categories. In validity studies, we suggest considering whether one wants an index of improvement beyond ""chance"" or beyond the best a priori strategy employing base rates. In the former case, considerations are similar to those in reliability studies with the marginals for the criterion measure considered as fixed. In the latter case, it is suggested that the largest marginal proportion for the criterion measure be used in place of the ""chance"" term in kappa. Similarities and differences among these statistics are discussed and illustrated with synthetic data.","Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2 .","['In general, dialogue act, agreement and turn anno- tations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement.', 'A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL.', ""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2 ."", 'Anvil divides the annotations in slices and compares each slice.', 'We used slices of 0.04 seconds.', 'The inter-coder agreement figures obtained for the three types of annotation are given in Table 2.']",5,"[""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2 .""]"
CC1075,P10-2059,Classification of Feedback Expressions in Multimodal Data,the hcrc map task corpus language and speech,"['Anne H Anderson', 'Miles Bader', 'Ellen Gurman Bard', 'Elizabeth Boyle', 'Gwyneth Doherty', 'Simon Garrod', 'Stephen Isard', 'Jacqueline Kowtko', 'Jan McAllister', 'Jim Miller', 'Catherine Sotillo', 'Henry S Thompson', 'Regina Weinert']",introduction,,"Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers .', 'Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",0,"['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers .', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']"
CC1076,P10-2059,Classification of Feedback Expressions in Multimodal Data,data mining practical machine learning tools and techniques,"['Ian H Witten', 'Eibe Frank']",,"As with any burgeoning technology that enjoys commercial attention, the use of data mining is surrounded by a great deal of hype. Exaggerated reports tell of secrets that can be uncovered by setting algorithms loose on oceans of data. But there is no magic in machine learning, no hidden power, no alchemy. Instead there is an identifiable body of practical techniques that can extract useful information from raw data. This book describes these techniques and shows how they work. The book is a major revision of the first edition that appeared in 1999. While the basic core remains the sam","These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #AUTHOR_TAG ) .","['The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4.', 'These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #AUTHOR_TAG ) .', 'We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table .', 'The best results on most of our data were obtained using Hidden Naive Bayes (HNB) (Zhang et al., 2005).', 'Therefore, here we show the results of this classifier.', 'Ten-folds crossvalidation was applied throughout.']",5,"['These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #AUTHOR_TAG ) .', 'We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table .', 'The best results on most of our data were obtained using Hidden Naive Bayes (HNB) (Zhang et al., 2005).', 'Therefore, here we show the results of this classifier.', 'Ten-folds crossvalidation was applied throughout.']"
CC1077,P10-2059,Classification of Feedback Expressions in Multimodal Data,combining lexical syntactic and prosodic cues for improved online dialog act tagging,"['Vivek Kumar Rangarajan Sridhar', 'Srinivas Bangaloreb', 'Shrikanth Narayanan']",introduction,,"#AUTHOR_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', '#AUTHOR_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",0,"['#AUTHOR_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .']"
CC1078,P10-2059,Classification of Feedback Expressions in Multimodal Data,a coefficient of agreement for nominal scales,['Jacob Cohen'],introduction,"CONSIDER Table 1. It represents in its formal characteristics a situation which arises in the clinical-social-personality areas of psychology, where it frequently occurs that the only useful level of measurement obtainable is nominal scaling (Stevens, 1951, pp. 2526), i.e. placement in a set of k unordered categories. Because the categorizing of the units is a consequence of some complex judgment process performed by a &dquo;two-legged meter&dquo; (Stevens, 1958), it becomes important to determine the extent to which these judgments are reproducible, i.e., reliable. The procedure which suggests itself is that of having two (or more) judges independently categorize a sample of units and determine the degree, significance, and","Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #AUTHOR_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 .","['In general, dialogue act, agreement and turn annotations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement.', 'A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL.', ""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #AUTHOR_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 ."", 'Anvil divides the annotations in slices and compares each slice.', 'We used slices of 0.04 seconds.', 'The inter-coder agreement figures obtained for the three types of annotation are given in Table 2.']",5,"['A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL.', ""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #AUTHOR_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 .""]"
CC1079,P10-2059,Classification of Feedback Expressions in Multimodal Data,head gestures for perceptual interfaces the role of context in improving recognition,"['Louis-Philippe Morency', 'Candace Sidner', 'Christopher Lee', 'Trevor Darrell']",introduction,"AbstractHead pose and gesture offer several conversational grounding cues and are used extensively in face-to-face interaction among people. To accurately recognize visual feedback, humans often use contextual knowledge from previous and current events to anticipate when feedback is most likely to occur. In this paper we describe how contextual information can be used to predict visual feedback and improve recognition of head gestures in human-computer interfaces. Lexical, prosodic, timing, and gesture features can be used to predict a user's visual feedback during conversational dialog with a robotic or virtual agent. In non-conversational interfaces, context features based on user-interface system events can improve detection of head gestures for dialog box confirmation or document browsing. Our user study with prototype gesture-based components indicate quantitative and qualitative benefits of gesture-based confirmation over conventional alternatives. Using a discriminative approach to contextual prediction and multi-modal integration, performance of head gesture detection was improved with context features even when the topic of the test set was significantly different than the training set","Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ) .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ) .', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",0,"['Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ) .', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']"
CC1080,P10-2059,Classification of Feedback Expressions in Multimodal Data,intercoder agreement for computational linguistics,"['Ron Artstein', 'Massimo Poesio']",introduction,,"Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971).","['It should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'For example, a yes can be an Answer to a question, an Agree and a TurnElicit at the same time, thus making the semantic classification very fine-grained.', ""Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971)."", 'Looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by ANVIL from the latest annotated element.']",0,"[""Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971).""]"
CC1081,P10-2059,Classification of Feedback Expressions in Multimodal Data,detecting action meetings in meetings,"['Gabriel Murray', 'Steve Renals']",introduction,,Related are also the studies by Rieks op den Akker and Schulz ( 2008 ) and #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus .,"['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'Related are also the studies by Rieks op den Akker and Schulz ( 2008 ) and #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus .']",0,['Related are also the studies by Rieks op den Akker and Schulz ( 2008 ) and #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus .']
CC1082,P10-2059,Classification of Feedback Expressions in Multimodal Data,clustering experiments on the communicative prop erties of gaze and gestures,"['Kristiina Jokinen', 'Anton Ragni']",introduction,,"For example , #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .","['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example , #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .', 'Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus.']",0,"['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'For example , #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .']"
CC1083,P10-2059,Classification of Feedback Expressions in Multimodal Data,distinguishing the communicative functions of gestures,"['Kristiina Jokinen', 'Costanza Navarretta', 'Patrizia Paggio']",introduction,"This paper deals with the results of a machine learning experiment conducted on annotated gesture data from two case studies (Danish and Estonian). The data concern mainly facial displays, that are annotated with attributes relating to shape and dynamics, as well as communicative function. The results of the experiments show that the granularity of the attributes used seems appropriate for the task of distinguishing the desired communicative functions. This is a promising result in view of a future automation of the annotation task.","For example , Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .","['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example , Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .', 'Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multi- modal corpus.']",0,"['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example , Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .']"
CC1084,P10-2059,Classification of Feedback Expressions in Multimodal Data,measuring nominal scale agreement among many raters,['Joseph L Fleiss'],introduction,,"Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG ) .","['It should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'For example, a yes can be an Answer to a question, an Agree and a TurnElicit at the same time, thus making the semantic classification very fine-grained.', ""Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG ) ."", 'Looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by ANVIL from the latest annotated element.']",0,"[""Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG ) .""]"
CC1085,P10-2059,Classification of Feedback Expressions in Multimodal Data,distinguishing the communicative functions of gestures,"['Kristiina Jokinen', 'Costanza Navarretta', 'Patrizia Paggio']",,"This paper deals with the results of a machine learning experiment conducted on annotated gesture data from two case studies (Danish and Estonian). The data concern mainly facial displays, that are annotated with attributes relating to shape and dynamics, as well as communicative function. The results of the experiments show that the granularity of the attributes used seems appropriate for the task of distinguishing the desired communicative functions. This is a promising result in view of a future automation of the annotation task.","These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG ) , but are still sat -isfactory given the high number of categories provided by the scheme.","['The head gestures in the DanPASS data have been coded by non expert annotators (one annotator per video) and subsequently controlled by a second annotator, with the exception of one video which was annotated independently and in parallel by two annotators.', 'The annotations of this video were then used to measure inter-coder agreement in ANVIL as it was the case for the annotations on feedback expressions.', 'In the case of gestures we also measured agreement on gesture segmentation.', 'The figures obtained are given in Table 3.', 'These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG ) , but are still sat -isfactory given the high number of categories provided by the scheme.']",1,"['The head gestures in the DanPASS data have been coded by non expert annotators (one annotator per video) and subsequently controlled by a second annotator, with the exception of one video which was annotated independently and in parallel by two annotators.', 'The annotations of this video were then used to measure inter-coder agreement in ANVIL as it was the case for the annotations on feedback expressions.', 'In the case of gestures we also measured agreement on gesture segmentation.', 'The figures obtained are given in Table 3.', 'These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG ) , but are still sat -isfactory given the high number of categories provided by the scheme.']"
CC1086,P10-2059,Classification of Feedback Expressions in Multimodal Data,feedback in head gesture and speech to appear in,"['Patrizia Paggio', 'Costanza Navarretta']",introduction,,"The results , which partly confirm those obtained on a smaller dataset in #AUTHOR_TAG , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results , which partly confirm those obtained on a smaller dataset in #AUTHOR_TAG , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions .', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",1,"['Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'The results , which partly confirm those obtained on a smaller dataset in #AUTHOR_TAG , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions .']"
CC1087,P10-2059,Classification of Feedback Expressions in Multimodal Data,hidden naive bayes,"['Harry Zhang', 'Liangxiao Jiang', 'Jiang Su']",,,The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #AUTHOR_TAG ) .,"['The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4.', 'These two sets of data were used for automatic dialogue act classification, which was run in the Weka system (Witten and Frank, 2005).', 'We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table .', 'The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #AUTHOR_TAG ) .', 'Therefore, here we show the results of this classifier.', 'Ten-folds crossvalidation was applied throughout.']",5,['The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #AUTHOR_TAG ) .']
CC1088,P10-2059,Classification of Feedback Expressions in Multimodal Data,gesture generation by imitation  from human behavior to computer character animation,['Michael Kipp'],introduction,"In an e ort to extend traditional human-computer interfaces research has introduced embodied agents to utilize the modalities of everyday human-human communication, like facial expression, gestures and body postures. However, giving computer agents a human-like body introduces new challenges. Since human users are very sensitive and critical concerning bodily behavior the agents must act naturally and individually in order to be believable. This dissertation focuses on conversational gestures. It shows how to generate conversational gestures for an animated embodied agent based on annotated text input. The central idea is to imitate the gestural behavior of a human individual. Using TV show recordings as empirical data, gestural key parameters are extracted for the generation of natural and individual gestures. The gesture generation task is solved in three stages: observation, modeling and generation. For each stage, a software module was developed. For observation, the video annotation research tool ANVIL was created. It allows the eAEcient transcription of gesture, speech and other modalities on multiple layers. ANVIL is application-independent by allowing users to de ne their own annotation schemes, it provides various import/export facilities and it is extensible via its plugin interface. Therefore, the tool is suitable for a wide variety of research elds. For this work, selected clips of the TV talk show  Das Literarische Quartett"" were transcribed and analyzed, arriving at a total of 1,056 gestures. For the modeling stage, the NOVALIS module was created to compute individual gesture pro les from these transcriptions with statistical methods. A gesture pro le models the aspects handedness, timing and function of gestures for a single human individual using estimated conditional probabilities. The pro les are based on a shared lexicon of 68 gestures, assembled from the data. Finally, for generation, the NOVA generator was devised to create gestures based on gesture pro les in an overgenerate-andlter approach. Annotated text input is processed in a graph-based representation in multiple stages where semantic data is added, the location of potential gestures is determined by heuristic rules, and gestures are added and ltered based on a gesture pro le. NOVA outputs a linear, player-independent action script in XML.",Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .,"['As already mentioned, all words in DanPASS are phonetically and prosodically annotated.', 'In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both.', 'For this study, we added semantic labels -including dialogue acts -and gesture annotation.', 'Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .', 'To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources.', 'This subset comprises the categories Accept, Decline, RepeatRephrase and Answer.', 'Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant.', 'Finally, the two turn management categories Turn-Take and TurnElicit were also coded.']",5,"['As already mentioned, all words in DanPASS are phonetically and prosodically annotated.', 'In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both.', 'For this study, we added semantic labels -including dialogue acts -and gesture annotation.', 'Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .', 'To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources.', 'Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant.']"
CC1089,P10-2059,Classification of Feedback Expressions in Multimodal Data,praat doing phonetics by computer retrieved,"['Paul Boersma', 'David Weenink']",introduction,,The Praat tool was used ( #AUTHOR_TAG ) .,"['Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see Gr�nnum (2006).', 'The Praat tool was used ( #AUTHOR_TAG ) .']",5,['The Praat tool was used ( #AUTHOR_TAG ) .']
CC1090,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,linking semantic and knowledge representations in a multidomain dialogue system,"['Myroslava O Dzikovska', 'James F Allen', 'Mary D Swift']",experiments,"We describe a two-layer architecture for supporting semantic interpretation and domain reasoning in dialogue systems. Building system that supports both semantic interpretation and domain reasoning in a transparent and well-integrated manner is an unresolved problem because of the diverging requirements of the semantic representations used in contextual interpretation versus the knowledge representations used in domain reasoning. We propose an architecture that provides both portability and efficiency in natural language interpretation by maintaining separate semantic and domain knowledge representations, and integrating them via an ontology mapping procedure. The ontology mapping is used to obtain representations of utterances in a form most suitable for domain reasoners and to automatically specialize the lexicon. The use of a linguistically motivated parser for producing semantic representations for complex natural language sentences facilitates building portable semantic interpretation components as well as connections with domain reasoners. Two evaluations demonstrate the effectiveness of our approach: we show that a small number of mapping rules are sufficient for customizing the generic semantic representation to a new domain, and that our automatic lexicon specialization technique improves parser speed and accuracy.","The diagnoser , based on #AUTHOR_TAGb ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer .","['The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""The diagnoser , based on #AUTHOR_TAGb ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer ."", 'At present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to Nielsen et al. (2008).']",2,"[""The diagnoser , based on #AUTHOR_TAGb ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer .""]"
CC1091,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,developing pedagogically effective tutorial dialogue tactics experiments and a testbed,"['Kurt VanLehn', 'Pamela Jordan', 'Diane Litman']",introduction,"Although effective tutorial dialogue strategies are well understood, tutorial tactics that govern brief episodes of tutoring, such as a single step, are not. Because better tactics seem to be crucial for further improving pedagogical effectiveness, we have begun investigating the effects of varying tutorial tactics. In this paper we describe two planned experiments and the testbed we have created to support this experimentation.","Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']"
CC1092,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,controlling content realization with functional unification grammars in,"['Michael Elhadad', 'Jacques Robin']",experiments,"Standard Functional Unification Grammars (FUGs) provide a structurally guided top-down control regime for sentence generation. When using FUGs to perform content realization as a whole, including lexical choice, this regime is no longer appropriate for two reasons: (1) the unification of non-lexicalized semantic input with an integrated lexico-grammar requires mapping ""floating"" semantic elements which can trigger extensive backtracking and (2) lexical choice requires accessing external constraint sources on demand to preserve the modularity between conceptual and linguistic knowledge.","The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE ( #AUTHOR_TAG ) generation system to produce the appropriate text .","[""The strategy decision made by the tutorial planner, together with relevant semantic content from the student's answer (e.g., part of the answer to confirm), is passed to content planning and generation."", 'The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE ( #AUTHOR_TAG ) generation system to produce the appropriate text .', 'Templates are used to generate some stock phrases such as ""When you are ready, go on to the next slide.""']",5,"['The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE ( #AUTHOR_TAG ) generation system to produce the appropriate text .']"
CC1093,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,cohesion and learning in a tutorial spoken dialog system,"['Arthur Ward', 'Diane Litman']",conclusion,"Two measures of lexical cohesion were developed and applied to a corpus of human-computer tutoring dialogs. For both measures, the amount of cohesion in the tutoring dialog was found to be significantly correlated to learning for students with below-mean pretest scores, but not for those with above-mean pre-test scores, even though both groups had similar amounts of cohesion. We also find that only cohesion between tutor and student is significant: the cohesiveness of tutor, or of student, utterances is not. These results are discussed in light of previous work in textual cohesion and recall.",Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG ) .,"['In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn't explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010)."", 'Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG ) .', 'Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail.']",3,['Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG ) .']
CC1094,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,a natural language tutorial dialogue system for physics,"['Pamela Jordan', 'Maxim Makatchev', 'Umarani Pappuswamy', 'Kurt VanLehn', 'Patricia Albacete']",introduction,Abstract : We describe the WHY2-ATLAS intelligent tutoring system for qualitative physics that interacts with students via natural language dialogue. We focus on the issue of analyzing and responding to multi-sentential explanations. We explore approaches for achieving a deeper understanding of these explanations and dialogue management approaches and strategies for providing appropriate feedback on them.,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']"
CC1095,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,information state and dialogue management in the trindi dialogue move engine toolkit,"['Staffan Larsson', 'David Traum']",experiments,"We introduce an architecture and toolkit for building dialogue managers currently being developed in the TRINDI project, based on the notions of information state and dialogue move engine. The aim is to provide a framework for experimenting with implementations of different theories of information state, information state update and dialogue control. A number of dialogue managers are currently being built using the toolkit, and we present overviews of two of them. We believe that this framework will make implementation of dialogue processing theories easier, also facilitating comparison of different types of dialogue systems, thus helping to achieve a prerequisite for arriving at a best practice for the development of the dialogue management component of a spoken dialogue system.",Interaction between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG ) .,"['Interaction between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG ) .', 'The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts of the answer.', 'Once the complete answer has been accumulated, the system accepts it and moves on.', 'Tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with the student.']",5,['Interaction between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG ) .']
CC1096,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,contentlearning correlations in spoken tutoring dialogs at word turn and discourse levels,"['Amruta Purandare', 'Diane Litman']",introduction,"We study correlations between dialog content and learning in a corpus of human-computer tutoring dialogs. Using an online encyclopedia, we first extract domainspecific concepts discussed in our dialogs. We then extend previously studied shallow dialog metrics by incorporating content at three levels of granularity (word, turn and discourse) and also by distinguishing between students' spoken and written contributions. In all experiments, our content metrics show strong correlations with learning, and outperform the corresponding shallow baselines. Our word-level results show that although verbosity in student writings is highly associated with learning, verbosity in their spoken turns is not. On the other hand, we notice that content along with conciseness in spoken dialogs is strongly correlated with learning. At the turn-level, we find that effective tutoring dialogs have more content-rich turns, but not necessarily more or longer turns. Our discourse-level analysis computes the distribution of content across larger dialog units and shows high correlations when student contributions are rich but unevenly distributed across dialog segments. Copyright (c) 2008, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ) .']"
CC1097,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,the impact of interpretation problems on tutorial dialogue,"['Myroslava O Dzikovska', 'Johanna D Moore', 'Natalie Steinhauser', 'Gwendolyn Campbell']",conclusion,"Supporting natural language input may improve learning in intelligent tutoring systems. However, interpretation errors are unavoidable and require an effective recovery policy. We describe an evaluation of an error recovery policy in the BEETLE II tutorial dialogue system and discuss how different types of interpretation problems affect learning gain and user satisfaction. In particular, the problems arising from student use of non-standard terminology appear to have negative consequences. We argue that existing strategies for dealing with terminology problems are insufficient and that improving such strategies is important in future ITS research.","The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG ) .","['In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG ) ."", 'Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006).', 'Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail.']",3,"['In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG ) ."", 'Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006).', 'Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail.']"
CC1098,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,learning to assess lowlevel conceptual understanding,"['Rodney D Nielsen', 'Wayne Ward', 'James H Martin']",introduction,,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.']"
CC1099,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,generalizing tutorial dialogue results,"['Diane Litman', 'Johanna Moore', 'Myroslava Dzikovska', 'Elaine Farrow']",introduction,,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']"
CC1100,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,dealing with interpretation errors in tutorial dialogue,"['Myroslava O Dzikovska', 'Charles B Callaway', 'Elaine Farrow', 'Johanna D Moore', 'Natalie B Steinhauser', 'Gwendolyn C Campbell']",experiments,"We describe an approach to dealing with interpretation errors in a tutorial dialogue system. Allowing students to provide explanations and generate contentful talk can be helpful for learning, but the language that can be understood by a computer system is limited by the current technology. Techniques for dealing with understanding problems have been developed primarily for spoken dialogue systems in informationseeking domains, and are not always appropriate for tutorial dialogue. We present a classification of interpretation errors and our approach for dealing with them within an implemented tutorial dialogue system.","At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #AUTHOR_TAG ) .","['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #AUTHOR_TAG ) .', 'Since the system accepts unrestricted input, interpretation errors are unavoidable.', 'Our recovery policy is modeled on the TargetedHelp (Hockey et al., 2003) policy used in task-oriented dialogue.', 'If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, ""I\'m sorry, I\'m having a problem understanding.', 'I don\'t know the word power.""', 'The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.']",5,"['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #AUTHOR_TAG ) .']"
CC1101,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,resolving pronominal reference to abstract entities,['Donna K Byron'],experiments,"This paper describes PHORA, a technique for resolving pronominal reference to either individual or abstract entities. It defines processes for evoking abstract referents from discourse and for resolving both demonstrative and personal pronouns. It successfully interprets 72% of test pronouns, compared to 37% for a leading technique without these features.","The contextual interpreter then uses a reference resolution approach similar to #AUTHOR_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student 's output .","['We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances.', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to #AUTHOR_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student 's output ."", 'Utterance content is represented as a set of extracted objects and relations between them.', 'Negation is supported, together with a heuristic scoping algorithm.', 'The interpreter also performs basic ellipsis resolution.', 'For example, it can determine that in the answer to the question ""Which bulbs will be on and which bulbs will be off in this diagram?"",', '""off"" can be taken to mean ""all bulbs in the di-agram will be off.""', 'The resulting output is then passed on to the domain reasoning and diagnosis components.']",1,"[""The contextual interpreter then uses a reference resolution approach similar to #AUTHOR_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student 's output ."", 'The interpreter also performs basic ellipsis resolution.']"
CC1102,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,the beetle and beediff tutoring systems,"['Charles B Callaway', 'Myroslava Dzikovska', 'Elaine Farrow', 'Manuel Marques-Pita', 'Colin Matheson', 'Johanna D Moore']",introduction,"We describe two tutorial dialogue systems that adapt techniques from task-oriented dialogue systems to tutorial dialogue. Both systems employ the same reusable deep natural language understanding and generation components to interpret students' written utterances and to automatically generate adaptive tutorial responses, with separate domain reasoners to provide the necessary knowledge about the correctness of student answers and hinting strategies. We focus on integrating the domain-independent language processing components with domain-specific reasoning and tutorial components in order to improve the dialogue interaction, and present a preliminary analysis of BeeDiff's evaluation. Index Terms: tutoring systems, dialogue, deep processing",The BEETLE II system architecture is designed to overcome these limitations ( #AUTHOR_TAG ) .,"['The BEETLE II system architecture is designed to overcome these limitations ( #AUTHOR_TAG ) .', 'It uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically.', 'This allows the system to consistently apply the same tutorial policy across a range of questions.', 'To some extent, this comes at the expense of being able to address individual student misconceptions.', ""However, the system's modular setup and extensibility make it a suitable testbed for both computational linguistics algorithms and more general questions about theories of learning.""]",0,['The BEETLE II system architecture is designed to overcome these limitations ( #AUTHOR_TAG ) .']
CC1103,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,learning to assess lowlevel conceptual understanding,"['Rodney D Nielsen', 'Wayne Ward', 'James H Martin']",experiments,,"At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #AUTHOR_TAG .","['The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""The diagnoser, based on Dzikovska et al. (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student's answer."", 'At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #AUTHOR_TAG .']",3,"['The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""The diagnoser, based on Dzikovska et al. (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student's answer."", 'At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #AUTHOR_TAG .']"
CC1104,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,simulated tutors in immersive learning environments empiricallyderived design principles,"['N B Steinhauser', 'L A Butler', 'G E Campbell']",introduction,,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ) .', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']"
CC1105,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,towards tutorial dialog to support selfexplanation adding natural language understanding to a cognitive tutor,"['V Aleven', 'O Popescu', 'K R Koedinger']",introduction,"Self-explanation is an effective metacognitive strategy, as a number of cognitive science studies have shown. In a previous study we showed that self-explanation can be supported effectively in a cognitive tutor for geometry problem solving. In that study, students explained their own problem-solving steps by selecting from a menu the name of a problem-solving principle that justifies the step. They learned with greater understanding, as compared to students who did not explain their reasoning. Currently, we are working toward testing the hypothesis that students will learn even better when they provide explanations in their own words rather than selecting them from a menu. We have implemented a prototype of a cognitive tutor that understands students' explanations and provides feedback. The tutor uses a knowledge-based approach to natural language understanding. We are entering a phase of pilot testing, both for the purpose of assessing the coverage of the natural language understanding component and for gaining insight into the kinds of dialog strategies that are needed.","Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']"
CC1106,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,linking semantic and knowledge representations in a multidomain dialogue system,"['Myroslava O Dzikovska', 'James F Allen', 'Mary D Swift']",experiments,"We describe a two-layer architecture for supporting semantic interpretation and domain reasoning in dialogue systems. Building system that supports both semantic interpretation and domain reasoning in a transparent and well-integrated manner is an unresolved problem because of the diverging requirements of the semantic representations used in contextual interpretation versus the knowledge representations used in domain reasoning. We propose an architecture that provides both portability and efficiency in natural language interpretation by maintaining separate semantic and domain knowledge representations, and integrating them via an ontology mapping procedure. The ontology mapping is used to obtain representations of utterances in a form most suitable for domain reasoners and to automatically specialize the lexicon. The use of a linguistically motivated parser for producing semantic representations for complex natural language sentences facilitates building portable semantic interpretation components as well as connections with domain reasoners. Two evaluations demonstrate the effectiveness of our approach: we show that a small number of mapping rules are sufficient for customizing the generic semantic representation to a new domain, and that our automatic lexicon specialization technique improves parser speed and accuracy.","The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #AUTHOR_TAGa ) to produce a domain-specific semantic representation of the student 's output .","['We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances.', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #AUTHOR_TAGa ) to produce a domain-specific semantic representation of the student 's output ."", 'Utterance content is represented as a set of extracted objects and relations between them.', 'Negation is supported, together with a heuristic scoping algorithm.', 'The interpreter also performs basic ellipsis resolution.', 'For example, it can determine that in the answer to the question ""Which bulbs will be on and which bulbs will be off in this diagram?"",', '""off"" can be taken to mean ""all bulbs in the di-agram will be off.""', 'The resulting output is then passed on to the domain reasoning and diagnosis components.']",5,"['We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances.', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #AUTHOR_TAGa ) to produce a domain-specific semantic representation of the student 's output .""]"
CC1107,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,the beetle and beediff tutoring systems,"['Charles B Callaway', 'Myroslava Dzikovska', 'Elaine Farrow', 'Manuel Marques-Pita', 'Colin Matheson', 'Johanna D Moore']",experiments,"We describe two tutorial dialogue systems that adapt techniques from task-oriented dialogue systems to tutorial dialogue. Both systems employ the same reusable deep natural language understanding and generation components to interpret students' written utterances and to automatically generate adaptive tutorial responses, with separate domain reasoners to provide the necessary knowledge about the correctness of student answers and hinting strategies. We focus on integrating the domain-independent language processing components with domain-specific reasoning and tutorial components in order to improve the dialogue interaction, and present a preliminary analysis of BeeDiff's evaluation. Index Terms: tutoring systems, dialogue, deep processing",Other factors such as student confidence could be considered as well ( #AUTHOR_TAG ) .,['Other factors such as student confidence could be considered as well ( #AUTHOR_TAG ) .'],3,['Other factors such as student confidence could be considered as well ( #AUTHOR_TAG ) .']
CC1108,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,autotutor a simulation of a human tutor cognitive systems research,"['A C Graesser', 'P Wiemer-Hastings', 'P WiemerHastings', 'R Kreuz']",introduction,,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']"
CC1109,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,towards modelling and using common ground in tutorial dialogue,"['Mark Buckley', 'Magdalena Wolska']",introduction,"In order to avoid miscommunication par-ticipants in dialogue continuously attempt to align their mutual knowledge (the ""common ground""). A setting that is per-haps most prone to misalignment is tu-toring. We propose a model of common ground in tutoring dialogues which explic-itly models the truth and falsity of do-main level contributions and show how it can be used to detect and repair stu-dents ' false conjectures and facilitate stu-dent modelling.","Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']"
CC1110,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,targeted help for spoken dialogue systems intelligent feedback improves naive users’ performance,"['Beth Ann Hockey', 'Oliver Lemon', 'Ellen Campana', 'Laura Hiatt', 'Gregory Aist', 'James Hieronymus', 'Alexander Gruenstein', 'John Dowding']",experiments,"We present experimental evidence that providing naive users of a spoken dialogue system with immediate help messages related to their out-of-coverage utterances improves their success in using the system. A grammar-based recognizer and a Statistical Language Model (SLM) recognizer are run simultaneously. If the grammar-based recognizer suceeds, the less accurate SLM recognizer hypothesis is not used. When the grammar-based recognizer fails and the SLM recognizer produces a recognition hypothesis, this result is used by the Targeted Help agent to give the user feedback on what was recognized, a diagnosis of what was problematic about the utterance, and a related in-coverage example. The in-coverage example is intended to encourage alignment between user inputs and the language model of the system. We report on controlled experiments on a spoken dialogue system for command and control of a simulated robotic helicopter.",Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .,"['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy .', 'Since the system accepts unrestricted input, interpretation errors are unavoidable.', 'Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .', 'If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, ""I\'m sorry, I\'m having a problem understanding.', 'I don\'t know the word power.""', 'The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.']",2,"['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy .', 'Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .', 'If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, ""I\'m sorry, I\'m having a problem understanding.', 'The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.']"
CC1111,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,deep linguistic processing for spoken dialogue systems,"['James Allen', 'Myroslava Dzikovska', 'Mehdi Manshadi', 'Mary Swift']",experiments,"We describe a framework for deep linguistic processing for natural language understanding in task-oriented spoken dialogue systems. The goal is to create domaingeneral processing techniques that can be shared across all domains and dialogue tasks, combined with domain-specific optimization based on an ontology mapping from the generic LF to the application ontology. This framework has been tested in six domains that involve tasks such as interactive planning, coordination operations, tutoring, and learning.",We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .,"['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008a) to produce a domain-specific semantic representation of the student's output."", 'Utterance content is represented as a set of extracted objects and relations between them.', 'Negation is supported, together with a heuristic scoping algorithm.', 'The interpreter also performs basic ellipsis resolution.', 'For example, it can determine that in the answer to the question ""Which bulbs will be on and which bulbs will be off in this diagram?"",', '""off"" can be taken to mean ""all bulbs in the di-agram will be off.""', 'The resulting output is then passed on to the domain reasoning and diagnosis components.']",5,['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .']
CC1112,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,interpretation and generation in a knowledgebased tutorial system,"['Myroslava O Dzikovska', 'Charles B Callaway', 'Elaine Farrow']",experiments,,"The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .","['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .', 'At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits.']",5,"['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .']"
CC1113,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,tree edit distance for textual entailment,"['Milen Kouleykov', 'Bernardo Magnini']",introduction,,"All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .","['All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', 'Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.', 'WordNet, the most widely used resource in TE, provides all the three types of information.', 'Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis.']",0,"['All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', 'Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.', 'WordNet, the most widely used resource in TE, provides all the three types of information.', 'Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis.']"
CC1114,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,statistical phrasebased translation,"['Philipp Koehn', 'Franz Josef Och', 'Daniel Marcu']",,"This work summarizes a comparison between two ap-proaches to Statistical Machine Translation (SMT), namely Ngram-based and Phrase-based SMT. In both approaches, the translation process is based on bilingual units related by word-to-word alignments (pairs of source and target words), while the main differences are based on the extraction process of these units and the sta-tistical modeling of the translation context. The study has been carried out on two different translation tasks (in terms of translation difculty and amount of available training data), and allowing for distortion (reordering) in the decoding pro-cess. Thus it extends a previous work were both approaches were compared under monotone conditions. We nally report comparative results in terms of trans-lation accuracy, computation time and memory size. Re-sults show how the ngram-based approach outperforms the phrase-based approach by achieving similar accuracy scores in less computational time and with less memory needs. 1",They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG ) .,"['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG ) .', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007).', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.', 'The resulting 1 http://www.statmt.org/wmt10/', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']",0,"['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG ) .', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007).', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']"
CC1115,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,dirt  discovery of inference rules from text,"['Dekang Lin', 'Patrick Pantel']",introduction,"In this paper, we propose an unsupervised method for discovering inference rules from text, such as ""X is author of Y  X wrote Y"", ""X solved Y  X found a solution to Y"", and ""X caused Y  Y is triggered by X"". Inference rules are extremely important in many fields such as natural language processing, information retrieval, and artificial intelligence in general. Our algorithm is based on an extended version of Harris' Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus.","ones , DIRT ( #AUTHOR_TAG ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .","['ones , DIRT ( #AUTHOR_TAG ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are fre- quently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similar- ity/relatedness scores.']",0,"['ones , DIRT ( #AUTHOR_TAG ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similar- ity/relatedness scores.']"
CC1116,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,a semantic approach to recognizing textual entailment,['Marta Tatu andDan Moldovan'],introduction,"Exhaustive extraction of semantic information from text is one of the formidable goals of state-of-the-art NLP systems. In this paper, we take a step closer to this objective. We combine the semantic information provided by different resources and extract new semantic knowledge to improve the performance of a recognizing textual entailment system. 1 Recognizing Textual Entailment While communicating, humans use different expressions to convey the same meaning. Therefore, numerous NLP applications, such as, Question Answering, Information Extraction, or Summarization require computational models of language that recognize if two texts semantically overlap. Trying to capture the major inferences needed to understand equivalent semantic expressions, the PASCAL Network proposed the Recognizing Textual Entailment (RTE) challenge (Dagan et al., 2005). Given two text fragments, the task is to determine if the meaning of one text (the entailed hypothesis, H) can be inferred from the meaning of the other text (the entailing text, T). Given the wide applicability of this task, there is an increased interest in creating systems which detect the semantic entailment between two texts. The systems that participated in the Pascal RTE challenge competition exploit various inference elements which, later, they combine within statistical models, scoring methods, or machine learning frameworks. Several systems (Bos and Markert, 2005; Herrera et al., 2005; Jijkoun and de Rijke, 2005; Kouylekov and Magnini, 2005; Newman et al., 2005) measured the word overlap between the two text strings. Using either statistical or Word-Net's relations, almost all systems considered lexical relationships that indicate entailment. The degree of similarity between the syntactic parse trees of the two texts was also used as a clue for entailment by several systems (Herrera et al., 2005; Kouylekov and Magnini, 2005; de Salvo Braz et al., 2005; Raina et al., 2005). Several groups used logic provers to show the entailment between T and H (Bayer e","All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and #AUTHOR_TAG ) , or adopting transformation-based techniques ( Kouleykov and Magnini , 2005 ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .","['All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and #AUTHOR_TAG ) , or adopting transformation-based techniques ( Kouleykov and Magnini , 2005 ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', 'Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.', 'WordNet, the most widely used resource in TE, provides all the three types of information.', 'Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis.']",0,"['All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and #AUTHOR_TAG ) , or adopting transformation-based techniques ( Kouleykov and Magnini , 2005 ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', 'Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.', 'WordNet, the most widely used resource in TE, provides all the three types of information.', 'Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis.']"
CC1117,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,creating a bilingual entailment corpus through translations with mechanical turk 100 for a 10day rush,"['Matteo Negri', 'Yashar Mehdad']",experiments,"This paper reports on experiments in the creation of a bi-lingual Textual Entailment corpus, using non-experts' workforce under strict cost and time limitations ($100, 10 days). To this aim workers have been hired for translation and validation tasks, through the Crowd-Flower channel to Amazon Mechanical Turk. As a result, an accurate and reliable corpus of 426 English/Spanish entailment pairs has been produced in a more cost-effective way compared to other methods for the acquisition of translations based on crowdsourcing. Focusing on two orthogonal dimensions (i.e. reliability of annotations made by non experts, and overall corpus creation costs), we summarize the methodology we adopted, the achieved results, the main problems encountered, and the lessons learned.","Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #AUTHOR_TAG ) .","['The dataset used for our experiments is an English-Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish.', 'It consists of 1600 pairs derived from the RTE3 development and test sets (800+800).', 'Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #AUTHOR_TAG ) .', ""The method relies on translation-validation cycles, defined as separate jobs routed to MTurk's workforce."", 'Translation jobs return one Spanish version for each hypothesis.', 'Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference.', 'At each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job.', 'Although the quality of the results is enhanced by the possibility to automatically weed out untrusted workers using gold units, we performed a manual quality check on a subset of the acquired CLTE corpus.', 'The validation, carried out by a Spanish native speaker on 100 randomly selected pairs after two translation-validation cycles, showed the good quality of the collected material, with only 3 minor ""errors"" consisting in controversial but substantially acceptable translations reflecting regional Spanish variations.']",5,"['The dataset used for our experiments is an English-Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish.', 'Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #AUTHOR_TAG ) .']"
CC1118,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,syntacticsemantic structures for textual entailment recognition,"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']",introduction,"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.","Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( #AUTHOR_TAG ) as an extension of Textual Entailment ( Dagan and Glickman , 2004 ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .","['Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( #AUTHOR_TAG ) as an extension of Textual Entailment ( Dagan and Glickman , 2004 ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'For instance, the reliance of cur- rent monolingual TE systems on lexical resources (e.g. WordNet, VerbOcean, FrameNet) and deep processing components (e.g. syntactic and semantic parsers, co-reference resolution tools, temporal ex- pressions recognizers and normalizers) has to con- front, at the cross-lingual level, with the limited availability of lexical/semantic resources covering multiple languages, the limited coverage of the ex- isting ones, and the burden of integrating language- specific components into the same cross-lingual ar- chitecture.']",0,"['Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( #AUTHOR_TAG ) as an extension of Textual Entailment ( Dagan and Glickman , 2004 ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.']"
CC1119,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,syntacticsemantic structures for textual entailment recognition,"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']",introduction,"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.","Using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :","['This paper investigates the idea, still unexplored, of a tighter integration of MT and TE algorithms and techniques.', 'Our aim is to embed cross-lingual processing techniques inside the TE recognition process in order to avoid any dependency on external MT components, and eventually gain full control of the system�s behaviour.', 'Along this direction, we start from the acquisition and use of lexical knowl- edge, which represents the basic building block of any TE system.', 'Using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :']",1,"['Along this direction, we start from the acquisition and use of lexical knowl- edge, which represents the basic building block of any TE system.', 'Using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :']"
CC1120,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,a systematic comparison of various statistical alignment models,"['Franz Josef Och', 'Hermann Ney']",,"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.","We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #AUTHOR_TAG ) to align the tokenized corpora at the word level .","['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #AUTHOR_TAG ) to align the tokenized corpora at the word level .', 'Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007).', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.', 'The resulting 1 http://www.statmt.org/wmt10/', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']",5,"['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'There are several methods to build phrase tables.', 'We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #AUTHOR_TAG ) to align the tokenized corpora at the word level .', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'The resulting 1 http://www.statmt.org/wmt10/', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']"
CC1121,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,tracking and summarizing news on a daily basis with columbias newsblaster,"['Kathleen R McKeown', 'Regina Barzilay', 'David Evans', 'Vasileios Hatzivassiloglou', 'Judith L Klavans', 'Ani Nenkova', 'Carl Sable', 'Barry Schiffman', 'Sergey Sigelman']",,"Recently, there have been significant advances in several areas of language technology, including clustering, text categorization, and summarization. However, efforts to combine technology from these areas in a practical system for information access have been limited. In this paper, we present Columbia's Newsblaster system for online news summarization. Many of the tools developed at Columbia over the years are combined together to produce a system that crawls the web for news articles, clusters them on specific topics and produces multidocument summaries for each cluster.","They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( #AUTHOR_TAG ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .","['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( #AUTHOR_TAG ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']",4,"['They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( #AUTHOR_TAG ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .']"
CC1122,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,paraphrasing with bilingual parallel corpora,"['Colin Bannard', 'Chris Callison-Burch']",introduction,"Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrase-based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments.","Some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .","['Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']",0,"['Some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']"
CC1123,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,making largescale support vector machine learning practical,['Thorsten Joachims'],,"Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, oo-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SV M light1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.","To combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight ( #AUTHOR_TAG ) , using each score as a feature .","['To combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight ( #AUTHOR_TAG ) , using each score as a feature .']",5,"['To combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight ( #AUTHOR_TAG ) , using each score as a feature .']"
CC1124,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,lexical selection and paraphrase in a meaning text generation model,"['Lidija Iordanskaja', 'Richard Kittredge', 'Alain Polg re']",,"We introduce a computationally tractable model for language generation based on the Meaning-Text Theory of Mel'cuk et al., in which the lexicon plays a central role. To illustrate the descriptive scope and paraphrase capabilities of the model, we show how the lexicon influences the set of choices at four different points during the multi-stage realization process: (1) semantic net simplification, (2) determination of root lexical node for the deep syntactic dependency tree, (3) possible application of deep paraphrase rules using lexical functions, and (4) surface syntactic realization. We also show some of the ways in which the theme/rheme specifications within the semantic net influence lexical and syntactic choices during realization. Examples are taken primarily from an implemented system which generates paragraph-length reports about the usage of operating systems.","They proved to be useful in a number of NLP applications such as natural language generation ( #AUTHOR_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .","['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( #AUTHOR_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']",4,"['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( #AUTHOR_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']"
CC1125,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,verbocean mining the web for finegrained semantic verb relations,"['Timothy Chklovski', 'Patrick Pantel']",introduction,"Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks. We present a semi-automatic method for extracting fine-grained semantic relations between verbs. We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web. On a set of 29,165 strongly associated verb pairs, our extraction algorithm yielded 65.5% accuracy. Analysis of error types shows that on the relation strength we achieved 75% accuracy. We provide the resource, called VERBOCEAN, for download at http://semantics.isi.edu/ocean/.","These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #AUTHOR_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .","['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #AUTHOR_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similar- ity/relatedness scores.']",0,"['These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #AUTHOR_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.']"
CC1126,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,multiwordnet developing and aligned multilingual database,"['Emanuele Pianta', 'Luisa Bentivogli', 'Christian Girardi']",introduction,,"Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #AUTHOR_TAG ) ) have been created for several languages , with different degrees of coverage .","['Addressing CLTE we have to face additional and more problematic issues related to: i) the stronger need of lexical knowledge, and ii) the limited availability of multilingual lexical resources.', 'As regards the first issue, it�s worth noting that in the monolingual scenario simple �bag of words� (or �bag of n- grams�) approaches are per se sufficient to achieve results above baseline.', 'In contrast, their application in the cross-lingual setting is not a viable solution due to the impossibility to perform direct lex- ical matches between texts and hypotheses in different languages.', 'This situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap.', 'However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English.', 'Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #AUTHOR_TAG ) ) have been created for several languages , with different degrees of coverage .', 'As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet�s synsets, thus making the coverage issue even more problematic than for TE.', 'As regards Wikipedia, the cross- lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for CLTE.', 'However, due to their relatively small number (especially for some languages), bilingual lexicons extracted from Wikipedia are still inadequate to provide acceptable coverage.', 'In addition, featuring a bias towards named entities, the information acquired through cross-lingual links can at most complement the lexical knowledge extracted from more generic multilingual resources (e.g bilingual dictionaries).']",0,"['Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #AUTHOR_TAG ) ) have been created for several languages , with different degrees of coverage .']"
CC1127,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,inference rules and their application to recognizing textual entailment,"['Georgiana Dinu', 'Rui Wang']",,"In this paper, we explore ways of improv-ing an inference rule collection and its ap-plication to the task of recognizing textual entailment. For this purpose, we start with an automatically acquired collection and we propose methods to refine it and ob-tain more rules using a hand-crafted lex-ical resource. Following this, we derive a dependency-based structure representa-tion from texts, which aims to provide a proper base for the inference rule appli-cation. The evaluation of our approach on the recognizing textual entailment data shows promising results on precision and the error analysis suggests possible im-provements.","They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( #AUTHOR_TAG ) .","['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( #AUTHOR_TAG ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']",4,"['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( #AUTHOR_TAG ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']"
CC1128,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,paraphrasing with bilingual parallel corpora,"['Colin Bannard', 'Chris Callison-Burch']",,"Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrase-based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments.",One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG ) .,"['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009).', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG ) .', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']",0,"['One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG ) .', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.']"
CC1129,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,syntacticsemantic structures for textual entailment recognition,"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']",introduction,"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.","These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 ) .","['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010;Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.']",0,"['These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 ) .', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.']"
CC1130,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,probabilistic textual entailment generic applied modeling of language variability,"['Ido Dagan', 'Oren Glickman']",introduction,,"Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of Textual Entailment ( #AUTHOR_TAG ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .","['Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of Textual Entailment ( #AUTHOR_TAG ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'For instance, the reliance of cur- rent monolingual TE systems on lexical resources (e.g. WordNet, VerbOcean, FrameNet) and deep processing components (e.g. syntactic and semantic parsers, co-reference resolution tools, temporal expressions recognizers and normalizers) has to confront, at the cross-lingual level, with the limited availability of lexical/semantic resources covering multiple languages, the limited coverage of the existing ones, and the burden of integrating language- specific components into the same cross-lingual architecture.']",0,"['Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of Textual Entailment ( #AUTHOR_TAG ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.']"
CC1131,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,the berkeley framenet project,"['Collin F Baker', 'Charles J Fillmore', 'John B Lowe']",introduction,"FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year #NSF IRI-9618838, #Tools for Lexicon Building&quot;#. The project&apos;s key features are #a# a commitment to corpus evidence for semantic and syntactic generalizations, and #b# the representation of the valences of its target words #mostly nouns, adjectives, and verbs# in which the semantic portion makes use of frame semantics. The resulting database will contain #a# descriptions of the semantic frames underlying the meanings of the words described, and #b# the valence representation #semantic and syntactic# of several thousand words and phrases, each accompanied by #c# a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between #frame elements&quot; and their syntactic realizations #e.g. grammatical function, phrase type, and other syntactic traits#. This report will present the project&apos;s goals and workflow, and information about the computational tools that have been adapted or created in-house for this work","These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .","['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.']",0,"['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.']"
CC1132,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,fluency adequacy or hter exploring different human judgments with a tunable mt metric,"['Matthew Snover', 'Nitin Madnani', 'Bonnie Dorr', 'Richard Schwartz']",,"Automatic Machine Translation (MT) evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance. Different types of human judgments, such as Fluency, Adequacy, and HTER, mea-sure varying aspects of MT performance that can be captured by automatic MT metrics. We explore these differences through the use of a new tunable MT met-ric: TER-Plus, which extends the Transla-tion Edit Rate evaluation metric with tun-able parameters and the incorporation of morphology, synonymy and paraphrases. TER-Plus was shown to be one of the top metrics in NIST's Metrics MATR 2008 Challenge, having the highest aver-age rank in terms of Pearson and Spear-man correlation. Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating sig-nificant differences between the types of human judgments.","After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .","['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009).', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']",0,"['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']"
CC1133,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,extending the meteor machine translation evaluation metric to the phrase level,"['Michael Denkowski', 'Alon Lavie']",,"This paper presents METEOR-NEXT, an ex-tended version of the METEOR metric de-signed to have high correlation with post-editing measures of machine translation qual-ity. We describe changes made to the met-ric's sentence aligner and scoring scheme as well as a method for tuning the metric's pa-rameters to optimize correlation with human-targeted Translation Edit Rate (HTER). We then show that METEOR-NEXT improves cor-relation with HTER over baseline metrics, in-cluding earlier versions of METEOR, and ap-proaches the correlation level of a state-of-the-art metric, TER-plus (TERp).","They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .","['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']",4,"['They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .']"
CC1134,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,extracting paraphrase patterns from bilingual parallel corpora,"['Shiqi Zhao', 'Haifeng Wang', 'Ting Liu', 'Sheng Li']",introduction,"Paraphrase patterns are semantically equivalent patterns, which are useful in both paraphrase recognition and generation. This paper presents a pivot approach for extracting paraphrase patterns from bilingual parallel corpora, whereby the paraphrase patterns in English are extracted using the patterns in another language as pivots. We make use of log-linear models for computing the paraphrase likelihood between pattern pairs and exploit feature functions based on maximum likelihood estimation (MLE), lexical weighting (LW), and monolingual word alignment (MWA). Using the presented method, we extract more than 1 million pairs of paraphrase patterns from about 2 million pairs of bilingual parallel sentences. The precision of the extracted paraphrase patterns is above 78%. Experimental results show that the presented method significantly outperforms a well-known method called discovery of inference rules from text (DIRT). Additionally, the log-linear model with the proposed feature functions are effective. The extracted paraphrase patterns are fully analyzed. Especially, we found that the extracted paraphrase patterns can be classified into five types, which are useful in multiple natural language processing (NLP) applications.","Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .","['Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']",0,"['Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']"
CC1135,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,syntacticsemantic structures for textual entailment recognition,"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']",experiments,"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.","For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) .","[""For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) ."", 'Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario.']",1,"[""For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) ."", 'Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario.']"
CC1136,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,a syntaxbased statistical translation model,"['Kenji Yamada', 'Kevin Knight']",conclusion,,"One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .","['Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE.', 'On one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .', 'Another interesting direction is to investigate the potential of paraphrase patterns (i.e.', 'patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (Zhao et al., 2009).', 'On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge.', 'As a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment.', '1343']",3,"['One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .']"
CC1137,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,jlsi a tool for latent semantic indexing software available at httptccitcitresearchtextectoolsresourcesjlsihtml,['Claudio Giuliano'],experiments,,We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .,"['Wikipedia (WIKI).', 'We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .', 'Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (Kouylekov et al., 2009).', 'In this way we obtained 13760 word pairs.']",5,['We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .']
CC1138,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,syntacticsemantic structures for textual entailment recognition,"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']",experiments,"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.","Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .","['The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations.', 'First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%.', 'This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table.', 'Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .', '63.50%).', 'Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%).', 'In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation.', ""Finally, it's worth remarking that applying our phrase matching method on the translated dataset without any additional source of knowledge would result in an overall accuracy of 62.12%, which is lower than the result obtained using only phrase tables on cross-lingual data (62.62%)."", 'This demonstrates that phrase tables can successfully replace MT systems in the CLTE task.']",1,"['The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations.', 'This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table.', 'Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .', 'Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%).', 'In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation.', ""Finally, it's worth remarking that applying our phrase matching method on the translated dataset without any additional source of knowledge would result in an overall accuracy of 62.12%, which is lower than the result obtained using only phrase tables on cross-lingual data (62.62%)."", 'This demonstrates that phrase tables can successfully replace MT systems in the CLTE task.']"
CC1139,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,moses open source toolkit for statistical machine translation,"['Philipp Koehn', 'Hieu Hoang', 'Alexandra Birch', 'Chris Callison-Burch', 'Marcello Federico', 'Nicola Bertoldi', 'Brooke Cowan', 'Wade Shen']",,"We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.","Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .","['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.', 'The resulting 1 http://www.statmt.org/wmt10/', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']",5,"['In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .']"
CC1140,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,recognizing textual entailment rational evaluation and approaches,"['Ido Dagan', 'Bill Dolan', 'Bernardo Magnini', 'Dan Roth']",introduction,"The goal of identifying textual entailment - whether one piece of text can be plausibly inferred  from another - has emerged in recent years as a generic core problem in natural language  understanding. Work in this area has been largely driven by the PASCAL Recognizing  Textual Entailment (RTE) challenges, which are a series of annual competitive meetings.  The current work exhibits strong ties to some earlier lines of research, particularly automatic  acquisition of paraphrases and lexical semantic relationships and unsupervised inference in  applications such as question answering, information extraction and summarization. It has  also opened the way to newer lines of research on more involved inference methods, on  knowledge representations needed to support this natural language understanding challenge  and on the use of learning methods in this context. RTE has fostered an active and growing  community of researchers focused on the problem of applied entailment. This special issue  of the JNLE provides an opportunity to showcase some of the most important work in this  emerging area","Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .","['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.']",0,"['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.']"
CC1141,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",morphology and meaning in the english mental lexicon,"['W D Marslen-Wilson', 'L K Tyler', 'R Waksler', 'L Older']",introduction,"The authors investigated the lexical entry for morphologically complex words in English. Six experiments, using a cross-modal repetition priming task, asked whether the lexical entry for derivationally suffixed and prefixed words is morphologically structured and how this relates to the semantic and phonological transparency of the surface relationship between stem and affix. There was clear evidence for morphological decomposition of semantically transparent forms. This was independent of phonological transparency, suggesting that morphemic representations are phonologically abstract. Semantically opaque forms, in contrast, behave like monomorphemic words. Overall, suffixed and prefixed derived words and their stems prime each other through shared morphemes in the lexical entry, except for pairs of suffixed forms, which show a cohort-based interference effect",Such questions are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG ) or other lexical decision tasks .,"['Such questions are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG ) or other lexical decision tasks .', 'The reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.', '(See Sec. 2 for models of morphological organization and access and related experiments).']",0,"['Such questions are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG ) or other lexical decision tasks .', 'The reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.', '(See Sec. 2 for models of morphological organization and access and related experiments).']"
CC1142,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",the measurement of interrater agreement statistical methods for rates and proportions2212–236,"['Joseph L Fleiss', 'Bruce Levin', 'Myunghee Cho Paik']",method,,We measure the inter annotator agreement using the Fleiss Kappa ( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0.79 .,"['Organization and Processing of Compound Verbs in the Mental Lexicon Compound verbs, as discussed above, are special type of verb sequences consisting of two or more verbs acting as a single verb and express a single expression of meaning.', 'The verb V1 is known as pole and V2 is called as vector.', 'For example, ""ওঠে পড়া "" (getting up) is a compound verb where individual words do not entirely reflects the meaning of the whole expression.', 'However, not all V1+V2 combinations are CVs.', 'For example, expressions like, ""নিঠে য়াও ""(take and then go) and "" নিঠে আঠ ়া"" (return back) are the examples of verb sequences where meaning of the whole expression can be derived from the meaning of the individual component and thus, these verb sequences are not considered as CV.', 'The key question linguists are trying to identify for a long time and debating a lot is whether to consider CVs as a single lexical units or consider them as two separate units.', 'Since linguistic rules fails to explain the process, we for the first time tried to perform cognitive experiments to understand the organization and processing of such verb sequences in the human mind.', 'A clear understanding about these phenomena may help us to classify or extract actual CVs from other verb sequences.', 'In order to do so, presently we have applied three different techniques to collect user data.', 'In the first technique, we annotated 4500 V1+V2 sequences, along with their example sentences, using a group of three linguists (the expert subjects).', 'We asked the experts to classify the verb sequences into three classes namely, CV, not a CV and not sure.', 'Each linguist has received 2000 verb pairs along with their respective example sentences.', 'Out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping.', 'We measure the inter annotator agreement using the Fleiss Kappa ( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0.79 .', 'Next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 V1+V2 pairs and presented them to 36 native Bangla speakers.', 'We ask each subjects to give a compositionality score of each verb sequences under 1-10 point scale, 10 being highly compositional and 1 for noncompositional.', 'We found an agreement of κ=0.69 among the subjects.', 'We also observe a continuum of compositionality score among the verb sequences.', 'This reflects that it is difficult to classify Bangla verb sequences discretely into the classes of CV and not a CV.', ""We then, compare the compositionality score with that of the expert user's annotation."", 'We found a significant correlation between the expert annotation and the compositionality score.', 'We observe verb sequences that are annotated as CVs (like, খেঠে খিল , ওঠে পড , কঠে খি ) have got low compositionality score (average score ranges between 1-4) on the other hand high compositional values are in general tagged as not a cv (নিঠে য়া (come and get), নিঠে আে (return back), তু ঠল খেঠেনি (kept), গনিঠে পিল (roll on floor)).', 'This reflects that verb sequences which are not CV shows high degree of compositionality.', 'In other words non CV verbs can directly interpret from their constituent verbs.', 'This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning.', 'In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences.', 'We followed the same experimental procedure as discussed in (Taft, 2004) for English polymorphemic words.', 'However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination.', 'The reaction time (RT) of each subject is recorded.', 'Our preliminarily observation from the RT analysis shows that as per our claim, RT of verb sequences having high compositionality value is significantly higher than the RTs for low or noncompositional verbs.', 'This proves our hypothesis that Bangla compound verbs that show less compositionality are stored as a hole in the mental lexicon and thus follows the full-listing model whereas compositional verb phrases are individually parsed.', 'However, we do believe that our experiment is composed of a very small set of data and it is premature to conclude anything concrete based only on the current experimental results.']",5,"['We measure the inter annotator agreement using the Fleiss Kappa ( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0.79 .', 'We found an agreement of k=0.69 among the subjects.']"
CC1143,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",serial verb construction in marathiquot,['R Pandharipande'],related work,,"Similar findings have been proposed by #AUTHOR_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints .","['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by #AUTHOR_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints .', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']",0,"['Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Similar findings have been proposed by #AUTHOR_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints .', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']"
CC1144,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",morphological decomposition and the reverse base frequency effect,['M Taft'],method,"If recognition of a polymorphemic word always takes place via its decomposition into stem and affix, then the higher the frequency of its stem (i.e., base frequency) the easier the lexical decision response should be when frequency of the word itself (i.e., surface frequency) is controlled. Past experiments have demonstrated such a base frequency effect, but not under all circumstances. Thus, a dual pathway notion has become dominant as an account of morphological processing whereby both decomposition and whole-word access is possible. Two experiments are reported here that demonstrate how an obligatory decomposition account can handle the absence of base frequency effects. In particular, it is shown that the later stage of recombining the stem and affix is harder for high base frequency words than for lower base frequency words when matched on surface frequency, and that this can counterbalance the advantage of easier access to the higher frequency stem. When the combination stage is crucial for discriminating the word items from the nonword items, a reverse base frequency effect emerges, revealing the disadvantage at this stage for high base frequency words. Such an effect is hard for the dual-pathway account to explain, but follows naturally from the idea of obligatory decomposition.",We followed the same experimental procedure as discussed in ( #AUTHOR_TAG ) for English polymorphemic words .,"['Organization and Processing of Compound Verbs in the Mental Lexicon Compound verbs, as discussed above, are special type of verb sequences consisting of two or more verbs acting as a single verb and express a single expression of meaning.', 'The verb V1 is known as pole and V2 is called as vector.', 'For example, ""ওঠে পড়া "" (getting up) is a compound verb where individual words do not entirely reflects the meaning of the whole expression.', 'However, not all V1+V2 combinations are CVs.', 'For example, expressions like, ""নিঠে য়াও ""(take and then go) and "" নিঠে আঠ ়া"" (return back) are the examples of verb sequences where meaning of the whole expression can be derived from the meaning of the individual component and thus, these verb sequences are not considered as CV.', 'The key question linguists are trying to identify for a long time and debating a lot is whether to consider CVs as a single lexical units or consider them as two separate units.', 'Since linguistic rules fails to explain the process, we for the first time tried to perform cognitive experiments to understand the organization and processing of such verb sequences in the human mind.', 'A clear understanding about these phenomena may help us to classify or extract actual CVs from other verb sequences.', 'In order to do so, presently we have applied three different techniques to collect user data.', 'In the first technique, we annotated 4500 V1+V2 sequences, along with their example sentences, using a group of three linguists (the expert subjects).', 'We asked the experts to classify the verb sequences into three classes namely, CV, not a CV and not sure.', 'Each linguist has received 2000 verb pairs along with their respective example sentences.', 'Out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping.', 'We measure the inter annotator agreement using the Fleiss Kappa (Fleiss et al., 1981) measure (κ) where the agreement lies around 0.79.', 'Next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 V1+V2 pairs and presented them to 36 native Bangla speakers.', 'We ask each subjects to give a compositionality score of each verb sequences under 1-10 point scale, 10 being highly compositional and 1 for noncompositional.', 'We found an agreement of κ=0.69 among the subjects.', 'We also observe a continuum of compositionality score among the verb sequences.', 'This reflects that it is difficult to classify Bangla verb sequences discretely into the classes of CV and not a CV.', ""We then, compare the compositionality score with that of the expert user's annotation."", 'We found a significant correlation between the expert annotation and the compositionality score.', 'We observe verb sequences that are annotated as CVs (like, খেঠে খিল , ওঠে পড , কঠে খি ) have got low compositionality score (average score ranges between 1-4) on the other hand high compositional values are in general tagged as not a cv (নিঠে য়া (come and get), নিঠে আে (return back), তু ঠল খেঠেনি (kept), গনিঠে পিল (roll on floor)).', 'This reflects that verb sequences which are not CV shows high degree of compositionality.', 'In other words non CV verbs can directly interpret from their constituent verbs.', 'This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning.', 'In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences.', 'We followed the same experimental procedure as discussed in ( #AUTHOR_TAG ) for English polymorphemic words .', 'However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination.', 'The reaction time (RT) of each subject is recorded.', 'Our preliminarily observation from the RT analysis shows that as per our claim, RT of verb sequences having high compositionality value is significantly higher than the RTs for low or noncompositional verbs.', 'This proves our hypothesis that Bangla compound verbs that show less compositionality are stored as a hole in the mental lexicon and thus follows the full-listing model whereas compositional verb phrases are individually parsed.', 'However, we do believe that our experiment is composed of a very small set of data and it is premature to conclude anything concrete based only on the current experimental results.']",5,"['In the first technique, we annotated 4500 V1+V2 sequences, along with their example sentences, using a group of three linguists (the expert subjects).', 'Out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping.', 'We ask each subjects to give a compositionality score of each verb sequences under 1-10 point scale, 10 being highly compositional and 1 for noncompositional.', 'We found an agreement of k=0.69 among the subjects.', 'We also observe a continuum of compositionality score among the verb sequences.', ""We then, compare the compositionality score with that of the expert user's annotation."", 'In other words non CV verbs can directly interpret from their constituent verbs.', 'This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning.', 'We followed the same experimental procedure as discussed in ( #AUTHOR_TAG ) for English polymorphemic words .', 'However, we do believe that our experiment is composed of a very small set of data and it is premature to conclude anything concrete based only on the current experimental results.']"
CC1145,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",causal chains and compound verbsquot,['E Bashir'],related work,,"#AUTHOR_TAG tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".","['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.', '#AUTHOR_TAG tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']",0,"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']"
CC1146,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",morphological decomposition and the reverse base frequency effect,['M Taft'],related work,"If recognition of a polymorphemic word always takes place via its decomposition into stem and affix, then the higher the frequency of its stem (i.e., base frequency) the easier the lexical decision response should be when frequency of the word itself (i.e., surface frequency) is controlled. Past experiments have demonstrated such a base frequency effect, but not under all circumstances. Thus, a dual pathway notion has become dominant as an account of morphological processing whereby both decomposition and whole-word access is possible. Two experiments are reported here that demonstrate how an obligatory decomposition account can handle the absence of base frequency effects. In particular, it is shown that the later stage of recombining the stem and affix is harder for high base frequency words than for lower base frequency words when matched on surface frequency, and that this can counterbalance the advantage of easier access to the higher frequency stem. When the combination stage is crucial for discriminating the word items from the nonword items, a reverse base frequency effect emerges, revealing the disadvantage at this stage for high base frequency words. Such an effect is hard for the dual-pathway account to explain, but follows naturally from the idea of obligatory decomposition.","Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose .","['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.']",0,"['Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose .']"
CC1147,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",lexical representation of derivational relation,['D Bradley'],related work,,"The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 ) .","['Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model.', 'The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 ) .', 'On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978).', 'Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'For instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model (Caramazza et al., 1988).']",0,"['Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model.', 'The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 ) .', 'On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978).', 'Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'For instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model (Caramazza et al., 1988).']"
CC1148,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",masked morphological priming in visual word recognition,"['J Grainger', 'P Cole', 'J Segui']",introduction,"Masked priming studies with adult readers have provided evidence for a form-based morpho-orthographic segmentation mechanism that ""blindly"" decomposes any word with the appearance of morphological complexity. The present studies investigated whether evidence for structural morphological decomposition can be obtained with developing readers. We used a masked primed lexical decision design first adopted by Rastle, Davis, and New (2004), comparing truly suffixed (golden-GOLD) and pseudosuffixed (mother-MOTH) prime-target pairs with nonsuffixed controls (spinach-SPIN). Experiment 1 tested adult readers, showing that priming from both pseudo- and truly suffixed primes could be obtained using our own set of high-frequency word materials. Experiment 2 assessed a group of Year 3 and Year 5 children, but priming only occurred when prime and target shared a true morphological relationship, and not when the relationship was pseudomorphological. This pattern of results indicates that morpho-orthographic decomposition mechanisms do not become automatized until a relatively late stage in reading development.21 page(s","There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ) .","['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']",0,"['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ) .', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).']"
CC1149,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",conscious choice and some light verbs in urduquot,['M Butt'],related work,,#AUTHOR_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .,"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']",0,"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']"
CC1150,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",morphology and meaning in the english mental lexicon,"['W D Marslen-Wilson', 'L K Tyler', 'R Waksler', 'L Older']",introduction,"The authors investigated the lexical entry for morphologically complex words in English. Six experiments, using a cross-modal repetition priming task, asked whether the lexical entry for derivationally suffixed and prefixed words is morphologically structured and how this relates to the semantic and phonological transparency of the surface relationship between stem and affix. There was clear evidence for morphological decomposition of semantically transparent forms. This was independent of phonological transparency, suggesting that morphemic representations are phonologically abstract. Semantically opaque forms, in contrast, behave like monomorphemic words. Overall, suffixed and prefixed derived words and their stems prime each other through shared morphemes in the lexical entry, except for pairs of suffixed forms, which show a cohort-based interference effect","With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #AUTHOR_TAG ; Bentin , S. and Feldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla .","['With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #AUTHOR_TAG ; Bentin , S. and Feldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla .', 'Our cross-modal and masked priming experiment on Bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when the forms are phonologically/orthographically unrelated.', 'These observations are similar to those reported for English and indicate that derivationally suffixed words in Bangla are in general accessed through decomposition of the word into its constituent morphemes.', 'Further, based on the experimental data we have developed a series of computational models that can be used to predict the decomposition of Bangla polymorphemic words.', 'Our evaluation result shows that decomposition of a polymorphemic word depends on several factors like, frequency, productivity of the suffix and the compositionality between the stem and the suffix.']",5,"['With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #AUTHOR_TAG ; Bentin , S. and Feldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla .', 'These observations are similar to those reported for English and indicate that derivationally suffixed words in Bangla are in general accessed through decomposition of the word into its constituent morphemes.']"
CC1151,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",repetition priming and frequency attenuation in lexical access,"['K I Forster', 'C Davis']",method,,"We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .","['We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .', 'Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming).', 'The subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'The same target word is again probed but with a different audio or visual probe called the control word.', 'The control shows no relationship with the target.', 'For example, baYaska (aged) and baYasa (age) is a prime-target pair, for which the corresponding control-target pair could be naYana (eye) and baYasa (age).']",5,"['We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .']"
CC1152,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",what can we learn from the morphology of hebrew a maskedpriming investigation of morphological representation,"['R Frost', 'K I Forster', 'A Deutsch']",introduction,"All Hebrew words are composed of 2 interwoven morphemes: a triconsonantal root and a phonological word pattern. the lexical representations of these morphemic units were examined using masked priming. When primes and targets shared an identical word pattern, neither lexical decision nor naming of targets was facilitated. In contrast root primes facilitated both lexical decisions and naming of target words that were derived from these roots. This priming effect proved to be independent of meaning similarity because no priming effects were found when primes and targets were semantically but not morphologically related. These results suggest that Hebrew roots are lexical units whereas word patterns are not. A working model of lexical organization in Hebrew is offered on the basis of these results.","There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; Grainger , et al. , 1991 ; Drews and Zwitserlood , 1995 ) .","['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; Grainger , et al. , 1991 ; Drews and Zwitserlood , 1995 ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']",0,"['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; Grainger , et al. , 1991 ; Drews and Zwitserlood , 1995 ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']"
CC1153,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",morphology and meaning in the english mental lexicon,"['W D Marslen-Wilson', 'L K Tyler', 'R Waksler', 'L Older']",method,"The authors investigated the lexical entry for morphologically complex words in English. Six experiments, using a cross-modal repetition priming task, asked whether the lexical entry for derivationally suffixed and prefixed words is morphologically structured and how this relates to the semantic and phonological transparency of the surface relationship between stem and affix. There was clear evidence for morphological decomposition of semantically transparent forms. This was independent of phonological transparency, suggesting that morphemic representations are phonologically abstract. Semantically opaque forms, in contrast, behave like monomorphemic words. Overall, suffixed and prefixed derived words and their stems prime each other through shared morphemes in the lexical entry, except for pairs of suffixed forms, which show a cohort-based interference effect","We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .","['We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .', 'Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming).', 'The subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'The same target word is again probed but with a different audio or visual probe called the control word.', 'The control shows no relationship with the target.', 'For example, baYaska (aged) and baYasa (age) is a prime-target pair, for which the corresponding control-target pair could be naYana (eye) and baYasa (age).']",5,"['We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .', 'The subjects were asked to make a lexical decision whether the given target is a valid word in that language.']"
CC1154,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",wordnet an electronic lexical database,['C Fellbaum'],introduction,"Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet.","Their computational significance arises from the issue of their storage in lexical resources like WordNet ( #AUTHOR_TAG ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency .","['A clear understanding of the structure and the processing mechanism of the mental lexicon will further our knowledge of how the human brain processes language.', 'Further, these linguistically important and interesting questions are also highly significant for computational linguistics (CL) and natural language processing (NLP) applications.', 'Their computational significance arises from the issue of their storage in lexical resources like WordNet ( #AUTHOR_TAG ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency .']",0,"['Further, these linguistically important and interesting questions are also highly significant for computational linguistics (CL) and natural language processing (NLP) applications.', 'Their computational significance arises from the issue of their storage in lexical resources like WordNet ( #AUTHOR_TAG ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency .']"
CC1155,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",morphological decomposition and the reverse base frequency effect,['M Taft'],introduction,"If recognition of a polymorphemic word always takes place via its decomposition into stem and affix, then the higher the frequency of its stem (i.e., base frequency) the easier the lexical decision response should be when frequency of the word itself (i.e., surface frequency) is controlled. Past experiments have demonstrated such a base frequency effect, but not under all circumstances. Thus, a dual pathway notion has become dominant as an account of morphological processing whereby both decomposition and whole-word access is possible. Two experiments are reported here that demonstrate how an obligatory decomposition account can handle the absence of base frequency effects. In particular, it is shown that the later stage of recombining the stem and affix is harder for high base frequency words than for lower base frequency words when matched on surface frequency, and that this can counterbalance the advantage of easier access to the higher frequency stem. When the combination stage is crucial for discriminating the word items from the nonword items, a reverse base frequency effect emerges, revealing the disadvantage at this stage for high base frequency words. Such an effect is hard for the dual-pathway account to explain, but follows naturally from the idea of obligatory decomposition.","On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG ) .","['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English, Hebrew, Italian, French, Dutch, and few other languages (Marslen-Wilson et al., 2008;Frost et al., 1997;Grainger, et al., 1991;Drews and Zwitserlood, 1995).', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG ) .', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']",0,"['On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG ) .']"
CC1156,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",lexical access and inflectional morphology,"['A Caramazza', 'A Laudanna', 'C Romani']",related work,,"For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #AUTHOR_TAG ) .","['Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model.', 'The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon (Bradley, 1980;Butterworth, 1983).', 'On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978).', 'Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #AUTHOR_TAG ) .']",0,"['For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #AUTHOR_TAG ) .']"
CC1157,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",hindi structures intermediate levelquot michigan papers on south and,['P E Hook'],related work,,#AUTHOR_TAG considers the second verb V2 as an aspectual complex comparable to the auxiliaries .,"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG considers the second verb V2 as an aspectual complex comparable to the auxiliaries .', 'Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']",0,"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG considers the second verb V2 as an aspectual complex comparable to the auxiliaries .', 'Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']"
CC1158,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",lexical representation of derivational relation,['D Bradley'],related work,,"Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .","['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.']",0,"['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.']"
CC1159,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",and orthographic similarity in visual word recognition,"['E Drews', 'P Zwitserlood']",introduction,,"There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .","['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']",0,"['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).']"
CC1160,P97-1063,A word-to-word model of translational equivalence,a geometric approach to mapping bitext correspondencequot,['I D Melamed'],method,,"Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #AUTHOR_TAGc ) .","['2We could just as easily use other symmetric ""association"" measures, such as ¢2  or the Dice coefficient (Smadja, 1992).', 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.', 'The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk.', 'Models of translational equivalence that are ignorant of indirect associations have ""a tendency ... to be confused by collocates"" (Dagan et al., 1993).', 'Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #AUTHOR_TAGc ) .', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']",0,"['2We could just as easily use other symmetric ""association"" measures, such as C/2  or the Dice coefficient (Smadja, 1992).', 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Models of translational equivalence that are ignorant of indirect associations have ""a tendency ... to be confused by collocates"" (Dagan et al., 1993).', 'Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #AUTHOR_TAGc ) .', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']"
CC1161,P97-1063,A word-to-word model of translational equivalence,a statistical approach to machine translationquot,"['P F Brown', 'J Cocke', 'S Della Pietra', 'V Della Pietra', 'F Jelinek', 'R Mercer', 'P Roossin']",introduction,,"Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; #AUTHOR_TAG ; Brown et al. , 1993a ) .","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; #AUTHOR_TAG ; Brown et al. , 1993a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.', '(Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; #AUTHOR_TAG ; Brown et al. , 1993a ) .']"
CC1162,P97-1063,A word-to-word model of translational equivalence,robust word alignment for machine aided translationquot,"['I Dagan', 'K Church', 'SZ W Gale']",method,,Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates '' ( #AUTHOR_TAG ) .,"['2We could just as easily use other symmetric ""association"" measures, such as ¢2  or the Dice coefficient (Smadja, 1992).', 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.', 'The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk.', ""Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates '' ( #AUTHOR_TAG ) ."", 'Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c).', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']",0,"['Now, suppose that uk and Uk+z often co-occur within their language.', ""Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates '' ( #AUTHOR_TAG ) ."", ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest.""]"
CC1163,P97-1063,A word-to-word model of translational equivalence,using bitextual alignment for translation validation the transcheck systemquot,['E Macklovitch'],introduction,,"Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).","['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"
CC1164,P97-1063,A word-to-word model of translational equivalence,a program for aligning sentences in bilingual corporaquot,"['W Gale', 'K W Church']",,,"This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .","['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']",0,"['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .']"
CC1165,P97-1063,A word-to-word model of translational equivalence,a program for aligning sentences in bilingual corporaquot,"['W Gale', 'K W Church']",introduction,,"Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).","['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"
CC1166,P97-1063,A word-to-word model of translational equivalence,a geometric approach to mapping bitext correspondencequot,['I D Melamed'],introduction,,"Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).","['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"
CC1167,P97-1063,A word-to-word model of translational equivalence,how to compile a bilingual collocational lexicon automaticallyquot,['F Smadja'],method,,"2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #AUTHOR_TAG ) .","['2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #AUTHOR_TAG ) .', 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.', 'The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk.', 'Models of translational equivalence that are ignorant of indirect associations have ""a tendency ... to be confused by collocates"" (Dagan et al., 1993).', 'Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c).', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']",1,"['2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #AUTHOR_TAG ) .', 'Now, suppose that uk and Uk+z often co-occur within their language.', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']"
CC1168,P97-1063,A word-to-word model of translational equivalence,a statistical approach to language translationquot,"['P F Brown', 'J Cocke', 'S Della Pietra', 'V Della Pietra', 'F Jelinek', 'R Mercer', 'P Roossin']",introduction,,"Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) .","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.', '(Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) .']"
CC1169,P97-1063,A word-to-word model of translational equivalence,a geometric approach to mapping bitext correspondencequot,['I D Melamed'],introduction,,"The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAGa ) .","['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAGa ) .', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.', 'The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995;.']",0,"['A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAGa ) .', 'The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995;.']"
CC1170,P97-1063,A word-to-word model of translational equivalence,a program for aligning sentences in bilingual corporaquot,"['W Gale', 'K W Church']",,,"We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG ) .","['We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG ) .']",5,"['We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG ) .']"
CC1171,P97-1063,A word-to-word model of translational equivalence,robust word alignment for machine aided translationquot,"['I Dagan', 'K Church', 'SZ W Gale']",method,,"It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']",0,"['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .']"
CC1172,P97-1063,A word-to-word model of translational equivalence,semiautomatic acquisition of domainspecific translation lexiconsquot,['personal communication Nasr'],,,#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .,"['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'For many applications, this is the desired behavior.', ""The most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained Brown et al.'s Model 2 on 74 million words of the Canadian Hansards."", 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.', 'The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.', 'Therefore, we ignored English words that were linked to nothing.']",0,"['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .', 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.']"
CC1173,P97-1063,A word-to-word model of translational equivalence,a program for aligning sentences in bilingual corporaquot,"['W Gale', 'K W Church']",introduction,,"Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).","['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other Melamed, 1996a).', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.', 'The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995;.']",0,"['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.']"
CC1174,P97-1063,A word-to-word model of translational equivalence,deriving translation data from bilingual textsquot,"['R Catizone', 'G Russell', 'S Warwick']",introduction,,"Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).","['Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"
CC1175,P97-1063,A word-to-word model of translational equivalence,how to compile a bilingual collocational lexicon automaticallyquot,['F Smadja'],conclusion,,Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) .,"['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy .', ""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) ."", 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.', 'In this manner, the model can account for a wider range of translation phenomena.']",3,"[""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) .""]"
CC1176,P97-1063,A word-to-word model of translational equivalence,accurate methods for the statistics of surprise and coincidencequot,['T Dunning'],method,,"For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( â , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG ) 2 .","['Our translation model consists of the hidden parameters A+ and A-, and likelihood ratios L(u, v).', 'The two hidden parameters are the probabilities of the model generating true and false positives in the data.', 'L(u,v) represents the likelihood that u and v can be mutual translations.', 'For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( â\x80\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG ) 2 .', ""When the L(u, v) are re-estimated, the model's hidden parameters come into play.""]",5,"['The two hidden parameters are the probabilities of the model generating true and false positives in the data.', 'For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( â\x80\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG ) 2 .']"
CC1177,P97-1063,A word-to-word model of translational equivalence,automatic evaluation and uniform filter cascades for inducing nbest translation lexiconsquot,['I D Melamed'],introduction,"This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources. The knowledge sources are cast as filters, so that any subset of them can be cascaded in a uniform framework. A new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades. The best filter cascades improve lexicon quality by up to 137% over the plain vanilla statistical method, and approach human performance. Drastically reducing the size of the training corpus has a much smaller impact on lexicon quality when these knowledge sources are used. This makes it practical to train on small hand-built corpora for language pairs where large bilingual corpora are unavailable. Moreover, three of the four filters prove useful even when used with large training corpora.","With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #AUTHOR_TAG ) .","['With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #AUTHOR_TAG ) .', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a).', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.', 'The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993; Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995; Melamed, 1997).']",0,"['With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #AUTHOR_TAG ) .']"
CC1178,P97-1063,A word-to-word model of translational equivalence,building probabilistic models for natural language,['S Chen'],method,"Building models of language is a central task in natural language processing. Traditionally, language has been modeled with manually-constructed grammars that describe which strings are grammatical and which are not; however, with the recent availability of massive amounts of on-line text, statistically-trained models are an attractive alternative. These models are generally probabilistic, yielding a score reflecting sentence frequency instead of a binary grammaticality judgement. Probabilistic models of language are a fundamental tool in speech recognition for resolving acoustically ambiguous utterances. For example, we prefer the transcription forbear to four bear as the former string is far more frequent in English text. Probabilistic models also have application in optical character recognition, handwriting recognition, spelling correction, part-of-speech tagging, and machine translation. In this thesis, we investigate three problems involving the probabilistic modeling of language: smoothing n-gram models, statistical grammar induction, and bilingual sentence alignment. These three problems employ models at three different levels of language; they involve word-based, constituent-based, and sentence-based models, respectively. We describe techniques for improving the modeling of language at each of these levels, and surpass the performance of existing algorithms for each problem. We approach the three problems using three different frameworks. We relate each of these frameworks to the Bayesian paradigm, and show why each framework used was appropriate for the given problem. Finally, we show how our research addresses two central issues in probabilistic modeling: the sparse data problem and the problem of inducing hidden structure.Engineering and Applied Science","It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']",1,"['It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .']"
CC1179,P97-1063,A word-to-word model of translational equivalence,maximum likelihood from incomplete data via the em algorithmquot,"['A P Dempster', 'N M Laird', 'D B Rubin']",,,"By using the EM algorithm ( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .","[""One advantage that Brown et al.'s Model i has over our word-to-word model is that their objective function has no local maxima."", 'By using the EM algorithm ( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .', 'In contrast, the dynamic nature of the competitive linking algorithm changes the Pr(datalmodel ) in a non-monotonic fashion.', 'We have adopted the simple heuristic that the model ""has converged"" when this probability stops increasing.']",0,"['By using the EM algorithm ( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .']"
CC1180,P97-1063,A word-to-word model of translational equivalence,measuring semantic entropyquot,['I D Melamed'],conclusion,,"Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .","['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .', 'Another interesting extension is to broaden the definition of a ""word"" to include multi-word lexical units (Smadja, 1992).', 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.', 'In this manner, the model can account for a wider range of translation phenomena.']",3,"['Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .', 'Another interesting extension is to broaden the definition of a ""word"" to include multi-word lexical units (Smadja, 1992).']"
CC1181,P97-1063,A word-to-word model of translational equivalence,measuring semantic entropyquot,['I D Melamed'],method,,"For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG ) .","['In the basic word-to-word model, the hidden parameters A + and A-depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'More accurate models can be induced by taking into account various features of the linked tokens.', 'For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG ) .']",0,"['More accurate models can be induced by taking into account various features of the linked tokens.', 'For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG ) .']"
CC1182,P97-1063,A word-to-word model of translational equivalence,a statistical approach to machine translationquot,"['P F Brown', 'J Cocke', 'S Della Pietra', 'V Della Pietra', 'F Jelinek', 'R Mercer', 'P Roossin']",method,,"It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']",1,"['It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .']"
CC1183,P97-1063,A word-to-word model of translational equivalence,a program for aligning sentences in bilingual corporaquot,"['W Gale', 'K W Church']",method,,"2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG ) or the Dice coefficient ( Smadja , 1992 ) .","[""2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG ) or the Dice coefficient ( Smadja , 1992 ) ."", 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.', 'The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk.', 'Models of translational equivalence that are ignorant of indirect associations have ""a tendency ... to be confused by collocates"" (Dagan et al., 1993).', 'Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c).', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']",1,"[""2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG ) or the Dice coefficient ( Smadja , 1992 ) ."", 'Now, suppose that uk and Uk+z often co-occur within their language.']"
CC1184,P97-1063,A word-to-word model of translational equivalence,the mathematics of statistical machine translation parameter estimationquot,"['P F Brown', 'V J Della Pietra', 'S A Della Pietra', 'R L Mercer']",introduction,,"Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.', '(Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .']"
CC1185,P97-1063,A word-to-word model of translational equivalence,building probabilistic models for natural language,['S Chen'],,"Building models of language is a central task in natural language processing. Traditionally, language has been modeled with manually-constructed grammars that describe which strings are grammatical and which are not; however, with the recent availability of massive amounts of on-line text, statistically-trained models are an attractive alternative. These models are generally probabilistic, yielding a score reflecting sentence frequency instead of a binary grammaticality judgement. Probabilistic models of language are a fundamental tool in speech recognition for resolving acoustically ambiguous utterances. For example, we prefer the transcription forbear to four bear as the former string is far more frequent in English text. Probabilistic models also have application in optical character recognition, handwriting recognition, spelling correction, part-of-speech tagging, and machine translation. In this thesis, we investigate three problems involving the probabilistic modeling of language: smoothing n-gram models, statistical grammar induction, and bilingual sentence alignment. These three problems employ models at three different levels of language; they involve word-based, constituent-based, and sentence-based models, respectively. We describe techniques for improving the modeling of language at each of these levels, and surpass the performance of existing algorithms for each problem. We approach the three problems using three different frameworks. We relate each of these frameworks to the Bayesian paradigm, and show why each framework used was appropriate for the given problem. Finally, we show how our research addresses two central issues in probabilistic modeling: the sparse data problem and the problem of inducing hidden structure.Engineering and Applied Science","This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .","['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']",0,"['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']"
CC1186,P97-1063,A word-to-word model of translational equivalence,the mathematics of statistical machine translation parameter estimationquot,"['P F Brown', 'V J Della Pietra', 'S A Della Pietra', 'R L Mercer']",method,,This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .,"['To account for this difference, we can estimate separate values of X + and A-for different ranges of n(u,v).', 'Similarly, the hidden parameters can be conditioned on the linked parts of speech.', 'Word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences.', 'Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not.', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .', 'for their models (Brown et al., 1993b).', 'When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class.']",1,"['To account for this difference, we can estimate separate values of X + and A-for different ranges of n(u,v).', 'Similarly, the hidden parameters can be conditioned on the linked parts of speech.', 'Word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences.', 'Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not.', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .']"
CC1187,P97-1063,A word-to-word model of translational equivalence,line em up advances in alignment technology and their impact on translation support toolsquot,"['E Macklovitch', 'M-L Hannan']",,"We present a quantitative evaluation of one well-known word-alignment algorithm, as well as an analysis of frequent errors in terms of this model's underlying assumptions. Despite error rates that range from 22% to 32%, we argue that this technology can be put to good use in certain automated aids for human translators. We support our contention by pointing to several successful applications and outline ways in which text alignments below the sentence level would allow us to improve the performance of other translation support tools.","The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .","['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', 'Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%.', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'For many applications, this is the desired behavior.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards ."", 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.', 'The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.', 'Therefore, we ignored English words that were linked to nothing.']",1,"['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', 'Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards ."", 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.']"
CC1188,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a wordclass approach to labeling pscfg rules for machine translation,"['Andreas Zollmann', 'Stephan Vogel']",experiments,"In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features.    Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chinese-to-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available.","11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .","['For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus.', 'The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine.', 'For the hyperparameters, we set Į to 0.1 and p expand = 1/3 to give a preference to the rules with small fragments.', 'We built an s2t translation system with the achieved U-trees after the 1000th iteration.', 'We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .', 'Thus, to be convenient, we only conduct experiments with the SAMT system.']",4,"['For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus.', 'For the hyperparameters, we set I to 0.1 and p expand = 1/3 to give a preference to the rules with small fragments.', 'We built an s2t translation system with the achieved U-trees after the 1000th iteration.', 'We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .']"
CC1189,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a bayesian model of syntaxdirected tree to string grammar induction,"['Trevor Cohn', 'Phil Blunsom']",method,"Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step.","Differently , #AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .","['Using the initial target U-trees, source sentences and word alignment, we extract minimal GHKM translation rules 7 in terms of frontier nodes (Galley et al., 2004).', 'Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.', 'In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.', 'Differently , #AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'This subject will be one of our future work topics.']",4,"['Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.', 'Differently , #AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.']"
CC1190,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,hierarchical phrasebased translation,['David Chiang'],related work,"Left-to-right (LR) decoding (Watanabe et al., 2006) is promising decoding algorithm for hierarchical phrase-based translation (Hiero) that visits input spans in arbitrary order producing the output translation in left to right order. This leads to far fewer language model calls, but while LR decod-ing is more efficient than CKY decoding, it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower transla-tion quality as a result. This paper in-troduces two improvements to LR decod-ing that make it comparable in translation quality to CKY-based Hiero.",The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG ) .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG ) .', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG ) .', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.']"
CC1191,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,bayesian synchronous grammar induction,"['Phil Blunsom', 'Trevor Cohn', 'Miles Osborne']",related work,"We present a novel method for inducing synchronous context free grammars (SCFGs) from a corpus of parallel string pairs. SCFGs can model equivalence between strings in terms of substitutions, insertions and deletions, and the reordering of sub-strings. We develop a non-parametric Bayesian model and apply it to a machine translation task, using priors to replace the various heuristics commonly used in this field. Using a variational Bayes training procedure, we learn the latent structure of translation equivalence through the induction of synchronous grammar categories for phrasal translations, showing improvements in translation performance over maximum likelihood models.","#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .","['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', '#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .']"
CC1192,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,spmt statistical machine translation with syntactified target language phrases,"['Daniel Marcu', 'Wei Wang', 'Abdessamad Echihabi', 'Kevin Knight']",experiments,"We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a human-based quality metric that ranks translations on a scale from 1 to 5.","The system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ) .","['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']",5,"['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']"
CC1193,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,treebased translation without using parse trees,"['Feifei Zhai', 'Jiajun Zhang', 'Yu Zhou', 'Chengqing Zong']",related work,"Parse trees are indispensable to the existing tree- based translation models. However, there exist two major challenges in utilizing parse trees: 1) Fo r most language pairs, it is hard to get parse trees due to the lack of syntactic resources for training. 2) Numero us parse trees are not compatible with word alignment which is generally learned by GIZA++. Therefore, a number of useful translation rules are often excluded. To overcome these two problems, in this paper we make a great effort to bypass the parse trees and induce effective unsupervised trees for treebased translation models. Our unsupervised trees depend only on the word alignment without utilizing any syntactic resource or linguistic pars er. Hence, they are very beneficial for the translation between resource-poor languages. Our experimental results have shown that the string-to-tree translation system using our unsupervised trees significantly outperforms th e stringto-tree system using parse trees.",Our previous work ( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work ( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['Our previous work ( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.']"
CC1194,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,binarizing syntax trees to improve syntaxbased machine translation accuracy,"['Wei Wang', 'Kevin Knight', 'Daniel Marcu']",experiments,We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result.,"Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system .","['To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006).', 'Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system .']",5,"['Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system .']"
CC1195,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,spmt statistical machine translation with syntactified target language phrases,"['Daniel Marcu', 'Wei Wang', 'Abdessamad Echihabi', 'Kevin Knight']",introduction,"We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a human-based quality metric that ranks translations on a scale from 1 to 5.","Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']",0,"['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']"
CC1196,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a new stringtodependency machine translation algorithm with a target dependency language model,"['Libin Shen', 'Jinxi Xu', 'Ralph Weischedel']",introduction,"In this paper, we propose a novel string-todependency algorithm for statistical machine translation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set.","Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011b ) .","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011b ) .']",0,"['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011b ) .']"
CC1197,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a bayesian model of syntaxdirected tree to string grammar induction,"['Trevor Cohn', 'Phil Blunsom']",related work,"Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step.",#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', '#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', '#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"
CC1198,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a bayesian model of syntaxdirected tree to string grammar induction,"['Trevor Cohn', 'Phil Blunsom']",method,"Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step.","Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :","['The base distribution 0 ( | ) P r N is designed to assign prior probabilities to the STSG production rules.', 'Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :']",5,"['Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :']"
CC1199,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a wordclass approach to labeling pscfg rules for machine translation,"['Andreas Zollmann', 'Stephan Vogel']",related work,"In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features.    Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chinese-to-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available.",#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"
CC1200,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,what’s in a translation rule,"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']",method,"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.","Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ) .","['Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ) .', 'Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.', 'In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.', 'Differently,  designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment.', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'This subject will be one of our future work topics.']",5,"['Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ) .']"
CC1201,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,retraining monolingual parser bilingually for syntactic smt,"['Shujie Liu', 'Chi-Ho Li', 'Mu Li', 'Ming Zhou']",related work,"The training of most syntactic SMT approaches involves two essential components, word alignment and monolingual parser. In the current state of the art these two components are mutually independent, thus causing problems like lack of rule generalization, and violation of syntactic correspondence in translation rules. In this paper, we propose two ways of re-training monolingual parser with the target of maximizing the consistency between parse trees and alignment matrices. One is targeted self-training with a simple evaluation function; the other is based on training data selection from forced alignment of bilingual data. We also propose an auxiliary method for boosting alignment quality, by symmetrizing alignment matrices with respect to parse trees. The best combination of these novel methods achieves 3 Bleu point gain in an IWSLT task and more than 1 Bleu point gain in NIST tasks.",#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', '#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,['#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .']
CC1202,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,statistical significance tests for machine translation evaluation,['Philipp Koehn'],experiments,"If two translation systems differ differ in perfor-mance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling meth-ods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real.",The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .,"['The experiments are conducted on Chinese-to-English translation.', 'The training data are the FBIS corpus with approximately 7.1 million Chinese words and 9.2 million English words.', 'We obtain the bidirectional word alignment with GIZA++, and then adopt the grow-diag-final-and strategy to obtain the final symmetric alignment.', 'We train a 5gram language model on the Xinhua portion of the English Gigaword corpus and the English part of the training data.', 'For tuning and testing, we use the NIST MT 2003 evaluation data as the development set, and use the NIST MT04 and MT05 data as the test set.', 'We use MERT (Och, 2004) to tune parameters.', 'Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set.', 'The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty.', 'The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .']",5,"['The experiments are conducted on Chinese-to-English translation.', 'The training data are the FBIS corpus with approximately 7.1 million Chinese words and 9.2 million English words.', 'We obtain the bidirectional word alignment with GIZA++, and then adopt the grow-diag-final-and strategy to obtain the final symmetric alignment.', 'We train a 5gram language model on the Xinhua portion of the English Gigaword corpus and the English part of the training data.', 'For tuning and testing, we use the NIST MT 2003 evaluation data as the development set, and use the NIST MT04 and MT05 data as the test set.', 'We use MERT (Och, 2004) to tune parameters.', 'Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set.', 'The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty.', 'The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .']"
CC1203,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,scalable inference and training of contextrich syntactic translation models,"['Michel Galley', 'Jonathan Graehl', 'Kevin Knight', 'Daniel Marcu', 'Steve DeNeefe', 'Wei Wang', 'Ignacio Thayer']",experiments,"Statistical MT has made great progress in the last few years, but current translation models are weak on re-ordering and target language fluency. Syntactic approaches seek to remedy these problems. In this paper, we take the framework for acquiring multi-level syntactic translation rules of (Galley et al., 2004) from aligned tree-string pairs, and present two main extensions of their approach: first, instead of merely computing a single derivation that minimally explains a sentence pair, we construct a large number of derivations that include contextually richer rules, and account for multiple interpretations of unaligned words. Second, we propose probability estimates and a training procedure for weighting these rules. We contrast different approaches on real examples, show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated, and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules.",The system is implemented based on ( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .,"['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']",5,"['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']"
CC1204,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a bayesian model for learning scfgs with discontiguous rules,"['Abby Levenberg', 'Chris Dyer', 'Phil Blunsom']",related work,"We describe a nonparametric model and corresponding inference algorithm for learning Synchronous Context Free Grammar derivations for parallel text. The model employs a Pitman-Yor Process prior which uses a novel base distribution over synchronous grammar rules. Through both synthetic grammar induction and statistical machine translation experiments, we show that our model learns complex translational correspondences--- including discontiguous, many-to-many alignments---and produces competitive translation results. Further, inference is efficient and we present results on significantly larger corpora than prior work.",#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', '#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,['#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .']
CC1205,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,two languages are better than one for syntactic parsing,"['David Burkett', 'Dan Klein']",related work,"We show that jointly parsing a bitext can substantially improve parse quality on both sides. In a maximum entropy bitext parsing model, we define a distribution over source trees, target trees, and node-to-node alignments between them. Features include monolingual parse scores and various measures of syntactic divergence. Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables. The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F1 at predicting English side trees and 1.8 F1 at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation.",#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', '#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment .', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment .', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.']"
CC1206,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,what’s in a translation rule,"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']",method,"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.",9 We only use the minimal GHKM rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler .,"['The sampler might reinforce the frequent alignment errors (AE), which would harm the translation model (TM).', 'Actually, the frequent AEs also greatly impair the conventional TM.', 'Besides, our sampler encourages the correct alignments and simultaneously discourages the infrequent AEs.', 'Thus, compared with the conventional TMs, we believe that our final TM would not be worse due to AEs.', 'Our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 We only use the minimal GHKM rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler .']",5,"['The sampler might reinforce the frequent alignment errors (AE), which would harm the translation model (TM).', 'Besides, our sampler encourages the correct alignments and simultaneously discourages the infrequent AEs.', '9 We only use the minimal GHKM rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler .']"
CC1207,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,what’s in a translation rule,"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']",experiments,"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.","In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .","['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on (Galley et al., 2006) and ).', 'In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .']",5,"['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .']"
CC1208,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,what’s in a translation rule,"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']",method,"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.","Using the GHKM algorithm ( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .","['Our first Gibbs operator, Rotate, just works by sampling value of the Ȍparameters, one at a time, and changing the U-tree accordingly.', 'For example, in Figure 3(a), the s-node is currently in the left VWDWHȌ :HVDPSOHWKHȌRIWKLVQRGHDQGLI WKHVDPSOHGYDOXHRIȌLVZHNHHSWKHVWUXFWXUH unchanged, i.e., in the left state.', 'Otherwise, we change its state to the right state Ȍ , and transform the U-tree to Figure 3 Obviously, towards an s-node for sampling, the two values of Ȍ would define two different U-trees.', 'Using the GHKM algorithm ( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .', 'Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own.', ""In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node's lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node."", 'For instance, in Figure 3(a), as the s-node is not a frontier node, the left state (Ȍ ) defines only one rule: Using these STSG rules, the two derivations are evaluated as follows (We use the value of Ȍ to denote the corresponding STSG derivation):']",5,"['Using the GHKM algorithm ( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .', 'Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own.']"
CC1209,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,what’s in a translation rule,"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']",introduction,"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.","Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']",0,"['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']"
CC1210,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,dependency treelet translation syntactically informed phrasal smt,"['Chris Quirk', 'Arul Menezes', 'Colin Cherry']",introduction,"We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation. This method requires a source-language dependency parser, target language word segmentation and an unsupervised word alignment component. We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model. We describe an efficient decoder and show that using these treebased models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser. 1","Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #AUTHOR_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #AUTHOR_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']",0,"['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #AUTHOR_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']"
CC1211,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,syntax augmented machine translation via chart parsing,"['Andreas Zollmann', 'Ashish Venugopal']",experiments,"We present translation results on the shared task ""Exploiting Parallel Texts for Statistical Machine Translation"" generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence. Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop's baseline system. Our translation system is available open-source under the GNU General Public License.","To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG ) respectively .","['To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG ) respectively .']",5,"['To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG ) respectively .']"
CC1212,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,learning accurate compact and interpretable tree annotation,"['Slav Petrov', 'Leon Barrett', 'Romain Thibaux', 'Dan Klein']",experiments,"We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank. Starting with a simple Xbar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals. In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2 % on the Penn Treebank, higher than fully lexicalized systems.","To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG ) .","['To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG ) .', 'Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system.']",5,"['To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG ) .', 'Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system.']"
CC1213,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,binarizing syntax trees to improve syntaxbased machine translation accuracy,"['Wei Wang', 'Kevin Knight', 'Daniel Marcu']",method,We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result.,"This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011a ) .","['is the probability of producing the target tree fragment frag.', 'To generate frag,  used a geometric prior to decide how many child nodes to assign each node.', 'Differently, we require that each multi-word non-terminal node must have two child nodes.', 'This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011a ) .']",4,"['This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011a ) .']"
CC1214,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,treetostring alignment template for statistical machine translation,"['Yang Liu', 'Qun Liu', 'Shouxun Lin']",introduction,"We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models.","Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .","['In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']",0,"['In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']"
CC1215,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,restructuring relabeling and realigning for syntaxbased machine translation,"['Wei Wang', 'Jonathan May', 'Kevin Knight', 'Daniel Marcu']",introduction,,This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG ) .,"['However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training.', '2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG ) .']",0,['This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG ) .']
CC1216,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,joint parsing and alignment with weakly synchronized grammars,"['David Burkett', 'John Blitzer', 'Dan Klein']",related work,"Syntactic machine translation systems extract rules from bilingual, word-aligned, syntacti-cally parsed text, but current systems for pars-ing and word alignment are at best cascaded and at worst totally independent of one an-other. This work presents a unified joint model for simultaneous parsing and word alignment. To flexibly model syntactic divergence, we de-velop a discriminative log-linear model over two parse trees and an ITG derivation which is encouraged but not forced to synchronize with the parses. Our model gives absolute improvements of 3.3 F1 for English pars-ing, 2.1 F1 for Chinese parsing, and 5.5 F1 for word alignment over each task's indepen-dent baseline, giving the best reported results for both Chinese-English word alignment and joint parsing on the parallel portion of the Chi-nese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments.",Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment .', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,['Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment .']
CC1217,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,transforming trees to improve syntactic convergence,"['David Burkett', 'Dan Klein']",related work,"We describe a transformation-based learning method for learning a sequence of mono-lingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. Using the manually annotated English Chinese Transla-tion Treebank, we show how our method au-tomatically discovers transformations that ac-commodate differences in English and Chi-nese syntax. Furthermore, when transforma-tions are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0.9 BLEU im-provement over baseline trees.",#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', '#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', '#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"
CC1218,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a bayesian model of syntaxdirected tree to string grammar induction,"['Trevor Cohn', 'Phil Blunsom']",method,"Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step.","Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .","['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']",4,"['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']"
CC1219,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a gibbs sampler for phrasal synchronous grammar induction,"['Phil Blunsom', 'Trevor Cohn', 'Chris Dyer', 'Miles Osborne']",method,"We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches.","Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .","['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']",4,"['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']"
CC1220,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a gibbs sampler for phrasal synchronous grammar induction,"['Phil Blunsom', 'Trevor Cohn', 'Chris Dyer', 'Miles Osborne']",,"We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches.","In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .","['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.', 'Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models.']",1,"['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .']"
CC1221,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,syntax augmented machine translation via chart parsing,"['Andreas Zollmann', 'Ashish Venugopal']",related work,"We present translation results on the shared task ""Exploiting Parallel Texts for Statistical Machine Translation"" generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence. Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop's baseline system. Our translation system is available open-source under the GNU General Public License.",#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"
CC1222,W00-1017,WIT,implementation of coordinative nodding behavior on spoken dialogue systems,"['Jun-ichi Hirasawa', 'Noboru Miyazaki', 'Mikio Nakano', 'Takeshi Kawabata']",,"This paper proposes a mechanism that contributes to the implementation of a spoken dialogue system with which a user can communicate e ortlessly. In a dialogue, exchanges between participants promote the establishment of shared information and this leads to e ortless communication. This is called  dialogue coordination"". In particular, revealing the respondent's internal state, such as through nodding and back-channel feedback, promotes the establishment of shared information. This is called  manifestation"", which is one aspect of coordinative behavior, and a mechanism for handling manifestation is introduced. In a human-human dialogue, the listener's manifestative behavior often occurs during a speaker's utterance. However, systems using conventional speech recognition technologies cannot respond during the speaker's utterance. In order to solve this problem, the proposed mechanism, ISTAR protocol transmission, utilizes the intermediate speech recognition results without waiting for the end of the speaker's utterance. This realizes a system with exible manifestative behavior.","This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .","['word hypotheses.', 'As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992).', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .', 'This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.', 'This module continuously runs and outputs recognition results when it detects a speech interval.', 'This enables the language generation module to react immediately to user interruptions while the system is speaking.', 'The language model for speech recognition is a network (regular) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'A phrase is a sequence of words, which is to be defined in a domain-dependent way.', 'Sentences can be decomposed into a couple of phrases.', 'The reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self-repairs and repetition.', 'In Japanese, bunsetsu is appropriate for defining phrases.', 'A bunsetsu consists of one content word and a number (possibly zero) of function words.', 'In the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation.']",0,"['As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .', 'This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.', 'This module continuously runs and outputs recognition results when it detects a speech interval.', 'This enables the language generation module to react immediately to user interruptions while the system is speaking.', 'The language model for speech recognition is a network (regular) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'Sentences can be decomposed into a couple of phrases.', 'The reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self-repairs and repetition.', 'In the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation.']"
CC1223,W00-1017,WIT,the philips automatic train timetable information system,"['Harald Aust', 'Martin Oerder', 'Frank Seide', 'Volker Steinbiss']",introduction,,"The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']",0,"['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']"
CC1224,W00-1017,WIT,gus a frame driven dialog system,"['Daniel G Bobrow', 'Ronald M Kaplan', 'Martin Kay', 'Donald A Norman', 'Henry Thompson', 'Teny Winograd']",conclusion,,"There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .","['There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .', 'Incorporating such techniques would deo crease the system developer workload.', 'However, there has been no work on domain-independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which WIT handles well.', 'Therefore incorporating those techniques remains as a future work.']",3,"['There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .']"
CC1225,W00-1017,WIT,mimic an adaptive mixed initiative spoken dialogue system for information queries,['Junnifer Chu-Carroll'],,"This paper describes MIMIC, an adaptive mixed initiative spoken dialogue system that provides movie showtime information. MIMIC improves upon previous dialogue systems in two respects. First, it employs initiative-oriented strategy adaptation to automatically adapt response generation strategies based on the cumulative effect of information dynamically extracted from user utterances during the dialogue. Second, MIMIC's dialogue management architecture decouples its initiative module from the goal and response strategy selection processes, providing a general framework for developing spoken dialogue systems with different adaptation behavior.",They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG .,"['Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG .']",1,"['Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG .']"
CC1226,W00-1017,WIT,asj continuous speech corpus for research,"['Tetsunori Kobayashi', 'Shuichi Itahashi', 'Satoru Hayamizu', 'Toshiyuld Takezawa']",,,Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .,"['word hypotheses.', 'As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (Hirasawa et al., 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses.', 'This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.', 'This module continuously runs and outputs recognition results when it detects a speech interval.', 'This enables the language generation module to react immediately to user interruptions while the system is speaking.', 'The language model for speech recognition is a network (regular) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'A phrase is a sequence of words, which is to be defined in a domain-dependent way.', 'Sentences can be decomposed into a couple of phrases.', 'The reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self-repairs and repetition.', 'In Japanese, bunsetsu is appropriate for defining phrases.', 'A bunsetsu consists of one content word and a number (possibly zero) of function words.', 'In the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation.']",5,['Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .']
CC1227,W00-1017,WIT,learning to predict problematic situations in a spoken dialogue system experiments with how may i help you,"['Marilyn Walker', 'Irene Langkilde', 'Jerry Wright Allen Gorin', 'Diane Litman']",introduction,"Current spoken dialogue systems are deficient in their strategies for preventing, identifying and repairing problems that arise in the conversation. This paper reports results on learning to automatically identify and predict problematic human-computer dialogues in a corpus of 4774 dialogues collected with the How May I Help You spoken dialogue system. Our expectation is that the ability to predict problematic dialogues will allow the system's dialogue manager to modify its behavior to repair problems, and even perhaps, to prevent them. We train a problematic dialogue classifier using automatically-obtainable features that can identify problematic dialogues significantly better (23%) than the baseline. A classifier trained with only automatic features from the first exchange in the dialogue can predict problematic dialogues 7% more accurately than the baseline, and one trained with automatic features from the first two exchanges can perform 14% better than the baseline.","The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']",0,"['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .']"
CC1228,W00-1017,WIT,jupiter a telephonebased conversational interface for weather information,"['Victor Zue', 'Stephanie Seneff', 'James Glass', 'Joseph Polifroni', 'Christine Pao', 'Timothy J Hazen', 'Lee Hetherington']",introduction,,"The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']",0,"['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .']"
CC1229,W00-1017,WIT,understanding unsegmented user utterances in realtime spoken dialogue systems,"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']",experiments,"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.","WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'A sample dialogue between this system and a naive user is shown in Figure 2.', 'This system employs HTK as the speech recognition engine.', ""The weather information system can answer the user's questions about weather forecasts in Japan."", 'The vocabulary size is around 500, and the number of phrase structure rules is 31.', 'The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000.']",2,"['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'A sample dialogue between this system and a naive user is shown in Figure 2.', 'This system employs HTK as the speech recognition engine.', 'The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000.']"
CC1230,W00-1017,WIT,a robust system for natural spoken dialogue,"['James F Allen', 'Bradford W Miller', 'Eric K Ringger', 'Teresa Sikorsld']",introduction,"This paper describes a system that leads us to believe in the feasibility of constructing natural spoken dialogue systems in task-oriented domains. It specifically addresses the issue of robust interpretation of speech in the presence of recognition errors. Robustness is achieved by a combination of statistical error post-correction, syntactically- and semantically-driven robust parsing, and extensive use of the dialogue context. We present an evaluation of the system using time-to-completion and the quality of the final solution that suggests that most native speakers of English can use the system successfully with virtually no training.Comment: uuencoded, gzipped PostScript. Includes extra Appendi","The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']",0,"['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .']"
CC1231,W00-1017,WIT,a grammar and a parser for spontaneous speech,"['Mikio Nakano', 'Akira Shimazu', 'Kiyoshi Kogure']",conclusion,"This paper classifies distinctive phenomena occurring in Japanese spontaneous speech, and proposes a grammar and processing techniques for handling them. Parsers using a grammar for written sentences cannot deal with spontaneous speech because in spontaneous speech there are phenomena that do not occur in written sentences. A grammar based on analysis of transcripts of dialogues was therefore developed. It has two distinctive features: it uses short units as input units instead of using sentences in grammars for written sentences, and it covers utterances including phrases peculiar to spontaneous speech. Since the grammar is an augmentation of a grammar for written sentences, it can also be used to analyze complex utterances. Incorporating the grammar into the distributed natural language processing model described elsewhere enables the handling of utterances including variety of phenomena peculiar to spontaneous speech.","For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .","['Whereas previous finite-state-model-based toolkits place many severe restrictions on domain descriptions, WIT has enough descriptive power to build a variety of dialogue systems.', 'Although the dialogue state is represented by a simple attributevalue matrix, since there is no limitation on the number of attributes, it can hold more complicated information.', 'For example, it is possible to represent a discourse stack whose depth is limited.', 'Recording some dialogue history is also possible.', 'Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .', 'The language generation module features Common Lisp functions, so there is no limitation on the description.', 'Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997).', 'It is also possible to build a simple finite-state-model-based dialogue system using WIT.', 'States can be represented by dialogue phases in WIT.']",3,"['Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .']"
CC1232,W00-1017,WIT,understanding unsegmented user utterances in realtime spoken dialogue systems,"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']",,"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.","Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .","['Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .', 'Thus the system can respond immediately after user pauses when the user has the initiative.', 'When the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (Dohsaka and Shimazu, 1997).']",0,"['Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .']"
CC1233,W00-1017,WIT,an efficient dialogue control method under systems limited knowledge,"['Kohji Dohsaka', 'Norihito Yasuda', 'Noboru Miyazaki', 'Mikio Nakano', 'Kiyoald Aikawa']",experiments,"This paper presents a novel method that controls a dialogue between a spoken dialogue system and a user efficiently so that the system responds as helpfully as possible within the limits of its knowledge. Due to speech recognition errors, a system and user must engage in a ""confirmation dialogue"" to clarify a user's request. Although a confirmation dialogue is unavoidable, it should be as concise as possible. Previous methods do not sufficiently allow for the effect of the limits of the system's knowledge on the efficiency of dialogue. The result is unnecessarily long dialogues to confirm a user's request minutely even if the request is beyond the system's knowledge. This paper describes a method that controls a dialogue efficiently so as to avoid an unnecessary confirmation dialogue and presents a computational efficiency criterion for dialogue control within the limits of the system's knowledge.","WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #AUTHOR_TAG ) .","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #AUTHOR_TAG ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'A sample dialogue between this system and a naive user is shown in Figure 2.', 'This system employs HTK as the speech recognition engine.', ""The weather information system can answer the user's questions about weather forecasts in Japan."", 'The vocabulary size is around 500, and the number of phrase structure rules is 31.', 'The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000.']",2,"['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #AUTHOR_TAG ) .', 'A sample dialogue between this system and a naive user is shown in Figure 2.', 'This system employs HTK as the speech recognition engine.', 'The vocabulary size is around 500, and the number of phrase structure rules is 31.']"
CC1234,W00-1017,WIT,understanding unsegmented user utterances in realtime spoken dialogue systems,"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']",introduction,"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.",WIT features an incremental understanding method ( #AUTHOR_TAGb ) that makes it possible to build a robust and real-time system .,"['This paper presents WIT 1, which is a toolkit IWIT is an acronym of Workable spoken dialogue lnter-for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'WIT features an incremental understanding method ( #AUTHOR_TAGb ) that makes it possible to build a robust and real-time system .', 'In addition, WIT compiles domain-dependent system specifications into internal knowledge sources so that building systems is easier.', 'Although WIT requires more domaindependent specifications than finite-state-modelbased toolkits, WIT-based systems are capable of taking full advantage of language processing technology.', 'WIT has been implemented and used to build several spoken dialogue systems.']",5,"['This paper presents WIT 1, which is a toolkit IWIT is an acronym of Workable spoken dialogue lnter-for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'WIT features an incremental understanding method ( #AUTHOR_TAGb ) that makes it possible to build a robust and real-time system .', 'WIT has been implemented and used to build several spoken dialogue systems.']"
CC1235,W00-1017,WIT,constraint projection an efficient treatment of disjunctive feature descriptions,['Mikio Nakano'],,"Unification of disjunctive feature descriptions is important for efficient unification-based parsing. This paper presents constraint projection, a new method for unification of disjunctive feature structures represented by logical constraints. Constraint projection is a generalization of constraint unification, and is more efficient because constraint projection has a mechanism for abandoning information irrelevant to a goal specified by a list of variables.",Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .,"['The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .', 'When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.', 'The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.', ""Two kinds of sentenees can be considered; domain-related ones that express the user's intention about the reser-vafion and dialogue-related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial."", 'Considering the meeting room reservation system, examples of domain-related sentences are ""I need to book Room 2 on Wednesday"", ""I need to book Room 2"", and ""Room 2"" and dialogue-related ones are ""yes"", ""no"", and ""Okay"".']",5,['Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .']
CC1236,W00-1017,WIT,understanding unsegmented user utterances in realtime spoken dialogue systems,"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']",experiments,"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.",The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .,"['These roles are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied.', 'It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper.', 'The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'The command is one of the following:']",5,"['The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state.""]"
CC1237,W00-1017,WIT,europa a generic framework for developing spoken dialogue systems,"['Munehiko Sasajima', 'Yakehide Yano', 'Yasuyuld Kono']",introduction,"Voice interfaces are not popular since they are neither useful nor user-friendly for non-specialist users. In this paper, EUROPA, a new framework for developing spoken dialogue systems, is introduced. In developing EUROPA, the authors focused on three points : (1) acceptance of spoken language, (2) portability in terms of domain and task, and (3) practical performance of the applied system. The framework is applied to prototyping a car navigation system called MINOS. MINOS is built on a portable PC, can process over 700 words of recognition vocabulary, and is able to respond to a user's question within a few seconds.","To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .","['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.', 'It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain.', 'However, it limits system functions; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.', 'Another is GALAXY-II (Seneffet al., 1998), *Mikio Nakano is currently a visiting scientist at MIT Laboratory for Computer Science. which enables modules in a dialogue system to communicate with each other.', 'It consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers.', 'Although it requires more specifications than finitestate-model-based toolkits, it places less limitations on system functions.']",0,"['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain.']"
CC1238,W00-1017,WIT,understanding unsegmented user utterances in realtime spoken dialogue systems,"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']",,"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.","The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .","['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.', 'ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time.']",5,"['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .']"
CC1239,W00-1312,Cross-lingual information retrieval using hidden Markov models,phrasal translation and query expansion techniques for crosslanguage information retrievalquot,"['L Ballesteros', 'W B Croft']",related work,"Dictionary methods for cross-language information retrieval give performance below that for mono-lingual retrieval. Failure to translate multi-term phrases has been shown to be one of the factors responsible for the errors associated with dictionary methods. First, we study the importance of phrasal translation for this approach. Second, we explore the role of phrases in query expansion via local context analysis and local feedback and show how they can be used to significantly reduce the error associated with automatic dictionary translation.","Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).","['Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.', 'Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR.']",1,"['Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).']"
CC1240,W00-1312,Cross-lingual information retrieval using hidden Markov models,using structured queries for disambiguation in crosslanguage information retrievalquot,['D A Hull'],method,,"There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .","['A second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.', 'However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'By contrast, our HMM approach supports translation probabilities.', ""The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function."", 'Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations.']",1,"['A second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.', 'By contrast, our HMM approach supports translation probabilities.', ""The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function."", 'Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations.']"
CC1241,W00-1312,Cross-lingual information retrieval using hidden Markov models,using statistical testing in the evaluation of retrieval experimentsquot,['D Hull'],,,The one-sided t-test ( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .,"['The results in Table 4 show that manual disambiguation improves performance by 17% on Trec5C, 4% on Trec4S, but not at all on Trec6C.', 'Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries.', 'The one-sided t-test ( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .']",5,"['Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries.', 'The one-sided t-test ( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .']"
CC1242,W00-1312,Cross-lingual information retrieval using hidden Markov models,translingual information retrieval a comparative evaluationquot,"['J Carbonell', 'Y Yang', 'R Frederlcing', 'R Brown', 'Y Geng', 'D Lee']",related work,Translingual information retrieval TIR con sists of providing a query in one language and searching document collections in one or more di erent languages This paper introduces new TIR methods and reports on comparative TIR experiments with these new methods and with previously reported ones in a realistic setting Methods fall into two categories query trans lation based and statistical IR approaches es tablishing translingual associations The re sults show that using bilingual corpora for au tomated extraction of term equivalences in con text outperforms other methods Translin gual versions of the Generalized Vector Space Model GVSM and Latent Semantic Indexing LSI perform relatively well as does translin gual pseudo relevance feedback PRF All showed relatively small performance loss be tween monolingual and translingual versions Query translation based on a general machine readable bilingual dictionary heretofore the most popular method did not match the per formance of other more sophisticated methods Also the previous very high LSI results in the literature were discon rmed by more realistic relevance based evaluations,"The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG ) .","['The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG ) .', 'We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources (WordNet in Diekema et al., 1999).']",1,"['The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG ) .']"
CC1243,W00-1312,Cross-lingual information retrieval using hidden Markov models,a tutorial on hidden markov models and selected applications in speech recognitionquot,['L Rabiner'],,,â¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG ) on the TREC4 ad-hoc query set .,['â\x80¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG ) on the TREC4 ad-hoc query set .'],5,['â\x80¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG ) on the TREC4 ad-hoc query set .']
CC1244,W00-1312,Cross-lingual information retrieval using hidden Markov models,word sense disambiguation and information retrievalquot,['M Sanderson'],related work,,#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M.,"['Another common approach is term translation, e.g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M.']",0,"['While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M.']"
CC1245,W00-1312,Cross-lingual information retrieval using hidden Markov models,using structured queries for disambiguation in crosslanguage information retrievalquot,['D A Hull'],related work,,"Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG .","['Another common approach is term translation, e.g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG .', 'Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR.']",0,"['Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG .']"
CC1246,W00-1312,Cross-lingual information retrieval using hidden Markov models,a comparative study of query and document translation for crosslanguage information retrievalquot,['D W Oard'],related work,"Cross-language retrieval systems use queries in one natural language to guide retrieval of documents that might be written in another. Acquisition and representation of translation knowledge plays a central role in this process. This paper explores the utility of two sources of translation knowledge for cross-language retrieval. We have implemented six query translation techniques that use bilingual term lists and one based on direct use of the translation output from an existing machine translation system; these are compared with a document translation technique that uses output from the same machine translation system. Average precision measures on a TREC collection suggest that arbitrarily selecting a single dictionary translation is typically no less effective than using every translation in the dictionary, that query translation using a machine translation system can achieve somewhat better effectiveness than simpler techniques, and that document translation may result in further improvements in retrieval effectiveness under some conditions.","One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .","['Many approaches to cross-lingual IR have been published.', 'One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .', 'For most languages, there are no MT systems at all.', 'Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived.']",1,"['One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .']"
CC1247,W00-1312,Cross-lingual information retrieval using hidden Markov models,the effects of query structure and dictionary setups in dictionarybased crosslanguage information retrievalquot,['An Pirkola'],method,,"There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .","['A second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.', 'However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'By contrast, our HMM approach supports translation probabilities.', ""The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function."", 'Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations.']",1,"['A second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.', 'However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'By contrast, our HMM approach supports translation probabilities.', ""The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function."", 'Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations.']"
CC1248,W00-1312,Cross-lingual information retrieval using hidden Markov models,resolving ambiguity for crosslanguage retrievalquot,"['L Ballesteros', 'W B Croft']",method,," However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG ) .","['In this section we compare our approach with two other approaches.', 'One approach is ""simple substitution"", i.e., replacing a query term with all its translations and treating the translated query as a bag of words in mono-lingual retrieval.', 'Suppose we have a simple query Q=(a, b), the translations for a are al, a2, a3, and the translations for b are bl, b2. The translated query would be (at, a2, a3, b~, b2).', 'Since all terms are treated as equal in the translated query, this gives terms with more translations (potentially the more common terms) more credit in retrieval, even though such terms should potentially be given less credit if they are more common.', 'Also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query.', 'That is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', ' However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG ) .']",0,"['One approach is ""simple substitution"", i.e., replacing a query term with all its translations and treating the translated query as a bag of words in mono-lingual retrieval.', 'Since all terms are treated as equal in the translated query, this gives terms with more translations (potentially the more common terms) more credit in retrieval, even though such terms should potentially be given less credit if they are more common.', ' However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG ) .']"
CC1249,W00-1312,Cross-lingual information retrieval using hidden Markov models,corpusbased stemming using cooccurrence of word variantsquot,"['J Xu', 'W B Croft']",experiments,,A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .,"['For Spanish, we downloaded a bilingual English-Spanish lexicon from the Internet (http://www.activa.arrakis.es)', 'containing around 22,000 English words (16,000 English stems) and processed it similarly.', 'Each English word has around 1.5 translations on average.', 'A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .', 'One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon.', 'This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon.']",5,['A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .']
CC1250,W00-1312,Cross-lingual information retrieval using hidden Markov models,a language modeling approach to information retrievalquot,"['J Ponte', 'W B Croft']",related work,,"Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .","['Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']",1,"['Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']"
CC1251,W00-1312,Cross-lingual information retrieval using hidden Markov models,on relevance probabilistic indexing and information retrievalquot,"['M E Maron', 'K L Kuhns']",related work,,"Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .","['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']",1,"['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']"
CC1252,W00-1312,Cross-lingual information retrieval using hidden Markov models,finding terminology translations from nonparallel corporaquot,"['P Fung', 'K Mckeown']",,"We present a statistical word feature, the Word Relation Matrix, which can be used to find translated pairs of words and terms from non-parallel corpora, across language groups. Online dictionary entries are used as seed words to generate Word Relation Matrices for the unknown words according to correlation measures. Word Relation Matrices are then mapped across the corpora to find translation pairs. Translation accuracies are around 30% when only the top candidate is counted. Nevertheless, top 20 candidate output give a 50.9% average increase in accuracy on human translator performance.",Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .,"['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']",0,['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .']
CC1253,W00-1312,Cross-lingual information retrieval using hidden Markov models,a hidden markov model information retrieval systemquot,"['D Miller', 'T Leek', 'R Schwartz']",,"We present a new method for information retrieval using hidden Markov models (HMMs). We develop a general framework for incorporating multiple word generation mechanisms within the same model. We then demonstrate that an extremely simple realization of this model substantially outperforms standard tf :idf ranking on both the TREC-6 and TREC-7 ad hoc retrieval tasks. We go on to present a novel method for performing blind feedback in the HMM framework, a more complex HMM that models bigram production, and several other algorithmic re nements. Together, these methods form a state-of-the-art retrieval system that ranked among the best on the TREC-7 ad hoc retrieval task.","Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .","['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).', 'The approach therefore estimates the probability that a query Q is generated, given the document D is relevant.', '(A glossary of symbols used appears below.)']",5,"['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).', 'The approach therefore estimates the probability that a query Q is generated, given the document D is relevant.', '(A glossary of symbols used appears below.)']"
CC1254,W01-0706,Exploring evidence for shallow parsing,three generative lexicalised models for statistical parsing,['M Collins'],introduction,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1255,W01-0706,Exploring evidence for shallow parsing,introduction to the conll2000 shared task chunking,"['E F Tjong Kim Sang', 'S Buchholz']",experiments,,The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .,"['The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .', 'In this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'The goal in this case is therefore to accurately predict a collection of ¢ £ ¢ different types of phrases.', 'The chunk types are based on the syntactic category part of the bracket label in the Treebank.', 'Roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name.', 'The phrases are: adjective phrase (ADJP), adverb phrase (ADVP), conjunction phrase (CONJP), interjection phrase (INTJ), list marker (LST), noun phrase (NP), preposition phrase (PP), particle (PRT), subordinated clause (SBAR), unlike coordinated phrase (UCP), verb phrase (VP).', '(See details in (Tjong Kim Sang and Buchholz, 2000).)']",5,['The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .']
CC1256,W01-0706,Exploring evidence for shallow parsing,cooccurrence and transformation in linguistic structure,['Z S Harris'],introduction,,"Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997a;Charniak, 1997b;Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns -syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000).']",0,"['Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .']"
CC1257,W01-0706,Exploring evidence for shallow parsing,statistical parsing with a contextfree grammar and word statistics,['E Charniak'],introduction,,"Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1258,W01-0706,Exploring evidence for shallow parsing,three generative lexicalised models for statistical parsing,['M Collins'],introduction,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 ) .","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 ) .']",0,"['However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 ) .']"
CC1259,W01-0706,Exploring evidence for shallow parsing,three generative lexicalised models for statistical parsing,['M Collins'],experiments,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .","['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'After that, it will choose the candidate parse tree with the highest probability as output.', 'The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank.', 'The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5 (Collins, 1997).']",5,"['For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .']"
CC1260,W01-0706,Exploring evidence for shallow parsing,statistical parsing with a contextfree grammar and word statistics,['E Charniak'],introduction,,"However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .']",0,"['However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .']"
CC1261,W01-0706,Exploring evidence for shallow parsing,the snow learning architecture,"['A Carleson', 'C Cumby', 'J Rosen', 'D Roth']",experiments,,"SNoW ( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .","['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).', ""The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.""]",5,"['SNoW ( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .']"
CC1262,W01-0706,Exploring evidence for shallow parsing,a stochastic parts program and noun phrase parser for unrestricted text,['Kenneth W Church'],introduction,A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.>,"Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1263,W01-0706,Exploring evidence for shallow parsing,the use of classifiers in sequential inference,"['V Punyakanok', 'D Roth']",introduction,"We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1264,W01-0706,Exploring evidence for shallow parsing,parsing by chunks,['S P Abney'],introduction,"I begin with an intuition: when I read a sentence, I read it a chunk at a time. For example, the previous sentence breaks up something like this:    (1)    [I begin] [with an intuition]: [when I read] [a sentence], [I read it] [a chunk] [at a time]              These chunks correspond in some way to prosodic patterns. It appears, for instance, that the strongest stresses in the sentence fall one to a chunk, and pauses are most likely to fall between chunks. Chunks also represent a grammatical watershed of sorts. The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template. A simple context-free grammar is quite adequate to describe the structure of chunks. By contrast, the relationships between chunks are mediated more by lexical selection than by rigid templates. Co-occurrence of chunks is determined not just by their syntactic categories, but is sensitive to the precise words that head them; and the order in which chunks occur is much more flexible than the order of words within chunks.","Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997a;Charniak, 1997b;Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns -syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000).']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.']"
CC1265,W01-0706,Exploring evidence for shallow parsing,evaluation techniques for automatic semantic extraction comparing semantic and window based approaches,['G Greffenstette'],introduction,"As large on-line corpora become more prevalent, a number of attempts have been made to automatically extract thesaurus-like relations directly from text using knowledge poor methods. In the absence of any specific application, comparing the results of these attempts is difficult. Here we propose an evaluation method using gold standards, i.e., pre-existing hand-compiled resources, as a means of comparing extraction techniques. Using this evaluation method, we compare two semantic extraction techniques which produce similar word lists, one using syntactic context of words , and the other using windows of heuristically tagged words. The two techniques are very similar except that in one case selective natural language processing, a partial syntactic analysis, is performed. On a 4 megabyte corpus, syntactic contexts produce significantly better results against the gold standards for the most characteristk: words in the corpus, while windows produce better results for rare words.","Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997a;Charniak, 1997b;Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns -syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000).']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.']"
CC1266,W01-0706,Exploring evidence for shallow parsing,fastus a finitestate processor for information extraction from realworld text,"['D Appelt', 'J Hobbs', 'J Bear', 'D Israel', 'M Tyson']",introduction,,"First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ) .","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis.', 'This can be augmented later if more information is available.', 'Finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low -sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes.']",0,"['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ) .', 'Finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low -sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes.']"
CC1267,W01-0706,Exploring evidence for shallow parsing,introduction to the conll2000 shared task chunking,"['E F Tjong Kim Sang', 'S Buchholz']",experiments,,"Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']",5,"['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']"
CC1268,W01-0706,Exploring evidence for shallow parsing,a learning approach to shallow parsing,"['M Munoz', 'V Punyakanok', 'D Roth', 'D Zimak']",introduction,"A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally. The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference. Two instantiations of this approach are studied and experimental results for Noun-Phrases (NP) and Subject-Verb (SV) phrases that compare favorably with the best published results are presented. In doing that, we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors.} thus contribute to the understanding of how to model shallow parsing tasks as learning problems.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1269,W01-0706,Exploring evidence for shallow parsing,a linear observed time statistical parser based on maximum entropy models,['A Ratnaparkhi'],introduction,"This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.","However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .']",0,"['However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .']"
CC1270,W01-0706,Exploring evidence for shallow parsing,the use of classifiers in sequential inference,"['V Punyakanok', 'D Roth']",experiments,"We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.","The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ) .","['The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).', ""The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.""]",5,"['The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).', ""The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.""]"
CC1271,W01-0706,Exploring evidence for shallow parsing,introduction to the conll2000 shared task chunking,"['E F Tjong Kim Sang', 'S Buchholz']",experiments,,Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG ) terminology .,"['We start by reporting the results in which we compare the full parser and the shallow parser on the ""clean"" WSJ data.', 'Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG ) terminology .', 'The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', 'Next, we compared the performance of the parsers on the task of identifying atomic phrases 2 .', 'Here, again, the shallow parser exhibits significantly better performance.', 'Table 3 shows the results of extracting atomic phrases.']",5,['Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG ) terminology .']
CC1272,W01-0706,Exploring evidence for shallow parsing,text chunking using transformationbased learning,"['L A Ramshaw', 'M P Marcus']",introduction,"Transformation-based learning, a technique introduced by Eric Brill (1993b), has been shown to do part-of-speech tagging with fairly high accuracy. This same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive ""baseNP"" chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 93% for baseNP chunks (trained on 950K words) and 88% for somewhat more complex chunks that partition the sentence (trained on 200K words). Working in this new application and with larger template and training sets has also required some interesting adaptations to the transformation-based learning approach.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1273,W01-0706,Exploring evidence for shallow parsing,a learning approach to shallow parsing,"['M Munoz', 'V Punyakanok', 'D Roth', 'D Zimak']",experiments,"A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally. The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference. Two instantiations of this approach are studied and experimental results for Noun-Phrases (NP) and Subject-Verb (SV) phrases that compare favorably with the best published results are presented. In doing that, we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors.} thus contribute to the understanding of how to model shallow parsing tasks as learning problems.","Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']",2,"['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']"
CC1274,W01-0706,Exploring evidence for shallow parsing,performance structuresa psycholinguistic and linguistic appraisal cognitive psychology,"['J P Gee', 'F Grosjean']",introduction,,"Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .","['Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .']",0,"['Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .']"
CC1275,W01-0706,Exploring evidence for shallow parsing,a new statistical parser based on bigram lexical dependencies,['M Collins'],experiments,"This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.  1 Introduction  Lexical information has been shown to be crucial for many parsing decisions, such as prepositional-phrase attachment (for example (Hindle and Rooth 93)). However, early approaches to probabilistic parsing (Pereira and Schabes 92; Magerman and Marcus 91; Briscoe and Carroll 93) conditioned probabil..","For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .","['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'After that, it will choose the candidate parse tree with the highest probability as output.', 'The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank.', 'The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5 (Collins, 1997).']",5,"['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.']"
CC1276,W01-0706,Exploring evidence for shallow parsing,errordriven pruning of treebanks grammars for base noun phrase identification,"['C Cardie', 'D Pierce']",introduction,"Finding simple, non-recursive, base noun phrases is an important subtask for many natural language processing applications. While previous empirical methods for base NP identification have been rather complex, this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task. In particular, we present a corpus-based approach for finding base NPs by matching part-of-speech tag sequences. The training phase of the algorithm is based on two successful techniques: first the base NP grammar is read from a ""treebank"" corpus; then the grammar is improved by selecting rules with high ""benefit"" scores. Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on the Penn Treebank Wall Street Journal.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1277,W01-0706,Exploring evidence for shallow parsing,a learning approach to shallow parsing,"['M Munoz', 'V Punyakanok', 'D Roth', 'D Zimak']",experiments,"A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally. The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference. Two instantiations of this approach are studied and experimental results for Noun-Phrases (NP) and Subject-Verb (SV) phrases that compare favorably with the best published results are presented. In doing that, we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors.} thus contribute to the understanding of how to model shallow parsing tasks as learning problems.","The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .","['The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).', ""The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.""]",5,"['The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .']"
CC1278,W01-0706,Exploring evidence for shallow parsing,learning to resolve natural language ambiguities a unified approach,['D Roth'],experiments,"We analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be recast as learning linear separators in the feature space. Each of the methods makes a priori assumptions which it employs, given the data, when searching for its hypothesis. Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem.We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems. The learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes.In particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging. In all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best.","SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .","['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).', ""The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.""]",5,"['SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .']"
CC1279,W01-0706,Exploring evidence for shallow parsing,introduction to the conll2000 shared task chunking,"['E F Tjong Kim Sang', 'S Buchholz']",introduction,,would be chunked as follows ( Tjong Kim #AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957; Abney, 1991; Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney�s work (Abney, 1991), who has suggested to �chunk� sentences to base level phrases.', 'For example, the sentence �He reckons the current account deficit will narrow to only $ 1.8 billion in September .�', 'would be chunked as follows ( Tjong Kim #AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'would be chunked as follows ( Tjong Kim #AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .']"
CC1280,W01-0706,Exploring evidence for shallow parsing,the nyu system for muc6 or where’s syntax in,['R Grishman'],introduction,,"First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ) .","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis.', 'This can be augmented later if more information is available.', 'Finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low -sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes.']",0,"['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ) .', 'If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis.', 'This can be augmented later if more information is available.', 'Finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low -sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes.']"
CC1281,W01-0706,Exploring evidence for shallow parsing,a memorybased approach to learning shallow natural language patterns,"['S Argamon', 'I Dagan', 'Y Krymolowski']",introduction,"Recognizing shallow linguistic patterns, such as basic syntactic relationships between words, is a common task in applied natural language and text processing. The common practice for approaching this task is by tedious manual definition of possible pattern structures, often in the form of regular expressions or finite automata. This paper presents a novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus. The training data are stored as-is, in efficient suffix-tree data structures. Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus. This way, no information in the training is lost, as can happen in other learning systems that construct a single generalized model at the time of training. The paper presents experimental results for recognizing noun phrase, subject-verb and verb-object patterns in English. Since the learning approach enables easy porting to new domains, we plan to apply it to syntactic patterns in other languages and to sub-language patterns for information extraction.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1282,W01-0706,Exploring evidence for shallow parsing,cascaded grammatical relation assignment,"['S Buchholz', 'J Veenstra', 'W Daelemans']",introduction,"In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1283,W01-0706,Exploring evidence for shallow parsing,a linear observed time statistical parser based on maximum entropy models,['A Ratnaparkhi'],introduction,"This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1284,W01-0706,Exploring evidence for shallow parsing,the use of classifiers in sequential inference,"['V Punyakanok', 'D Roth']",experiments,"We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.","Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']",2,"['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']"
CC1285,W01-0706,Exploring evidence for shallow parsing,building a large annotated corpus of english the penn treebank,"['M P Marcus', 'B Santorini', 'M Marcinkiewicz']",experiments,"Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant.","Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .","['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', 'To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations.', 'This is done using the ""Chunklink"" program provided for CoNLL-2000 (Tjong Kim Sang andBuchholz, 2000).']",5,"['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .']"
CC1286,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,translating the xtag english grammar to hpsg,"['Yuka Tateisi', 'Kentaro Torisawa', 'Yusuke Miyao', 'Jun’ichi Tsujii']",introduction,,Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .,"['Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .', 'However, their method depended on translator�s intuitive analy- sis of the original grammar.', 'Thus the transla- tion was manual and grammar dependent.', 'The manual translation demanded considerable efforts from the translator, and obscures the equiva- lence between the original and obtained gram- mars.', 'Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars.', 'However, given the greater ex- pressive power of HPSG, it is impossible to con- vert an arbitrary HPSG grammar into an LTAG grammar.', 'Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capac- ity.', 'Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above ad- vantages.']",1,"['Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .', 'Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars.', 'However, given the greater ex- pressive power of HPSG, it is impossible to con- vert an arbitrary HPSG grammar into an LTAG grammar.', 'Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capac- ity.']"
CC1287,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,hybrid japanese parser with handcrafted grammar and statistics,"['Hiroshi Kanayama', 'Kentaro Torisawa', 'Yutaka Mitsuisi', 'Jun’ichi Tsujii']",introduction,,"There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).","['There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']",0,"['There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).']"
CC1288,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,efficient ltag parsing using hpsg parsers,"['Naoki Yoshinaga', 'Yusuke Miyao', 'Kentaro Torisawa', 'Jun’ichi Tsujii']",introduction,,A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG ) .,"['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar (The XTAG Research Group, 2001), which is a large-scale FB-LTAG grammar.', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG ) .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']",1,"['A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG ) .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']"
CC1289,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,an hpsg parser with cfg filtering,"['Kentaro Torisawa', 'Kenji Nishida', 'Yusuke Miyao', 'Jun’ichi Tsujii']",experiments,,"TNT refers to the HPSG parser ( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .","['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser ( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper  describes the detailed analysis on the factor of the difference of parsing performance.']",1,"['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'TNT refers to the HPSG parser ( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']"
CC1290,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan A Sag']",introduction,,"This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']",0,"['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.']"
CC1291,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,feature structures based tree adjoining grammars,"['K Vijay-Shanker', 'Aravind K Joshi']",introduction,"We have embedded Tree Adjoining Grammars (TAG) in a  feature structure based unification system. The resulting system, Feature Structure based Tree Adjoining Grammars (FTAG), captures the principle of factoring dependencies and recursion, fundamental to TAG u27s. We show that FTAG has an enhanced descriptive capacity compared to TAG formalism. We consider some restricted versions of this system and some possible linguistic stipulations that can be made. We briefly describe a calculus to represent the structures used by this system, extending on the work of Rounds, and Kasper [Rounds et al. 1986, Kasper et al. 1986)involving the logical formulation of feature structures","FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .","['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with ¥).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeillé and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']",0,"['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with Y=).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with PS).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled U onto an internal node of another tree with the same symbol U (Figure 4).', 'FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.']"
CC1292,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,typing as a means for validating feature structures,"['Anoop Sarkar', 'Shuly Wintner']",introduction,"We present a method for validating the consistency of feature structure speci cations by imposing a type discipline. A typed system facilitates a great number of compile-time checks: many possible errors can be detected before the grammar is used for parsing. We have constructed a type signature for an existing broad-coverage grammar of English, and implemented a type inference algorithm that operates on the feature structure speci cations in the grammar. The algorithm reports occurrences of incompatibility with the type signature. We have detected a large number of errors in the grammar; four types of errors are described in the paper.","ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .","['ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .', 'These works are re- stricted to each closed community, and the rela- tion between them is not well discussed.', 'Investi- gating the relation will be apparently valuable for both communities.']",0,"['ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .', 'Investi- gating the relation will be apparently valuable for both communities.']"
CC1293,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,feature structures based tree adjoining grammars,"['K Vijay-Shanker', 'Aravind K Joshi']",introduction,"We have embedded Tree Adjoining Grammars (TAG) in a  feature structure based unification system. The resulting system, Feature Structure based Tree Adjoining Grammars (FTAG), captures the principle of factoring dependencies and recursion, fundamental to TAG u27s. We show that FTAG has an enhanced descriptive capacity compared to TAG formalism. We consider some restricted versions of this system and some possible linguistic stipulations that can be made. We briefly describe a calculus to represent the structures used by this system, extending on the work of Rounds, and Kasper [Rounds et al. 1986, Kasper et al. 1986)involving the logical formulation of feature structures","This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']",0,"['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .']"
CC1294,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,parsing strategies with ‘lexicalized’ grammars application to tree adjoining grammars,"['Yves Schabes', 'Anne Abeille', 'Aravind K Joshi']",introduction,"In this paper we present a general parsing strategy that arose from the development of an Earley-type parsing algorithm for TAGs (Schabes and Joshi 1988) and from recent linguistic work in TAGs (Abeille 1988).In our approach elementary structures are associated with their lexical heads. These structures specify extended domains of locality (as compared to a context-free grammar) over which constraints can be stated. These constraints either hold within the elementary structure itself or specify what other structures can be composed with a given elementary structure.We state the conditions under which context-free based grammars can be 'lexicalized' without changing the linguistic structures originally produced. We argue that even if one extends the domain of locality of CFGs to trees, using only substitution does not give the freedom to choose the head of each structure. We show how adjunction allows us to 'lexicalize' a CFG freely.We then show how a 'lexicalized' grammar naturally follows from the extended domain of locality of TAGs and present some of the linguistic advantages of our approach.A novel general parsing strategy for 'lexicalized' grammars is discussed. In a first stage, the parser builds a set structures corresponding to the input sentence and in a second stage, the sentence is parsed with respect to this set. The strategy is independent of the linguistic theory adopted and of the underlying grammar formalism. However, we focus our attention on TAGs. Since the set of trees needed to parse an input sentence is supposed to be finite, the parser can use in principle any search strategy. Thus, in particular, a top-down strategy can be used since problems due to recursive structures are eliminated. The parser is also able to use non-local information to guide the search.We then explain how the Earley-type parser for TAGs can be modified to take advantage of this approach.",LTAG ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar,['LTAG ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar'],0,['LTAG ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar']
CC1295,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,verbmobil a translation system for facetoface dialog,"['M Kay', 'J Gawron', 'P Norvig']",introduction,,"In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG ) .","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG ) .', 'Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000).']",0,"['In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG ) .']"
CC1296,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,hpsgstyle underspecified japanese grammar with wide coverage,"['Yutaka Mitsuishi', 'Kentaro Torisawa', 'Jun’ichi Tsujii']",introduction,"This paper describes a wide-coverage Japanese grammar based on HPSG. The aim of this work is to see the coverage and accuracy attainable using an underspecified grammar. Underspecification, allowed in a typed feature structure formalism, enables us to write down a wide-coverage grammar concisely. The grammar we have implemented consists of only 6 ID schemata, 68 lexical entries (assigned to functional words), and 63 lexical entry templates (assigned to parts of speech (POSs) ). Furthermore, word-specific constraints such as subcategorization of verbs are not fixed in the grammar. However, this grammar can generate parse trees for 87% of the 10000 sentences in the Japanese EDR corpus. The dependency accuracy is 78% when a parser uses the heuristic that every bunsetsu is attached to the nearest possible one.","Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .']",0,"['Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .']"
CC1297,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,adapting hpsgtotag compilation to widecoverage grammars,"['Tilman Becker', 'Patrice Lopez']",introduction,,"Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .","['Figure 1 depicts a brief sketch of the RenTAL system.', 'The system consists of the following four modules: Tree converter, Type hierarchy extractor, Lexicon converter and Derivation translator.', 'The tree converter module is a core module of the system, which is an implementation of the grammar conversion algorithm given in Section 3. The type hierarchy extractor module extracts the symbols of the node, features, and feature values from the LTAG elementary tree templates and lexicon, and construct the type hierarchy from them.', 'The lexicon converter module converts LTAG elementary tree templates into HPSG lexical entries.', 'The derivation translator module takes HPSG parse  (Tateisi et al., 1998).', ""However, their method depended on translator's intuitive analysis of the original grammar."", 'Thus the translation was manual and grammar dependent.', 'The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .', 'However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar.', 'Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity.', 'Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages.']",1,"['Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .']"
CC1298,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,hybrid japanese parser with handcrafted grammar and statistics,"['Hiroshi Kanayama', 'Kentaro Torisawa', 'Yutaka Mitsuisi', 'Jun’ichi Tsujii']",introduction,,"Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG ) .","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG ) .']",0,"['Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG ) .']"
CC1299,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,the logic of typed feature structures,['Bob Carpenter'],introduction,"For those of us who belonged to the ""Bay Area (Computational) Linguistics Community,"" the early eighties were a heady time. Local researchers working on linguistics, computational linguistics, and logic programming were investigating notions of category, type, feature, term, and partial specification that appeared to converge to a powerful new approach for describing (linguistic) objects and their relationships by monotonic accumulation of constraints between their features. The seed notions had almost independently arisen in generalized phrase structure grammar (GPSG) (Gazdar et al. 1985), lexical-functional grammar (LFG) (Bresnan and Kaplan 1982), functionalunification grammar (FUG) (Kay 1985), logic programming (Colmerauer 1978, Pereira and Warren 1980), and terminological reasoning systems (Ait-Kaci 1984). It took, however, a lot of experimental and theoretical work to identify precisely what the core notions were, how particular systems related to the core notions, and what were the most illuminating mathematical accounts of that core. The development of the unificationbased formalism PATR-II (Shieber 1984) was an early step toward the definition of the core, but its mathematical analysis, and the clarification of the connections between the various systems, are only now coming to a reasonable closure. The Logic of Typed Feature Structures is the first monograph that brings all the main theoretical ideas into one place where they can be related and compared in a unified setting. Carpenter's book touches most of the crucial questions of the developments during the decade, provides proofs for central results, and reaches right up to the edge of current research in the field. These contributions alone make it an indispensable compendium for the researcher or graduate student working on constraint-based grammatical formalisms, and they also make it a very useful reference work for researchers in object-oriented databases and logic programming. Having discharged the main obligation of the reviewer of saying who should read the book under review and why, I will now survey each of the book's four parts while raising some more general questions impinging on the whole book as they arise from the discussion of each part.","An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG ) .","['An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG ) .', 'A lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.', 'An ID grammar rule represents a relation between a mother and its daughters, and is independent of lexical characteristics.', 'Figure 6 illustrates an example of bottom-up parsing with an HPSG grammar.', 'First, lexical entries for ""can"" and ""run"" are unified respectively with the daughter feature structures of']",0,"['An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG ) .', 'First, lexical entries for ""can"" and ""run"" are unified respectively with the daughter feature structures of']"
CC1300,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,a lexicalized tree adjoining grammar for english,['The XTAG Research Group'],introduction,"This paper presents a sizable grammar for English written in the Tree Adjoining grammar (TAG) formalism. The grammar uses a TAG that is both lexicalized (Schabes, Abeille, Joshi 1988) and feature-based (VijayShankar, Joshi 1988). In this paper, we describe a wide range of phenomena that it covers. A Lexicalized TAG (LTAG) is organized around a lexicon, which associates sets of elementary trees (instead of just simple categories) with the lexical items. A Lexicalized TAG consists of a finite set of trees associated with lexical items, and operations (adjunction and substitution) for composing the trees. A lexical item is called the anchor of its corresponding tree and directly determines both the tree's structure and its syntactic features. In particular, the trees define the domain of locality over which constraints are specified and these constraints are local with respect to their anchor. In this paper, the basic tree structures of the English LTAG are described, along with some relevant features. The interaction between the morphological and the syntactic components of the lexicon is also explained. Next, the properties of the different tree structures are discussed. The use of S complements exclusively allows us to take full advantage of the treatment of unbounded dependencies originally presented in Joshi (1985) and Kroch and Joshi (1985). Structures for auxiliaries and raising-verbs which use adjunction trees are also discussed. We present a representation of prepositional complements that is based on extended elementary trees. This representation avoids the need for preposition incorporation in order to account for double whquestions (preposition stranding and pied-piping) and the pseudo-passive. A treatment of light verb constructions is also given, similar to what Abeille (1988c) has presented. Again, neither noun nor adjective incorporation is needed to handle double passives and to account for CNPC violations in these constructions. TAG'S extended domain of locality allows us to handle, within a single level of syntactic description, phenomena that in other frameworks require either dual analyses or reanalysis. In addition, following Abeille and Schabes (1989), we describe how to deal with semantic non compositionality in verb-particle combinations, light verb constructions and idioms, without losing the internal syntactic composition of these structures. The last sections discuss current work on PRO, case, anaphora and negation, and outline future work on copula constructions and small clauses, optional arguments, adverb movement and the nature of syntactic rules in a lexicalized framework. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-24. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/527 A Lexicalized Tree Adjoining Grammar For English MS-CIS-90-24 LINC LAB 170 Anne Abeillh Kathleen Bishop Sharon Cote Yves Schabes Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19104-6389","We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .","['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']",5,"['We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .']"
CC1301,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,evolution of the xtag system,"['Christy Doran', 'Beth Ann Hockey', 'Anoop Sarkar', 'B Srinivas', 'Fei Xia']",introduction,,"The XTAG group ( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .","['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with ¥).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FB-LTAG (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) is an extension of the LTAG formalism.', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group ( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .', 'Development of a large-scale French grammar (Abeillé and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']",0,"['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with Y=).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with PS).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled U onto an internal node of another tree with the same symbol U (Figure 4).', 'FB-LTAG (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) is an extension of the LTAG formalism.', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group ( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .']"
CC1302,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,grammar conversion from fbltag to hpsg,"['Naoki Yoshinaga', 'Yusuke Miyao']",introduction,,The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG ) .,"['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoin-ing Grammar (FB-LTAG 1 ) (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) by a method of grammar conversion.', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG ) .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']",0,['The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG ) .']
CC1303,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,some experiments on indicators of parsing complexity for lexicalized grammars,"['Anoop Sarkar', 'Fei Xia', 'Aravind Joshi']",experiments,"In this paper, we identify syntactic lexical ambiguity and sentence complexity as factors that contribute to parsing complexity in fully lexicalized grammar formalisms such as Lexicalized Tree Adjoining Grammars. We also report on experiments that explore the effects of these factors on parsing complexity. We discuss how these constraints can be exploited in improving efficiency of parsers for such grammar formalisms.","In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .","['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .', 'TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2).', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper  describes the detailed analysis on the factor of the difference of parsing performance.']",1,"['In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper  describes the detailed analysis on the factor of the difference of parsing performance.']"
CC1304,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,a study of tree adjoining grammars,['K Vijay-Shanker'],introduction,"Constrained grammatical system have been the object of study in computational linguistics over the last few years, both with respect to their linguistic adequacy and their computational properties. A Tree Adjoining Grammar (TAG) is a tree rewriting system whose linguistic relevance has been extensively studied. A key property of these systems is that a TAG factors recursion from the co-occurrence restrictions. In this thesis, we study some mathematical properties of TAG u27s. We show that TAG u27s have several interesting properties and are a natural generalization of Context Free Grammars. We show the equivalence of the classes of languages generated by TAG u27s with those generated by Head Grammars and a linear version of Indexed Grammars, which have been studied for their linguistic applicability. We define the embedded pushdown automaton, an extention of the pushdown automaton, and prove that they are equivalent to TAG u27s. We show that the class of Tree Adjoining Languages form a substitution closed abstract family of languages, and that each Tree Adjoining Language is a semilinear language. We show that a TAG can be parsed in polynomial time by adapting the Cocke-Kasami-Younger algorithm for CFL u27s. Feature structures, essentially a set of attribute value pairs, have been used in computational linguistics to make statements of equality to capture some linguistic phenomena such as subcategorization and agreement. We embed TAG u27s in a feature structure based framework. We show that the resulting system has several advantages over TAG u27s. We give a mathematical model of this system based on the logical calculus developed by Rounds, Kasper, and Manaster-Ramer. Finally, we propose a restriction of this system and show how parsing of such a system can be done efficiently","FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .","['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with ¥).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeillé and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']",0,"['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with Y=).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with PS).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled U onto an internal node of another tree with the same symbol U (Figure 4).', 'FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeille and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']"
CC1305,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,on building a more efficient grammar by exploiting types,['Dan Flickinger'],introduction,"Modern grammar development platforms often support multiple devices for representing properties of a natural language, giving the grammar writer some freedom in implementing analyses of linguistic phenomena. These design alternatives can have dramatic consequences for efficiency both in processing and in grammar building. In this paper I report on three experiments in making systematic modifications to a broad-coverage grammar of English in order to gain efficiency without loss of linguistic elegance. While the experiments are to some degree both platform-dependant and theory-bound, the kinds of modifications reported should be applicable to any unification-based grammar which makes use of types. The results make a strong case for a more visible role for the linguist in the collaborative effort to achieve greater processing efficiency.","Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG ) .","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG ) .', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000).']",0,"['Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG ) .', 'Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000).']"
CC1306,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,evolution of the xtag system,"['Christy Doran', 'Beth Ann Hockey', 'Anoop Sarkar', 'B Srinivas', 'Fei Xia']",introduction,,"There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .","['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']",0,"['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']"
CC1307,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,an hpsg parser with cfg filtering,"['Kentaro Torisawa', 'Kenji Nishida', 'Yusuke Miyao', 'Jun’ichi Tsujii']",experiments,,"LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ) .","['The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ) .', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (Marcus et al., 1994) 6 (the average length is 6.32 words).', 'This result empirically attested the strong equivalence of our algorithm.']",0,"['LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ) .']"
CC1308,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,efficient ltag parsing using hpsg parsers,"['Naoki Yoshinaga', 'Yusuke Miyao', 'Kentaro Torisawa', 'Jun’ichi Tsujii']",experiments,,Another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .,"['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2).', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .']",0,"['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2).', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .']"
CC1309,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,building a large annotated corpus of english the penn treebank computational linguistics,"['Mitchell Marcus', 'Beatrice Santorini', 'Mary Ann Marcinkiewicz']",experiments,,The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .,"['The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .', 'This result empirically attested the strong equivalence of our algorithm.']",5,"['We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .']"
CC1310,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,lilfes — towards a practical hpsg parsers,"['Takaki Makino', 'Minoru Yoshida', 'Kentaro Torisawa', 'Jun’ichi Tsujii']",experiments,,The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .,"['The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (Marcus et al., 1994) 6 (the average length is 6.32 words).', 'This result empirically attested the strong equivalence of our algorithm.']",5,"['The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (Marcus et al., 1994) 6 (the average length is 6.32 words).', 'This result empirically attested the strong equivalence of our algorithm.']"
CC1311,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,a study of tree adjoining grammars,['K Vijay-Shanker'],introduction,"Constrained grammatical system have been the object of study in computational linguistics over the last few years, both with respect to their linguistic adequacy and their computational properties. A Tree Adjoining Grammar (TAG) is a tree rewriting system whose linguistic relevance has been extensively studied. A key property of these systems is that a TAG factors recursion from the co-occurrence restrictions. In this thesis, we study some mathematical properties of TAG u27s. We show that TAG u27s have several interesting properties and are a natural generalization of Context Free Grammars. We show the equivalence of the classes of languages generated by TAG u27s with those generated by Head Grammars and a linear version of Indexed Grammars, which have been studied for their linguistic applicability. We define the embedded pushdown automaton, an extention of the pushdown automaton, and prove that they are equivalent to TAG u27s. We show that the class of Tree Adjoining Languages form a substitution closed abstract family of languages, and that each Tree Adjoining Language is a semilinear language. We show that a TAG can be parsed in polynomial time by adapting the Cocke-Kasami-Younger algorithm for CFL u27s. Feature structures, essentially a set of attribute value pairs, have been used in computational linguistics to make statements of equality to capture some linguistic phenomena such as subcategorization and agreement. We embed TAG u27s in a feature structure based framework. We show that the resulting system has several advantages over TAG u27s. We give a mathematical model of this system based on the logical calculus developed by Rounds, Kasper, and Manaster-Ramer. Finally, we propose a restriction of this system and show how parsing of such a system can be done efficiently","This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']",0,"['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .']"
CC1312,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,lilfes — towards a practical hpsg parsers,"['Takaki Makino', 'Minoru Yoshida', 'Kentaro Torisawa', 'Jun’ichi Tsujii']",introduction,,"There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .","['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']",0,"['There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .']"
CC1313,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,twostep tag parsing revisited,"['Peter Poller', 'Tilman Becker']",introduction,"Based on the work in (Poller, 1994) and a minor assumption about a normal form for TAGs, we present a highly simplified version of the twostep parsing approach for TAGs which allows for a much easier analysis of run-time and space complexity. It also snggests how restrictions on the grammars might result in improvements in run-time complexity. The main advantage of a two-step parsing system shows in practical applications like Verbmobil (Bub et al., 1997) where the parser must look at multiple hypotheses supplied by a speech recognizer (encoded in a word hypotheses lattice) and filter out illicit hypotheses as early as possible. The first (context-free) step of our parser filters out some illicit hypotheses fast (O(n3 )); the constructed parsing matrix is then reused for the second step, the complete (O(n6 )) TAG parse.","There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).","['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']",0,"['There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).']"
CC1314,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,grammar conversion from fbltag to hpsg,"['Naoki Yoshinaga', 'Yusuke Miyao']",,,The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .,['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .'],0,['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .']
CC1315,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,a lexicalized tree adjoining grammar for english,['The XTAG Research Group'],introduction,"This paper presents a sizable grammar for English written in the Tree Adjoining grammar (TAG) formalism. The grammar uses a TAG that is both lexicalized (Schabes, Abeille, Joshi 1988) and feature-based (VijayShankar, Joshi 1988). In this paper, we describe a wide range of phenomena that it covers. A Lexicalized TAG (LTAG) is organized around a lexicon, which associates sets of elementary trees (instead of just simple categories) with the lexical items. A Lexicalized TAG consists of a finite set of trees associated with lexical items, and operations (adjunction and substitution) for composing the trees. A lexical item is called the anchor of its corresponding tree and directly determines both the tree's structure and its syntactic features. In particular, the trees define the domain of locality over which constraints are specified and these constraints are local with respect to their anchor. In this paper, the basic tree structures of the English LTAG are described, along with some relevant features. The interaction between the morphological and the syntactic components of the lexicon is also explained. Next, the properties of the different tree structures are discussed. The use of S complements exclusively allows us to take full advantage of the treatment of unbounded dependencies originally presented in Joshi (1985) and Kroch and Joshi (1985). Structures for auxiliaries and raising-verbs which use adjunction trees are also discussed. We present a representation of prepositional complements that is based on extended elementary trees. This representation avoids the need for preposition incorporation in order to account for double whquestions (preposition stranding and pied-piping) and the pseudo-passive. A treatment of light verb constructions is also given, similar to what Abeille (1988c) has presented. Again, neither noun nor adjective incorporation is needed to handle double passives and to account for CNPC violations in these constructions. TAG'S extended domain of locality allows us to handle, within a single level of syntactic description, phenomena that in other frameworks require either dual analyses or reanalysis. In addition, following Abeille and Schabes (1989), we describe how to deal with semantic non compositionality in verb-particle combinations, light verb constructions and idioms, without losing the internal syntactic composition of these structures. The last sections discuss current work on PRO, case, anaphora and negation, and outline future work on copula constructions and small clauses, optional arguments, adverb movement and the nature of syntactic rules in a lexicalized framework. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-24. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/527 A Lexicalized Tree Adjoining Grammar For English MS-CIS-90-24 LINC LAB 170 Anne Abeillh Kathleen Bishop Sharon Cote Yves Schabes Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19104-6389","Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .","['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with ¥).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FB-LTAG (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) is an extension of the LTAG formalism.', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeillé and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']",0,"['In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .']"
CC1316,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,a lexicalized tree adjoining grammar for english,['The XTAG Research Group'],experiments,"This paper presents a sizable grammar for English written in the Tree Adjoining grammar (TAG) formalism. The grammar uses a TAG that is both lexicalized (Schabes, Abeille, Joshi 1988) and feature-based (VijayShankar, Joshi 1988). In this paper, we describe a wide range of phenomena that it covers. A Lexicalized TAG (LTAG) is organized around a lexicon, which associates sets of elementary trees (instead of just simple categories) with the lexical items. A Lexicalized TAG consists of a finite set of trees associated with lexical items, and operations (adjunction and substitution) for composing the trees. A lexical item is called the anchor of its corresponding tree and directly determines both the tree's structure and its syntactic features. In particular, the trees define the domain of locality over which constraints are specified and these constraints are local with respect to their anchor. In this paper, the basic tree structures of the English LTAG are described, along with some relevant features. The interaction between the morphological and the syntactic components of the lexicon is also explained. Next, the properties of the different tree structures are discussed. The use of S complements exclusively allows us to take full advantage of the treatment of unbounded dependencies originally presented in Joshi (1985) and Kroch and Joshi (1985). Structures for auxiliaries and raising-verbs which use adjunction trees are also discussed. We present a representation of prepositional complements that is based on extended elementary trees. This representation avoids the need for preposition incorporation in order to account for double whquestions (preposition stranding and pied-piping) and the pseudo-passive. A treatment of light verb constructions is also given, similar to what Abeille (1988c) has presented. Again, neither noun nor adjective incorporation is needed to handle double passives and to account for CNPC violations in these constructions. TAG'S extended domain of locality allows us to handle, within a single level of syntactic description, phenomena that in other frameworks require either dual analyses or reanalysis. In addition, following Abeille and Schabes (1989), we describe how to deal with semantic non compositionality in verb-particle combinations, light verb constructions and idioms, without losing the internal syntactic composition of these structures. The last sections discuss current work on PRO, case, anaphora and negation, and outline future work on copula constructions and small clauses, optional arguments, adverb movement and the nature of syntactic rules in a lexicalized framework. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-24. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/527 A Lexicalized Tree Adjoining Grammar For English MS-CIS-90-24 LINC LAB 170 Anne Abeillh Kathleen Bishop Sharon Cote Yves Schabes Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19104-6389","We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .","['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']",5,"['We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .']"
CC1317,W02-0309,Biomedical text retrieval in languages with a complex morphology,how effective is suffixing,['D Harman'],conclusion,"s and titles from the Cranfield collection (with 225 queries and 1400 documents), comprised the major test collection for this study. The Medlars collection (30 queries and 1033 documents), and the CACM collection (64 queries and 3204 documents) were used to provide information about the variation of stemming performance across different subject areas and test collections. In addition to the standard recall/precision measures, with SMART system averaging (Salton, 1971), several methods more suited to an interactive retrieval environment were adopted. The interactive environment returns lists of the top ranked documents, and allows the users to scan titles of a group of documents a screenful at a time, so that the ranking of individual documents within the screenful is not as important as the total number of relevant titles within a screen. Furthermore, the number of relevant documents in the first few screens is far more important for the user than the number of relevant in the last screenfuls. Three measures were selected which evaluate performance at given rank cutoff points, such as those corresponding to a screenful of document titles. The first measure, the E measure (Van Rijsbergen, 1979), is a weighted combination of recall and precision that evaluates a set of retrieved documents at a given cutoff, ignoring the ranking within that set. The measure may have weights of 0.5, 1.0, and 2.0 which correspond, respectively, to attaching half the importance to recall as to precision, equal importance to both, and double importance to recall. A lower E value indicates a more effective performance. A second measure, the total number of relevant documents retrieved by a given cutoff, was also calculated. Cutoffs of 10 and 30 documents were used, with ten reflecting a minimum number a user might be expected to TABLE 2. Retrieval performance for Cranfteld 225. scan, and 30 being an assumed upper limit of what a user would scan before query modification. The third measure applicable to the interactive environment is the number of queries that retrieve no relevant documents by the given cutoff. This measure is important because many types of query modification techniques, such as relevance feedback, require relevant documents to be in the retrieved set to work well. These measures were all used in Croft (1983) as complementary measures to the standard recall/precision evaluation.","There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']",0,"['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .']"
CC1318,W02-0309,Biomedical text retrieval in languages with a complex morphology,morphosemantic parsing of medical expressions,"['R Baud', 'C Lovis', 'A-M Rassinoux', 'J-R Scherrer']",introduction,,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .']",0,"['The efforts required for performing morphological analysis vary from language to language.', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .']"
CC1319,W02-0309,Biomedical text retrieval in languages with a complex morphology,development of a stemming algorithm,['J Lovins'],introduction,"A stemming algorithm, a procedure to reduce all words with the same stem to a common form, is useful in many areas of computational linguistics and information-retrieval work. While the form of the algorithm varies with its application, certain linguistic problems are common to any stemming procedure. As a basis for evaluation of previous attempts to deal with these problems, this paper first discusses the theoretical and practical attributes of stemming algorithms. Then a new version of a context-sensitive, longest-match stemming algorithm for English is proposed; though developed for use in a library information transfer system, it is of general application. A major linguistic problem in stemming, variation in spelling of stems, is discussed in some detail and several feasible programmed solutions are outlined, along with sample results of one of these methods.","mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998).']",0,"['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.']"
CC1320,W02-0309,Biomedical text retrieval in languages with a complex morphology,stemming algorithms a case study for detailed evaluation,['D A Hull'],conclusion,"The majority of information retrieval experiments are evaluated by measures such as average precision and average recall. Fundamental decisions about the superiority of one retrieval technique over another are made solely on the basis of these measures. We claim that average performance figures need to be validated with a careful statistical analysis and that there is a great deal of additional information that can be uncovered by looking closely at the results of individual queries. This article is a case study of stemming algorithms which describes a number of novel approaches to evaluation and demonstrates their value. (c) 1996 John Wiley & Sons, Inc.","There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']",0,"['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .']"
CC1321,W02-0309,Biomedical text retrieval in languages with a complex morphology,an algorithm for suffix stripping,['M Porter'],introduction,"Purpose - The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. This work was originally published in Program in 1980 and is republished as part of a series of articles commemorating the 40th anniversary of the journal. Design/methodology/approach - An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. Findings - Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length. Originality/value - The piece provides a useful historical document on information retrieval.","mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998).']",0,"['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.']"
CC1322,W02-0309,Biomedical text retrieval in languages with a complex morphology,viewing stemming as recall enhancement,"['W Kraaij', 'R Pohlmann']",introduction,"Previous research on stemming has shown both positive and negative effects on retrieval performance. This paper describes an experiment in which several linguistic and non-linguistic stemmers are evaluated on a Dutch test collection. Experiments especially focus on the measurement of Recall. Results show that linguistic stemming restricted to inflection yields a significant improvement over full linguistic and non-linquistic stemming, both in average Precision and R-Recall. Best results are obtained with a linguistic stemmer which is enhanced with compound analysis. This version has a significantly better Recall than a system without stemming, without a significant deterioration of Precision.","Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J Â¨ appinen and Niemist Â¨ o , 1988 ; #AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J Â¨ appinen and Niemist Â¨ o , 1988 ; #AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]",0,"['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J Â¨ appinen and Niemist Â¨ o , 1988 ; #AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure']).""]"
CC1323,W02-0309,Biomedical text retrieval in languages with a complex morphology,the use of morphosemantic regularities in the medical vocabulary for automatic lexical coding methods ofinformation in,['S Wolff'],introduction,The use of morphosemantic regularities in the medical vocabulary for automatic lexical coding. -,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']",0,"[""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']"
CC1324,W02-0309,Biomedical text retrieval in languages with a complex morphology,an algorithm for suffix stripping,['M Porter'],conclusion,"Purpose - The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. This work was originally published in Program in 1980 and is republished as part of a series of articles commemorating the 40th anniversary of the journal. Design/methodology/approach - An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. Findings - Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length. Originality/value - The piece provides a useful historical document on information retrieval.","There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']",0,"['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .']"
CC1325,W02-0309,Biomedical text retrieval in languages with a complex morphology,automated coding of diagnoses three methods compared,"['P Franz', 'A Zaiss', 'S Schulz', 'U Hahn', 'R Klar']",experiments,"In Germany, new legal requirements have raised the importance of the accurate encoding of admission and discharge diseases for in- and outpatients. In response to emerging needs for computer-supported tools we examined three methods for automated coding of German-language free-text diagnosis phrases. We compared a language-independent lexicon-free n-gram approach with one which uses a dictionary of medical morphemes and refines the query by a mapping to SNOMED codes. Both techniques produced a ranked output of possible diagnoses within a vector space framework for retrieval. The results did not reveal any significant difference: The correct diagnosis was found in approximately 40% for three-digit codes, and 30% for four-digit codes. The lexicon-based method was then modified by substituting the vector space ranking by a heuristic approach that capitalizes on the semantic structure of SNOMED, thus raising the number of correct diagnoses significantly (approximately 50% for three-digit codes, and 40% for four-digit codes). As a result, we claim that lexicon-based retrieval methods do not perform better than the lexicon-free ones, unless conceptual knowledge is added.",Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method .,"['), but at high ones its precision decreases almost dramatically.', 'Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method .']",1,"['), but at high ones its precision decreases almost dramatically.', 'Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method .']"
CC1326,W02-0309,Biomedical text retrieval in languages with a complex morphology,morphologic analysis of compound words,['F Wingert'],introduction,,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']",0,"['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jappinen and Niemisto, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekcioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']"
CC1327,W02-0309,Biomedical text retrieval in languages with a complex morphology,viewing stemming as recall enhancement,"['W Kraaij', 'R Pohlmann']",conclusion,"Previous research on stemming has shown both positive and negative effects on retrieval performance. This paper describes an experiment in which several linguistic and non-linguistic stemmers are evaluated on a Dutch test collection. Experiments especially focus on the measurement of Recall. Results show that linguistic stemming restricted to inflection yields a significant improvement over full linguistic and non-linquistic stemming, both in average Precision and R-Recall. Best results are obtained with a linguistic stemmer which is enhanced with compound analysis. This version has a significantly better Recall than a system without stemming, without a significant deterioration of Precision.","Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 ) .","['There has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 ) .']",0,"['There has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 ) .']"
CC1328,W02-0309,Biomedical text retrieval in languages with a complex morphology,automatic text processing the transformation analysis and retrieval ofinformation by computer,['Gerard Salton'],experiments,,"The retrieval process relies on the vector space model ( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .","['For unbiased evaluation of our approach, we used a home-grown search engine (implemented in the PYTHON script language).', 'It crawls text/HTML files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf-idf metric.', 'The retrieval process relies on the vector space model ( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .', 'The search engine produces a ranked output of documents.']",5,"['For unbiased evaluation of our approach, we used a home-grown search engine (implemented in the PYTHON script language).', 'It crawls text/HTML files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf-idf metric.', 'The retrieval process relies on the vector space model ( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .', 'The search engine produces a ranked output of documents.']"
CC1329,W02-0309,Biomedical text retrieval in languages with a complex morphology,development of a stemming algorithm,['J Lovins'],conclusion,"A stemming algorithm, a procedure to reduce all words with the same stem to a common form, is useful in many areas of computational linguistics and information-retrieval work. While the form of the algorithm varies with its application, certain linguistic problems are common to any stemming procedure. As a basis for evaluation of previous attempts to deal with these problems, this paper first discusses the theoretical and practical attributes of stemming algorithms. Then a new version of a context-sensitive, longest-match stemming algorithm for English is proposed; though developed for use in a library information transfer system, it is of general application. A major linguistic problem in stemming, variation in spelling of stems, is discussed in some detail and several feasible programmed solutions are outlined, along with sample results of one of these methods.","There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .","['There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']",0,"['There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .']"
CC1330,W02-0309,Biomedical text retrieval in languages with a complex morphology,the use of morphosemantic regularities in the medical vocabulary for automatic lexical coding methods ofinformation in,['S Wolff'],introduction,The use of morphosemantic regularities in the medical vocabulary for automatic lexical coding. -,"While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG ) .","['Furthermore, medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language (e.g., German), often referred to as neo-classical compounding (Mc-Cray et al., 1988).', 'While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG ) .']",0,"['While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG ) .']"
CC1331,W02-0309,Biomedical text retrieval in languages with a complex morphology,medical subject headings,['NLM'],experiments,"Automatically assigning MeSH (Medical Subject Headings) to articles is an active research topic. Recent work demonstrated the feasibility of improving the existing automated Medical Text Indexer (MTI) system, developed at the National Library of Medicine (NLM). Encouraged by this work, we propose a novel data-driven approach that uses semantic distances in the MeSH ontology for automated MeSH assignment. Specifically, we developed a graphical model to propagate belief through a citation network to provide robust MeSH main heading (MH) recommendation. Our preliminary results indicate that this approach can reach high Mean Average Precision (MAP) in some scenarios","This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG ) ) are incorporated into our system .","['Generalizing the interpretation of our data in the light of these findings, we recognize a substantial increase of retrieval performance when query and text tokens are segmented according to the principles of the subword model.', 'The gain is still not overwhelming.', ""With regard to orthographic normalization, we expected a higher performance benefit because of the well-known spelling problems for German medical terms of Latin or Greek origin (such as in 'Zäkum ', 'Cäkum', 'Zaekum', 'Caekum', 'Zaecum', 'Caecum')."", 'For our experiments, however, we used quite a homogeneous document collection following the spelling standards of medical publishers.', 'The same standards apparently applied to the original multiple choice questions, by which the acquisition of expert queries was guided (cf.', 'Section 3).', 'In the layman queries, there were only few Latin or Greek terms, and, therefore, they did not take advantage of the spelling normalization.', 'However, the experience with medical text retrieval (especially on medical reports which exhibit a high rate of spelling variations) shows that orthographic normalization is a desider-  Whereas the usefulness of Subword Indexing became evident, we could not provide sufficient evidence for Synonym Class Indexing, so far.', 'However, synonym mapping is still incomplete in the current state of our subword dictionary.', 'A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing.', 'We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5).', 'It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG ) ) are incorporated into our system .', 'Alternatively, we may think of user-centered comparative studies (Hersh et al., 1995).']",3,"['For our experiments, however, we used quite a homogeneous document collection following the spelling standards of medical publishers.', 'However, the experience with medical text retrieval (especially on medical reports which exhibit a high rate of spelling variations) shows that orthographic normalization is a desider-  Whereas the usefulness of Subword Indexing became evident, we could not provide sufficient evidence for Synonym Class Indexing, so far.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG ) ) are incorporated into our system .']"
CC1332,W02-0309,Biomedical text retrieval in languages with a complex morphology,viewing morphology as an inference process,['R Krovetz'],conclusion,"AbstractMorphology is the area of linguistics concerned with the internal structure of words. Information retrieval has generally not paid much attention to word structure, other than to account for some of the variability in word forms via the use of stemmers. We report on our experiments to determine the importance of morphology, and the effect that it has on performance. We found that grouping morphological variants makes a significant improvement in retrieval performance. Improvements are seen by grouping inflectional as well as derivational variants. We also found that performance was enhanced by recognizing lexical phrases. We describe the interaction between morphology and lexical ambiguity, and how resolving that ambiguity will lead to further improvements in performance","There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']",0,"['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .']"
CC1333,W02-0309,Biomedical text retrieval in languages with a complex morphology,the effectiveness of stemming for natural language access to slovene textual data,"['M Popovic', 'P Willett']",introduction,"There have been several studies of the use of stemming algorithms for conflating morphological variants in freetext retrieval systems. Comparison of stemmed and nonconflated searches suggests that there are no significant increases in the effectiveness of retrieval when stemming is applied to English-language documents and queries. This article reports the use of stemming on Slovene-language documents and queries, and demonstrates that the use of an appropriate stemming algorithm results in a large, and statistically significant, increase in retrieval effectiveness when compared with nonconflated processing; similar comments apply to the use of manual, right-hand truncation. A comparison is made with stemming of English versions of the same documents and queries and it is concluded that the effectiveness of a stemming algorithm is determined by the morphological complexity of the language that it is designed to process.","This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc Â¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) .","['The efforts required for performing morphologi- cal analysis vary from language to language.', 'For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc Â¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) .', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980; Norton and Pacak, 1983; Wolff, 1984; Wingert, 1985; Dujols et al., 1991; Baud et al., 1998).']",0,"['The efforts required for performing morphologi- cal analysis vary from language to language.', 'For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc Â¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) .', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980; Norton and Pacak, 1983; Wolff, 1984; Wingert, 1985; Dujols et al., 1991; Baud et al., 1998).']"
CC1334,W02-0309,Biomedical text retrieval in languages with a complex morphology,morphosemantic analysis of compound word forms denoting surgical procedures methods ofinformation in medicine,"['L Norton', 'M Pacak']",introduction,,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']",0,"['From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']"
CC1335,W02-0309,Biomedical text retrieval in languages with a complex morphology,morphosemantic analysis of itis forms in medical language,"['M Pacak', 'L Norton', 'G Dunham']",introduction,,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']",0,"['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jappinen and Niemisto, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekcioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']"
CC1336,W02-0309,Biomedical text retrieval in languages with a complex morphology,towards new measures of information retrieval evaluation,"['W Hersh', 'D Elliot', 'D Hickam', 'S Wolf', 'A Molnar', 'C Leichtenstien']",experiments,"All of the methods currently used to assess information retrieval (IR) systems have limitations in their ability to measure how well users are able to acquire information. We utilized a new approach to assessing information obtained, based on a short-answer test given to senior medical students. Students took the ten-question test and then searched one of two IR systems on the five questions for which they were least certain of their answer Our results showed that pre-searching scores on the test were low but that searching yielded a high proportion of answers with both systems. These methods are able to measure information obtained, and will be used in subsequent studies to assess differences among IR systems.","Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG ) .","['Generalizing the interpretation of our data in the light of these findings, we recognize a substantial increase of retrieval performance when query and text tokens are segmented according to the principles of the subword model.', 'The gain is still not overwhelming.', ""With regard to orthographic normalization, we expected a higher performance benefit because of the well-known spelling problems for German medical terms of Latin or Greek origin (such as in 'Zäkum ', 'Cäkum', 'Zaekum', 'Caekum', 'Zaecum', 'Caecum')."", 'For our experiments, however, we used quite a homogeneous document collection following the spelling standards of medical publishers.', 'The same standards apparently applied to the original multiple choice questions, by which the acquisition of expert queries was guided (cf.', 'Section 3).', 'In the layman queries, there were only few Latin or Greek terms, and, therefore, they did not take advantage of the spelling normalization.', 'However, the experience with medical text retrieval (especially on medical reports which exhibit a high rate of spelling variations) shows that orthographic normalization is a desider-  Whereas the usefulness of Subword Indexing became evident, we could not provide sufficient evidence for Synonym Class Indexing, so far.', 'However, synonym mapping is still incomplete in the current state of our subword dictionary.', 'A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing.', 'We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5).', 'It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system.', 'Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG ) .']",3,"['Generalizing the interpretation of our data in the light of these findings, we recognize a substantial increase of retrieval performance when query and text tokens are segmented according to the principles of the subword model.', 'For our experiments, however, we used quite a homogeneous document collection following the spelling standards of medical publishers.', 'In the layman queries, there were only few Latin or Greek terms, and, therefore, they did not take advantage of the spelling normalization.', 'However, the experience with medical text retrieval (especially on medical reports which exhibit a high rate of spelling variations) shows that orthographic normalization is a desider-  Whereas the usefulness of Subword Indexing became evident, we could not provide sufficient evidence for Synonym Class Indexing, so far.', 'A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing.', 'We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5).', 'It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG ) .']"
CC1337,W02-0309,Biomedical text retrieval in languages with a complex morphology,effective use of natural language processing techniques for automatic conflation of multiword terms the role of derivational morphology part of speech tagging and shallow parsing,"['E Tzoukermann', 'J Klavans', 'C Jacquemin']",conclusion,,"Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG ) .","['There has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG ) .']",0,"['There has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG ) .']"
CC1338,W02-0309,Biomedical text retrieval in languages with a complex morphology,the contribution of morphological knowledge to french mesh mapping for information retrieval,"['P Zweigenbaum', 'S Darmoni', 'N Grabar']",introduction,"MeSH-indexed Internet health directories must provide a mapping from natural language queries to MeSH terms so that both health professionals and the general public can query their contents. We describe here the design of lexical knowledge bases for mapping French expressions to MeSH terms, and the initial evaluation of their contribution to Doc'CISMeF, the search tool of a MeSH-indexed directory of French-language medical Internet resources. The observed trend is in favor of the use of morphological knowledge as a moderate (approximately 5%) but effective factor for improving query to term mapping capabilities.","Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages .","['While one may argue that single-word compounds are quite rare in English (which is not the case in the medical domain either), this is certainly not true for German and other basically agglutinative languages known for excessive single-word nominal compounding.', 'This problem becomes even more pressing for technical sublanguages, such as medical German (e.g., �Blut druck mess gera__t� translates to �device for measuring blood pressure�).', 'The problem one faces from an IR point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents.', 'Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages .']",0,"['Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages .']"
CC1339,W02-0309,Biomedical text retrieval in languages with a complex morphology,morphological typology of languages for ir,['A Pirkola'],introduction,"This paper presents a morphological classification of languages from the IR perspective. Linguistic typology research has shown that the morphological complexity of every language in the world can be described by two variables, index of synthesis and index of fusion. These variables provide a theoretical basis for IR research handling morphological issues. A common theoretical framework is needed in particular because of the increasing significance of cross-language retrieval research and CLIR systems processing different languages. The paper elaborates the linguistic morphological typology for the purposes of IR research. It studies how the indexes of synthesis and fusion could be used as practical tools in mono- and cross-lingual IR research. The need for semantic and syntactic typologies is discussed. The paper also reviews studies made in different languages on the effects of morphology and stemming in IR.","This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc Â¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ) .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc Â¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ) .', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998).']",0,"['The efforts required for performing morphological analysis vary from language to language.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc Â¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ) .', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.']"
CC1340,W02-0309,Biomedical text retrieval in languages with a complex morphology,responsa an operational fulltext retrieval system with linguistic components for large corpora,['Y Choueka'],introduction,,"Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J Â¨ appinen and Niemist Â¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J Â¨ appinen and Niemist Â¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]",0,"['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J Â¨ appinen and Niemist Â¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .']"
CC1341,W02-0309,Biomedical text retrieval in languages with a complex morphology,the semantic structure of neoclassical compounds,"['A McCray', 'A Browne', 'D Moore']",introduction,,"Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .","['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (Wolff, 1984).']",0,"['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .']"
CC1342,W02-1601,A synchronization structure of SSTC and its applications in machine translation,finding structural correspondences from bilingual parsed corpus for corpusbased translation,"['H Watanabe', 'S Kurohashi', 'E Aramaki']",,"In this paper, we describe a system and methods for finding structural correspondences from the paired dependency structures of a source sentence and its translation in a target language. The system we have developed finds word correspondences first, then finds phrasal correspondences based on word correspondences. We have also developed a GUI system with which a user can check and correct the correspondences retrieved by the system. These structural correspondences will be used as raw translation patterns in a corpus-based translation system.","For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #AUTHOR_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #AUTHOR_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #AUTHOR_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1343,W02-1601,A synchronization structure of SSTC and its applications in machine translation,examplebased machine translation based on the synchronous sstc annotation schema,"['M H Al-Adhaileh', 'E K Tang']",,"In this paper, we describe an Example-Based Machine Translation (EBMT) system for English-Malay translation. Our approach is an example-based approach which relies sorely on example translations kept in a Bilingual Knowledge Bank (BKB). In our approach, a flexible annotation schema called Structured String-Tree Correspondence (SSTC) is used to annotate both the source and target sentences of a translation pair. Each SSTC describes a sentence, a representation tree as well as the correspondences between substrings in the sentence and subtrees in the representation tree. With both the source and target SSTCs established, a translation example in the BKB can then be represented effectively in terms of a pair of synchronous SSTCs. In the process of translation, we first try to build the representation tree for the source sentence (English) based on the example-based parsing algorithm as presented in [1]. By referring to the resultant source parse tree, we then proceed to synthesis the target sentence (Malay) based on the target SSTCs as pointed to by the synchronous SSTCs which encode the relationship between source and target SSTCs.","For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #AUTHOR_TAG ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #AUTHOR_TAG ) .']",0,"['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #AUTHOR_TAG ) .']"
CC1344,W02-1601,A synchronization structure of SSTC and its applications in machine translation,representation trees and stringtree correspondences,"['C Boitet', 'Y Zaharin']",introduction,,"In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .","['In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .', 'The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation.', 'Such correspondence is defined in a way that is able to handle some non-standard cases (e.g.', 'non-projective correspondence).']",0,"['In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .', 'The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation.', 'Such correspondence is defined in a way that is able to handle some non-standard cases (e.g.', 'non-projective correspondence).']"
CC1345,W02-1601,A synchronization structure of SSTC and its applications in machine translation,handling crossed dependencies with the stcg,"['E K Tang', 'Y Zaharin']",,,"These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG ) .","['The SSTC is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be nonprojective (Boitet & Zaharin, 1988).', 'These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG ) .', 'crossed dependencies (Tang & Zaharin, 1995).']",0,"['These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG ) .', 'crossed dependencies (Tang & Zaharin, 1995).']"
CC1346,W02-1601,A synchronization structure of SSTC and its applications in machine translation,representation trees and stringtree correspondences,"['C Boitet', 'Y Zaharin']",,,"Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .","['In this section, we stress on the fact that in order to describe Natural Language (NL) in a natural manner, three distinct components need to be expressed by the linguistic formalisms; namely, the text, its corresponding abstract linguistic representation and the mapping (correspondence) between these two.', 'Actually, NL is not only a correspondence between different representation levels, as stressed by MTT postulates, but also a sub-correspondence between them.', 'For instance, between the string in a language and its representation tree structure, it is important to specify the sub-correspondences between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation in NLP.', 'It is well known that many linguistic constructions are not projective (e.g.', 'scrambling, cross serial dependencies, etc.).', 'Hence, it is very much desired to define the correspondence in a way to be able to handle the non-standard cases (e.g.', 'non-projective correspondence), see Figure 1.', 'Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .']",0,"['Actually, NL is not only a correspondence between different representation levels, as stressed by MTT postulates, but also a sub-correspondence between them.', 'For instance, between the string in a language and its representation tree structure, it is important to specify the sub-correspondences between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation in NLP.', 'non-projective correspondence), see Figure 1.', 'Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .']"
CC1347,W02-1601,A synchronization structure of SSTC and its applications in machine translation,converting a bilingual dictionary into a bilingual knowledge bank based on the synchronous sstc annotation schema,"['M H Al-Adhaileh', 'E K Tang']",,"In this paper, we would like to present an approach to construct a huge Bilingual Knowledge Bank (BKB) from an English Malay bilingual dictionary based on the idea of synchronous Structured String-Tree Correspondence (SSTC). The SSTC is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be non-projective. With this structure, we are able to match linguistic units at different inter levels of the structure (i.e. define the correspondence between substrings in the sentence, nodes in the tree, subtrees in the tree and sub-correspondences in the SSTC). This flexibility makes synchronous SSTC very well suited for the construction of a Bilingual Knowledge Bank we need for the English-Malay MT application.",#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC .,"['However, what has so far been lacking is a schema or a framework to annotate and express such extracted lexical and structural correspondences in a flexible and powerful manner.', ""The proposed S-SSTC annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages' structures."", 'S-SSTC very well suited for the construction of a BKB, which is needed for the EBMT applications.', '#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC .']",0,"['However, what has so far been lacking is a schema or a framework to annotate and express such extracted lexical and structural correspondences in a flexible and powerful manner.', ""The proposed S-SSTC annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages' structures."", '#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC .']"
CC1348,W02-1601,A synchronization structure of SSTC and its applications in machine translation,chartbased transfer rule application in machine translation,"['A Meyers', 'M Kosaka', 'R Grishman']",,"Transfer-based Machine Translation systems require a procedure for choosing the set of transfer rules for generating a target language translation from a given source language sentence. In an MT system with many competing transfer rules, choosing the best set of transfer rules for translation may involve the evaluation of an explosive number of competing sets. We propose a solution to this problem based on current best-first chart parsing algorithms.","For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( #AUTHOR_TAG ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( #AUTHOR_TAG ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( #AUTHOR_TAG ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1349,W02-1601,A synchronization structure of SSTC and its applications in machine translation,nonisomorphic synchronous tags,"['K Harbusch', 'P Poller']",,"Synchronous tree{adjoining grammars (S{TAGs) combine two standard tree{adjoining grammars (TAGs), e.g., for language transduction in Machine Translation (MT). Recent advances show that the restriction to isomorphic derivation trees (IS{TAGs) ensures eecient transduction because only tree{adjoining languages can be formed in each component. As a result IS{TAGs only allow for  triv-ial"" transfer rules, due to the fact that only isomorphic derivations can be synchronized. This means that only very similar constructions in the two languages can be translated into each other. To overcome these limitations and provide a way of realizing more complex translation phenomena, this paper introduces a new formalism, the dynamic link synchronous tree{adjoining grammars or DLS{TAGs. This formalism allows for the synchronization of non{isomorphic derivation trees by introducing the new concept of dynamic links. DLS{TAGs are more powerful than IS-TAGs. More precisely speaking, DLS{TAGs allow for the formulation of a non{tree{adjoining language in one of the two components. This makes the translation problem more diicult but not untractable as outlined in this paper. However , there remain non{isomorphic translation phenomena which cannot be handled by DLS-TAGs as we also show in this paper.","It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .","['The idea of parallelized formalisms is widely used one, and one which has been applied in many different ways.', 'The use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures.', 'For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeillé et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990).', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .', 'As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.', 'In this case only TAL can be formed in each component.', 'This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994).']",0,"['The use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures.', 'For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeille et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990).', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .', 'As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.', 'This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994).']"
CC1350,W02-1601,A synchronization structure of SSTC and its applications in machine translation,what is a natural language and how to describe it meaningtext approaches in contrast with generative approaches,['S Kahane'],,"The paper expounds the general conceptions of the Meaning- Text theory about what a natural language is and how it must be de- scribed. In a second part, a formalization of these conceptions - the transductive grammars - is proposed and compared with generative ap- proaches.","From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ) .","['From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ) .', 'The MTT point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community.']",0,"['From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ) .']"
CC1351,W02-1601,A synchronization structure of SSTC and its applications in machine translation,towards memorybased translation,"['S Sato', 'M Nagao']",,,"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1352,W02-1601,A synchronization structure of SSTC and its applications in machine translation,restricting the weak generative capacity of synchronous tree adjoining grammar,['S Shieber'],,,"In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG cases ) .","['As mentioned earlier, there are some non-standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG cases ) .', 'Shieber (1994) cases).', 'Due to lack of space we will only brief on some of these non-standard cases without going into the details.']",0,"['As mentioned earlier, there are some non-standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG cases ) .']"
CC1353,W02-1601,A synchronization structure of SSTC and its applications in machine translation,structural matching of parallel texts,"['Y Matsumoto', 'H Ishimoto', 'T Utsuro']",,"This paper describes a nethod for finding structural matching between parallel sentences of two lauguages, (such as Japanese and English). Par- allel sentences are analyzed based on unification grammars, and structural matching is performed by making use of a similarity measure of word pairs in the two languages. Syntactic ambiguities are resolved simultaneously in the matching process. The results serve as a useful source for extracting linguistic and lexical knowledge","For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1354,W02-1601,A synchronization structure of SSTC and its applications in machine translation,synchronous models of language,"['O Rambow', 'G Satta']",,"In synchronous rewriting, the productions of two rewriting systems are paired and applied synchronously in the derivation of a pair of strings. We present a new synchronous rewriting system and argue that it can handle certain phenomena that are not covered by existing synchronous systems. We also prove some interesting formal/computational properties of our system.Comment: 8 pages uuencoded gzipped ps fil","Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) , such as the relation between syntax and semantic .","['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) , such as the relation between syntax and semantic .']",0,"['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) , such as the relation between syntax and semantic .']"
CC1355,W02-1601,A synchronization structure of SSTC and its applications in machine translation,pilot implementation of a bilingual knowledge bank,"['V Sadler', 'R Vendelmans']",,"A Bilingual Knowledge Bank is a syntactically and referentially structured pair of corpora, one being a  translation of the other, in which translation units are  cross-codexl between the corpora. A pilot implementation  is described for a corpus of some 20,000 words  each in English, French and Esperanto which has been cross-coded between English and Esperanto and &apos;between Esperanto and French. The aim is to develop a corpus-based general-purpose knowledge sontee for applicatious in machine translation and computer-  aided translation","For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus","[') are governed by the following constraints:  .', 'This means allowing one-to-one, one-to-many and many-to-many, but the mappings do not overlap.', 'Note that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two SSTCs of the S-SSTC (i.e. between the two languages).', ""For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus""]",0,"[""For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus""]"
CC1356,W02-1601,A synchronization structure of SSTC and its applications in machine translation,a bestfirst algorithm for automatic extraction of transfer mappings from bilingual corpora,"['A Menezes', 'S Richardson']",,"Translation systems that automatically extract transfer mappings (rules or examples) from bilingual corpora have been hampered by the difficulty of achieving accurate alignment and acquiring high quality mappings. We describe an algorithm that uses a best-first strategy and a small alignment grammar to significantly improve the quality of the transfer mappings extracted. For each mapping, frequencies are computed and sufficient context is retained to distinguish competing mappings during translation. Variants of the algorithm are run against a corpus containing 200K sentence pairs and evaluated based on the quality of resulting translations.","For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e.', 'Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1357,W02-1601,A synchronization structure of SSTC and its applications in machine translation,synchronous models of language,"['O Rambow', 'G Satta']",introduction,"In synchronous rewriting, the productions of two rewriting systems are paired and applied synchronously in the derivation of a pair of strings. We present a new synchronous rewriting system and argue that it can handle certain phenomena that are not covered by existing synchronous systems. We also prove some interesting formal/computational properties of our system.Comment: 8 pages uuencoded gzipped ps fil",Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) .,"['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) .']",0,"['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) .']"
CC1358,W02-1601,A synchronization structure of SSTC and its applications in machine translation,representation trees and stringtree correspondences,"['C Boitet', 'Y Zaharin']",,,"For more details on the proprieties of SSTC , see #AUTHOR_TAG .","['The case depicted in Figure 2, describes how the SSTC structure treats some non-standard linguistic phenomena.', 'The particle ""up"" is featurised into the verb ""pick"" and in discontinuous manner (e.g.', '""up"" (4-5) in ""pick-up"" (1-2+4-5)) in the sentence ""He picks the box up"".', 'For more details on the proprieties of SSTC , see #AUTHOR_TAG .']",0,"['For more details on the proprieties of SSTC , see #AUTHOR_TAG .']"
CC1359,W02-1601,A synchronization structure of SSTC and its applications in machine translation,natural language analysis in machine translation mt based on the stringtree correspondence grammar stcg,['E K Tang'],,"The formalism is argued to be a totally declarative grammar formalism that can associate, to strings in a language, arbitrary tree structures as desired by the grammar writer to be the linguistic representation structures of the strings. More importantly is the facility to specify the correspondence between the string and the associated tree in a very natural manner. These features are very much desired in grammar writing, in particular for the treatment of certain linguistic phenomena which are 'non-standard', namely featurisation, lexicalisation and crossed dependencies [2,3]. Furthermore, a grammar written in this way naturally inherits the desired property of bi-directionality (in fact non-directionality [4]) such that the same grammar can be interpreted for both analysis and generation.",A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .,"['Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'It contains a nonprojective correspondence.', 'An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']",5,"['A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']"
CC1360,W02-1601,A synchronization structure of SSTC and its applications in machine translation,achieving commercialquality translation with examplebased methods,"['S Richardson', 'W Dolan', 'A Menezes', 'J Pinkham']",,,"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1361,W02-1601,A synchronization structure of SSTC and its applications in machine translation,restricting the weak generative capacity of synchronous tree adjoining grammar,['S Shieber'],,,"It allows the construction of a non-TAL ( #AUTHOR_TAG ) , ( Harbusch & Poller , 2000 ) .","['The idea of parallelized formalisms is widely used one, and one which has been applied in many different ways.', 'The use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures.', 'For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeillé et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990).', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( #AUTHOR_TAG ) , ( Harbusch & Poller , 2000 ) .', 'As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.', 'In this case only TAL can be formed in each component.', 'This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994).']",0,"['The idea of parallelized formalisms is widely used one, and one which has been applied in many different ways.', 'The use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures.', 'For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeille et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990).', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( #AUTHOR_TAG ) , ( Harbusch & Poller , 2000 ) .', 'As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.', 'This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994).']"
CC1362,W02-1601,A synchronization structure of SSTC and its applications in machine translation,examplebased machine translation,['S Sato'],,"Example-based machine translation (EBMT) systems, so far, rely on heuristic measures in re-trieving translation examples. Such a heuristic measure costs time to adjust, and might make its algorithm unclear. This paper presents a probabilistic model for EBMT. Under the pro-posed model, the system searches the transla-tion example combination which has the high-est probability. The proposed model clearly for-malizes EBMT process. In addition, the model can naturally incorporate the context similarity of translation examples. The experimental re-sults demonstrate that the proposed model has a slightly better translation quality than state-of-the-art EBMT systems.","For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1363,W02-1601,A synchronization structure of SSTC and its applications in machine translation,representation trees and stringtree correspondences,"['C Boitet', 'Y Zaharin']",,,"A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .","['Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'It contains a nonprojective correspondence.', 'An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']",5,"['It contains a nonprojective correspondence.', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']"
CC1364,W03-0806,Blueprint for a high performance NLP infrastructure,english gigaword corpus catalogue number ldc2003t05,['Linguistic Data Consortium'],introduction,,"However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG ) .","['However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG ) .', 'Recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', 'Other potential applications must process text online or in realtime.', 'For example, Google currently answers 250 million queries per day, thus processing time must be minimised.', 'Clearly, efficient NLP components will need to be developed.', 'At the same time, state-of-the-art performance will be needed for these systems to be of practical use.']",0,"['However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG ) .']"
CC1365,W03-0806,Blueprint for a high performance NLP infrastructure,dialogue interaction with the darpa communicator infrastructure the development of useful software,"['Samuel Bayer', 'Christine Doran', 'Bryan George']",,"To support engaging human users in robust, mixed-initiative speech dialogue interactions which reach beyond current capabilities in dialogue systems, the DARPA Communicator program [1] is funding the development of a distributed message-passing infrastructure for dialogue systems which all Communicator participants are using. In this presentation, we describe the features of and requirements for a genuinely useful software infrastructure for this purpose.","There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ) .","['The final interface we intend to implement is a collection of web services for NLP.', 'A web service provides a remote procedure that can be called using XML based encodings (XMLRPC or SOAP) of function names, arguments and results transmitted via internet protocols such as HTTP.', 'Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']",0,"['Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ) .']"
CC1366,W03-0806,Blueprint for a high performance NLP infrastructure,combining labeled and unlabeled data with cotraining,"['Avrim Blum', 'Tom Mitchell']",,,"Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .","['As discussed earlier, there are two main requirements of the system that are covered by ""high performance"": speed and state of the art accuracy.', 'Efficiency is required both in training and processing.', 'Efficient training is required because the amount of data available for training will increase significantly.', 'Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .', 'Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly.']",0,"['Efficient training is required because the amount of data available for training will increase significantly.', 'Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .']"
CC1367,W03-0806,Blueprint for a high performance NLP infrastructure,gate – a general architecture for text engineering,"['Hamish Cunningham', 'Yorick Wilks', 'Robert J Gaizauskas']",experiments,"This paper presents the design, implementation and evaluation of GATE, a General Architecture for Text Engineering.GATE lies at the intersection of human language computation and software engineering, and constitutes aninfrastructural system supporting research and development of languageprocessing software.","Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']",0,"['Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .']"
CC1368,W03-0806,Blueprint for a high performance NLP infrastructure,a distributed architecture for robust automatic speech recognition,"['Kadri Hacioglu', 'Bryan Pellom']",,"In this paper, we attempt to decompose a state-of-the-art speech recognition system into its components and define an infrastructure that allows a flexible, efficient and effective interaction among the components. Motivated by the success of DARPA Communicator program, we select the open source Galaxy architecture as our development test bed. It consists of a hub that allows communication among servers connected to it by message passing and supports the plug-and-play paradigm. In addition to message passing it supports high bandwidth data (binary or audio) transfer between servers via a brokering scheme. For several reasons, we believe that it is the right time to start developing a distributed framework for speech recognition along with data and protocol standards supporting interoperability. We present our work towards that goal using the Colorado University (CU) Sonic recognizer. We divide Sonic into a number of components and structure it around the Hub. We describe the system in some detail and report on its present status with some possibilities for future development. 1","There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .","['The final interface we intend to implement is a collection of web services for NLP.', 'A web service provides a remote procedure that can be called using XML based encodings (XMLRPC or SOAP) of function names, arguments and results transmitted via internet protocols such as HTTP.', 'Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']",0,"['There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .']"
CC1369,W03-0806,Blueprint for a high performance NLP infrastructure,a maximum entropy partofspeech tagger,['Adwait Ratnaparkhi'],conclusion,,"For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .","['The Generative Programming approach to NLP infrastructure development will allow tools such as sentence boundary detectors, POS taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components.', 'For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .']",3,"['For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .']"
CC1370,W03-0806,Blueprint for a high performance NLP infrastructure,bootstrapping postaggers using unlabelled data,"['Stephen Clark', 'James R Curran', 'Miles Osborne']",experiments,"This paper investigates booststrapping part-of-speech taggers using co-training, in which two taggers are iteratively re-trained on each other's output. Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set. We investigate selecting examples by directly maximising tagger agreement on unlabelled data, a method which has been theoretically and empirically motivated in the co-training literature. Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost.","The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).', 'We expect even faster training times when we move to conjugate gradient methods.']",4,"['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).', 'We expect even faster training times when we move to conjugate gradient methods.']"
CC1371,W03-0806,Blueprint for a high performance NLP infrastructure,scaling context space,"['James R Curran', 'Marc Moens']",experiments,"This paper proposes a computationally feasible method for measuring the context-sensitive semantic distance between words. The distance is computed by adaptive scaling of a semantic space. In the semantic space, each word in the vocabulary V is represented by a multidimensional vector which is extracted from an English dictionary through principal component analysis. Given a word set C which specifies a context, each dimension of the semantic space is scaled up or down according to the distribution of C in the semantic space. In the space thus transformed, the distance between words in V  becomes dependent on the context C. An evaluation through a word prediction task shows that the proposed measurement successfully extracts the context of a text. 1 Introduction  Semantic distance (or similarity) between words is one of the basic measurements used in many fields of natural language processing, information retrieval, etc. Word distance provides bottom-up information for text understandi..","The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .","['The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .', 'We have already implemented a P O S tagger, chunker, C C G supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training ma- terials and tag faster than T N T , the fastest existing P O S tagger.', 'These tools use a highly optimised G I S imple- mentation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).', 'We expect even faster train- ing times when we move to conjugate gradient methods.']",4,"['The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .', 'These tools currently train in less than 10 minutes on the standard training ma- terials and tag faster than T N T , the fastest existing P O S tagger.', 'These tools use a highly optimised G I S imple- mentation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).', 'We expect even faster train- ing times when we move to conjugate gradient methods.']"
CC1372,W03-0806,Blueprint for a high performance NLP infrastructure,maximum entropy models for natural language ambiguity resolution,['Adwait Ratnaparkhi'],experiments,"This thesis demonstrates that several important kinds of natural language ambiguities can be resolved to state-of-the-art accuracies using a single statistical modeling technique based on the principle of maximum entropy.  We discuss the problems of sentence boundary detection, part-of-speech tagging, prepositional phrase attachment, natural language parsing, and text categorization under the maximum entropy framework. In practice, we have found that maximum entropy models offer the following advantages:  State-of-the-art accuracy. The probability models for all of the tasks discussed perform at or near state-of-the-art accuracies, or outperform competing learning algorithms when trained and tested under similar conditions. Methods which outperform those presented here require much more supervision in the form of additional human involvement or additional supporting resources.  Knowledge-poor features. The facts used to model the data, or features, are linguistically very simple, or ""knowledge-poor"", but yet succeed in approximating complex linguistic relationships.  Reusable software technology. The mathematics of the maximum entropy framework are essentially independent of any particular task, and a single software implementation can be used for all of the probability models in this thesis.  The experiments in this thesis suggest that experimenters can obtain state-of-the-art accuracies on a wide range of natural language tasks, with little task-specific effort, by using maximum entropy probability models.","An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']",0,"['An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .']"
CC1373,W03-0806,Blueprint for a high performance NLP infrastructure,a rational design for a weighted finitestate transducer library,"['Mehryar Mohri', 'Fernando C N Pereira', 'Michael Riley']",experiments,,"Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .","['A number of stand-alone tools have also been developed.', 'For example, the suite of LT tools (Mikheev et al., 1999;Grover et al., 2000) perform tokenization, tagging and chunking on XML marked-up text directly.', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .', 'However, the source code for these tools is not freely available, so they cannot be extended.']",0,"['Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .']"
CC1374,W03-0806,Blueprint for a high performance NLP infrastructure,scaling context space,"['James R Curran', 'Marc Moens']",introduction,"This paper proposes a computationally feasible method for measuring the context-sensitive semantic distance between words. The distance is computed by adaptive scaling of a semantic space. In the semantic space, each word in the vocabulary V is represented by a multidimensional vector which is extracted from an English dictionary through principal component analysis. Given a word set C which specifies a context, each dimension of the semantic space is scaled up or down according to the distribution of C in the semantic space. In the space thus transformed, the distance between words in V  becomes dependent on the context C. An evaluation through a word prediction task shows that the proposed measurement successfully extracts the context of a text. 1 Introduction  Semantic distance (or similarity) between words is one of the basic measurements used in many fields of natural language processing, information retrieval, etc. Word distance provides bottom-up information for text understandi..","Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .","['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', 'Other potential applications must process text online or in realtime.', 'For example, Google currently answers 250 million queries per day, thus processing time must be minimised.', 'Clearly, efficient NLP components will need to be developed.', 'At the same time, state-of-the-art performance will be needed for these systems to be of practical use.']",0,"['Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .']"
CC1375,W03-0806,Blueprint for a high performance NLP infrastructure,building a large annotated corpus of english the penn treebank computational linguistics,"['Mitchell Marcus', 'Beatrice Santorini', 'Mary Marcinkiewicz']",introduction,,"For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG ) , currently used for training POS taggers .","['NLP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']",0,"['Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG ) , currently used for training POS taggers .']"
CC1376,W03-0806,Blueprint for a high performance NLP infrastructure,nltk the natural language toolkit,"['Edward Loper', 'Steven Bird']",,,It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG ) .,"['Although C++ is extremely efficient, it is not suitable for rapidly gluing components together to form new tools.', 'To overcome this problem we have implemented an interface to the infrastructure in the Python scripting language.', 'Python has a number of advantages over other options, such as Java and Perl.', 'Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG ) .']",2,"['Although C++ is extremely efficient, it is not suitable for rapidly gluing components together to form new tools.', 'To overcome this problem we have implemented an interface to the infrastructure in the Python scripting language.', 'Python has a number of advantages over other options, such as Java and Perl.', 'Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG ) .']"
CC1377,W03-0806,Blueprint for a high performance NLP infrastructure,mixedinitiative development of language processing systems,"['David Day', 'John Aberdeen', 'Lynette Hirschman', 'Robyn Kozierok', 'Patricia Robinson', 'Marc Vilain']",experiments,"Historically, tailoring language processing systems to specific domains and languages for which they were not originally built has required a great deal of effort. Recent advances in corpus-based manual and automatic training methods have shown promise in reducing the time and cost of this porting process. These developments have focused even greater attention on the bottleneck of acquiring reliable, manually tagged training data. This paper describes a new set of integrated tools, collectively called the Alembic Workbench, that uses a mixed-initiative approach to ""bootstrapping"" the manual tagging process, with the goal of reducing the overhead associated with corpus development. Initial empirical studies using the Alembic Workbench to annotate ""named entities"" demonstrates that this approach can approximately double the production rate. As an added benefit, the combined efforts of machine and user produce domain specific annotation rules that can be used to annotate similar texts automatically through the Alembic-NLP system. The ultimate goal of this project is to enable end users to generate a practical domain-specific information extraction system within a single session.","Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']",0,"['Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .']"
CC1378,W03-0806,Blueprint for a high performance NLP infrastructure,developing language processing components with gate,"['Hamish Cunningham', 'Diana Maynard', 'C Ursu K Bontcheva', 'V Tablan', 'M Dimitrov']",experiments,"Fluid leakage through soil in a region thereof is controlled by sequentially passing over the region to dig a plurality of parallel, laterally displaced grooves in the surface. Soil dug from each groove is temporarily stored, and a strip of sheet material is laid over a groove as it is created during each pass, the width of the strip being greater than the width of the groove. Thereafter, the temporarily stored soil is deposited on the strip such that it is covered with soil except along one edge, the other edge of the strip overlying the uncovered edge of an adjacent strip laid down during a previous pass over the region. As a consequence, a first layer of overlapping strips of sheet material covered with soil is installed over the region.","For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ) .","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ) .', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']",0,"['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ) .', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']"
CC1379,W03-0806,Blueprint for a high performance NLP infrastructure,generative programming methods tools and applications,"['Krzysztof Czarnecki', 'Ulrich W Eisenecker']",introduction,"1. What Is This Book About? From Handcrafting to Automated Assembly Lines. Generative Programming. Benefits and Applicability. I. ANALYSIS AND DESIGN METHODS AND TECHNIQUES. 2. Domain Engineering. Why Is This Chapter Worth Reading? What Is Domain Engineering? Domain Analysis. Domain Design and Domain Implementation. Application Engineering. Product-Line Practices. Key Domain Engineering Concepts. Domain. Domain Scope and Scoping. Relationships between Domains. Features and Feature Models. Method Tailoring and Specialization. Survey of Domain Analysis and Domain Engineering Methods. Feature-Oriented Domain Analysis (FODA). Organization Domain Modeling (ODM). Draco. Capture. Domain Analysis and Reuse Environment (DARE). Domain-Specific Software Architecture (DSSA) Approach. Algebraic Approach. Other Approaches. Domain Engineering and Related Approaches. Historical Notes. Summary. 3. Domain Engineering and Object-Oriented Analysis and Design. Why Is This Chapter Worth Reading? OO Technology and Reuse. Solution Space. Problem Space. Relationship between Domain Engineering and Object-Oriented Analysis and Design (OOA/D) Methods. Aspects of Integrating Domain Engineering and OOA/D Methods. Horizontal versus Vertical Methods. Selected Methods. Rational Unified Process. 00ram. Reuse-Driven Software Engineering Business (RSEB). FeatuRSEB. Domain Engineering Method for Reusable Algorithmic Libraries (DEMRAL). 4. Feature Modeling. Why Is This Chapter Worth Reading? Features Revisited. Feature Modeling. Feature Models. Feature Diagrams. Other Infon-Nation Associated with Feature Diagrams in a Feature Model. Assigning Priorities to Variable Features. Availability Sites, Binding Sites, and Binding Modes. Relationship between Feature Diagrams and Other Modeling Notations and Implementation Techniques. Single Inheritance. Multiple Inheritance. Parameterized Inheritance. Static Parameterization. Dynamic Parameterization. Implementing Constraints. Tool Support for Feature Models. Frequently Asked Questions about Feature Diagrams. Feature Modeling Process. How to Find Features. Role of Variability in Modeling. 5. The Process of Generative Programming. Why Is This Chapter Worth Reading? Generative Domain Models. Main Development Steps in Generative Programming. Adapting Domain Engineering for Generative Programming. Domain-Specific Languages. DEMRAL: Example of a Domain Engineering Method for Generative Programming. Outline of DEMRAL. Domain Analysis. Domain Definition. Domain Modeling. Domain Design. Scope Domain Model for Implementation. Identify Packages. Develop Target Architectures and Identify the Implementation Components. Identify User DSLs. Identify Interactions between DSLs. Specify DSLs and Their Translation. Configuration DSLs. Expression DSLs. Domain Implementation. II. IMPLEMENTATION TECHNOLOGIES. 6. Generic Programming. Why Is This Chapter Worth Reading? What Is Generic Programming? Generic versus Generative Programming. Generic Parameters. Parametric versus Subtype Polymorphism. Genericity in Java. Bounded versus Unbounded Polymorphism. A Fresh Look at Polymorphism. Parameterized Components. Parameterized Programming. Types, Interfaces, and Specifications. Adapters. Vertical and Horizontal Parameters. Module Expressions. C++ Standard Template Library. Iterators. Freestanding Functions versus Member Functions. Generic Methodology. Historical Notes. 7. Component-Oriented Template-Based C++ Programming Techniques. Why Is This Chapter Worth Reading? Types of System Configuration. C++ Support for Dynamic Configuration. C++ Support for Static Configuration. Static Typing. Static Binding. Inlining. Templates. Parameterized Inheritance. typedefs. Member Types. Nested Classes. Prohibiting Certain Template Instantiations. Static versus Dynamic Parameterization. Wrappers Based on Parameterized Inheritance. Template Method Based on Parameterized Inheritance. Parameterizing Binding Mode. Consistent Parameterization of Multiple Components. Static Interactions between Components. Components with Influence. Components under Influence. Structured Configurations. Recursive Components. Intelligent Configuration. 8. Aspect-Oriented Decomposition and Composition. Why Is This Chapter Worth Reading? What Is Aspect-Oriented Programming? Aspect-Oriented Decomposition Approaches. Subject-Oriented Programming. Composition Filters. Demeter / Adaptive Programming. Aspect-Oriented Decomposition and Domain Engineering. How Aspects Arise. Composition Mechanisms. Requirements on Composition Mechanisms. Example: Synchronizing a Bounded Buffer. ""Tangled"" Synchronized Stack. Separating Synchronization Using Design Patterns. Separating Synchronization Using SOP. Some Problems with Design Patterns and Some Solutions. Implementing Noninvasive, Dynamic Composition in Smalltalk. Kinds of Crosscutting. How to Express Aspects in Programming Languages. Separating Synchronization Using AspectJ Cool. Implementing Dynamic Cool in Smalltalk. Implementation Technologies for Aspect-Oriented Programming. Technologies for Implementing Aspect-Specific Abstractions. Technologies for Implementing Weaving. AOP and Specialized Language Extensions. AOP and Active Libraries. Final Remarks. 9. Generators. Why Is This Chapter Worth Reading? What Are Generators? Transformational Model of Software Development. Technologies for Building Generators. Compositional versus Transformational Generators. Kinds of Transformations. Compiler Transformations. Source-to-Source Transformations. Transformation Systems. Scheduling Transformations. Existing Transformation Systems and Their Applications. Selected Approaches to Generation. Draco. GenVoca. Approaches Based on Algebraic Specifications. 10. Static Metaprogramming in C++. Why Is This Chapter Worth Reading? What Is Metaprogramming? A Quick Tour of Metaprogramming. Static Metaprogramming. C++ as a Two-Level Language. Functional Flavor of the Static Level. Class Templates as Functions. Integers and Types as Data. Symbolic Names Instead of Variables. Constant Initialization and typedef-Statements Instead of Assignment. Template Recursion Instead of Loops. Conditional Operator and Template Specialization as Conditional Constructs. Template Metaprogramming. Template Metafunctions. Metafinctions as Arguments and Return Values of Other Metafinctions. Representing Metainformation. Member Traits. Traits Classes. Traits Templates. Example: Using Template Metafunctions and Traits Templates to Implement Type Promotions. Compile-Time Lists and Trees as Nested Templates. Compile-Time Control Structures. Explicit Selection Constructs. Template Recursion as a Looping Construct. Explicit Looping Constructs. Code Generation. Simple Code Selection. Composing Templates. Generators Based on Expression Templates. Recursive Code Expansion. Explicit Loops for Generating Code. Example: Using Static Execute Loops to Test Metafunctions. Partial Evaluation in C++. Workarounds for Partial Template Specialization. Problems of Template Metaprogramming. Historical Notes. 11. Intentional Programming. Why Is This Chapter Worth Reading? What Is Intentional Programming? Technology behind IP. System Architecture. Representing Programs in IP: The Source Graph. Source Graph + Methods = Active Source. Working with the IP Programming Environment. Editing. Further Capabilities of the IP Editor. Extending the IP System with New Intentions. Advanced Topics. Questions, Methods, and a Frameworklike Organization. Source-Pattem-Based Polymorphism. Methods as Visitors. Asking Questions Synchronously and Asynchronously. Reduction. The Philosophy behind IP. Why Do We Need Extendible Programming Environments? or What Is the Problem with Fixed Programming Languages? Moving Focus from Fixed Languages to Language Features and the Emergence of an Intention Market. Intentional Programming and Component-Based Development. Frequently Asked Questions. Summary. III. APPLICATION EXAMPLES. 12. List Container. Why Is This Chapter Worth Reading? Overview. Domain Analysis. Domain Design. Implementation Components. Manual Assembly. Specifying Lists. The Generator. Extensions. 13. Bank Account. Why Is This Chapter Worth Reading? The Successful Programming Shop. Design Pattems, Frameworks, and Components. Domain Engineering and Generative Programming. Feature Modeling. Architecture Design. Implementation Components. Configurable Class Hierarchies. Designing a Domain-Specific Language. Bank Account Generator. Testing Generators and Their Products. 14. Generative Matrix Computation Library (GMCL). Why Is This Chapter Worth Reading? Why Matrix Computations? Domain Analysis. Domain Definition. Domain Modeling. Domain Design and Implementation. Matrix Type Generation. Generating Code for Matrix Expressions. Implementing the Matrix Component in IP. APPENDICES. Appendix A: Conceptual Modeling. What Are Concepts? Theories of Concepts. Basic Terminology. The Classical View. The Probabilistic View. The Exemplar View. Summary of the Three Views. Important Issues Concerning Concepts. Stability of Concepts. Concept Core. Informational Contents of Features. Feature Composition and Relationships between Features. Quality of Features. Abstraction and Generalization. Conceptual Modeling, Object-Orientation, and Software Reuse. Appendix B: Instance-Specific Extension Protocol for Smalltalk. Appendix C: Protocol for Attaching Listener Objects in Smalltalk. Appendix D: Glossary of Matrix Computation Terms. Appendix E: Metafunction for Evaluating Dependency Tables. Glossary of Generative Programming Terms. References. Index. 020130977T04062001",Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .,"['Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .', 'Our infrastructure for NLP will provide high performance 1 components inspired by Generative Programming principles.']",0,"['Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .', 'Our infrastructure for NLP will provide high performance 1 components inspired by Generative Programming principles.']"
CC1380,W03-0806,Blueprint for a high performance NLP infrastructure,the american national corpus more than the web can provide,"['Nancy Ide', 'Randi Reppen', 'Keith Suderman']",introduction,"The American National Corpus (ANC) project is developing a corpus comparable to the British National Corpus (BNC), covering American English. Recent interest in the web as a source of corpus materials has caused some in the language processing community to suggest that the development of a corpus of American English is unnecessary. However, we argue that far from being rendered superfluous by the availability of web materials, the ANC is likely to provide a resource for developing web acquisition techniques to support tasks such as genre and language detection and automatic annotation. This paper presents a comparison of the ANC in terms of both content and format with a test corpus compiled from web data, and a discussion of points of intersection and divergence.","For example , 10 million words of the American National Corpus ( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .","['NLP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']",0,"['NLP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .']"
CC1381,W03-0806,Blueprint for a high performance NLP infrastructure,lt ttt  a flexible tokenisation tool,"['Claire Grover', 'Colin Matheson', 'Andrei Mikheev', 'Marc Moens']",experiments,,"For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .","['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).', 'However, the source code for these tools is not freely available, so they cannot be extended.']",0,"['For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .']"
CC1382,W03-0806,Blueprint for a high performance NLP infrastructure,modern c design generic programming and design patterns applied c indepth series,['Andrei Alexandrescu'],experiments,"Modern C++ Designis an important book. Fundamentally, it demonstrates 'generic patterns' or 'pattern templates' as a powerful new way of creating extensible designs in C++i??a new way to combine templates and patterns that you may never have dreamt was possible, but is. If your work involves C++ design and coding, you should read this book. Highly recommended. i??Herb SutterWhat's left to say about C++ that hasn't already been said? Plenty, it turns out. i??From the Foreword by John VlissidesIn Modern C++ Design, Andrei Alexandrescu opens new vistas for C++ programmers. Displaying extraordinary creativity and programming virtuosity, Alexandrescu offers a cutting-edge approach to design that unites design patterns, generic programming, and C++, enabling programmers to achieve expressive, flexible, and highly reusable code.This book introduces the concept of generic componentsi??reusable design templates that produce boilerplate code for compiler consumptioni??all within C++. Generic components enable an easier and more seamless transition from design to application code, generate code that better expresses the original design intention, and support the reuse of design structures with minimal recoding.The author describes the specific C++ techniques and features that are used in building generic components and goes on to implement industrial strength generic components for real-world applications. Recurring issues that C++ developers face in their day-to-day activity are discussed in depth and implemented in a generic way. These include: Policy-based design for flexibility Partial template specialization Typelistsi??powerful type manipulation structures Patterns such as Visitor, Singleton, Command, and Factories Multi-method enginesFor each generic component, the book presents the fundamental problems and design options, and finally implements a generic solution.In addition, an accompanying Web site, http://www.awl.com/cseng/titles/0-201-70431-5, makes the code implementations available for the generic components in the book and provides a free, downloadable C++ library, called Loki, created by the author. Loki provides out-of-the-box functionality for virtually any C++ project.Get a value-added service! Try out all the examples from this book at www.codesaw.com. CodeSaw is a free online learning tool that allows you to experiment with live code from your book right in your browser. 0201704315B11102003","To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes .","['The infrastructure will be implemented in C/C++.', 'Templates will be used heavily to provide generality without significantly impacting on efficiency.', 'However, because templates are a static facility we will also provide dynamic versions (using inheritance), which will be slower but accessible from scripting languages and user interfaces.', 'To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes .']",5,"['The infrastructure will be implemented in C/C++.', 'Templates will be used heavily to provide generality without significantly impacting on efficiency.', 'However, because templates are a static facility we will also provide dynamic versions (using inheritance), which will be slower but accessible from scripting languages and user interfaces.', 'To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes .']"
CC1383,W03-0806,Blueprint for a high performance NLP infrastructure,scaling to very very large corpora for natural language disambiguation,"['Michele Banko', 'Eric Brill']",introduction,"The amount of readily available online  text has reached hundreds of  billions of words and continues to  grow. Yet for most core natural  language tasks, algorithms continue  to be optimized, tested and compared  after training on corpora consisting  of only one million words or less. In  this paper, we evaluate the  performance of different learning  methods on a prototypical natural  language disambiguation task,  confusion set disambiguation, when  trained on orders of magnitude more  labeled data than has previously been  used. We are fortunate that for this  particular application, correctly  labeled training data is free. Since  this will often not be the case, we  examine methods for effectively  exploiting very large corpora when  labeled data comes at a cost","Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .","['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', 'Other potential applications must process text online or in realtime.', 'For example, Google currently answers 250 million queries per day, thus processing time must be minimised.', 'Clearly, efficient NLP components will need to be developed.', 'At the same time, state-of-the-art performance will be needed for these systems to be of practical use.']",0,"['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.']"
CC1384,W03-0806,Blueprint for a high performance NLP infrastructure,xml tools and architecture for named entity recognition,"['Andrei Mikheev', 'Claire Grover', 'Marc Moens']",experiments,This paper reports on the development of a Named Entity recognition system developed fully within the xml paradigm,"For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .","['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).', 'However, the source code for these tools is not freely available, so they cannot be extended.']",0,"['For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .']"
CC1385,W03-0806,Blueprint for a high performance NLP infrastructure,a corpusbased appreach to language learning,['Eric Brill'],,,"Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .","['Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .', 'We will base these components on the design of Weka (Witten and Frank, 1999).']",4,"['Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .', 'We will base these components on the design of Weka (Witten and Frank, 1999).']"
CC1386,W03-0806,Blueprint for a high performance NLP infrastructure,transformationbased learning in the fast lane,"['Grace Ngai', 'Radu Florian']",experiments,,"Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']",0,"['Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .']"
CC1387,W03-0806,Blueprint for a high performance NLP infrastructure,bootstrapping postaggers using unlabelled data,"['Stephen Clark', 'James R Curran', 'Miles Osborne']",,"This paper investigates booststrapping part-of-speech taggers using co-training, in which two taggers are iteratively re-trained on each other's output. Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set. We investigate selecting examples by directly maximising tagger agreement on unlabelled data, a method which has been theoretically and empirically motivated in the co-training literature. Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost.","The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'An example of using the Python tagger interface is shown in Figure 1.']",0,"['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .']"
CC1388,W03-0806,Blueprint for a high performance NLP infrastructure,a gaussian prior for smoothing maximum entropy models,"['Stanley Chen', 'Ronald Rosenfeld']",experiments,"Abstract : In certain contexts, maximum entropy (ME) modeling can be viewed as maximum likelihood training for exponential models, and like other maximum likelihood methods is prone to overfitting of training data. Several smoothing methods for maximum entropy models have been proposed to address this problem, but previous results do not make it clear how these smoothing methods compare with smoothing methods for other types of related models. In this work, we survey previous work in maximum entropy smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing n-gram language models. Because of the mature body of research in n-gram model smoothing and the close connection between maximum entropy and conventional n-gram models, this domain is well-suited to gauge the performance of maximum entropy smoothing methods. Over a large number of data sets, we find that an ME smoothing method proposed to us by Lafferty performs as well as or better than all other algorithms under consideration. This general and efficient method involves using a Gaussian prior on the parameters of the model and selecting maximum a posteriori instead of maximum likelihood parameter values. We contrast this method with previous n-gram smoothing methods to explain its superior performance.",These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG ) .,"['The implementation has been inspired by experience in extracting information from very large corpora (Curran and Moens, 2002) and performing experiments on maximum entropy sequence tagging (Curran and .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG ) .', 'We expect even faster training times when we move to conjugate gradient methods.']",5,['These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG ) .']
CC1389,W03-0806,Blueprint for a high performance NLP infrastructure,deterministic partofspeech tagging with finitestate transducers,"['Emmanuel Roche', 'Yves Schabes']",experiments,,"Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG ) .","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']",0,"['Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG ) .']"
CC1390,W03-0806,Blueprint for a high performance NLP infrastructure,nltk the natural language toolkit,"['Edward Loper', 'Steven Bird']",experiments,,"Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG ) .","['Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG ) .', 'Python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple.']",0,"['Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG ) .']"
CC1391,W03-0806,Blueprint for a high performance NLP infrastructure,investigating gis and smoothing for maximum entropy taggers,"['James R Curran', 'Stephen Clark']",experiments,"This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (GIS) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar.","The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).', 'We expect even faster training times when we move to conjugate gradient methods.']",4,"['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .']"
CC1392,W03-0806,Blueprint for a high performance NLP infrastructure,a comparison of algorithms for maximum entropy parameter estimation,['Robert Malouf'],experiments,"Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing. However, the flexibility of ME models is not without cost. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters. In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Sur-prisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limited-memory variable metric algorithm outperformed the other choices.","An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']",0,"['An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .']"
CC1393,W03-0806,Blueprint for a high performance NLP infrastructure,investigating gis and smoothing for maximum entropy taggers,"['James R Curran', 'Stephen Clark']",,"This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (GIS) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar.","The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example of using the Python tagger interface is shown in Figure 1.']",0,"['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .']"
CC1394,W03-0806,Blueprint for a high performance NLP infrastructure,tnt  a statistical partofspeech tagger,['Thorsten Brants'],experiments,,"The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']",0,"['The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']"
CC1395,W03-0806,Blueprint for a high performance NLP infrastructure,software architecture for language engineering,['Hamish Cunningham'],experiments,"Every building, and every computer program, has an architecture: structural and organisational principles that underpin its design and construction. The garden shed  once built by one of the authors had an ad hoc architecture, extracted (somewhat painfully) from the imagination during a slow and non-deterministic process that, luckily, resulted in a structure which keeps the rain on the outside and the mower on the inside (at least for the time being). As well as being ad hoc (i.e. not informed by analysis of similar practice or relevant science or engineering) this architecture is implicit: no explicit design was made, and no records or documentation kept of the construction process.  The pyramid in the courtyard of the Louvre, by contrast, was constructed in a process involving explicit design performed by qualified engineers with a wealth of theoretical and practical knowledge of the properties of materials, the relative merits and strengths of different construction techniques, et cetera.  So it is with software: sometimes it is thrown together by  enthusiastic amateurs; sometimes it is architected, built to last, and intended to be 'not something you finish, but something you start' (to paraphrase Brand (1994). A number of researchers argued in the early and middle 1990s that the field of computational infrastructure or architecture for human language computation merited an increase in attention. The reasoning was that the increasingly large-scale and technologically significant nature of language processing science was placing increasing burdens of an engineering nature on research and development workers seeking robust and practical methods (as was the increasingly collaborative nature of research in this field, which puts a large premium on software integration and interoperation). Over the intervening period a number of significant systems and practices have been developed in what we may call Software Architecture for Language Engineering (SALE).  This special issue represented an opportunity for practitioners in this area to report their work in a coordinated setting, and to present a snapshot of the state-ofthe-art in infrastructural work, which may indicate where further development and further take-up of these systems can be of benefit",GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .,"['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']",0,"['General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .']"
CC1396,W04-0910,Paraphrastic grammars,an open source grammar development environment and broadcoverage english grammar using hpsg,"['Ann Copestake', 'Dan Flickinger']",,"The LinGO (Linguistic Grammars Online) project's English Resource Grammar and the LKB grammar development environment are language resources which are freely available for download for any purpose, including commercial use (see http://lingo.stanford.edu). Executable programs and source code are both included. In this paper, we give an outline of the LinGO English grammar and LKB system, and discuss the ways in which they are currently being used. The grammar and processing system can be used independently or combined to give a central component which can be exploited in a variety of ways. Our intention in writing this paper is to encourage more people to use the technology, which supports collaborative development on many levels.","Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']",0,"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"
CC1397,W04-0910,Paraphrastic grammars,modlisation et traitement informatique de la synonymi linguisticae investigationes,['S Ploux'],,,Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ) .,"['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity (Pereira et al., 1993;Lin, 1998).', 'techniques.']",0,['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ) .']
CC1398,W04-0910,Paraphrastic grammars,automatic paraphrase acquisition from news articles,"['Y Shinyanma', 'S Sekine', 'K Sudo', 'R Grishman']",introduction,"Paraphrases play an important role in the variety and complexity of natural language documents. However, they add to the difficulty of natural language processing. Here we describe a procedure for ob-taining paraphrases from news articles. Articles derived from dif-ferent newspapers can contain paraphrases if they report the same event on the same day. We exploit this feature by using Named Entity recognition. Our approach is based on the assumption that Named Entities are preserved across paraphrases. We applied our method to articles of two domains and obtained notable examples. Although this is our initial attempt at automatically extracting para-phrases from a corpus, the results are promising. 1","Similarly , ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .","['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']",0,"['Similarly , ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .']"
CC1399,W04-0910,Paraphrastic grammars,identifying lexical paraphrases from a single corpus a case study for verbs,"['O Glickman', 'I Dagan']",introduction,,And ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .,"['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .']",0,"['Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'And ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .']"
CC1400,W04-0910,Paraphrastic grammars,un outil multilingue de generation de ltag  application au francais et a l’italien,['M H Candito'],,,"To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ) .","['Modeling intercategorial synonymic links.', ""A first investigation of Anne Abeillé's TAG for French suggests that modeling the synonymic relations across syntactic constructs is reasonably straightforward."", 'For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeillé FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above.', 'Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising.', 'To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ) .', 'This metagrammar allows us to factorise both syntactic and semantic information.', 'Syntactic information is factorised in the usual way.', 'For instance, there will be a class NOVN1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur.', 'But additionnally there will be semantic classes such as, ""binary predicate of semantic type X"" which will be associated with the relevant syntactic classes for instance, NOVN1 (the class of transitive verbs with nominal arguments), BINARY NPRED (the class of binary predicative nouns), NOVSUPNN1 , the class of support verb constructions taking two nominal arguments.', 'By further associating semantic units (e.g., ""cost"") with the appropriate semantic classes (e.g., ""binary predicate of semantic type X""), we can in this way capture both intra and intercategorial paraphrasing links in a general way.']",3,"['To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ) .', 'This metagrammar allows us to factorise both syntactic and semantic information.', 'Syntactic information is factorised in the usual way.']"
CC1401,W04-0910,Paraphrastic grammars,learning to paraphrase an unsupervised approahc using mutliplesequence alignment,"['R Barzilay', 'L Lee']",introduction,,"Similarly , ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .","['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']",0,"['Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'Similarly , ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .']"
CC1402,W04-0910,Paraphrastic grammars,framenet theory and practice,"['C Fillmore C Johnson', 'M Petruckand C Baker', 'M Ellsworth', 'J Ruppenhofer']",,,"To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .","['To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .', 'Johnson et al., 2002).', 'FrameNet is an online lexical resource for English based on the principles of Frame Semantics.', 'In this approach, a word evokes a frame i.e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame.', 'Finally each frame is associated with a set of target words, the words that evoke that frame.']",5,"['To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .', 'Johnson et al., 2002).', 'FrameNet is an online lexical resource for English based on the principles of Frame Semantics.', 'Finally each frame is associated with a set of target words, the words that evoke that frame.']"
CC1403,W04-0910,Paraphrastic grammars,discovery of inference rules for question answering natural language engineering,"['Dekang Lin', 'Patrick Pantel']",introduction,,"For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .","['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']",0,"['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']"
CC1404,W04-0910,Paraphrastic grammars,automatic retrieval and clustering of similar words,['D Lin'],,Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.,"For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ) .","['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ) .', 'techniques.']",3,"['For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ) .']"
CC1405,W04-0910,Paraphrastic grammars,towards evaluation of nlp systems,"['D Flickinger', 'J Nerbonne', 'I Sag', 'T Wasow']",,,"For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) .","['Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?)', 'and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) .', 'Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'The construction and annotation of the paraphrases reflects the paraphrase typology.', 'In a first phase, we concentrate on simple, non-recursive predicate/argument structure.', 'Given such a structure, the construction and annotation of a test item proceeds as follows.']",1,"['For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) .']"
CC1406,W04-0910,Paraphrastic grammars,minimal recursion semantics an introduction,"['A Copestake', 'D Flickinger', 'I Sag', 'C Pollard']",,"Minimal recursion semantics (MRS) is a framework for computational semantics that is suitable for parsing and generation and that can be implemented in typed feature structure formalisms. We discuss why, in general, a semantic representation with minimal structure is desirable and illustrate how a descriptively adequate representation with a nonrecursive structure may be achieved. MRS enables a simple formulation of the grammatical constraints on lexical and phrasal semantics, including the principles of semantic composition. We have integrated MRS with a broad-coverage HPSG grammar.","The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']",1,"['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']"
CC1407,W04-0910,Paraphrastic grammars,a procedure for quantitatively comparing the syntactic coverage of english grammars,"['A Black', 'S Abney', 'D Flickinger', 'C Gdaniec', 'R Grishman', 'P Harrison', 'D Hindel', 'R INgria', 'F Jelinek', 'F Klaavans', 'M Liberman', 'M Marcus', 'S Roukos', 'B Santorini', 'T Strzalkowski']",,"The problem of quantitatively comparing the performance of different broad-coverage grammars of English has to date resisted solution. Prima facie, known English grammars appear to disagree strongly with each other as to the elements of even the simplest sentences. For instance, the grammars of Steve Abney (Bellcore), Ezra Black (IBM), Dan Flickinger (Hewlett Packard), Claudia Gdaniec (Logos), Ralph Grishman and Tomek Strzalkowski (NYU), Phil Harrison (Boeing), Don Hindle (AT&T), Bob Ingria (BBN), and Mitch Marcus (U. of Pennsylvania) recognize in common only the following constituents, when each grammarian provides the single parse which he/she would ideally want his/her grammar to specify for three sample Brown Corpus sentences:The famed Yankee Clipper, now retired, has been assisting (as (a batting coach)).One of those capital-gains ventures, in fact, has saddled him (with Gore Court).He said this constituted a (very serious) misuse (of the (Criminal court) processes).","While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .","['Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?) and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english, there is for instance the 15 year old Hewlett- Packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al., 1987); and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information (Oepen and Flickinger, 1998).']",0,"['Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?) and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .', 'For english, there is for instance the 15 year old Hewlett- Packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al., 1987); and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information (Oepen and Flickinger, 1998).']"
CC1408,W04-0910,Paraphrastic grammars,predicate logic unplugged,['J Bos'],,"Die vorliegende Arbeit wurde im Rahmen des Verbundvorhabens Verbmobil vom Bundes-ministerium f ur Bildung, Wissenschaft, Forschung und Technologie (BMBF) unter dem FF orderkennzeichen 01 IV 101 R geff ordert. Die Verantwortung f ur den Inhalt dieser Arbeit liegt bei dem Autor.","The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .","['The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']",1,"['The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']"
CC1409,W04-0910,Paraphrastic grammars,les constructions converses du francais,['G Gross'],,,"In particular , ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns .","['For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular , ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns .']",3,"['For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular , ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns .']"
CC1410,W04-0910,Paraphrastic grammars,un outil multilingue de generation de ltag  application au francais et a l’italien,['M H Candito'],introduction,,"As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .","['As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .', 'The metagrammar is an abstract specification of the linguistic properties (phrase structure, valency, realisation of grammatical functions etc.) encoded in the grammar basic units.', 'This specification is then compiled to automatically produce a specific grammar.']",5,"['As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .', 'The metagrammar is an abstract specification of the linguistic properties (phrase structure, valency, realisation of grammatical functions etc.) encoded in the grammar basic units.', 'This specification is then compiled to automatically produce a specific grammar.']"
CC1411,W04-0910,Paraphrastic grammars,m´ethodes en syntase,['M Gross'],,,"For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .","['For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']",3,"['For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .']"
CC1412,W04-0910,Paraphrastic grammars,towards systematic grammar profiling test suite technology 10 years after computer speech and language,"['S Oepen', 'D Flickinger']",,,"For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ) .","['Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?)', 'and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ) .', 'Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'The construction and annotation of the paraphrases reflects the paraphrase typology.', 'In a first phase, we concentrate on simple, non-recursive predicate/argument structure.', 'Given such a structure, the construction and annotation of a test item proceeds as follows.']",0,"['Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?)', 'While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ) .']"
CC1413,W04-0910,Paraphrastic grammars,distributional clustering of english words,"['F Pereira', 'N Tishby', 'L Lee']",,"We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical ""soft"" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.","For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ) .","['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ) .', 'techniques.']",3,"['For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ) .']"
CC1414,W04-0910,Paraphrastic grammars,semantics and syntax in lexical functional grammar,['M Dalrymple'],,,"Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']",0,"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"
CC1415,W04-0910,Paraphrastic grammars,semantic construction in ftag,"['C Gardent', 'L Kallmeyer']",,,Semantic construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .,"['Semantic construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .', 'This is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation.', 'The association between tree nodes and unification variables encodes the syntax/semantics interface -it specifies which node in the tree provides the value for which variable in the final semantic representation.']",0,"['Semantic construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .', 'This is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation.']"
CC1416,W04-0910,Paraphrastic grammars,an algebra for semantic construction in constraintbased grammars,"['A Copestake', 'A Lascarides', 'D Flickinger']",,,"Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']",0,"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"
CC1417,W04-0910,Paraphrastic grammars,an algebra for semantic construction in constraintbased grammars,"['A Copestake', 'A Lascarides', 'D Flickinger']",,,"The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']",1,"['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .']"
CC1418,W04-0910,Paraphrastic grammars,alternations and verb semantic classes for french analysis and class formation chapter 5,['P Saint-Dizier'],,"In this paper, we show how alternations (called here contexts) can be defined for French, what their semantic properties are and how verb semantic classes can be constructed from syntactic criteria following (Levin 93). We then analyze the global quality of the results in terms of overlap with classes formed from the same verb-senses using WordNetlike classification criteria. Finally, we propose a method which combines these two approaches to form verb semantic classes better suited for natural language processing.","For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .","['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']",0,"['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .']"
CC1419,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,modern information retrieval,"['R B Yates', 'B R Neto']",,"Digital technologies can help scholars to navigate the vast quantities of musical data and source materials now available to them, but an imaginative leap is needed in order to conceptualize the kinds of musicological research questions we might ask of electronic corpora. In particular, our data-rich digital world offers enormous potential for the exploration of musical transmission and relatedness.  In this article, we explore the 16th- and 17th-century instrumental battaglia (battle piece), a genre with a very distinctive collective identity arising from the use of numerous shared ingredients (including melodic, motivic, textural, harmonic and rhythmic features). However, a battaglia is not defined by the presence of a core set of essential features, and exact concordance between these pieces is often remarkably low. This kind of musical 'family resemblance' (formulated after Wittgenstein) poses a serious challenge to both traditional musicological apparatus (for example, finding aids such as thematic catalogues) and Music Information Retrieval (which has often privileged melodic similarity at the expense of other kinds of musical relatedness).  This case study provides a stimulus for rethinking the complex nature of musical similarity. In doing this, we outline a set of requirements for digital tools that could support the discovery, exploration and representation of these kinds of relationships","It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .","['The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form.', 'This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.', 'In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']",4,"['It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .']"
CC1420,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,towards an arabic information retrieval system,['Y Houmame'],,"Arabic, the mother tongue of over 300 million people around the world, is known as one of the most difficult languages in Automatic Natural Language processing (NLP) in general and information retrieval in particular. Hence, Arabic cannot trust any web information retrieval system as reliable and relevant as Google search engine. In this context, we dared to focus all our researches to implement an Arabic web-based information retrieval system entitled ARABIRS (ARABic Information Retrieval System). Therefore, to launch such a process so long and hard, we will start with Indexing as one of the crucial steps of the system","It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .","['The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form.', 'This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.', 'In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']",4,"['It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .']"
CC1421,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,automatic indexing based on bayesian inference networksquot,"['K Tzeras', 'S Hartman']",experiments,"In this paper, a Bayesian inference network model for automatic indexing with index terms (descriptors) from a prescribed vocabulary is presented. It requires an indexing dictionary with rules mapping terms of the respective subject field onto descriptors and inverted lists for terms occuring in a set of documents of the subject field and descriptors manually assigned to these documents. The indexing dictionary can be derived automatically from a set of manually indexed documents. An application of the network model is described, followed by an indexing example and some experimental results about the indexing performance of the network model.","Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic .","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic .', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the χ 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the χ 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']",0,"['Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic .']"
CC1422,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,toward an arabic web page classifierquot master project,['M Yahyaoui'],related work,,"This work is a continuation of that initiated in ( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .","['Concerning Arabic, one automatic categorizer has been reported to have been put under operational use to classify Arabic documents; it is referred to as ""Sakhr\'s categorizer"" (Sakhr, 2004).', 'Unfortunately, there is no technical documentation or specification concerning this Arabic categorizer.', ""Sakhr's marketing literature claims that this categorizer is based on Arabic morphology and some research that has been carried out on natural language processing."", 'The present work evaluates the performance on Arabic documents of the Naïve Bayes algorithm (NB) -one of the simplest algorithms applied to English document categorization (Mitchell, 1997).', 'The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets.', 'This work is a continuation of that initiated in ( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .', 'A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories).', 'The present work expands the work in (Yahyaoui, 2001) by experimenting with the use of a better root extraction algorithm (El Kourdi, 2004) for document preprocessing, and using a different data set, collected from the largest Arabic site on the web: aljazeera.net.']",2,"['Concerning Arabic, one automatic categorizer has been reported to have been put under operational use to classify Arabic documents; it is referred to as ""Sakhr\'s categorizer"" (Sakhr, 2004).', 'Unfortunately, there is no technical documentation or specification concerning this Arabic categorizer.', 'The present work evaluates the performance on Arabic documents of the Naive Bayes algorithm (NB) -one of the simplest algorithms applied to English document categorization (Mitchell, 1997).', 'The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets.', 'This work is a continuation of that initiated in ( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .']"
CC1423,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,toward an arabic web page classifierquot master project,['M Yahyaoui'],conclusion,,"To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .","['To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .', 'In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments.', 'The corresponding performances in (Yahyaoui, 2001) are 75.6% and 50%, respectively.', 'Thus, the overall performance (including cross validation and evaluation set experiments) in this work is comparable to that in (Yahyaoui, 2001).', 'This offers some indication that the performance of NB algorithm in classifying Arabic documents is not sensitive to the Arabic root extraction algorithm.', 'Future work will be directed at experimenting with other root extraction algorithms.', ""Further improvement of NB's performance may be effected by using unlabeled documents; e.g., EM has been used successfully for this purpose in (Nigam et al.,200), where EM has increased the classification accuracy by 30% for classifying English documents.""]",1,"['To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .']"
CC1424,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,a reexamination of text categorization methods”,"['Y Yang', 'X Liu']",related work,,A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ) .,"['A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ) .', 'More recently, (Sebastiani, 2002) has performed a good survey of document categorization; recent works can also be found in (Joachims, 2002), (Crammer and Singer, 2003), and (Lewis et al., 2004).']",0,['A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ) .']
CC1425,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,a computational morphology system for arabic”,"['R Al-Shalabi', 'M Evens']",,"This paper describes a new algorithm for morphological analysis of Arabic words, which has been tested on a corpus of 242 abstracts from the Saudi Arabian National Computer Conference . It runs an order of magnitude faster than other algorithms in the literature.","This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .","['In Arabic, however, the use of stems will not yield satisfactory categorization.', 'This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .', 'The infix form (or stem) needs further to be processed in order to obtain the root.', 'This processing is not straightforward: it necessitates expert knowledge in Arabic language word morphology (Al-Shalabi and Evens, 1998).', 'As an example, two close roots (i.e., roots made of the same letters), but semantically different, can yield the same infix form thus creating ambiguity.']",0,"['In Arabic, however, the use of stems will not yield satisfactory categorization.', 'This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .', 'This processing is not straightforward: it necessitates expert knowledge in Arabic language word morphology (Al-Shalabi and Evens, 1998).']"
CC1426,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,a comparative study on feature selection in text categorization,"['Y Yang', 'J P Pedersen']",experiments,This paper is a comparative study of feature selection methods in statistical learning of text categorization The focus is on aggres sive dimensionality reduction Five meth ods were evaluated including term selection based on document frequency DF informa tion gain IG mutual information MI a test CHI and term strength TS We found IG and CHI most e ective in our ex periments Using IG thresholding with a k nearest neighbor classi er on the Reuters cor pus removal of up to removal of unique terms actually yielded an improved classi cation accuracy measured by average preci sion DF thresholding performed similarly Indeed we found strong correlations between the DF IG and CHI values of a term This suggests that DF thresholding the simplest method with the lowest cost in computation can be reliably used instead of IG or CHI when the computation of these measures are too expensive TS compares favorably with the other methods with up to vocabulary reduction but is not competitive at higher vo cabulary reduction levels In contrast MI had relatively poor performance due to its bias towards favoring rare terms and its sen sitivity to probability estimation errors,"( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the χ 2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .', 'On the other hand, (Rogati and Yang, 2002) reports the χ 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']",0,"['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the kh 2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .', 'On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']"
CC1427,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,machine learning in automated text categorization”,['F Sebastiani'],related work,"The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last ten years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert manpower, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely document representation, classifier construction, and classifier evaluation.Comment: Accepted for publication on ACM Computing Survey","More recently , ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .","['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']",0,"['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']"
CC1428,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,an evaluation of statistical approaches to text categorization”,['Y Yang'],introduction,"This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.","Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG ) .","['With the explosive growth of text documents on the web, relevant information retrieval has become a crucial task to satisfy the needs of different end users.', 'To this end, automatic text categorization has emerged as a way to cope with such a problem.', 'Automatic text (or document) categorization attempts to replace and save human effort required in performing manual categorization.', 'It consists of assigning and labeling documents using a set of predefined categories based on document contents.', 'As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes.', 'Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG ) .', 'Such applications have included electronic email filtering, newsgroups classification, and survey data grouping.', 'Barq for instance uses automatic categorization to provide similar documents feature (Rachidi et al., 2003).', 'In this paper, NB which is a statistical machine learning algorithm is used to learn to classify non-vocalized 1 Arabic web text documents.']",0,"['To this end, automatic text categorization has emerged as a way to cope with such a problem.', 'Automatic text (or document) categorization attempts to replace and save human effort required in performing manual categorization.', 'Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG ) .']"
CC1429,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,toward an arabic web page classifierquot master project,['M Yahyaoui'],experiments,,"In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG ) .","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the χ 2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the χ 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the χ 2 to produce best performance.', 'In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG ) .', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']",1,"['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the kh 2 statistic.', 'In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG ) .', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']"
CC1430,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,on the specification of term values in automatic indexingquot,"['G Salton', 'C S Yang']",experiments,,"TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .","['While the TF measurement concerns the importance of a term in a given document, IDF seeks to measure the relative importance of a term in a collection of documents.', 'The importance of each term is assumed to be inversely proportional to the number of documents that contain that term.', 'TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .', 'Thus, the weight of each term/root in a document is given by w D,t = TF D,t * IDF t .']",4,"['While the TF measurement concerns the importance of a term in a given document, IDF seeks to measure the relative importance of a term in a collection of documents.', 'The importance of each term is assumed to be inversely proportional to the number of documents that contain that term.', 'TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .']"
CC1431,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,newsweeder learning to filter netnewsquot,['K Lang'],experiments,,"Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( #AUTHOR_TAG ) , and the X2 statistic .","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( #AUTHOR_TAG ) , and the X2 statistic .', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the χ 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the χ 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']",0,"['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( #AUTHOR_TAG ) , and the X2 statistic .', 'On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']"
CC1432,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,comparison of two learning algorithms for text categorizationquot,"['D Lewis', 'M Ringnette']",related work,,"include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively .","['Many machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively .']",0,"['Many machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively .']"
CC1433,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,modern information retrieval,"['R B Yates', 'B R Neto']",experiments,"Digital technologies can help scholars to navigate the vast quantities of musical data and source materials now available to them, but an imaginative leap is needed in order to conceptualize the kinds of musicological research questions we might ask of electronic corpora. In particular, our data-rich digital world offers enormous potential for the exploration of musical transmission and relatedness.  In this article, we explore the 16th- and 17th-century instrumental battaglia (battle piece), a genre with a very distinctive collective identity arising from the use of numerous shared ingredients (including melodic, motivic, textural, harmonic and rhythmic features). However, a battaglia is not defined by the presence of a core set of essential features, and exact concordance between these pieces is often remarkably low. This kind of musical 'family resemblance' (formulated after Wittgenstein) poses a serious challenge to both traditional musicological apparatus (for example, finding aids such as thematic catalogues) and Music Information Retrieval (which has often privileged melodic similarity at the expense of other kinds of musical relatedness).  This case study provides a stimulus for rethinking the complex nature of musical similarity. In doing this, we outline a set of requirements for digital tools that could support the discovery, exploration and representation of these kinds of relationships",TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .,"['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the χ 2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the χ 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the χ 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']",0,"['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .']"
CC1434,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,learning to classify text using svm,['T Joachims'],related work,,"More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .","['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']",0,"['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']"
CC1435,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,using clustering to boost text classificationquot,"['Y C Fang', 'S Parthasarathy', 'F Schwartz']",related work,"In recent years we have seen a tremendous growth in the number of text document collections available on the Internet. Automatic text categorization, the process of assigning unseen documents to user-defined categories, is an important task that can help in the organization and querying of such collections. In this article we consider the problem of classifying online papers from a specific journal in the geological sciences, over a set of expert defined categories. We evaluate two general strategies and several variants thereof. The first strategy is based on Naive Bayes, a popular text classification algorithm. The second strategy is based on Principle Direction Divisive Partitioning, an unsupervised document clustering algorithm. While the performance of both approaches is quite good, some of the new variants that we propose including one, which involves a combination of these two approaches yield even better results.","For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .","['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']",0,"['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']"
CC1436,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,wordsketch extraction and display of significant collocations for lexicography,"['Adam Kilgarriff', 'David Tugwell']",related work,,"Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG , for example ) .","['On the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG , for example ) .', 'It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs.', 'The original acquisition methodology we present in the next section will allow us to overcome this limitation.']",1,"['On the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG , for example ) .']"
CC1437,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,two methods for extracting quotspecificquot singleword terms from specialized corpora,"['Chantal Lemay', ""Marie-Claude L'Homme"", 'Patrick Drouin']",,,"The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG ) .","['To construct this test set, we have focused our attention on ten domain-specific terms: commande (command), configuration, fichier (file), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), syst�me (system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called TermoStat.', 'The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG ) .', 'Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '(They were removed from the example set.)']",5,"['The terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called TermoStat.', 'The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG ) .']"
CC1438,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,symbolic word clustering for mediumsized corpora,"['Benoit Habert', 'Ellie Naulleau', 'Adeline Nazarenko']",related work,"When trying to identify essential concepts and relationships in a medium-size corpus, it is not always possible to rely on statistical methods, as the frequencies are too low. We present an alternative method, symbolic, based on the simplification of parse trees. We discuss the results on nominal phrases of two technical corpora, analyzed by two different robust parsers used for terminology updating in an industrial company. We compare our results with Hindle's scores of similarity.","This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG , for example ) , does not specify the relationship itself .","['A number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG , for example ) , does not specify the relationship itself .', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not di\x1berentiated.']",0,"['A number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG , for example ) , does not specify the relationship itself .', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not di\x1berentiated.']"
CC1439,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,conceptual structuring through term variation,['Beatrice Daille'],related work,"Term extraction systems are now an integral part of the compiling of specialized dictionaries and updating of term banks. In this paper, we present a term detection approach that discovers, structures, and infers conceptual relationships between terms for French. Conceptual relationships are deduced from specific types of term variations, morphological and syntagmatic, and are expressed through lexical functions. The linguistic precision of the conceptual structuring through morphological variations is of 95%.","More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .']",0,"['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .']"
CC1440,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,termextraction using nontechnical corpora as a point of leverage,['Patrick Drouin'],,,The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called TER1vloSTAT .,"['To construct this test set, we have focused our attention on ten domain-speci\x1cc terms: commande (command), con\x1cguration, \x1cchier (\x1cle), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), syst�me (system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called TER1vloSTAT .', 'The ten most speci\x1cc nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles (Lemay et al., 2004).', 'Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '(They were removed from the example set.)']",5,['The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called TER1vloSTAT .']
CC1441,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,lexicallybased terminology structuring some inherent limits,"['Natalia Grabar', 'Pierre Zweigenbaum']",related work,"Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying hierarchical (or other types of) relations between these terms. The present paper focusses on terminology structuring by lexical methods, which match terms on the basis on their content words, taking morphological variants into account. Experiments are done on a 'flat' list of terms obtained from an originally hierarchically-structured terminology: the French version of the US National Library of Medicine MeSH thesaurus. We compare the lexically-induced relations with the original MeSH relations: after a quantitative evaluation of their congruence through recall and precision metrics, we perform a qualitative, human analysis of the 'new' relations not present in the MeSH. This analysis shows, on the one hand, the limits of the lexical structuring method. On the other hand, it also reveals some specific structuring choices and naming conventions made by the MeSH designers, and emphasizes ontological commitments that cannot be left to automatic structuring.","More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use , as a starting point , a number of identical characters .","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use , as a starting point , a number of identical characters .']",0,"['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use , as a starting point , a number of identical characters .']"
CC1442,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,aide a lacquisition de connaissances a partir de corpus,['Rochdi Oueslati'],related work,"Le probleme d'identification des termes presente un interet particulier pour les applications du taln. En effet, la conception d'outils d'identification de termes et de relations entre termes est d'une aide considerable aux terminologues et aux cogniticiens qui veulent analyser un domaine nouveau. Les terminologues s'interessent surtout a l'etude des termes particulierement dans les domaines de specialite ou les termes designent des objets du domaine de facon la moins ambigue possible. Pour construire une terminologie on part souvent de textes et on applique un ensemble de methodes qui facilitent l'identification des termes. Les methodes classiques utilisent souvent des grammaires et des dictionnaires afin d'acquerir des concepts du domaine d'etude. L'approche que nous presentons dans cette these utilise une approche distributionnelle basee sur les travaux de z. Harris et utilise des algorithmes originaux pour la synthese automatique de contextes entre termes afin d'identifier des relations semantiques propres au domaine. Les resultats obtenus sont d'abord filtres puis valides par un linguiste avant d'etre structures sous forme hierarchique. Ils sont ensuite exploites afin d'acquerir d'autres connaissances en utilisant un processus iteratif et incremental base sur l'inference. L'utilisation d'un langage d'expression de contraintes entre termes du domaines permet de reperer un nombre fini de schemas morphosyntaxiques qui expriment des relations generiques notamment des definitions et des proprietes d'objets. Les resultats obtenus peuvent interesser d'autres travaux comme ceux lies a la construction de bases de connaissances terminologiques ou a la construction d'ontologies partielles propres au domaine.","Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #AUTHOR_TAG ) or, more frequently, on a combination of the two (Smadja, 1993; Kilgarri\x1b and Tugwell, 2001, for example).","['On the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #AUTHOR_TAG ) or, more frequently, on a combination of the two (Smadja, 1993; Kilgarri\x1b and Tugwell, 2001, for example).', 'It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs.', 'The original acquisition methodology we present in the next section will allow us to overcome this limitation.']",1,"['On the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #AUTHOR_TAG ) or, more frequently, on a combination of the two (Smadja, 1993; Kilgarri\\x1b and Tugwell, 2001, for example).', 'On the other hand, other work has been carried out in order to acquire collocations.']"
CC1443,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,inductive logic programming theory and methods,"['Stephen Muggleton', 'Luc De-Raedt']",method,"AbstractInductive Logic Programming (ILP) is a new discipline which investigates the inductive construction of first-order clausal theories from examples and background knowledge. We survey the most important theories and methods of this new field. First, various problem specifications of ILP are formalized in semantic settings for ILP, yielding a ""model-theory"" for ILP. Second, a generic ILP algorithm is presented. Third, the inference rules and corresponding operators used in ILP are presented, resulting in a ""proof-theory"" for ILP. Fourth, since inductive inference does not produce statements which are assured to follow from what is given, inductive inferences require an alternative form of justification. This can take the form of either probabilistic support or logical constraints on the hypothesis language. Information compression techniques used within ILP are presented within a unifying Bayesian approach to confirmation and corroboration of hypotheses. Also, different ways to constrain the hypothesis language or specify the declarative bias are presented. Fifth, some advanced topics in ILP are addressed. These include aspects of computational learning theory as applied to ILP, and the issue of predicate invention. Finally, we survey some applications and implementations of ILP. ILP applications fall under two different categories: first, scientific discovery and knowledge acquisition, and second, programming assistants","ASARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP ) ( #AUTHOR_TAG ) , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E â\x88\x92 ) of the elements one wants to acquire and their context.","['ASARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP ) ( #AUTHOR_TAG ) , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E â\x88\x92 ) of the elements one wants to acquire and their context.', 'The contextual patterns produced can then be applied to the corpus in order to retrieve new elements.', 'The acquisition process can be summarized in 3 steps:']",0,"['ASARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP ) ( #AUTHOR_TAG ) , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E â\\x88\\x92 ) of the elements one wants to acquire and their context.']"
CC1444,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,lexicallybased terminology structuring some inherent limits,"['Natalia Grabar', 'Pierre Zweigenbaum']",introduction,"Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying hierarchical (or other types of) relations between these terms. The present paper focusses on terminology structuring by lexical methods, which match terms on the basis on their content words, taking morphological variants into account. Experiments are done on a 'flat' list of terms obtained from an originally hierarchically-structured terminology: the French version of the US National Library of Medicine MeSH thesaurus. We compare the lexically-induced relations with the original MeSH relations: after a quantitative evaluation of their congruence through recall and precision metrics, we perform a qualitative, human analysis of the 'new' relations not present in the MeSH. This analysis shows, on the one hand, the limits of the lexical structuring method. On the other hand, it also reveals some specific structuring choices and naming conventions made by the MeSH designers, and emphasizes ontological commitments that cannot be left to automatic structuring.","However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .","['However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .', '(In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process.)', 'The work reported here infers specific semantic relationships based on sets of examples and counterexamples.']",1,"['However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .', '(In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process.)', 'The work reported here infers specific semantic relationships based on sets of examples and counterexamples.']"
CC1445,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,selection de termes dans un dictionnaire dinformatique  comparaison de corpus et criteres lexicosemantiques,"[""Marie-Claude L'Homme""]",introduction,"Resume Le present article propose une methode de selection des termes devant faire partie d'un dictionnaire specialise et, plus precisement, un dictionnaire fondamental d'informatique. La methode repose principalement sur un ensemble de criteres lexico-semantiques appliques a un corpus specialise. Elle tient egalement compte de la frequence et de la repartition des unites dans ce corpus. Dans ce travail, nous avons voulu savoir jusqu'a quel point des techniques de comparaison de corpus permettaient de ramener des termes coincidant avec la liste obtenue par l'application des criteres. L'examen de la liste generee automatiquement montre qu'un peu plus de 50 % des unites classees comme etant specifiques par la metrique sont egalement retenues par le terminographe. Les resultats revelent que la technique revet un interet dans la mesure ou elle permet d'aligner des choix sur des donnees extraites de corpus. Toutefois, la selection automatique recele un certain nombre d'imperfections qui doivent etre corrigees par une analyse terminographique.",The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG ) .,"['In this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization.', 'The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG ) .']",4,['The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG ) .']
CC1446,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,acquisition of qualia elements from corpora — evaluation of a symbolic learning method,"['Pierrette Bouillon', 'Vincent Claveau', 'Cecile Fabre', 'Pascale Sebillot']",method,"This paper presents and evaluates a system extracting from a corpus noun-verb pairs whose components are related by a special kind of link: the qualia roles as defined in the Generative Lexicon. This system is based on a symbolic learning method that automatically learns, from noun-verb pairs that are or are not related by a qualia link, rules characterizing positive examples from negative ones in terms of their surrounding part-of-speech or semantic contexts. The qualia noun-verb pair extraction is thus performed by applying the learnt rules on a part-of-speech or semantically tagged text. Stress is put on the quality of the learning when compared with traditional statistical or syntactical-based approaches. The linguistic relevance of the rules is also evaluated through a comparison with manually acquired qualia patterns.","In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG ) .","['In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG ) .']",4,"['In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG ) .']"
CC1447,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,the generative lexicon,['James Pustejovsky'],method,"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.","ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) .","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', 'However, the N-V combinations sought are more specific than those that were identified in these previous experiments.']",0,"['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) .', 'However, the N-V combinations sought are more specific than those that were identified in these previous experiments.']"
CC1448,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,query expansion using lexicalsemantic relations,['Ellen M Voorhees'],introduction,"Applications such as office automation, news filtering, help facilities in complex systems, and the like require the ability to retrieve documents from full-text databases where vocabulary problems can be particularly severe. Experiments performed on small collections with single-domain thesauri suggest that expanding query vectors with words that are lexically related to the original query words can ameliorate some of the problems of mismatched vocabularies. This paper examines the utility of lexical query expansion in the large, diverse TREC collection. Concepts are represented by WordNet synonym sets and are expanded by following the typed links included in WordNet. Experimental results show this query expansion technique makes little difference in retrieval effectiveness if the original queries are relatively complete descriptions of the information being sought even when the concepts to be expanded are selected by hand. Less well developed queries can be significantly improved by expansion of hand-chosen concepts. However, an automatic procedure that can approximate the set of hand picked synonym sets has yet to be devised, and expanding by the synonym sets that are automatically generated can degrade retrieval performance.","Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .","['Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval.', 'Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .']",1,"['Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .']"
CC1449,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,using partofspeech and semantic tagging for the corpusbased learning of qualia structure elements,"['Pierrette Bouillon', 'Vincent Claveau', 'Cecile Fabre', 'Pascale Sebillot']",method,This paper describes the im plementation and results of a machine learning method de veloped within the inductive logic programming ILP frame work Muggleton and De Raedt to automatically extract from a corpus tagged with parts of speech POS and semantic classes noun verb pairs whose components are bound by one of the relations de ned in the qualia structure in the Genera tive Lexicon Pustejovsky We demonstrate that the seman tic tagging of the corpus improves the quality of the learning both on a theoretical and an empiri cal point of view We also show that a set of the rules learnt by our ILP method have a linguistic signi cance regarding the detec tion of the clues that distinguish in terms of POS and seman tic surrounding context noun verb pairs that are linked by one qualia role from others that are not semantically related,"ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', 'However, the N-V combinations sought are more specific than those that were identified in these previous experiments.']",0,"['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .']"
CC1450,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,a comparative evaluation of collocation extraction techniques,['Darren Pearce'],method,"This paper describes an experiment that attempts to compare a range of existing collocation extraction techniques as well as the implementation of a new technique based on tests for lexical substitutability. After a description of the experiment details, the techniques are discussed with particular emphasis on any adaptations that are required in order to evaluate it in the way proposed. This is followed by a discussion on the relative strengths and weaknesses of the techniques with reference to the results obtained. Since there is no general agreement on the exact nature of collocation, evaluating techniques with reference to any single standard is somewhat controversial. Departing from this point, part of the concluding discussion includes initial proposals for a common framework for evaluation of collocation extraction techniques.","Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :","['The main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']",0,"['Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']"
CC1451,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,learning semantic lexicons from a partofspeech and semantically tagged corpus using inductive logic programming,"['Vincent Claveau', 'Pascale Sebillot', 'Cecile Fabre', 'Pierrette Bouillon']",method,"This paper describes an inductive logic programming learning method designed to acquire from a corpus specific Noun-Verb (N-V) pairs---relevant in information retrieval applications to perform index expansion---in order to build up semantic lexicons based on Pustejovsky's generative lexicon (GL) principles (Pustejovsky, 1995). In one of the components of this lexical model, called the qualia structure, words are described in terms of semantic roles. For example, the telic role indicates the purpose or function of an item (cut for knife), the agentive role its creation mode (build for house), etc. The qualia structure of a noun is mainly made up of verbal associations, encoding relational information. The learning method enables us to automatically extract, from a morpho-syntactically and semantically tagged corpus, N-V pairs whose elements are linked by one of the semantic relations defined in the qualia structure in GL. It also infers rules explaining what in the surrounding context distinguishes such pairs from others also found in sentences of the corpus but which are not relevant. Stress is put here on the learning efficiency that is required to be able to deal with all the available contextual information, and to produce linguistically meaningful rules.",ASARES is presented in detail in ( #AUTHOR_TAG ) .,"['The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in ( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein.']",5,['ASARES is presented in detail in ( #AUTHOR_TAG ) .']
CC1452,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,structures mathematiques du langage,['Zellig Harris'],related work,,A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .,"['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself.', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated.']",0,"['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated.']"
CC1453,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,trec2001 crosslingual retrieval at bbn,"['J Xu', 'A Fraser', 'R Weischedel']",introduction,,"It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 ) .","['We separate the EDR task into two parts: a mention detection step, which identifies and classifies all the mentions in a text -and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object.', 'In its entirety, the EDR task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non-named mentions (nominal and pronominal) and the requirement of grouping mentions into entities.', 'This is particularly true for Arabic where nominals and pronouns are also attached to the word they modify.', 'In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words).', 'In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 ) .']",0,"['We separate the EDR task into two parts: a mention detection step, which identifies and classifies all the mentions in a text -and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object.', 'In its entirety, the EDR task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non-named mentions (nominal and pronominal) and the requirement of grouping mentions into entities.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 ) .']"
CC1454,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,improving machine learning approaches to coreference resolution,"['V Ng', 'C Cardie']",introduction,"Human speakers generally have no difficulty in determining which noun phrases in a text or dialogue refer to the same real-world entity. This task of identifying co-referring noun phrases --- noun phrase coreference resolution --- can present a serious challenge to a natural language processing system, however. Indeed, it is one of the critical problems that currently limits the performance of many practical natural language processing tasks.    State-of-the-art coreference resolution systems operate by relying on a set of hand-crafted heuristics that requires a lot of time and linguistic expertise to develop. Recently, machine learning techniques have been used to circumvent both of these problems by automating the acquisition of coreference resolution heuristics, yielding coreference systems that offer performance comparable to their heuristic-based counterparts. In this dissertation, we present a machine learning-based solution to noun phrase coreference that extends eariler work in the area and outperforms the best existing learning-based coreference engine on a suite of standard coreference data sets. Performance gains accrue from more effective use of the available training data via a set of linguistic and extra-linguistic extensions to the standard machine learning framework for coreference resolution","The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]"
CC1455,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,proceedings of ace evaluation and pi meeting,['NIST'],introduction,,"Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the fo-cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL'02 and CoNLL'03 shared tasks."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",5,"[""The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the fo-cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL'02 and CoNLL'03 shared tasks."", 'Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .']"
CC1456,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a stochastic finitestate wordsegmentation algorithm for chinese,"['R Sproat', 'C Shih', 'W Gale', 'N Chang']",,,"In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG ) .","['In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG ) .', 'For these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'Here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n-gram character contexts that appear in the training data.', 'The character based model alone achieves a 94.5% exact match segmentation accuracy, considerably less accurate then the dictionary based model.']",1,"['In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG ) .']"
CC1457,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,automatic content extraction,['ACE'],conclusion,"This paper presents an integrated approach to automatically provide an overview of content on Thai websites based on tag cloud. This approach is intended to address the information overload issue by presenting the overview to users in order that they could assess whether the information meets their needs. The approach has incorporated Web content extraction, Thai word segmentation, and information presentation to generate a tag cloud in Thai language as an overview of the key content in the webpage. From the experimental study, the generated Thai Tag clouds are able to provide an overview of the tags which frequently appear in the title and body of the content. Moreover, the first few lines in the tag cloud offer an improved readability","These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .","['These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .', 'The experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'In addition, we also report results on the official test data.']",5,"['These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .', 'The experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'In addition, we also report results on the official test data.']"
CC1458,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,the unicode standard,"['Joan Aliprand', 'Julie Allen', 'Joe Becker', 'Mark Davis', 'Michael Everson']",,"The Unicode Standard is a global character set for worldwide computing covering the major modern scripts of the world as well as classical forms of Greek, Sanskrit, and Pali. The history and implications of Unicode Standard are discussed. The principles underpinning the design of the Unicode Standard are described with reference to those principles that also are present in USMARC and UNIMARC. Unicode give the potential to support every script. Expanding the character set would have consequences for transcription. Faithfulness of transcription has implications for retrieval. The addition of more characters to support more exact cataloging affects the economic cost of cataloging. The need for characters should be related not to the production of a surrogate for the physical item that has been cataloged, but to facilitating retrieval.","Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG ) .","['(2) .', 'We define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters.', 'Each punctuation symbol is considered a separate token.', 'Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG ) .']",5,"['(2) .', 'Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG ) .']"
CC1459,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,empirical studies in strategies for arabic information retrieval,"['J Xu', 'A Fraser', 'R Weischedel']",introduction,,"It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG ) .","['We separate the EDR task into two parts: a mention detection step, which identifies and classifies all the mentions in a text -and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object.', 'In its entirety, the EDR task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non-named mentions (nominal and pronominal) and the requirement of grouping mentions into entities.', 'This is particularly true for Arabic where nominals and pronouns are also attached to the word they modify.', 'In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words).', 'In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG ) .']",0,"['In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words).', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG ) .']"
CC1460,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a mentionsynchronous coreference resolution algorithm based on the bell tree,"['Xiaoqiang Luo', 'Abe Ittycheriah', 'Hongyan Jing', 'Nanda Kambhatla', 'Salim Roukos']",,This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.,The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG ) .,['The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG ) .'],1,['The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG ) .']
CC1461,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,arabic verbs and essentials of grammar,"['J Wightwick', 'M Gaafar']",,"Your one-stop guide to mastering the basics of Arabic Can one book have all you need to communicate confidently in a new language? Yes, and that book is Arabic Verbs & Essentials of Grammar. It offers a solid foundation of major verbal and grammatical concepts of the language, from pronouns to idioms and expressions and from irregular verbs to expressions of time. Each unit is devoted to one topic, so you can find what you need right away and get focused instruction immediately. Concise yet thorough, the explanations are supported by numerous examples to help you master the different concepts. And for those tricky verbs, Arabic Verbs & Essentials of Grammar includes a Verb Index of the most common verbs, cross-referenced with the abundant verb tables appearing throughout the book. This book will give you: An excellent introduction to the basics of Arabic if you are a beginner or a quick, thorough reference if you already have experience in the language Contemporary usage of verbs, adjectives, pronouns, prepositions, conjunctions, and other grammar essentials Examples that reflect contemporary usage and real-life situations","Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ) .","['Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .', 'The plural form of the noun (book) is (books), which is formed by deleting the infix .', 'The plural form and the singular form may also be completely different (e.g. for woman, but for women).', 'The sound plurals are formed by adding plural suffixes to singular nouns (e.g., meaning researcher): the plural suffix is for feminine nouns in grammatical cases (e.g.,']",0,"['Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ) .']"
CC1462,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a statistical model for multilingual entity detection and tracking,"['R Florian', 'H Hassan', 'A Ittycheriah', 'H Jing', 'N Kambhatla', 'X Luo', 'N Nicolov', 'S Roukos']",introduction,"Abstract : Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks. In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text. Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features. In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers. The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages.","The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.']"
CC1463,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a maximum entropy approach to natural language processing,"['A Berger', 'S Della Pietra', 'V Della Pietra']",,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.","The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ) .","['We formulate the mention detection problem as a classification problem, which takes as input segmented Arabic text.', 'We assign to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.', 'We use a maximum entropy Markov model (MEMM) classifier.', 'The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ) .', 'One big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision.']",0,"['The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ) .']"
CC1464,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,automatic content extraction,['ACE'],,"This paper presents an integrated approach to automatically provide an overview of content on Thai websites based on tag cloud. This approach is intended to address the information overload issue by presenting the overview to users in order that they could assess whether the information meets their needs. The approach has incorporated Web content extraction, Thai word segmentation, and information presentation to generate a tag cloud in Thai language as an overview of the key content in the webpage. From the experimental study, the generated Thai Tag clouds are able to provide an overview of the tags which frequently appear in the title and body of the content. Moreover, the first few lines in the tag cloud offer an improved readability","the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .","['the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .']",5,"['the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .']"
CC1465,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a maximum entropy approach to named entity recognition,['A Borthwick'],introduction,"This thesis describes a novel statistical named-entity (i.e. ""proper name"") recognition system known as ""MENE"" (Maximum Entropy Named Entity). Named entity (N.E.) recognition is a form of information extraction in which we seek to classify every word in a document as being a person-name, organization, location, date, time, monetary value, percentage, or ""none of the above"". The task has particular significance for Internet search engines, machine translation, the automatic indexing of documents, and as a foundation for work on more complex information extraction tasks.  Two of the most significant problems facing the constructor of a named entity system are the questions of portability and system performance. A practical N.E. system will need to be ported frequently to new bodies of text and even to new languages. The challenge is to build a system which can be ported with minimal expense (in particular minimal programming by a computational linguist) while maintaining a high degree of accuracy in the new domains or languages.  MENE attempts to address these issues through the use of maximum entropy probabilistic modeling. It utilizes a very flexible object-based architecture which allows it to make use of a broad range of knowledge sources in making its tagging decisions. In the DARPA-sponsored MUC-7 named entity evaluation, the system displayed an accuracy rate which was well-above the median, demonstrating that it can achieve the performance goal. In addition, we demonstrate that the system can be used as a post-processing tool to enhance the output of a hand-coded named entity recognizer through experiments in which MENE improved on the performance of N.E. systems from three different sites. Furthermore, when all three external recognizers are combined under MENE, we are able to achieve very strong results which, in some cases, appear to be competitive with human performance.  Finally, we demonstrate the trans-lingual portability of the system. We ported the system to two Japanese-language named entity tasks, one of which involved a new named entity category, ""artifact"". Our results on these tasks were competitive with the best systems built by native Japanese speakers despite the fact that the author speaks no Japanese.","The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.']"
CC1466,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,language model based arabic word segmentation,"['Y-S Lee', 'K Papineni', 'S Roukos', 'O Emam', 'H Hassan']",,"We approximate Arabic's rich morphology by a model that a word consists of a sequence of morphemes in the pattern prefix*-stem-suffix * (* denotes zero or more occurrences of a morpheme). Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus. The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input. The language model is initially estimated from a small manually segmented corpus of about 110,000 words. To improve the segmentation accuracy, we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus, and re-estimate the model parameters with the expanded vocabulary and training corpus. The resulting Arabic word segmentation system achieves around 97 % exact match accuracy on a test corpus containing 28,449 word tokens. We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest",#AUTHOR_TAG demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .,"['#AUTHOR_TAG demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .', 'A trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules.']",5,['#AUTHOR_TAG demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .']
CC1467,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a maximum entropy approach to natural language processing,"['A Berger', 'S Della Pietra', 'V Della Pietra']",introduction,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.",Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG ) .,"['Both tasks are performed with a statistical framework: the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in .', 'Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG ) .', 'We formulate the mention detection task as a sequence classification problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.', 'The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'We begin with a segmentation of the written text before starting the classification.', 'This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens).', 'The resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions).', 'Additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness.']",5,['Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG ) .']
CC1468,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a statistical model for multilingual entity detection and tracking,"['R Florian', 'H Hassan', 'A Ittycheriah', 'H Jing', 'N Kambhatla', 'X Luo', 'N Nicolov', 'S Roukos']",introduction,"Abstract : Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks. In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text. Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features. In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers. The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages.","Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classi cation problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.', 'The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'We begin with a segmentation of the written text before starting the classification.', 'This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens).', 'The resulting granulariity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions).', 'Additionally, because the pre xes and su_xes are quite frequent, directly processing unsegmented words results in signi cant data sparseness.']",1,"['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .']"
CC1469,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a statistical model for multilingual entity detection and tracking,"['R Florian', 'H Hassan', 'A Ittycheriah', 'H Jing', 'N Kambhatla', 'X Luo', 'N Nicolov', 'S Roukos']",,"Abstract : Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks. In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text. Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features. In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers. The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages.",The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ) .,"['The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ) .', 'We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of t i respectively.', 'For a token t i , the backward token n-gram feature will contains the previous n − 1 tokens in the history (t i−n+1 , . . .', 't i−1 ) and the forward token n-gram feature will contains the next n − 1 tokens (t i+1 , . . .', 't i+n−1 ).']",0,['The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ) .']
CC1470,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a mentionsynchronous coreference resolution algorithm based on the bell tree,"['Xiaoqiang Luo', 'Abe Ittycheriah', 'Hongyan Jing', 'Nanda Kambhatla', 'Salim Roukos']",,This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.,Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ) .,"['Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ) .', 'For Arabic, since words are morphologically derived from a list of roots (stems), we expected that a feature based on the right and left stems would lead to improvement in system accuracy.']",5,['Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ) .']
CC1471,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a machine learning approach to coreference resolution of noun phrases,"['W M Soon', 'H T Ng', 'C Y Lim']",introduction,"this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data set","The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]"
CC1472,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,language model based arabic word segmentation,"['Y-S Lee', 'K Papineni', 'S Roukos', 'O Emam', 'H Hassan']",,"We approximate Arabic's rich morphology by a model that a word consists of a sequence of morphemes in the pattern prefix*-stem-suffix * (* denotes zero or more occurrences of a morpheme). Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus. The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input. The language model is initially estimated from a small manually segmented corpus of about 110,000 words. To improve the segmentation accuracy, we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus, and re-estimate the model parameters with the expanded vocabulary and training corpus. The resulting Arabic word segmentation system achieves around 97 % exact match accuracy on a test corpus containing 28,449 word tokens. We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest","As in ( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .","['However, an analysis of the errors indicated that the character based model is more effective at segmenting words that do not appear in the training data.', 'We seeked to exploit this ability to generalize to improve the dictionary based model.', 'As in ( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .', 'In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus.', 'From this, we create a vocabulary of stems and affixes by requiring that tokens appear more than twice in the supervised training data or more than ten times in the unsupervised, segmented corpus.']",5,"['As in ( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .', 'In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus.']"
CC1473,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,named entity recognition without gazetteers,"['A Mikheev', 'M Moens', 'C Grover']",introduction,"It is often claimed that Named Entity  recognition systems need extensive  gazetteers|lists of names of people, organisations,  locations, and other named  entities. Indeed, the compilation of such  gazetteers is sometimes mentioned as a  bottleneck in the design of Named Entity  recognition systems.  We report on a Named Entity recognition  system which combines rule-based  grammars with statistical (maximum entropy)  models. We report on the system  &apos;s performance with gazetteers of different  types and dierent sizes, using test  material from the muc{7 competition.  We show that, for the text type and task  of this competition, it is sucient to use  relatively small gazetteers of well-known  names, rather than large gazetteers of  low-frequency names. We conclude with  observations about the domain independence  of the competition and of our experiments.  1 Introduction  Named Entity recognition involves processing a text and identifying certain occurrences of words or expressions ..","The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]"
CC1474,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,automatic content extraction,['ACE'],experiments,"This paper presents an integrated approach to automatically provide an overview of content on Thai websites based on tag cloud. This approach is intended to address the information overload issue by presenting the overview to users in order that they could assess whether the information meets their needs. The approach has incorporated Web content extraction, Thai word segmentation, and information presentation to generate a tag cloud in Thai language as an overview of the key content in the webpage. From the experimental study, the generated Thai Tag clouds are able to provide an overview of the tags which frequently appear in the title and body of the content. Moreover, the first few lines in the tag cloud offer an improved readability","We introduce here a clearly defined and replicable split of the #AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here .","['The system is trained on the Arabic ACE 2003 and part of the 2004 data.', 'We introduce here a clearly defined and replicable split of the #AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here .']",5,"['The system is trained on the Arabic ACE 2003 and part of the 2004 data.', 'We introduce here a clearly defined and replicable split of the #AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here .']"
CC1475,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,an empirical study of smoothing techinques for language modeling,"['S F Chen', 'J Goodman']",,"We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including t...","The final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG ) based backoff language model .","['In our latest implementation of this algorithm, we have recast this segmentation strategy as the composition of three distinct finite state machines.', 'The first machine, illustrated in Figure 1 encodes the prefix and suffix expansion rules, producing a lattice of possible segmentations.', 'The second machine is a dictionary that accepts characters and produces identifiers corresponding to dictionary entries.', 'The final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG ) based backoff language model .', 'Differing from (Lee et al., 2003), we have also introduced an explicit model for un-known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.', 'Using 0.5M words from the combined Arabic Treebanks 1V2, 2V2 and 3V1, the dictionary based segmenter achieves a exact word match 97.8% correct segmentation.']",5,"['The final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG ) based backoff language model .']"
CC1476,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,bbn description of the sift system as used for muc7,"['S Miller', 'M Crystal', 'H Fox', 'L Ramshaw', 'R Schwarz', 'R Stone', 'R Weischedel']",introduction,,"The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]"
CC1477,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,nymble a highperformance learning namefinder,"['D M Bikel', 'S Miller', 'R Schwartz', 'R Weischedel']",introduction,,"The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]"
CC1478,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a mentionsynchronous coreference resolution algorithm based on the bell tree,"['Xiaoqiang Luo', 'Abe Ittycheriah', 'Hongyan Jing', 'Nanda Kambhatla', 'Salim Roukos']",experiments,This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.,"ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .","['In this section, we present the coreference results on the devtest defined earlier.', 'First, to see the effect of stem matching features, we compare two coreference systems: one with the stem features, the other with- out.', 'We test the two systems on both ""true"" and system mentions of the devtest set.', 'True mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.', 'We report results with two metrics: ECM-F and ACE- Value.', 'ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .', 'The result is shown in Table 4: the baseline numbers without stem features are listed under \\Base,"" and the results of the coreference system with stem features are listed under \\Base+Stem.""']",5,"['In this section, we present the coreference results on the devtest defined earlier.', 'First, to see the effect of stem matching features, we compare two coreference systems: one with the stem features, the other with- out.', 'We test the two systems on both ""true"" and system mentions of the devtest set.', 'True mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.', 'ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .', 'The result is shown in Table 4: the baseline numbers without stem features are listed under \\Base,"" and the results of the coreference system with stem features are listed under \\Base+Stem.""']"
CC1479,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,building an arabic stemmer for information retrieval,"['Aitao Chen', 'Fredic Gey']",,"In TREC 2002 the Berkeley group participated only in the English-Arabic cross-language retrieval (CLIR) track. One Arabic monolingual run and three English-Arabic cross-language runs were submitted. Our approach to the crosslanguage retrieval was to translate the English topics into Arabic using online English-Arabic machine translation systems. The four official runs are named as BKYMON, BKYCL1, BKYCL2, and BKYCL3. The BKYMON is the Arabic monolingual run, and the other three runs are English-to-Arabic cross-language runs. This paper reports on the construction of an Arabic stoplist and two Arabic stemmers, and the experiments on Arabic monolingual retrieval, English-to-Arabic cross-language retrieval.","Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .","['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .', 'The plural form of the noun (book) is (books), which is formed by deleting the infix .', 'The plural form and the singular form may also be completely different (e.g. for woman, but for women).', 'The sound plurals are formed by adding plural suffixes to singular nouns (e.g., meaning researcher): the plural suffix is for feminine nouns in grammatical cases (e.g.,']",0,"['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .']"
CC1480,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a mentionsynchronous coreference resolution algorithm based on the bell tree,"['Xiaoqiang Luo', 'Abe Ittycheriah', 'Hongyan Jing', 'Nanda Kambhatla', 'Salim Roukos']",introduction,This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.,"Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classification problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.', 'The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'We begin with a segmentation of the written text before starting the classification.', 'This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens).', 'The resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions).', 'Additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness.']",1,"['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .']"
CC1481,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,proceedings of ace evaluation and pi meeting,['NIST'],experiments,,"As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) .","['We want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system.', 'The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class.', 'Hence, to build our mention detection system we spent a lot of effort in improving the rst step: detecting the mention boundary and their main type.', 'In this paper, we report the results in terms of precision, recall, and F-measure3.']",5,"[""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class.', 'In this paper, we report the results in terms of precision, recall, and F-measure3.']"
CC1482,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a maximum entropy approach to natural language processing,"['A Berger', 'S Della Pietra', 'V Della Pietra']",,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.","where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .","['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']",5,"['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']"
CC1483,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing semantic relatedness across parts of speech,['Iryna Gurevych'],experiments,,#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .,"['We developed a web-based interface to obtain human judgments of semantic relatedness for each automatically generated concept pair.', 'Test subjects were invited via email to participate in the experiment.', 'Thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .', 'Their results differed particularly in cases of antonymy or distributionally related pairs.', 'We created a manual with a detailed introduction to SR stressing the crucial points.', 'The manual was presented to the subjects before the experiment and could be re-accessed at any time.', 'During the experiment, one concept pair at a time was presented to the test subjects in random ordering.', 'Subjects had to assign a discrete relatedness value {0,1,2,3,4} to each pair.', ""Figure 2 shows the system's GUI.""]",4,['#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .']
CC1484,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing similarity,"['Ludovic Lebart', 'Martin Rajman']",introduction,"In this paper we study similarity measures for moving curves which can, for example, model changing coastlines or retreating glacier termini. Points on a moving curve have two parameters, namely the position along the curve as well as time. We therefore focus on similarity measures for surfaces, specifically the Fr 'echet distance between surfaces. While the Fr 'echet distance between surfaces is not even known to be computable, we show for variants arising in the context of moving curves that they are polynomial-time solvable or NP-complete depending on the restrictions imposed on how the moving curves are matched. We achieve the polynomial-time solutions by a novel approach for computing a surface in the so-called free-space diagram based on max-flow min-cut duality","words and their surrounding context), words or concepts ( #AUTHOR_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness.","['Linguistic distance plays an important role in many applications like information retrieval, word sense disambiguation, text summarization or spelling correction.', 'It is defined on different kinds of textual units, e.g.', 'documents, parts of a document (e.g.', 'words and their surrounding context), words or concepts ( #AUTHOR_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness.']",0,"['Linguistic distance plays an important role in many applications like information retrieval, word sense disambiguation, text summarization or spelling correction.', 'words and their surrounding context), words or concepts ( #AUTHOR_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness.']"
CC1485,W06-1104,Automatically creating datasets for measures of semantic relatedness,identifying semantic relations and functional properties of human verb associations,"['Sabine Schulte im Walde', 'Alissa Melinger']",related work,"This paper uses human verb associations as the basis for an investigation of verb properties, focusing on semantic verb relations and prominent nominal features. First, the lexical semantic taxonymy GermaNet is checked on the types of classic semantic relations in our data; verb-verb pairs not covered by GermaNet can help to detect missing links in the taxonomy, and provide a useful basis for defining non-classical relations. Second, a statistical grammar is used for determining the conceptual roles of the noun responses. We present prominent syntax-semantic roles and evidence for the usefulness of co-occurrence information in distributional verb descriptions.","In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG ) . Table 1: Comparison of previous experiments.","['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG ) . Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']",0,"['In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG ) . Table 1: Comparison of previous experiments.']"
CC1486,W06-1104,Automatically creating datasets for measures of semantic relatedness,evaluating wordnetbased measures of semantic distance,"['Alexander Budanitsky', 'Graeme Hirst']",,,"According to #AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .","['According to #AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .']",0,"['According to #AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .']"
CC1487,W06-1104,Automatically creating datasets for measures of semantic relatedness,evaluating wordnetbased measures of semantic distance,"['Alexander Budanitsky', 'Graeme Hirst']",experiments,,#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .,"['The distribution of averaged human judgments on the whole test set (see Figure 3) is almost balanced with a slight underrepresentation of highly related concepts.', 'To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain- ing could be used.', 'However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .', 'This empty band is not observed here.', 'However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8).', 'The plot clearly shows an empty horizontal band with no judgments.', 'The connection between averaged judgments and standard deviation is plotted in Figure 5.']",1,"['The distribution of averaged human judgments on the whole test set (see Figure 3) is almost balanced with a slight underrepresentation of highly related concepts.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .', 'The connection between averaged judgments and standard deviation is plotted in Figure 5.']"
CC1488,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing semantic relatedness across parts of speech,['Iryna Gurevych'],related work,,This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG .,"['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG .', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']",0,"['Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', 'This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG .', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'Therefore, we automatically create word pairs using a corpus-based approach.']"
CC1489,W06-1104,Automatically creating datasets for measures of semantic relatedness,using information content to evaluate semantic similarity,['Philip Resnik'],related work,"Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments.",This experiment was again replicated by #AUTHOR_TAG with 10 subjects .,"['In the seminal work by Rubenstein and Goodenough (1965), similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', 'Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'This experiment was again replicated by #AUTHOR_TAG with 10 subjects .', 'Table 1']",0,"['In the seminal work by Rubenstein and Goodenough (1965), similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', 'This experiment was again replicated by #AUTHOR_TAG with 10 subjects .', 'Table 1']"
CC1490,W06-1104,Automatically creating datasets for measures of semantic relatedness,project ‘semantic information retrieval’,['SIR Project'],experiments,"The aim of the SCHEMA Network of Excellence is to bring together a critical mass of universities, research centers, industrial partners and end users, in order to design a reference system for content-based semantic scene analysis, interpretation and understanding. Relevant research areas include: content-based multimedia analysis and automatic annotation of semantic multimedia content, combined textual and multimedia information retrieval, semantic -web, MPEG-7 and MPEG-21 standards, user interfaces and human factors. In this paper, recent advances in content-based analysis, indexing and retrieval of digital media within the SCHEMA Network are presented. These advances will be integrated in the SCHEMA module-based, expandable reference system","In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .","['We extracted word pairs from three different domain-specific corpora (see Table 2).', 'This is motivated by the aim to enable research in information retrieval incorporating SR measures.', ""In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .""]",4,"[""In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .""]"
CC1491,W06-1104,Automatically creating datasets for measures of semantic relatedness,cooccurrence retrieval a flexible framework for lexical distributional similarity,"['Julie Weeds', 'David Weir']",,"Techniques that exploit knowledge of distributional similarity between words have been proposed in many areas of Natural Language Processing. For example, in language modeling, the sparse data problem can be alleviated by estimating the probabilities of unseen co-occurrences of events from the probabilities of seen co-occurrences of similar events. In other applications, distributional similarity is taken to be an approximation to semantic similarity. However, due to the wide range of potential applications and the lack of a strict definition of the concept of distributional similarity, many methods of calculating distributional similarity have been proposed or adopted. In this work, a flexible, parameterized framework for calculating distributional similarity is proposed. Within this framework, the problem of finding distributionally similar words is cast as one of co-occurrence retrieval (CR) for which precision and recall can be measured by analogy with the way they are measured in document retrieval. As will be shown, a number of popular existing measures of distributional similarity are simulated with parameter settings within the CR framework. In this article, the CR framework is then used to systematically investigate three fundamental questions concerning distributional similarity. First, is the relationship of lexical similarity necessarily symmetric, or are there advantages to be gained from considering it as an asymmetric relationship? Second, are some co-occurrences inherently more salient than others in the calculation of distributional similarity? Third, is it necessary to consider the difference in the extent to which each word occurs in each co-occurrence type? Two application-based tasks are used for evaluation: automatic thesaurus generation and pseudo-disambiguation. It is possible to achieve significantly better results on both these tasks by varying the parameters within the CR framework rather than using other existing distributional similarity measures; it will also be shown that any single unparameterized measure is unlikely to be able to do better on both tasks. This is due to an inherent asymmetry in lexical substitutability and therefore also in lexical distributional similarity.","dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ) .","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']",0,"['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ) .']"
CC1492,W06-1104,Automatically creating datasets for measures of semantic relatedness,lexikalischsemantische wortnetze chapter computerlinguistik und sprachtechnologie,['Claudia Kunze'],experiments,,"Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .","['We implemented a set of filters for word pairs.', 'One group of filters removed unwanted word pairs.', 'Word pairs are filtered if they contain at least one word that a) has less than three letters b) contains only uppercase letters (mostly acronyms) or c) can be found in a stoplist.', 'Another filter enforced a specified fraction of combinations of nouns (N), verbs (V) and adjectives (A) to be present in the result set.', 'We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on.', 'Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .', 'It is the most complete resource of this type for German.']",5,"['We implemented a set of filters for word pairs.', 'One group of filters removed unwanted word pairs.', 'Word pairs are filtered if they contain at least one word that a) has less than three letters b) contains only uppercase letters (mostly acronyms) or c) can be found in a stoplist.', 'Another filter enforced a specified fraction of combinations of nouns (N), verbs (V) and adjectives (A) to be present in the result set.', 'We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on.', 'Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .', 'It is the most complete resource of this type for German.']"
CC1493,W06-1104,Automatically creating datasets for measures of semantic relatedness,an informationtheoretic definition of similarity in,['Dekang Lin'],,,"Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG ) .4","['Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG ) .4', 'However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application.']",0,"['Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG ) .4']"
CC1494,W06-1104,Automatically creating datasets for measures of semantic relatedness,contextual correlates of synonymy,"['Herbert Rubenstein', 'John B Goodenough']",experiments,"Experimentol corroboration was obtained for the hypothesis that the proportion of words common to the contexts of word A and to the contexts of word B is a function of the degree to which A and B are similar in meaning. The tests were carried out for variously defined contexts. The shapes of the functions, however, indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning.",#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .,"['In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647.', '#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .', 'The values may again not be compared directly.', 'Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low.']",1,['#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .']
CC1495,W06-1104,Automatically creating datasets for measures of semantic relatedness,using the structure of a conceptual network in computing semantic relatedness,['Iryna Gurevych'],related work,"Abstract. We present a new method for computing semantic relatedness of concepts. The method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis. The network structure is employed to generate artificial conceptual glosses. They replace textual definitions proper written by humans and are processed by a dictionary based metric of semantic relatedness [1]. We implemented the metric on the basis of GermaNet, the German counterpart of WordNet, and evaluated the results on a German dataset of 57 word pairs rated by human subjects for their semantic relatedness. Our approach can be easily applied to compute semantic relatedness based on alternative conceptual networks, e.g. in the domain of life sciences.","We used the revised experimental setup ( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .","['In our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006).', 'We used the revised experimental setup ( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.', 'Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure.']",5,"['We used the revised experimental setup ( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.']"
CC1496,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing semantic relatedness across parts of speech,['Iryna Gurevych'],related work,,"In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG .","['In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG .', 'We used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.', 'Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure.']",1,"['In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG .', 'We used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.']"
CC1497,W06-1104,Automatically creating datasets for measures of semantic relatedness,wordnet an electronic lexical database chapter combining local context and wordnet similarity for word sense identification,"['Claudia Leacock', 'Martin Chodorow']",,,"Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']",0,"['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']"
CC1498,W06-1104,Automatically creating datasets for measures of semantic relatedness,evaluating wordnetbased measures of semantic distance,"['Alexander Budanitsky', 'Graeme Hirst']",introduction,,Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG ) .,"['Semantic similarity is typically defined via the lexical relations of synonymy (automobile -car) and hypernymy (vehicle -car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist be-tween two words (Gurevych, 2005). 3', 'Dissimilar words can be semantically related, e.g.', 'via functional relationships (night -dark) or when they are antonyms (high -low).', 'Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG ) .']",0,"['Semantic similarity is typically defined via the lexical relations of synonymy (automobile -car) and hypernymy (vehicle -car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist be-tween two words (Gurevych, 2005). 3', 'Dissimilar words can be semantically related, e.g.', 'Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG ) .']"
CC1499,W06-1104,Automatically creating datasets for measures of semantic relatedness,contextual correlates of synonymy,"['Herbert Rubenstein', 'John B Goodenough']",related work,"Experimentol corroboration was obtained for the hypothesis that the proportion of words common to the contexts of word A and to the contexts of word B is a function of the degree to which A and B are similar in meaning. The tests were carried out for variously defined contexts. The shapes of the functions, however, indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning.","In the seminal work by #AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .","['In the seminal work by #AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', 'Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'This experiment was again replicated by Resnik (1995) with 10 subjects.', 'Table 1']",0,"['In the seminal work by #AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .']"
CC1500,W06-1104,Automatically creating datasets for measures of semantic relatedness,using measures of semantic relatedness for word sense disambiguation,"['Siddharth Patwardhan', 'Satanjeev Banerjee', 'Ted Pedersen']",,This paper generalizes the Adapted Lesk Algorithm of Banerjee and Pedersen (2002) to a method of word sense disambiguation based on semantic relatedness. This is possible since Lesk&apos;s original algorithm (1986) is based on gloss overlaps which can be viewed as a measure of semantic relatedness. We evaluate a variety of measures of semantic relatedness when applied to word sense disambiguation by carrying out experiments using the English lexical sample data of Senseval-2. We find that the gloss overlaps of Adapted Lesk and the semantic distance measure of Jiang and Conrath (1997) result in the highest accuracy,"The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.","['The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.', 'But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'A certain measure may work well in one application, but not in another.', 'Application-based evaluation can only state the fact, but give little explanation about the reasons.']",0,"['The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.']"
CC1501,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing similarity,"['Ludovic Lebart', 'Martin Rajman']",,"In this paper we study similarity measures for moving curves which can, for example, model changing coastlines or retreating glacier termini. Points on a moving curve have two parameters, namely the position along the curve as well as time. We therefore focus on similarity measures for surfaces, specifically the Fr 'echet distance between surfaces. While the Fr 'echet distance between surfaces is not even known to be computable, we show for variants arising in the context of moving curves that they are polynomial-time solvable or NP-complete depending on the restrictions imposed on how the moving curves are matched. We achieve the polynomial-time solutions by a novel approach for computing a surface in the so-called free-space diagram based on max-flow min-cut duality","#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .","['The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g.', 'word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006).', '#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .', 'But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'A certain measure may work well in one application, but not in another.', 'Application-based evaluation can only state the fact, but give little explanation about the reasons.']",0,"['#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .']"
CC1502,W06-1104,Automatically creating datasets for measures of semantic relatedness,automatic text processing the transformation analysis and retrieval of information by computer,['Gerard Salton'],experiments,,The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .,"['The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (Schmid, 1995).', ""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .""]",5,"['The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (Schmid, 1995).', ""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .""]"
CC1503,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing semantic relatedness across parts of speech,['Iryna Gurevych'],experiments,,#AUTHOR_TAG reported a correlation of r = .69 .,"['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', 'The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', 'Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', '#AUTHOR_TAG reported a correlation of r = .69 .', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']",1,"['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', 'The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG reported a correlation of r = .69 .', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']"
CC1504,W06-1104,Automatically creating datasets for measures of semantic relatedness,using the structure of a conceptual network in computing semantic relatedness,['Iryna Gurevych'],experiments,"Abstract. We present a new method for computing semantic relatedness of concepts. The method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis. The network structure is employed to generate artificial conceptual glosses. They replace textual definitions proper written by humans and are processed by a dictionary based metric of semantic relatedness [1]. We implemented the metric on the basis of GermaNet, the German counterpart of WordNet, and evaluated the results on a German dataset of 57 word pairs rated by human subjects for their semantic relatedness. Our approach can be easily applied to compute semantic relatedness based on alternative conceptual networks, e.g. in the domain of life sciences.","As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG ) .","['GermaNet contains only a few conceptual glosses.', 'As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG ) .', 'We removed words which had more than three senses.']",5,"['GermaNet contains only a few conceptual glosses.', 'As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG ) .', 'We removed words which had more than three senses.']"
CC1505,W06-1104,Automatically creating datasets for measures of semantic relatedness,nonclassical lexical semantic relations,"['Jane Morris', 'Graeme Hirst']",related work,,#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .,"['Furthermore, manually selected word pairs are often biased towards highly related pairs (Gurevych, 2006), because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .']",0,['#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .']
CC1506,W06-1104,Automatically creating datasets for measures of semantic relatedness,placing search in context the concept revisited,"['Lev Finkelstein', 'Evgeniy Gabrilovich', 'Yossi Matias', 'Ehud Rivlin', 'Zach Solan', 'Gadi Wolfman']",related work,"Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (""the context""). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web.","#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .","['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']",0,"['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']"
CC1507,W06-1104,Automatically creating datasets for measures of semantic relatedness,probabilistic partofspeech tagging using decision trees,['Helmut Schmid'],experiments,"In this paper, a new probabilistic tagging method is presented which avoids problems that Markov Model based taggers face, when they have to estimate transition probabilities from sparse data. In this tagging method, transition probabilities are estimated using a decision tree. Based on this method, a part-of-speech tagger (called TreeTagger) has been implemented which achieves 96.36 % accuracy on Penn-Treebank data which is better than that of a trigram tagger (96.06 %) on the same data.","The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG ) .","['The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG ) .', ""The resulting list of POS-tagged lemmas is weighted using the SMART 'ltc' 8 tf.idf-weighting scheme (Salton, 1989).""]",5,"['The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG ) .']"
CC1508,W06-1104,Automatically creating datasets for measures of semantic relatedness,verb semantics and lexical selection,"['Zhibiao Wu', 'Martha Palmer']",,"This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT). Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentence as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection.Comment: 6 pages, Figures and bib files are in part","dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']",0,"['dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']"
CC1509,W06-1104,Automatically creating datasets for measures of semantic relatedness,using information content to evaluate semantic similarity,['Philip Resnik'],,"Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments.","dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']",0,"['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']"
CC1510,W06-1104,Automatically creating datasets for measures of semantic relatedness,using information content to evaluate semantic similarity,['Michael Lesk'],,"Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments.","dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']",0,"['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.']"
CC1511,W06-1104,Automatically creating datasets for measures of semantic relatedness,automatic generation of a coarse grained wordnet,"['Rada Mihalcea', 'Dan Moldovan']",experiments,,"If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6","['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.', 'To limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'Words with a number of senses above the threshold are removed from the list.']",0,"['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.']"
CC1512,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing semantic relatedness across parts of speech,['Iryna Gurevych'],related work,,"Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .","['We assume that due to lexical-semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'Resulting from our corpus-based approach, test sets will also contain domain-specific terms.', 'Previous studies only included general terms as opposed to domain-specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain-specific or technical terms.', 'This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car).', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', 'Morris and Hirst (2004) pointed out that many relations between words in a text are non-classical (i.e.', 'other than typical taxonomic relations like synonymy or hypernymy) and therefore not covered by semantic similarity.']",0,"['We assume that due to lexical-semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'Resulting from our corpus-based approach, test sets will also contain domain-specific terms.', 'This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car).', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', 'Morris and Hirst (2004) pointed out that many relations between words in a text are non-classical (i.e.', 'other than typical taxonomic relations like synonymy or hypernymy) and therefore not covered by semantic similarity.']"
CC1513,W06-1104,Automatically creating datasets for measures of semantic relatedness,placing search in context the concept revisited,"['Lev Finkelstein', 'Evgeniy Gabrilovich', 'Yossi Matias', 'Ehud Rivlin', 'Zach Solan', 'Gadi Wolfman']",experiments,"Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (""the context""). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web.",#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .,"['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', 'The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']",1,"['This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', 'The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']"
CC1514,W06-1104,Automatically creating datasets for measures of semantic relatedness,using information content to evaluate semantic similarity,['Philip Resnik'],experiments,"Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments.","#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .","['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .', 'Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']",1,"['#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .']"
CC1515,W06-1104,Automatically creating datasets for measures of semantic relatedness,using the structure of a conceptual network in computing semantic relatedness,['Iryna Gurevych'],related work,"Abstract. We present a new method for computing semantic relatedness of concepts. The method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis. The network structure is employed to generate artificial conceptual glosses. They replace textual definitions proper written by humans and are processed by a dictionary based metric of semantic relatedness [1]. We implemented the metric on the basis of GermaNet, the German counterpart of WordNet, and evaluated the results on a German dataset of 57 word pairs rated by human subjects for their semantic relatedness. Our approach can be easily applied to compute semantic relatedness based on alternative conceptual networks, e.g. in the domain of life sciences.",#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .,"['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']",0,"['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'Therefore, we automatically create word pairs using a corpus-based approach.']"
CC1516,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing semantic relatedness across parts of speech,['Iryna Gurevych'],experiments,,"Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .","['Correlation coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations (like cooccurrence of words) and is thus a more complicated task.', 'Judgments for such relations strongly depend on experience and cultural background of the test subjects.', 'While most people may agree 10 Note that Resnik used the averaged correlation coefficient.', 'We computed the summarized correlation coefficient using a Fisher Z-value transformation.', 'that (car -vehicle) are highly related, a strong connection between (parts -speech) may only be established by a certain group.', 'Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']",1,"['Correlation coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations (like cooccurrence of words) and is thus a more complicated task.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']"
CC1517,W06-1104,Automatically creating datasets for measures of semantic relatedness,semantic similarity based on corpus statistics and lexical taxonomy,"['Jay J Jiang', 'David W Conrath']",,"This paper presents a new approach for measuring semantic similarity/distance between words and concepts. It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data. Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation. When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models. It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task.","Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']",0,"['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .']"
CC1518,W06-1705,Annotated web as corpus,facilitating the compilation and dissemination of adhoc web corpora,['W H Fletcher'],method,"Since the World Wide Web gained prominence in the mid-1990s it has tantalized language investigators and ins ructors as a virtually unlimited source of machine-readable texts for compiling corpora and developing teaching materials. The broad range of languages and content domains found online also offers translators enormous promise both for translation by-example and as a comprehensive supplement to published reference works This paper surveys the impediments which s ill prevent the Web from realizing its full potential as a linguistic resource and discusses tools to overcome the remaining hurdles. Identifying online documents which are both relevant and reliable presents a major challenge. As a partial solution the author's Web concordancer KWiCFinder au omates the process of seeking and retrieving webpages Enhancements which permit more focused queries than existing search engines and provide search results in an interactive explora ory environmen are described in detail. Despite the efficiency of automated downloading and excerpting, selecting Web documents still entails significant time and effort. To multiply the benefits of a search, an online forum for sharing annotated search reports and linguistically interesting texts with other users is outlined. Furthermore, the orien ation of commercial sea ch engines toward the general public makes them less beneficial for linguistic research. The author sketches plans for a specialized Search Engine for Applied Linguis s and a selective Web Corpus Archive which build on his experience with KWiCFinder. He compares his available and proposed solutions to existing resou ces, and su veys ways to exploi them in language teaching. Together these proposed services will enable language learners and professionals to tap into the Web effectively and efficiently for instruction research and translation. t","Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) .","['• Site based corpus annotation -in which the user can specify a web site to annotate • Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate • Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger).', 'Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) .']",3,"['* Site based corpus annotation -in which the user can specify a web site to annotate * Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate * Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger).', 'Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) .']"
CC1519,W06-1705,Annotated web as corpus,linguistic search engine,['A Kilgarriff'],related work,"Users of search engines often have specific questions which they hope or believe a particular resource can answer. The problem, from the computer system's perspective, is cognitive understanding of the contents in the source and finding the desired answer. Most of the search engines, with Google on the top, able to retrieve most likely relevant information based on a query. But not capable of providing answer to a question due to lack of deduction capability. In order to find a specific answer to a question, the engine needs to understand the information content and able to do deductive reasoning. Conventional information representation models used in the search engines rely on an extensive use of keywords and their frequencies in storing and retrieving information and other characteristic data on specific body of information. It is believed that we need new approaches for the development of future search engines which will be more effective. Semantic model is an alternative to conventional approach. We have proposed logical-linguistic model where logic and linguistic formalism are used in providing mechanism for computer to understand the contents of the source and deduce answers to questions. The capability of deduction is much depended on the knowledge representation framework used. The approach applies semantic analysis in transforming and normalising information from natural language texts into a declarative knowledge based representation of first order predicate logic. Retrieval of relevant information can then be performed through plausible logical implication and answer to query is carried out using a theorem proving technique. This paper elaborates on the model and how it is used in search engine and question answering system as one unified model","Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 ) .""]",0,"[""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 ) .""]"
CC1520,W06-1705,Annotated web as corpus,introduction to the special issue on the web as corpus,"['A Kilgarriff', 'G Grefenstette']",introduction,"The Web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is a fabulous linguists' playground. This special issue of Computational Linguistics explores ways in which this dream is being explored.","In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']",0,"['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).']"
CC1521,W06-1705,Annotated web as corpus,supporting text mining for escience the challenges for gridenabled natural language processing,"['J Carroll', 'R Evans', 'E Klein']",related work,"Over the last few years, language technology has moved rapidly from 'applied research' to 'engineering', and from small-scale to large-scale engineering. Applications such as advanced text mining systems are feasible, but very resource-intensive, while research seeking to address the underlying language processing questions faces very real practical and methodological limitations. The e-Science vision, and the creation of the e-Science Grid, promises the level of integrated large-scale technological support required to sustain this important and successful new technology area. In this paper, we discuss the foundations for the deployment of text mining and other language technology on the Grid - the protocols and tools required to build distributed large-scale language technology systems, meeting the needs of users, application builders and researchers.","In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 ) .","['In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 ) .', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.', 'In simple terms, P2P is a technology that takes advantage of the resources and services available at the edge of the Internet (Shirky, 2001).', 'Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems.', 'Examples include SETI@home (looking for radio evidence of extraterrestrial life), ClimatePrediction.net (studying climate change), Predictor@home (investigating protein-related diseases) and Einstein@home (searching for gravitational signals).']",0,"['In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 ) .']"
CC1522,W06-1705,Annotated web as corpus,the linguists search engine getting started guide,"['P Resnik', 'A Elkiss']",related work,,"Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .""]",0,"[""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .""]"
CC1523,W06-1705,Annotated web as corpus,facilitating the compilation and dissemination of adhoc web corpora,['W H Fletcher'],related work,"Since the World Wide Web gained prominence in the mid-1990s it has tantalized language investigators and ins ructors as a virtually unlimited source of machine-readable texts for compiling corpora and developing teaching materials. The broad range of languages and content domains found online also offers translators enormous promise both for translation by-example and as a comprehensive supplement to published reference works This paper surveys the impediments which s ill prevent the Web from realizing its full potential as a linguistic resource and discusses tools to overcome the remaining hurdles. Identifying online documents which are both relevant and reliable presents a major challenge. As a partial solution the author's Web concordancer KWiCFinder au omates the process of seeking and retrieving webpages Enhancements which permit more focused queries than existing search engines and provide search results in an interactive explora ory environmen are described in detail. Despite the efficiency of automated downloading and excerpting, selecting Web documents still entails significant time and effort. To multiply the benefits of a search, an online forum for sharing annotated search reports and linguistically interesting texts with other users is outlined. Furthermore, the orien ation of commercial sea ch engines toward the general public makes them less beneficial for linguistic research. The author sketches plans for a specialized Search Engine for Applied Linguis s and a selective Web Corpus Archive which build on his experience with KWiCFinder. He compares his available and proposed solutions to existing resou ces, and su veys ways to exploi them in language teaching. Together these proposed services will enable language learners and professionals to tap into the Web effectively and efficiently for instruction research and translation. t","Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( #AUTHOR_TAGa ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( #AUTHOR_TAGa ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) .""]",0,"['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( #AUTHOR_TAGa ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) .""]"
CC1524,W06-1705,Annotated web as corpus,concordancing the web with kwicfinder third north american,['W H Fletcher'],related work,,"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]",0,"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]"
CC1525,W06-1705,Annotated web as corpus,web googles missing pages mystery solved httpaixtalblogspotcom200502webgooglesmissingpagesmysteryhtml accessed,['J Veronis'],related work,,Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ) .,"['A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ) .', 'Tools based on static corpora do not suffer from this problem, e.g.', 'BNCweb 7 , developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) are both based on the British National Corpus.', 'Both BNCweb and View enable access to annotated corpora and facilitate searching on part-ofspeech tags.', 'In addition, PIE 9 (Phrases in English), developed at USNA, which performs searches on n-grams (based on words, parts-ofspeech and characters), is currently restricted to the British National Corpus as well, although other static corpora are being added to its database.', 'In contrast, little progress has been made toward annotating sizable sample corpora from the web.']",0,['Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ) .']
CC1526,W06-1705,Annotated web as corpus,google as a corpus tool in,['T Robb'],related work,,"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two ap- proaches e.g. automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS4, Amal- gam5, Connexor6) are available, for example, in the application of part-of-speech tags to corpora.', 'Existing tagging systems are �small scale� and typically impose some limitation to prevent over- load (e.g. restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin- gle-server systems.', 'This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', 'Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist�s Search En- gine (Kilgarriff, 2003; Resnik and Elkiss, 2003).']",0,"['This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.']"
CC1527,W06-1705,Annotated web as corpus,automatic profiling of learner texts,"['S Granger', 'P Rayson']",introduction,"In this chapter Crystal's (1991) notion of 'profiling', i.e. the identification of the most salient features in a particular person (clinical linguistics) or register (stylistics), is applied to the field of interlanguage studies. Starting from the assumption that every interlanguage is characterized by a 'unique matrix of frequencies of various linguistic forms' (Krzeszowski 1990: 212), we have submitted two similar-sized corpora of native and non-native writing to a lexical frequency software program to uncover some of the distinguishing features of learner writing. The non-native speaker corpus is taken from the International Corpus of Learner English (ICLE) database. It consists of argumentative essay writing by advanced French-speaking learners of English. The control corpus of similar writing is taken from the Louvain Corpus of Native English Essays (LOCNESS) database. Though limited to one specific type of interlanguage, the approach presented here is applicable to any learner variety and demonstrates a potential of automatic profiling for revealing the stylistic characteristics of EFL texts. In the present study, the learner data is shown to display many of the stylistic features of spoken, rather than written, English.","In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ) .","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']",0,"['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ) .']"
CC1528,W06-1705,Annotated web as corpus,an introduction to corpus linguistics,['G Kennedy'],introduction,"On first looking into a corpus, teachers as well as students may well be blinded by the sheer scale of the resource and by the possibilities for research that it offers. As the author of this book puts it, ""The research topics in a machine-readable cor pus are potentially as various and wide ranging as are the facts about a language and the use of that language"" (274). The value of this book is that it provides practical examples of the range of research possibilities that a corpus offers, as well as indi cating how corpus-based research projects may be undertaken. It is informed throughout by the view that corpus linguistics is not a separate branch of linguistics, but rather ""descriptive linguistics aided by new technology"" (268). There is some theoretical discussion of the place of corpus linguistics in the wider field, but in general, the author's approach is to let the results speak for themselves. The author's declared aim is to whet the appetites of teachers and students, and in this he clearly succeeds. In the ""Introduction,"" the author suggests that some readers might usefully begin with chapter 3, ""Corpus-Based Descriptions of English."" This is the central part of the book and by far the most valuable in terms of whetting the appetite. It consists of a very comprehensive and wide-ranging review of previous corpus-based research, divided into the following sections: lexical description, grammatical studies cen tered on morphemes or words, grammatical studies centered on the sentence, pragmatics and spoken discourse, and studies of variation. The first section of the chapter investigates how computerized corpora are increasingly being used in lexi cography and continues with a review of collocational studies based on the LOB corpus, as well as work by Sinclair and Renouf on collocational frameworks. Under ""word-centered"" grammatical studies, previous work on modals, voice, aspect, the subjunctive, as well as prepositions and conjunctions are very comprehensively ex emplified and described. The list continues. The latter sections of this chapter re view work by Kuc*era and Francis on sentence length, Altenberg on verb complementation, Mair on nonfinite complementation, Meyer on apposition,","In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']",0,"['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .']"
CC1529,W06-1705,Annotated web as corpus,distributed video encoding over a peertopeer network,"['D Hughes', 'J Walkerdine']",method,"How does the work advance the state-of-the-art?: Current video encoding technologies tend to focus on single machine solutions, while little or no work on distributed video encoding systems has been undertaken. Current work on distributed computation over peer-to-peer networks primarily focuses upon systems with heavily centralised control [1] [2]. The Distributed Video Encoder is a novel example of fully decentralized ad-hoc distributed computation.","This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) .","['The second stage of our work will involve im- plementing the framework within a P2P environment.', ""We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API)."", 'We have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) .', 'It is our intention to implement our distributed corpus annotation framework as a plug- in.', 'This will involve implementing new functionality and integrating this with our existing annotation tools (such as CLAWS11).', 'The development environment is also flexible enough to utilise the BOINC platform, and such support will be built into it.']",0,"['This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) .']"
CC1530,W06-1705,Annotated web as corpus,listening to napster in peertopeer harnessing the power of disruptive technologies,['C Shirky'],related work,,"In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG ) .","['In the areas of Natural Language Processing (NLP) and computational linguistics, proposals have been made for using the computational Grid for data-intensive NLP and text-mining for e-Science (Carroll et al., 2005;Hughes et al, 2004).', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.', 'In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG ) .', 'Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems.', 'Examples include SETI@home (looking for radio evidence of extraterrestrial life), ClimatePrediction.net (studying climate change), Predictor@home (investigating protein-related diseases) and Einstein@home (searching for gravitational signals).']",0,"['In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG ) .']"
CC1531,W06-1705,Annotated web as corpus,p2p4dl digital library over peertopeer,"['J Walkerdine', 'P Rayson']",method,,"This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ) .","['The second stage of our work will involve implementing the framework within a P2P environment.', ""We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API)."", 'We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ) .', 'It is our intention to implement our distributed corpus annotation framework as a plugin.', 'This will involve implementing new functionality and integrating this with our existing annotation tools (such as CLAWS 11 ).', 'The development environment is also flexible enough to utilise the BOINC platform, and such support will be built into it.']",0,"['The second stage of our work will involve implementing the framework within a P2P environment.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ) .']"
CC1532,W06-1705,Annotated web as corpus,the biggest corpus of allquot,['M Rundell'],related work,,"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]",0,"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.']"
CC1533,W06-1705,Annotated web as corpus,introduction to the special issue on the web as corpus,"['A Kilgarriff', 'G Grefenstette']",related work,"The Web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is a fabulous linguists' playground. This special issue of Computational Linguistics explores ways in which this dream is being explored.","The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]",0,"['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG ) .', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.']"
CC1534,W06-1705,Annotated web as corpus,bootcat bootstrapping corpora and terms from the web in,"['M Baroni', 'S Bernardini']",related work,"This paper introduces the BootCaT toolkit, a suite of perl programs implementing an iterative procedure to bootstrap specialized corpora and terms from the web. The procedure requires only a small set of seed terms as input. The seeds are used to build a corpus via automated Google queries, and more terms are extracted from this corpus. In turn, these new terms are used as seeds to build a larger corpus via automated queries, and so forth. The corpus and the unigram terms are then used to extract multi-word terms. We conducted an evaluation of the tools by applying them to the construction of English and Italian corpora and term lists from the domain of psychiatry. The results illustrate the potential usefulness of the tools.",#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .,"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]",0,"['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', '#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .']"
CC1535,W06-1705,Annotated web as corpus,creating specialized and general corpora using automated search engine queries web as corpus workshop,"['M Baroni', 'S Sharoff']",introduction,,"Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG ) .","['A key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG ) .']",0,"['A key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG ) .']"
CC1536,W06-1705,Annotated web as corpus,using the web to overcome data sparseness,"['F Keller', 'M Lapata', 'O Ourioupina']",introduction,"This paper shows that the web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the web by querying a search engine. We evaluate this method by demonstrating that web frequencies and correlate with frequencies obtained from a carefully edited, balanced corpus. We also perform a task-based evaluation, showing that web frequencies can reliably predict human plausibility judgments.","Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG ) .","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG ) .', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']",0,"['Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG ) .']"
CC1537,W06-1705,Annotated web as corpus,finding syntactic structure in unparsed corpora the gsearch corpus query system computers and the humanities,"['S Corley', 'M Corley', 'F Keller', 'M Crocker', 'S Trewin']",related work,,The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .,"['""Real-time"" linguistic analysis of web data at the syntactic level has been piloted by the Linguist\'s Search Engine (LSE).', 'Using this tool, linguists can either perform syntactic searches via parse trees on a pre-analysed web collection of around three million sentences from the Internet Archive (www.archive.org)', 'or build their own collections from AltaVista search engine results.', 'The second method pushes the new collection onto a queue for the LSE annotator to analyse.', 'A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server.', 'The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .', 'Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up.', 'In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora.', ""A word sketch is an automatic one-page corpus-derived summary of a word's grammatical and collocational behaviour."", 'Word Sketches were first used to prepare the Macmillan English Dictionary for Advanced Learners (2002, edited by Michael Rundell).', 'They have also served as the starting point for high-accuracy Word Sense Disambiguation.', 'More recently, the Sketch Engine was used to develop the new edition of the Oxford Thesaurus of English (2004, edited by Maurice Waite).']",0,"['Using this tool, linguists can either perform syntactic searches via parse trees on a pre-analysed web collection of around three million sentences from the Internet Archive (www.archive.org)', 'The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .', 'Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up.']"
CC1538,W06-1705,Annotated web as corpus,introduction to the special issue on evaluating word sense disambiguation systems,"['P Edmonds', 'A Kilgarriff']",introduction,"Has system performance on Word Sense Disambiguation (WSD) reached a limit? Automatic systems don't perform nearly as well as humans on the task, and from the results of the SENSEVAL exercises, recent improvements in system performance appear negligible or even negative. Still, systems do perform much better than the baselines, so something is being done right. System evaluation is crucial to explain these results and to show the way forward. Indeed, the success of any project in WSD is tied to the evaluation methodology used, and especially to the formalization of the task that the systems perform. The evaluation of WSD has turned out to be as difficult as designing the systems in the first place.","Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .","['Linguistic annotation of corpora contributes crucially to the study of language at several levels: morphology, syntax, semantics, and discourse.', 'Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .']",0,"['Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .']"
CC1539,W06-1705,Annotated web as corpus,web as corpus,['A Kilgarriff'],related work,,"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g. automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS4, Amal- gam5, Connexor6) are available, for example, in the application of part-of-speech tags to corpora.', 'Existing tagging systems are �small scale� and typically impose some limitation to prevent over- load (e.g. restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', 'Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist�s Search En- gine (Kilgarriff, 2003; Resnik and Elkiss, 2003).']",0,"['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.']"
CC1540,W06-1705,Annotated web as corpus,word sense disambiguation by web mining for word cooccurrence probabilities,['P Turney'],related work,"This paper describes the National Research Council (NRC) Word Sense Disambiguation (WSD) system, as applied to the English Lexical Sample (ELS) task in Senseval-3. The NRC system approaches WSD as a classical supervised machine learning problem, using familiar tools such as the Weka machine learning software and Brill's rule-based part-of-speech tagger. Head words are represented as feature vectors with several hundred features. Approximately half of the features are syntactic and the other half are semantic. The main novelty in the system is the method for generating the semantic features, based on word co-occurrence probabilities. The probabilities are estimated using the Waterloo MultiText System with a corpus of about one terabyte of unlabeled text, collected by a web crawler.",#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .,"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', '#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]",0,['#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .']
CC1541,W06-1705,Annotated web as corpus,facilitating the compilation and dissemination of adhoc web corpora,['W H Fletcher'],,"Since the World Wide Web gained prominence in the mid-1990s it has tantalized language investigators and ins ructors as a virtually unlimited source of machine-readable texts for compiling corpora and developing teaching materials. The broad range of languages and content domains found online also offers translators enormous promise both for translation by-example and as a comprehensive supplement to published reference works This paper surveys the impediments which s ill prevent the Web from realizing its full potential as a linguistic resource and discusses tools to overcome the remaining hurdles. Identifying online documents which are both relevant and reliable presents a major challenge. As a partial solution the author's Web concordancer KWiCFinder au omates the process of seeking and retrieving webpages Enhancements which permit more focused queries than existing search engines and provide search results in an interactive explora ory environmen are described in detail. Despite the efficiency of automated downloading and excerpting, selecting Web documents still entails significant time and effort. To multiply the benefits of a search, an online forum for sharing annotated search reports and linguistically interesting texts with other users is outlined. Furthermore, the orien ation of commercial sea ch engines toward the general public makes them less beneficial for linguistic research. The author sketches plans for a specialized Search Engine for Applied Linguis s and a selective Web Corpus Archive which build on his experience with KWiCFinder. He compares his available and proposed solutions to existing resou ces, and su veys ways to exploi them in language teaching. Together these proposed services will enable language learners and professionals to tap into the Web effectively and efficiently for instruction research and translation. t","Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAGa ) .","['We will explore techniques to make the resulting annotated web corpus data available in static form to enable replication and verification of corpus studies based on such data.', 'The initial solution will be to store the resulting reference 11 http://www.comp.lancs.ac.uk/ucrel/claws/ corpus in the Sketch Engine.', 'We will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web-based corpus studies based in general.', 'Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAGa ) .', 'Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here.']",0,"['We will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web-based corpus studies based in general.', 'Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAGa ) .']"
CC1542,W06-1705,Annotated web as corpus,the corpusbased study of language change in progress the extra value of tagged corpora,['C Mair'],introduction,,"In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']",0,"['In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .']"
CC1543,W06-1705,Annotated web as corpus,blueprint for a high performance nlp infrastructure,['J R Curran'],related work,"Natural Language Processing (NLP) system de-velopers face a number of new challenges. In-terest is increasing for real-world systems that use NLP tools and techniques. The quantity of text now available for training and processing is increasing dramatically. Also, the range of languages and tasks being researched contin-ues to grow rapidly. Thus it is an ideal time to consider the development of new experimen-tal frameworks. We describe the requirements, initial design and exploratory implementation of a high performance NLP infrastructure.",#AUTHOR_TAG,['#AUTHOR_TAG'],0,['#AUTHOR_TAG']
CC1544,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,free culture how big media uses technology and the law to lock down culture and control creativity,['Lawrence Lessig'],,"espanolEl presente texto publicado en la Zona REMIX de la revista Communiars se corresponde con la introduccion del afamado libro Cultura Libre, del profesor Lawrence Lessig, presidente de la organizacion Creative Commons, dedicada a promover el acceso e intercambio culturales. El texto > (en espanol Cultura Libre. Como los grandes medios usan la tecnologia y la ley para bloquear la cultura y controlar la creatividad) es un libro publicado en 2004 y centrado en presentar otra manera de organizar la cultura y el conocimiento, abriendo las restricciones del obsoleto paradigma del copyright, y apoyandose en el modelo copyleft promovido desde el software libre. La introduccion que aqui se presenta traduce el espiritu abierto de un texto clave para la comprension y evolucion de la actualidad cultural. La version que se publica procede la version PDF de Free Culture, licenciada bajo Creative Commons en su variante BY-NC 1.0. EnglishThe present text published in the REMIX Zone of the Communiars Journal corresponds to the introduction of the famous book Free Culture, by Professor Lawrence Lessig, president of the Creative Commons organization, dedicated to promoting cultural access and exchange. The text Free Culture. How big media uses technology and the law to lock down culture and control creativity is a book published in 2004 and focused on presenting another way of organizing culture and knowledge, opening the restrictions of the obsolete paradigm of copyright, and relying on the copyleft model promoted by free software. The introduction presented here translates the open spirit of a key text for the understanding and evolution of current cultural reality. The version that is published is part of PDF version of Free Culture, licensed under Creative Commons in its variant BY-NC 1.0.",We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ) .,"['We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ) .', 'Narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (Stallman, 2001).']",5,['We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ) .']
CC1545,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,hypertext 20 the convergence of contemporary critical theory and technology the johns hopkins,['George P Landow'],related work,,Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ) .,"['Apart from wikis, blogs, and cognitive mapping, we were also inspired by the experiences of early hypertext writing tools, in particular Intermedia and Storyspace.', 'In fact, they were used expecially in academic writing with some success.', 'Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ) .', 'Storyspace is currently distributed by Eastgate ( 2005), and we have used it for a time.', ""However, in our opinion Storyspace is a product of its time and in fact it isn't a web application."", 'Although it is possible to label links, it lacks a lot of features we need.', 'Moreover, no hypertext writing tool available is released under an open source licence.', 'We hope that Novelle will bridge this gap -we will choose the exact licence when our first public release is ready.']",0,"['Apart from wikis, blogs, and cognitive mapping, we were also inspired by the experiences of early hypertext writing tools, in particular Intermedia and Storyspace.', 'In fact, they were used expecially in academic writing with some success.', 'Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ) .', 'Storyspace is currently distributed by Eastgate ( 2005), and we have used it for a time.', ""However, in our opinion Storyspace is a product of its time and in fact it isn't a web application."", 'Moreover, no hypertext writing tool available is released under an open source licence.']"
CC1546,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,hypertext 20 the convergence of contemporary critical theory and technology the johns hopkins,['George P Landow'],introduction,,"Following the example of #AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by Roland Barthes ( 1970 ) .","[""Following the example of #AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by Roland Barthes ( 1970 ) ."", 'Consequently, a hypertext is a set of lexias.', 'In hypertexts transitions from one lexia to another are not necessarily sequential, but navigational.', 'The main problems of hypertexts, acknowledged since the beginning, have been traced as follows (Nelson, 1992):']",5,"[""Following the example of #AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by Roland Barthes ( 1970 ) ."", 'Consequently, a hypertext is a set of lexias.', 'The main problems of hypertexts, acknowledged since the beginning, have been traced as follows (Nelson, 1992):']"
CC1547,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,understanding comics,['Scott McCloud'],introduction,"During the spring semester of 2010, as part of my graduate program in English Education, I took a class titled American Comic Book. I took what I learned there, turned around, and immediately applied it to my own teaching. I teach 7th grade language arts and developed a unit on understanding and creating comics, pulling from what I was learning in the class at the University of Iowa, and utilized some other resources including Great Source u27s Daybook of Critical Reading and Writing, and ideas from other books on using graphic novels as a teaching tool. The unit was taught during April and May of this year. I have collected my lesson plans, examples of student work, and much, much more on a website, http://sites.google.com/site/7thgradecomicsunit/","For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) .","['1.1 Hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e."", 'literary criticism -unlike in the previous times (Eisenstein, 1983).', 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', ""For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) ."", 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962)."", 'From a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis (McNeill, 2005) emphasize annotation, comment, and strong editing.', 'They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.', 'This situation could make new problems rise up: Who owns the text?', 'Which role is suitable for authors?', 'We have to analyse them before presenting the architecture of Novelle.']",0,"[""For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e."", ""For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) ."", 'This situation could make new problems rise up: Who owns the text?']"
CC1548,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,free software free society selected essays of,['Richard M Stallman'],,"Please contact the GNU Press for information regarding bulk purchases for classroom or user group use, reselling, or any other questions or comments. Permission is granted to make and distribute verbatim copies of this book provided the copyright notice and this permission notice are preserved on all copies. Permission is granted to copy and distribute translations of this book into another language, from the original English, with respect to the conditions on distribution of modified versions above, provided that it has been approved by the Free Software Foundation. The waning days of the 20th century seemed like an Orwellian nightmare: laws preventing publication of scientific research on software; laws preventing sharing software; an overabundance of software patents preventing development; and end-user license agreements that strip the user of all freedoms--including ownership, privacy, sharing, and understanding how their software works. This collection of essays and speeches by Richard M. Stallman addresses many of these issues. Above all, Stallman discusses the philosophy underlying the free software movement. This movement combats the oppression of federal laws and evil end-user license agreements in hopes of spreading the idea of software freedom. With the force of hundreds of thousands of developers working to create GNU software and the GNU/Linux operating system, free software has secured a spot on the servers that control the Internet, and--as it moves into the desktop computer market--is a threat to Microsoft and other proprietary software companies. These essays cater to a wide audience; you do not need a computer science background to understand the philosophy and ideas herein. However, there is a "" Note on Software, "" to help the less technically inclined reader become familiar with some common computer science jargon and concepts, as well as footnotes throughout. Many of these essays have been updated and revised from their originally published version. Each essay carries permission to redistribute verbatim copies. The ordering of the essays is fairly arbitrary, in that there is no required order to read the essays in, for they were written independently of each other over a period of 18 years. The first section, "" The GNU Project and Free Software, "" is intended to familiarize you with the history and philosophy of free software and the GNU project. Furthermore, it provides a road map for developers, educators, and business people to pragmatically incorporate free software into society, business, and life. The second section, "" Copyright, ...","Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG ) .","['We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve (Lessig, 2004).', 'Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG ) .']",0,"['Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG ) .']"
CC1549,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,wikipedia from wikipedia the free encyclopedia,['Wikipedia'],,"administrator, born in Campinas, Brazil. He holds a doctoral degree in physiology from the University of Sao Paulo. His main contributions as a researcher and publisher concern the following areas[1]: neuroethology; medical informatics; health sciences; internet and web applications in medicine, biology and health; ; artificial neural networks; distance education and e-learning; telemedicine; biological and health impacts of non-ionizing radiation and history of neuroscience. He is currently the president of the Edumed Institute, a non-profit R&amp;D institution and director of Edulogica Educacao &amp; Tecnologia, a consultant and commercial enterprise specializing in distance education. artificial intelligenc Research and education Sabbatini began his scientific career in neurophysiology in 1966, while he was a medical student at the Medical School of the University of Sao Paulo at Ribeirao Preto [2]. He began to work in basic biomedical research under the supervision of Prof. Miguel Rolando Covian, an Argentine neurophysiologist, who encouraged him to found, after he graduated in 1968, the first research laboratory of neuroethology in Latin America [3] , at the Department of Physiology. Also there, he started in 1970 one of the first Brazilian and Latin American groups of research, development and education on the computer applications in biomedicine. Sabbatini got a doctorate in behavioral neuroscience in 1977 and immediately thereafter went to spend two and a half years doing postdoctoral work i","On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #AUTHOR_TAG .","['The emphasis on narrativity takes into account the use of blogs as public diaries on the web, that is still the main current interpretation of this literary genre, or metagenre (McNeill, 2005).', 'Furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'So blogs are a literary metagenre which started as authored personal diaries or journals.', ""Now they try to collect themselves in so-called 'blogspheres'."", 'On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #AUTHOR_TAG .', 'Wikipedia (2005).', 'Now personal wiki tools are arising for brainstorming and mind mapping.', 'See Section 4 for further aspects.']",0,"['Furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #AUTHOR_TAG .', 'Wikipedia (2005).', 'Now personal wiki tools are arising for brainstorming and mind mapping.', 'See Section 4 for further aspects.']"
CC1550,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,free culture how big media uses technology and the law to lock down culture and control creativity,['Lawrence Lessig'],,"espanolEl presente texto publicado en la Zona REMIX de la revista Communiars se corresponde con la introduccion del afamado libro Cultura Libre, del profesor Lawrence Lessig, presidente de la organizacion Creative Commons, dedicada a promover el acceso e intercambio culturales. El texto > (en espanol Cultura Libre. Como los grandes medios usan la tecnologia y la ley para bloquear la cultura y controlar la creatividad) es un libro publicado en 2004 y centrado en presentar otra manera de organizar la cultura y el conocimiento, abriendo las restricciones del obsoleto paradigma del copyright, y apoyandose en el modelo copyleft promovido desde el software libre. La introduccion que aqui se presenta traduce el espiritu abierto de un texto clave para la comprension y evolucion de la actualidad cultural. La version que se publica procede la version PDF de Free Culture, licenciada bajo Creative Commons en su variante BY-NC 1.0. EnglishThe present text published in the REMIX Zone of the Communiars Journal corresponds to the introduction of the famous book Free Culture, by Professor Lawrence Lessig, president of the Creative Commons organization, dedicated to promoting cultural access and exchange. The text Free Culture. How big media uses technology and the law to lock down culture and control creativity is a book published in 2004 and focused on presenting another way of organizing culture and knowledge, opening the restrictions of the obsolete paradigm of copyright, and relying on the copyleft model promoted by free software. The introduction presented here translates the open spirit of a key text for the understanding and evolution of current cultural reality. The version that is published is part of PDF version of Free Culture, licensed under Creative Commons in its variant BY-NC 1.0.",Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .,"['If a user lets others edit some lexias, he has the right to retain or refuse the attribution when other users have edited it.', 'In the first instance, the edited version simply moves ahead the document history.', 'In the second one, the last user, who has edited the lexia, may claim the attribution for himself.', 'The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2).', 'Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .', 'If nobody claims the document for himself, it will fall in the public domain.', 'The set of lexias in the public domain will form a special document, owned by a special user, called Public Domain.', ""If the author refuses the permission to create derivative works, i.e. to edit his own lexias, users still have the right to comment the author's work."", 'So as to come to terms with this idea, we need a concept invented by Nelson (1992), i.e. transclusion.', ""Rather than copy-and-paste contents from a lexia, a user may recall a quotation of the author's lexia and write a comment in the surroundings."", ""In doing so, the link list of the author's lexia will be updated with a special citation link marker, called quotation link (see later for details)."", ""Usually, the quotation will be 'frozen', as in the moment where it was transcluded (see Figure 3)."", 'Consequently the transclusion resembles a copiedand-pasted text chunk, but the link to the original document will always be consistent, i.e. neither it expires nor it returns an error.', 'Otherwise the user who has transcluded the quotation may choose to keep updated the links to the original document.', 'This choice has to be made when the transclusion is done.', 'If so, the transcluded quotation will update automatically, following the history timeline of the original document.', 'For example, if the original document changes topic from stars to pentagons, the quotation transcluded will change topic too (see Figure 4).']",0,['Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .']
CC1551,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,the wiki way  quick collaboration on the web,"['Ward Cunningham', 'Bo Leuf']",related work,"Foreword. Preface. Why This Book? Why You Want to Read This. Book Structure. The Authors. Contributors and Colleagues. Errata and Omissions. Contacting Us. Read the Book, Use the Wiki! I. FROM CONCEPTS TO USING WIKI. 1. Introduction to Discussion and Collaboration Servers. In this Chapter. Collaboration and Discussion Tools. Collaboration Models. Who Uses Collaborative Discussion Servers? Whatever For? Features of a Web-Based Collaboration. On the Horizon: WebDAV. Comparing Wiki to Other Collaboration Tools. 2. What's a ""Wiki""? The Wiki Concept. The Essence of Wiki. The User Experience. Usefulness Criteria. Wiki Basics. Wiki Clones. Wiki Implementations by Language. Other Wiki Offerings. Non-Wiki Servers. Wiki Application. Pros and Cons of a Wiki-Style Server. Why Consider Setting Up a Wiki? Other Issues. 3. Installing Wiki. QuickiWiki--Instant Serve. Installing Perl. Installing QuickiWiki. Multiple Instances. Wiki and Webserver. Wiki on IIS or PWS. The Apache Webserver. Installing Apache. Reconfiguring Apache. Testing Webserver Wiki. Wrapper Scripts. General Security Issues. Security and Database Integrity. Server Vulnerabilities. Addressing wiki Vulnerabilities. Configuring Your Browser Client. Fonts, Size and Layout. 4. Using Wiki. In this Chapter. Quicki Quick-Start. A Virtual Notebook. Making Wiki Notes, A Walkthrough. Wiki as PIM. A Working Example. The Content Model. Internal and External Hyperlink Models. Browsing Pages. Editing Pages. The Browser Editing Model. Building Wiki Content. Editing and Markup Conventions. 5. Structuring Wiki Content. In this Chapter. Wiki Structure. Structure Types. Only a Click Away. How Hard to Try. When to Impose Structure. When Not to Impose Structure. What is the Purpose of the Wiki? Structure Patterns. When to Spin Off New Wiki Servers. II. UNDERSTANDING THE HACKS. 6. Customizing Your Wiki. In this Chapter. Hacking Your Wiki Source. Copyright and Open Source License Policy. Why Customize? What to Customize. 7. Wiki Components Examined. In this Chapter. Dissecting QuickiWiki. QuickiWiki Component Model. Core QuickiWiki Modules. Sever Component. Optional Extended Components. Analyzing Page Content. Managing User Access. 8. Alternatives and Extensions. Parsing the Requests. ClusterWiki Component Model. The Library Module. Special Features. Spell Checking. Uploading Files. A Standard Wiki? 9. Wiki Administration and Tools. In this Chapter. Events History. Tracking Page Edits. Usage Statistics. Abuse Management. Access Management. Permission Models. Adding Authentication and Authorization. Administering the Database. Page Conversions. Page Management. Backup Issues. Server Resources and Wiki Loading. Avoiding User Waits. Implementing Wiki Constraints. Debugging a Wiki. Programming Resources. Backups. Low-Tech Debugging. Higher-Level Debugging. III. IMAGINE THE POSSIBILITIES. 10. Insights and Other Voices. In this Chapter. Wiki Culture. Wiki as Open Community. Writing Style Contention. Why Wiki Works. The Open-Edit Issue. When Wiki Doesn't Work. Public Wiki Issues. Wiki Style Guidelines. Notifying About Update. Design and Portability. Wiki Trade-Offs. Portability. The Future of Wiki. 11. Wiki Goes Edu. In this Chapter. CoWeb at Georgia Tech. Introduction to CoWeb. CoWeb Usage. Supported CoWeb User Roles. CoWeb Open Authoring Projects. Overall Conclusions. 12. Wiki at Work. In this Chapter. Case Studies. WikiWikiWeb. New York Times Digital. TWiki at TakeFive. TWiki at Motorola. Kehei Wiki Case Studies. A Rotary Wiki. Wiki Workplace Essentials. Why a Workplace Wiki? Planning the Wiki. Selection Stage. Implementation Stage. Day-to-Day Operations. Appendix A: Syntax Comparisons. Hyperlink Anchors. Markup Conventions. Escaped Blocks. HTML Tag Inclusion. Other Syntax Extensions Seen. Appendix B: Wiki Resources. Book Resources. Internet Resources. Appendix C: List of Tips. Index. 020171499XTO5232001","While wikis have spread from a detailed design ( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .","['The main source of Novelle are wikis and blogs.', 'While wikis have spread from a detailed design ( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .', 'So we have tested and compared the most used tools available for blogging: Bloggers, WordPress, MovableType and LiveJournal.']",0,"['While wikis have spread from a detailed design ( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .']"
CC1552,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,free culture how big media uses technology and the law to lock down culture and control creativity,['Lawrence Lessig'],,"espanolEl presente texto publicado en la Zona REMIX de la revista Communiars se corresponde con la introduccion del afamado libro Cultura Libre, del profesor Lawrence Lessig, presidente de la organizacion Creative Commons, dedicada a promover el acceso e intercambio culturales. El texto > (en espanol Cultura Libre. Como los grandes medios usan la tecnologia y la ley para bloquear la cultura y controlar la creatividad) es un libro publicado en 2004 y centrado en presentar otra manera de organizar la cultura y el conocimiento, abriendo las restricciones del obsoleto paradigma del copyright, y apoyandose en el modelo copyleft promovido desde el software libre. La introduccion que aqui se presenta traduce el espiritu abierto de un texto clave para la comprension y evolucion de la actualidad cultural. La version que se publica procede la version PDF de Free Culture, licenciada bajo Creative Commons en su variante BY-NC 1.0. EnglishThe present text published in the REMIX Zone of the Communiars Journal corresponds to the introduction of the famous book Free Culture, by Professor Lawrence Lessig, president of the Creative Commons organization, dedicated to promoting cultural access and exchange. The text Free Culture. How big media uses technology and the law to lock down culture and control creativity is a book published in 2004 and focused on presenting another way of organizing culture and knowledge, opening the restrictions of the obsolete paradigm of copyright, and relying on the copyleft model promoted by free software. The introduction presented here translates the open spirit of a key text for the understanding and evolution of current cultural reality. The version that is published is part of PDF version of Free Culture, licensed under Creative Commons in its variant BY-NC 1.0.","We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #AUTHOR_TAG ) .","['With our typology of links, we aim to solve the framing problem as defined in Section 1.2.', 'We want to model views as dynamic objects -the creation of context will be still arbitrary, but changes are very easily.', 'We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #AUTHOR_TAG ) .']",5,"['We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #AUTHOR_TAG ) .']"
CC1553,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,genre under construction the diary on the internet,['Laurie McNeill'],related work,"The emergence of the diary as a digital form has generated the kinds of introduction and explanation that typically accumulate around emerging genres, even though online diarists in many ways strive to reproduce the stereotypical print diary. However, as diarists and readers explore the nature of blogs, both in diary entries and comments pages, a tension is apparent between users ' accounts or explanations of the genre and their actual practices, and this tension provides a rich site for studying the evolution of the diary genre. Readers ' and writers ' comments illustrate the blogging community's ideas about genre as a concept and how these ideas transfer to the ""new "" world of online media. In this paper, I look at the diary's transition from page to screen, and consider how readers and writers build on and diverge from print culture practices in establishing expectations and ""rules "" for Weblogs. Examining how diarists and their communities establish and police the digital diary, and how generic knowledge is circulated and codified, helps understand the particular social actions the diary can perform only on the Internet. In contemporary Western culture the diary is among the most familiar genres, an enduring everyday form that has evolved and adapted across time yet has remained recognizable in each instance. As a genre that straddles and confuses ideals of the literary and non-literary","Generally speaking , we find that the personal public diary metaphor behind blogs ( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context .","['Generally speaking , we find that the personal public diary metaphor behind blogs ( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context .', ""The only way to retrieve information is through a search engine or a calendar, i.e. the date of the 'post' -a lexia in the jargon of bloggers.""]",0,"['Generally speaking , we find that the personal public diary metaphor behind blogs ( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context .']"
CC1554,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,genre under construction the diary on the internet,['Laurie McNeill'],,"The emergence of the diary as a digital form has generated the kinds of introduction and explanation that typically accumulate around emerging genres, even though online diarists in many ways strive to reproduce the stereotypical print diary. However, as diarists and readers explore the nature of blogs, both in diary entries and comments pages, a tension is apparent between users ' accounts or explanations of the genre and their actual practices, and this tension provides a rich site for studying the evolution of the diary genre. Readers ' and writers ' comments illustrate the blogging community's ideas about genre as a concept and how these ideas transfer to the ""new "" world of online media. In this paper, I look at the diary's transition from page to screen, and consider how readers and writers build on and diverge from print culture practices in establishing expectations and ""rules "" for Weblogs. Examining how diarists and their communities establish and police the digital diary, and how generic knowledge is circulated and codified, helps understand the particular social actions the diary can perform only on the Internet. In contemporary Western culture the diary is among the most familiar genres, an enduring everyday form that has evolved and adapted across time yet has remained recognizable in each instance. As a genre that straddles and confuses ideals of the literary and non-literary","The emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre ( #AUTHOR_TAG ) .","['The emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre ( #AUTHOR_TAG ) .', 'Furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'So blogs are a literary metagenre which started as authored personal diaries or journals.', ""Now they try to collect themselves in so-called 'blogspheres'."", 'On the other side, wikis started as collective works where each entry is not owned by a single author -e.g.', 'Wikipedia (2005).', 'Now personal wiki tools are arising for brainstorming and mind mapping.', 'See Section 4 for further aspects.']",0,"['The emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre ( #AUTHOR_TAG ) .', 'So blogs are a literary metagenre which started as authored personal diaries or journals.']"
CC1555,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,the printing revolution in early modern europe,['Elizabeth L Eisenstein'],introduction,"What difference did printing make? Although the importance of the advent of printing for the Western world has long been recognized, it was Elizabeth Eisenstein in her monumental, two-volume work, The Printing Press as an Agent of Change, who provided the first full-scale treatment of the subject. This illustrated and abridged edition provides a stimulating survey of the communications revolution of the fifteenth century. After summarizing the initial changes, and introducing the establishment of printing shops, it considers how printing effected three major cultural movements: the Renaissance, the Reformation, and the rise of modern science. First Edition Hb (1984) 0-521-25858-8 First Edition Pb (1984) 0-521-27735-3","For example , when books should n't be copied by hand any longer , authors took the advantage and start writing original books and evaluation -- i.e. literary criticism -- unlike in the previous times ( #AUTHOR_TAG ) .","['Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', 'Terms as �chapter�, �page� or �foot- note� simply become meaningless in the new texts, or they highly change their meaning.', 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example , when books should n't be copied by hand any longer , authors took the advantage and start writing original books and evaluation -- i.e. literary criticism -- unlike in the previous times ( #AUTHOR_TAG ) ."", 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', 'For example, a �web page� is more similar to an infinite canvas than a written page (McCloud, 2001).', 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', 'From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an �opera aperta� (open work), as Eco would define it (1962).', 'From a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis (McNeill, 2005) emphasize annotation, comment, and strong editing.', 'They give more power to readers, eventually filling the gap - the so-called active readers become authors as well.', 'This situation could make new problems rise up: Who owns the text? Which role is suitable for authors? We have to analyse them before presenting the architecture of Novelle.']",0,"['Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', 'Terms as chapter, page or foot- note simply become meaningless in the new texts, or they highly change their meaning.', 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example , when books should n't be copied by hand any longer , authors took the advantage and start writing original books and evaluation -- i.e. literary criticism -- unlike in the previous times ( #AUTHOR_TAG ) .""]"
CC1556,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,writing space the computer hypertext and the history of writing erlbaum associates,['Jay David Bolter'],introduction,Getting the books writing space the computer hypertext and the history of writing now is not type of challenging means. You could not solitary going bearing in mind books deposit or library or borrowing from your friends to gain access to them. This is an totally easy means to specifically acquire lead by on-line. This online declaration writing space the computer hypertext and the history of writing can be one of the options to accompany you gone having new time.,1.1 Hypertext as a New Writing Space #AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.,"['1.1 Hypertext as a New Writing Space #AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e."", 'literary criticism -unlike in the previous times (Eisenstein, 1983).', 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', ""For example, a 'web page' is more similar to an infinite canvas than a written page (McCloud, 2001)."", 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962)."", 'From a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis (McNeill, 2005) emphasize annotation, comment, and strong editing.', 'They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.', 'This situation could make new problems rise up: Who owns the text?', 'Which role is suitable for authors?', 'We have to analyse them before presenting the architecture of Novelle.']",0,['1.1 Hypertext as a New Writing Space #AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.']
CC1557,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,wikipedia from wikipedia the free encyclopedia,['Wikipedia'],,"administrator, born in Campinas, Brazil. He holds a doctoral degree in physiology from the University of Sao Paulo. His main contributions as a researcher and publisher concern the following areas[1]: neuroethology; medical informatics; health sciences; internet and web applications in medicine, biology and health; ; artificial neural networks; distance education and e-learning; telemedicine; biological and health impacts of non-ionizing radiation and history of neuroscience. He is currently the president of the Edumed Institute, a non-profit R&amp;D institution and director of Edulogica Educacao &amp; Tecnologia, a consultant and commercial enterprise specializing in distance education. artificial intelligenc Research and education Sabbatini began his scientific career in neurophysiology in 1966, while he was a medical student at the Medical School of the University of Sao Paulo at Ribeirao Preto [2]. He began to work in basic biomedical research under the supervision of Prof. Miguel Rolando Covian, an Argentine neurophysiologist, who encouraged him to found, after he graduated in 1968, the first research laboratory of neuroethology in Latin America [3] , at the Department of Physiology. Also there, he started in 1970 one of the first Brazilian and Latin American groups of research, development and education on the computer applications in biomedicine. Sabbatini got a doctorate in behavioral neuroscience in 1977 and immediately thereafter went to spend two and a half years doing postdoctoral work i","In other words AJAX is a web development technique for creating interactive web applications using a combination of XHTML and CSS , Document Object Model ( or DOM ) , the XMLHTTPRequest object ( #AUTHOR_TAG ) .","['AJAX is not a technology in itself but a term that refers to the use of a group of technologies together, in particular Javascript and XML.', 'In other words AJAX is a web development technique for creating interactive web applications using a combination of XHTML and CSS , Document Object Model ( or DOM ) , the XMLHTTPRequest object ( #AUTHOR_TAG ) .']",0,"['AJAX is not a technology in itself but a term that refers to the use of a group of technologies together, in particular Javascript and XML.', 'In other words AJAX is a web development technique for creating interactive web applications using a combination of XHTML and CSS , Document Object Model ( or DOM ) , the XMLHTTPRequest object ( #AUTHOR_TAG ) .']"
CC1558,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,learning creating and using knowledge concept maps as facilitative tools in schools and corporations lawrence erlbaum associates,['Joseph Donald Novak'],related work,,"Every arc always has a definite direction , i.e. arcs are arrows ( #AUTHOR_TAG ) .","[""Concept mapping has been used at least in education for over thirty years, in particular at the Cornell University, where Piaget's ideas gave the roots to the assimilation theory by David Ausubel."", 'Very briefly, concept maps show the relationships between concepts labelling both nodes and arcs.', 'Every arc always has a definite direction , i.e. arcs are arrows ( #AUTHOR_TAG ) .']",0,"[""Concept mapping has been used at least in education for over thirty years, in particular at the Cornell University, where Piaget's ideas gave the roots to the assimilation theory by David Ausubel."", 'Very briefly, concept maps show the relationships between concepts labelling both nodes and arcs.', 'Every arc always has a definite direction , i.e. arcs are arrows ( #AUTHOR_TAG ) .']"
CC1559,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,ruby on rails web developement that doesn’t hurt url httpwwwrubyonrailsorg retrieved the 03rd of january,['Ruby on Rails'],,,The Ruby on #AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes .,['The Ruby on #AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes .'],5,['The Ruby on #AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes .']
CC1560,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,the wiki way  quick collaboration on the web,"['Ward Cunningham', 'Bo Leuf']",introduction,"Foreword. Preface. Why This Book? Why You Want to Read This. Book Structure. The Authors. Contributors and Colleagues. Errata and Omissions. Contacting Us. Read the Book, Use the Wiki! I. FROM CONCEPTS TO USING WIKI. 1. Introduction to Discussion and Collaboration Servers. In this Chapter. Collaboration and Discussion Tools. Collaboration Models. Who Uses Collaborative Discussion Servers? Whatever For? Features of a Web-Based Collaboration. On the Horizon: WebDAV. Comparing Wiki to Other Collaboration Tools. 2. What's a ""Wiki""? The Wiki Concept. The Essence of Wiki. The User Experience. Usefulness Criteria. Wiki Basics. Wiki Clones. Wiki Implementations by Language. Other Wiki Offerings. Non-Wiki Servers. Wiki Application. Pros and Cons of a Wiki-Style Server. Why Consider Setting Up a Wiki? Other Issues. 3. Installing Wiki. QuickiWiki--Instant Serve. Installing Perl. Installing QuickiWiki. Multiple Instances. Wiki and Webserver. Wiki on IIS or PWS. The Apache Webserver. Installing Apache. Reconfiguring Apache. Testing Webserver Wiki. Wrapper Scripts. General Security Issues. Security and Database Integrity. Server Vulnerabilities. Addressing wiki Vulnerabilities. Configuring Your Browser Client. Fonts, Size and Layout. 4. Using Wiki. In this Chapter. Quicki Quick-Start. A Virtual Notebook. Making Wiki Notes, A Walkthrough. Wiki as PIM. A Working Example. The Content Model. Internal and External Hyperlink Models. Browsing Pages. Editing Pages. The Browser Editing Model. Building Wiki Content. Editing and Markup Conventions. 5. Structuring Wiki Content. In this Chapter. Wiki Structure. Structure Types. Only a Click Away. How Hard to Try. When to Impose Structure. When Not to Impose Structure. What is the Purpose of the Wiki? Structure Patterns. When to Spin Off New Wiki Servers. II. UNDERSTANDING THE HACKS. 6. Customizing Your Wiki. In this Chapter. Hacking Your Wiki Source. Copyright and Open Source License Policy. Why Customize? What to Customize. 7. Wiki Components Examined. In this Chapter. Dissecting QuickiWiki. QuickiWiki Component Model. Core QuickiWiki Modules. Sever Component. Optional Extended Components. Analyzing Page Content. Managing User Access. 8. Alternatives and Extensions. Parsing the Requests. ClusterWiki Component Model. The Library Module. Special Features. Spell Checking. Uploading Files. A Standard Wiki? 9. Wiki Administration and Tools. In this Chapter. Events History. Tracking Page Edits. Usage Statistics. Abuse Management. Access Management. Permission Models. Adding Authentication and Authorization. Administering the Database. Page Conversions. Page Management. Backup Issues. Server Resources and Wiki Loading. Avoiding User Waits. Implementing Wiki Constraints. Debugging a Wiki. Programming Resources. Backups. Low-Tech Debugging. Higher-Level Debugging. III. IMAGINE THE POSSIBILITIES. 10. Insights and Other Voices. In this Chapter. Wiki Culture. Wiki as Open Community. Writing Style Contention. Why Wiki Works. The Open-Edit Issue. When Wiki Doesn't Work. Public Wiki Issues. Wiki Style Guidelines. Notifying About Update. Design and Portability. Wiki Trade-Offs. Portability. The Future of Wiki. 11. Wiki Goes Edu. In this Chapter. CoWeb at Georgia Tech. Introduction to CoWeb. CoWeb Usage. Supported CoWeb User Roles. CoWeb Open Authoring Projects. Overall Conclusions. 12. Wiki at Work. In this Chapter. Case Studies. WikiWikiWeb. New York Times Digital. TWiki at TakeFive. TWiki at Motorola. Kehei Wiki Case Studies. A Rotary Wiki. Wiki Workplace Essentials. Why a Workplace Wiki? Planning the Wiki. Selection Stage. Implementation Stage. Day-to-Day Operations. Appendix A: Syntax Comparisons. Hyperlink Anchors. Markup Conventions. Escaped Blocks. HTML Tag Inclusion. Other Syntax Extensions Seen. Appendix B: Wiki Resources. Book Resources. Internet Resources. Appendix C: List of Tips. Index. 020171499XTO5232001","Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .","['In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .', 'Figure 1 shows the model.', 'History snapshots of the timeline may be considered as permanent views, i.e. views with a timestamp.', 'Consequently, except in the case of sandboxes, every change in the document cannot be erased.', 'This model will have a strong impact on the role of links and on the underpinning structure of Novelle itself.']",0,"['In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .', 'Figure 1 shows the model.', 'This model will have a strong impact on the role of links and on the underpinning structure of Novelle itself.']"
CC1561,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,genre under construction the diary on the internet,['Laurie McNeill'],introduction,"The emergence of the diary as a digital form has generated the kinds of introduction and explanation that typically accumulate around emerging genres, even though online diarists in many ways strive to reproduce the stereotypical print diary. However, as diarists and readers explore the nature of blogs, both in diary entries and comments pages, a tension is apparent between users ' accounts or explanations of the genre and their actual practices, and this tension provides a rich site for studying the evolution of the diary genre. Readers ' and writers ' comments illustrate the blogging community's ideas about genre as a concept and how these ideas transfer to the ""new "" world of online media. In this paper, I look at the diary's transition from page to screen, and consider how readers and writers build on and diverge from print culture practices in establishing expectations and ""rules "" for Weblogs. Examining how diarists and their communities establish and police the digital diary, and how generic knowledge is circulated and codified, helps understand the particular social actions the diary can perform only on the Internet. In contemporary Western culture the diary is among the most familiar genres, an enduring everyday form that has evolved and adapted across time yet has remained recognizable in each instance. As a genre that straddles and confuses ideals of the literary and non-literary","Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .","['1.1 Hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e."", 'literary criticism -unlike in the previous times (Eisenstein, 1983).', 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', ""For example, a 'web page' is more similar to an infinite canvas than a written page (McCloud, 2001)."", 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962)."", 'From a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .', 'They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.', 'This situation could make new problems rise up: Who owns the text?', 'Which role is suitable for authors?', 'We have to analyse them before presenting the architecture of Novelle.']",0,"[""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', 'literary criticism -unlike in the previous times (Eisenstein, 1983).', 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', 'Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .', 'This situation could make new problems rise up: Who owns the text?']"
CC1562,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,the wiki way  quick collaboration on the web,"['Ward Cunningham', 'Bo Leuf']",,"Foreword. Preface. Why This Book? Why You Want to Read This. Book Structure. The Authors. Contributors and Colleagues. Errata and Omissions. Contacting Us. Read the Book, Use the Wiki! I. FROM CONCEPTS TO USING WIKI. 1. Introduction to Discussion and Collaboration Servers. In this Chapter. Collaboration and Discussion Tools. Collaboration Models. Who Uses Collaborative Discussion Servers? Whatever For? Features of a Web-Based Collaboration. On the Horizon: WebDAV. Comparing Wiki to Other Collaboration Tools. 2. What's a ""Wiki""? The Wiki Concept. The Essence of Wiki. The User Experience. Usefulness Criteria. Wiki Basics. Wiki Clones. Wiki Implementations by Language. Other Wiki Offerings. Non-Wiki Servers. Wiki Application. Pros and Cons of a Wiki-Style Server. Why Consider Setting Up a Wiki? Other Issues. 3. Installing Wiki. QuickiWiki--Instant Serve. Installing Perl. Installing QuickiWiki. Multiple Instances. Wiki and Webserver. Wiki on IIS or PWS. The Apache Webserver. Installing Apache. Reconfiguring Apache. Testing Webserver Wiki. Wrapper Scripts. General Security Issues. Security and Database Integrity. Server Vulnerabilities. Addressing wiki Vulnerabilities. Configuring Your Browser Client. Fonts, Size and Layout. 4. Using Wiki. In this Chapter. Quicki Quick-Start. A Virtual Notebook. Making Wiki Notes, A Walkthrough. Wiki as PIM. A Working Example. The Content Model. Internal and External Hyperlink Models. Browsing Pages. Editing Pages. The Browser Editing Model. Building Wiki Content. Editing and Markup Conventions. 5. Structuring Wiki Content. In this Chapter. Wiki Structure. Structure Types. Only a Click Away. How Hard to Try. When to Impose Structure. When Not to Impose Structure. What is the Purpose of the Wiki? Structure Patterns. When to Spin Off New Wiki Servers. II. UNDERSTANDING THE HACKS. 6. Customizing Your Wiki. In this Chapter. Hacking Your Wiki Source. Copyright and Open Source License Policy. Why Customize? What to Customize. 7. Wiki Components Examined. In this Chapter. Dissecting QuickiWiki. QuickiWiki Component Model. Core QuickiWiki Modules. Sever Component. Optional Extended Components. Analyzing Page Content. Managing User Access. 8. Alternatives and Extensions. Parsing the Requests. ClusterWiki Component Model. The Library Module. Special Features. Spell Checking. Uploading Files. A Standard Wiki? 9. Wiki Administration and Tools. In this Chapter. Events History. Tracking Page Edits. Usage Statistics. Abuse Management. Access Management. Permission Models. Adding Authentication and Authorization. Administering the Database. Page Conversions. Page Management. Backup Issues. Server Resources and Wiki Loading. Avoiding User Waits. Implementing Wiki Constraints. Debugging a Wiki. Programming Resources. Backups. Low-Tech Debugging. Higher-Level Debugging. III. IMAGINE THE POSSIBILITIES. 10. Insights and Other Voices. In this Chapter. Wiki Culture. Wiki as Open Community. Writing Style Contention. Why Wiki Works. The Open-Edit Issue. When Wiki Doesn't Work. Public Wiki Issues. Wiki Style Guidelines. Notifying About Update. Design and Portability. Wiki Trade-Offs. Portability. The Future of Wiki. 11. Wiki Goes Edu. In this Chapter. CoWeb at Georgia Tech. Introduction to CoWeb. CoWeb Usage. Supported CoWeb User Roles. CoWeb Open Authoring Projects. Overall Conclusions. 12. Wiki at Work. In this Chapter. Case Studies. WikiWikiWeb. New York Times Digital. TWiki at TakeFive. TWiki at Motorola. Kehei Wiki Case Studies. A Rotary Wiki. Wiki Workplace Essentials. Why a Workplace Wiki? Planning the Wiki. Selection Stage. Implementation Stage. Day-to-Day Operations. Appendix A: Syntax Comparisons. Hyperlink Anchors. Markup Conventions. Escaped Blocks. HTML Tag Inclusion. Other Syntax Extensions Seen. Appendix B: Wiki Resources. Book Resources. Internet Resources. Appendix C: List of Tips. Index. 020171499XTO5232001","The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) .","['On the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'In fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself.', 'Generally, people avoid commenting, preferring to edit each document.', 'The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) .']",0,"['On the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) .']"
CC1563,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,ajax a new approach to web applications url httpwwwadaptivepathcompublicationsessays archives000385php retrieved the 22nd of december,['Jesse James Garrett'],,,AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .,"['We used the Asyncronous Javascript And XML (or AJAX) paradigm to create the graphical user interface.', 'AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .']",0,"['We used the Asyncronous Javascript And XML (or AJAX) paradigm to create the graphical user interface.', 'AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .']"
CC1564,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,floresta sint´actica” a treebank for portuguese,"['S Afonso', 'E Bick', 'R Haber', 'D Santos']",experiments,,"This is noticeable for German ( Brants et al. , 2002 ) and Portuguese ( #AUTHOR_TAG ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .","['A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'This is noticeable for German ( Brants et al. , 2002 ) and Portuguese ( #AUTHOR_TAG ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .', 'On the other hand, all three languages behave like high-accuracy languages with respect to attachment score.', 'A very similar pattern is found for Spanish (Civit Torruella and Martí Antonín, 2002), although this cannot be explained by a high proportion of non-projective structures.', 'One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features.']",1,"['This is noticeable for German ( Brants et al. , 2002 ) and Portuguese ( #AUTHOR_TAG ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .', 'One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features.']"
CC1565,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,pseudoprojective dependency parsing,"['J Nivre', 'J Nilsson']",introduction,,â¢ Graph transformations for recovering nonprojective structures ( #AUTHOR_TAG ) .,"['• A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', '• History-based feature models for predicting the next parser action (Black et al., 1992).', '• Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', 'â\x80¢ Graph transformations for recovering nonprojective structures ( #AUTHOR_TAG ) .']",5,"['* A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', '* Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', 'â\x80¢ Graph transformations for recovering nonprojective structures ( #AUTHOR_TAG ) .']"
CC1566,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,the tiger treebank,"['S Brants', 'S Dipper', 'S Hansen', 'W Lezius', 'G Smith']",experiments,"Proceedings of the 16th Nordic Conference   of Computational Linguistics NODALIDA-2007.  Editors: Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek and Mare Koit.  University of Tartu, Tartu, 2007.  ISBN 978-9985-4-0513-0 (online)  ISBN 978-9985-4-0514-7 (CD-ROM)  pp. 81-88","This is noticeable for German ( #AUTHOR_TAG ) and Portuguese ( Afonso et al. , 2002 ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .","['A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'This is noticeable for German ( #AUTHOR_TAG ) and Portuguese ( Afonso et al. , 2002 ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .', 'On the other hand, all three languages behave like high-accuracy languages with respect to attachment score.', 'A very similar pattern is found for Spanish (Civit Torruella and Martí Antonín, 2002), although this cannot be explained by a high proportion of non-projective structures.', 'One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features.']",1,"['A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'This is noticeable for German ( #AUTHOR_TAG ) and Portuguese ( Afonso et al. , 2002 ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .', 'On the other hand, all three languages behave like high-accuracy languages with respect to attachment score.', 'One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features.']"
CC1567,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,memorybased dependency parsing,"['J Nivre', 'J Hall', 'J Nilsson']",method,,The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre ( 2003 ) and extended to labeled dependency parsing by #AUTHOR_TAG .,['The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre ( 2003 ) and extended to labeled dependency parsing by #AUTHOR_TAG .'],5,['The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre ( 2003 ) and extended to labeled dependency parsing by #AUTHOR_TAG .']
CC1568,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,mamba meets tiger reconstructing a swedish treebank from antiquity,"['J Nilsson', 'J Hall', 'J Nivre']",experiments,,"Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( #AUTHOR_TAG ) .","['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( #AUTHOR_TAG ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']",0,"['Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( #AUTHOR_TAG ) .']"
CC1569,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,design and implementation of the bulgarian hpsgbased treebank,"['K Simov', 'P Osenova', 'A Simov', 'M Kouylekov']",experiments,,"Typical examples are Bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .","['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']",0,"['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']"
CC1570,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,sinica treebank design criteria representational issues and implementation,"['K Chen', 'C Luo', 'M Chang', 'F Chen', 'C Chen', 'C Huang', 'Z Gao']",experiments,"The disclosed apparatus measures the mass rate of flow of gas through an orifice of a flow nozzle under critical flow conditions and comprises a reservoir having an inlet connected to a source of high pressure through a first valve and an outlet connected to a flow nozzle adapted to conduct pressurized gas from said reservoir under critical flow conditions to a region of low pressure downstream of the flow nozzle. The critical flow conditions are established by a second valve mounted downstream of the nozzle orifice. Density and pressure measuring devices are coupled to the reservoir and provide signals representative of the density and pressure, respectively, of the gas flowing through the flow nozzle. The density and pressure signals are applied to a device which calculates the square root of the product of density and pressure to provide a signal proportional to the mass rate of gas flow.","Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( #AUTHOR_TAG ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .","['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( #AUTHOR_TAG ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']",0,"['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( #AUTHOR_TAG ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']"
CC1571,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,towards historybased grammars using richer models for probabilistic parsing,"['E Black', 'F Jelinek', 'J D Lafferty', 'D M Magerman', 'R L Mercer', 'S Roukos']",introduction,"We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.",â¢ History-based feature models for predicting the next parser action ( #AUTHOR_TAG ) .,"['• A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', 'â\x80¢ History-based feature models for predicting the next parser action ( #AUTHOR_TAG ) .', '• Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', '• Graph transformations for recovering nonprojective structures .']",5,['â\x80¢ History-based feature models for predicting the next parser action ( #AUTHOR_TAG ) .']
CC1572,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,maltparser a datadriven parsergenerator for dependency parsing,"['J Nivre', 'J Hall', 'J Nilsson']",introduction,,"All experiments have been performed using MaltParser ( #AUTHOR_TAG ) , version 0.4 , which is made available together with the suite of programs used for preand post-processing .1","['All experiments have been performed using MaltParser ( #AUTHOR_TAG ) , version 0.4 , which is made available together with the suite of programs used for preand post-processing .1']",5,"['All experiments have been performed using MaltParser ( #AUTHOR_TAG ) , version 0.4 , which is made available together with the suite of programs used for preand post-processing .1']"
CC1573,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,libsvm a library for support vector machines software available at httpwwwcsientuedutw cjlinlibsvm,"['C-C Chang', 'C-J Lin']",method,,"More specifically , we use LIBSVM ( #AUTHOR_TAG ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification .","['We use support vector machines to predict the next parser action from a feature vector representing the history.', 'More specifically , we use LIBSVM ( #AUTHOR_TAG ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification .', 'Symbolic features are converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components. 4', 'or some languages, we divide the training data into smaller sets, based on some feature s (normally the CPOS or POS of the next input token), which may reduce training times without a significant loss in accuracy (Yamada and Matsumoto, 2003).', 'To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t.']",5,"['More specifically , we use LIBSVM ( #AUTHOR_TAG ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification .']"
CC1574,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,headdriven statistical models for natural language parsing,['M Collins'],experiments,"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.",6The analysis is reminiscent of the treatment of coordination in the Collins parser ( #AUTHOR_TAG ) .,['6The analysis is reminiscent of the treatment of coordination in the Collins parser ( #AUTHOR_TAG ) .'],1,['6The analysis is reminiscent of the treatment of coordination in the Collins parser ( #AUTHOR_TAG ) .']
CC1575,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,pseudoprojective dependency parsing,"['J Nivre', 'J Nilsson']",method,,"Although the parser only derives projective graphs , the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudoprojective approach of #AUTHOR_TAG .","['Although the parser only derives projective graphs , the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudoprojective approach of #AUTHOR_TAG .']",0,"['Although the parser only derives projective graphs , the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudoprojective approach of #AUTHOR_TAG .']"
CC1576,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,statistical dependency analysis with support vector machines,"['H Yamada', 'Y Matsumoto']",method,"In this paper, we propose a method for analyzing word-word dependencies using deterministic bottom-up manner using Support Vector machines. We experimented with dependency trees converted from Penn treebank data, and achieved over 90% accuracy of word-word dependency. Though the result is little worse than the most up-to-date phrase structure based parsers, it looks satisfactorily accurate considering that our parser uses no information from phrase structures.","For some languages , we divide the training data into smaller sets , based on some feature s ( normally the CPOS or POS of the next input token ) , which may reduce training times without a significant loss in accuracy ( #AUTHOR_TAG ) .","['For some languages , we divide the training data into smaller sets , based on some feature s ( normally the CPOS or POS of the next input token ) , which may reduce training times without a significant loss in accuracy ( #AUTHOR_TAG ) .', 'To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t.']",0,"['For some languages , we divide the training data into smaller sets , based on some feature s ( normally the CPOS or POS of the next input token ) , which may reduce training times without a significant loss in accuracy ( #AUTHOR_TAG ) .']"
CC1577,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,practical annotation scheme for an hpsg treebank of bulgarian in,"['K Simov', 'P Osenova']",experiments,"The paper presents an HPSG-based annotation scheme for constructing a Bulgarian treebank: BulTreeBank. It differs from other grammar-based annotation schemes in having a hybrid status with respect to the partial parsing component and the full parsing module. As the parsing complexity is handled preferably by the pre-processing step, the task of the HPSG module is maximally facilitated and simplified.","Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .","['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']",0,"['before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .']"
CC1578,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,stylebook for the japanese treebank in verbmobil verbmobilreport 240 seminar f¨ur sprachwissenschaft,"['Y Kawata', 'J Bartels']",experiments,,"Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .","['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian (Simov et al., 2005;Simov and Osenova, 2003), Chinese (Chen et al., 2003), Danish (Kromann, 2003), and Swedish .', 'Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .']",1,"['before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .']"
CC1579,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,japanese dependency analysis using cascaded chunking,"['T Kudo', 'Y Matsumoto']",introduction,"In this paper, we propose a new statistical Japanese dependency parser using a cascaded chunking model. Conventional Japanese statistical dependency parsers are mainly based on a probabilistic model, which is not always efficient or scalable. We propose a new method that is simple and efficient, since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side. Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency.",Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .,"['• A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', '• History-based feature models for predicting the next parser action (Black et al., 1992).', 'Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .', '• Graph transformations for recovering nonprojective structures .']",5,"['* A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', 'Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .']"
CC1580,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,the annotation process in the turkish treebank,"['N B Atalay', 'K Oflazer', 'B Say']",experiments,"We present a progress report of the Turkish Treebank concentrating on various aspects of its design and implementation. In addition to a review of the corpus compilation process and the design  of the annotation scheme, we describe the details of various pre-processing stages and the computer-assisted annotation process","By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .","['The results for Arabic (Hajič et al., 2004;Smrž et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2).', 'By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .', 'It is noteworthy that Arabic and Turkish, being ""typological outliers"", show patterns that are different both from each other and from most of the other languages.']",1,"['By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .', 'It is noteworthy that Arabic and Turkish, being ""typological outliers"", show patterns that are different both from each other and from most of the other languages.']"
CC1581,W06-3309,Generative content models for structural analysis of medical abstracts,categorization of sentence types in medical abstracts,"['Larry McKnight', 'Padmini Srinivasan']",introduction,"This study evaluated the use of machine learning techniques in the classification of sentence type. 7253 structured abstracts and 204 unstructured abstracts of Randomized Controlled Trials from MedLINE were parsed into sentences and each sentence was labeled as one of four types (Introduction, Method, Result, or Conclusion). Support Vector Machine (SVM) and Linear Classifier models were generated and evaluated on cross-validated data. Treating sentences as a simple ""bag of words"", the SVM model had an average ROC area of 0.92. Adding a feature of relative sentence location improved performance markedly for some models and overall increasing the average ROC to 0.95. Linear classifier performance was significantly worse than the SVM in all datasets. Using the SVM model trained on structured abstracts to predict unstructured abstracts yielded performance similar to that of models trained with unstructured abstracts in 3 of the 4 types. We conclude that classification of sentence type seems feasible within the domain of RCT's. Identification of sentence types may be helpful for providing context to end users or other text summarization techniques.",#AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques .,"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', '#AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques .', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,['#AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques .']
CC1582,W06-3309,Generative content models for structural analysis of medical abstracts,semiautomatic indexing of full text biomedical articles,"['Clifford W Gay', 'Mehmet Kayaalp', 'Alan R Aronson']",introduction,"The main application of U.S. National Library of Medicine's Medical Text Indexer (MTI) is to provide indexing recommendations to the Library's indexing staff. The current input to MTI consists of the titles and abstracts of articles to be indexed. This study reports on an extension of MTI to the full text of articles appearing in online medical journals that are indexed for Medline. Using a collection of 17 journal issues containing 500 articles, we report on the effectiveness of the contribution of terms by the whole article and also by each section. We obtain the best results using a model consisting of the sections Results, Results and Discussion, and Conclusions together with the article's title and abstract, the captions of tables and figures, and sections that have no titles. The resulting model provides indexing significantly better (7.4%) than what is currently achieved using only titles and abstracts.","For example , #AUTHOR_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example , #AUTHOR_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score .', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orasan, 2001).', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example , #AUTHOR_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score .', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.']"
CC1583,W06-3309,Generative content models for structural analysis of medical abstracts,modern applied statistics with splus,"['William N Venables', 'Brian D Ripley']",method,,"#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space .","['In an attempt to further boost performance, we employed Linear Discriminant Analysis (LDA) to find a linear projection of the four-dimensional vec- tors that maximizes the separation of the Gaussians (corresponding to the HMM states).', '#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space .', 'Each sentence/vector is then mul- tiplied by this matrix, and new HMM models are re-computed from the projected data.']",5,"['#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space .']"
CC1584,W06-3309,Generative content models for structural analysis of medical abstracts,knowledge extraction for clinical question answering preliminary results,"['Dina Demner-Fushman', 'Jimmy Lin']",conclusion,"The combination of recent developments in question an-swering research and the unparalleled resources devel-oped specifically for automatic semantic processing of text in the medical domain provides a unique opportu-nity to explore complex question answering in the clin-ical domain. In this paper, we attempt to operationalize major aspects of evidence-based medicine in the form of knowledge extractors that serve as the fundamental building blocks of a clinical question answering sys-tem. Our evaluations demonstrate that domain-specific knowledge can be effectively leveraged to extract PICO frame elements from MEDLINE abstracts. Clinical in-formation systems in support of physicians ' decision-making process have the potential to improve the qual-ity of patient care in real-world settings","Such a component would serve as the first stage of a clinical question answering system ( #AUTHOR_TAG ) or summarization system ( McKeown et al. , 2003 ) .","['Although exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process.', 'In truth, our crossvalidation experiments do not correspond to any meaningful naturally-occurring task-structured abstracts are, after all, already appropriately labeled.', 'The true utility of content models is to structure abstracts that have no structure to begin with.', 'Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance.', 'Such a component would serve as the first stage of a clinical question answering system ( #AUTHOR_TAG ) or summarization system ( McKeown et al. , 2003 ) .', 'We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured.']",3,"['Such a component would serve as the first stage of a clinical question answering system ( #AUTHOR_TAG ) or summarization system ( McKeown et al. , 2003 ) .']"
CC1585,W06-3309,Generative content models for structural analysis of medical abstracts,on discriminative vs generative classifiers a comparison of logistic regression and naive bayes,"['Andrew Y Ng', 'Michael Jordan']",introduction,"Comparison of generative and discriminative classifiers is an ever-lasting topic. As an important contribution to this topic, based on their theoretical and empirical comparisons between the naive Bayes classifier and linear logistic regression, Ng and Jordan (NIPS 841-848, 2001) claimed that there exist two distinct regimes of performance between the generative and discriminative classifiers with regard to the training-set size. In this paper, our empirical and simulation studies, as a complement of their work, however, suggest that the existence of the two distinct regimes may not be so reliable. In addition, for real world datasets, so far there is no theoretically correct, general criterion for choosing between the discriminative and the generative approaches to classification of an observation x into a class y; the choice depends on the relative confidence we have in the correctness of the specification of either p(y vertical bar x) or p(x, y) for the data. This can be to some extent a demonstration of why Efron (J Am Stat Assoc 70(352):892-898, 1975) and O'Neill (J Am Stat Assoc 75(369):154-160, 1980) prefer normal-based linear discriminant analysis (LDA) when no model mis-specification occurs but other empirical studies may prefer linear logistic regression instead. Furthermore, we suggest that pairing of either LDA assuming a common diagonal covariance matrix (LDA-A) or the naive Bayes classifier and linear logistic regression may not be perfect, and hence it may not be reliable for any claim that was derived from the comparison between LDA-A or the naive Bayes classifier and linear logistic regression to be generalised to all generative and discriminative classifiers","Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( Joachims , 1998 ; #AUTHOR_TAG ) .","['Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( Joachims , 1998 ; #AUTHOR_TAG ) .', 'However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing.', 'Under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'Since HMMs are very wellsuited to modeling sequences, our discourse modeling task lends itself naturally to this particular generative approach.', 'In fact, we demonstrate that HMMs are competitive with SVMs, with the added advantage of lower computational complexity.', 'In addition, generative models can be directly applied to tackle certain classes of problems, such as sentence ordering, in ways that discriminative approaches cannot readily.', 'In the context of machine learning, we see our work as contributing to the ongoing debate between generative and discriminative approacheswe provide a case study in an interesting domain that begins to explore some of these tradeoffs.']",0,"['Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( Joachims , 1998 ; #AUTHOR_TAG ) .', 'In the context of machine learning, we see our work as contributing to the ongoing debate between generative and discriminative approacheswe provide a case study in an interesting domain that begins to explore some of these tradeoffs.']"
CC1586,W06-3309,Generative content models for structural analysis of medical abstracts,catching the drift probabilistic content models with applications to generation and summarization,"['Regina Barzilay', 'Lillian Lee']",method,"We consider the problem of modeling the content structure of texts within a specic domain, in terms of the topics the texts address and the order in which these topics appear. We rst present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.","Following Ruch et al. ( 2003 ) and #AUTHOR_TAG , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts .","['Following Ruch et al. ( 2003 ) and #AUTHOR_TAG , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts .', 'The four states in our HMMs correspond to the information that characterizes each section (""introduction"", ""methods"", ""results"", and ""conclusions"") and state transitions capture the discourse flow from section to section.']",5,"['Following Ruch et al. ( 2003 ) and #AUTHOR_TAG , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts .']"
CC1587,W06-3309,Generative content models for structural analysis of medical abstracts,what’s yours and what’s mine determining intellectual attribution in scientific text,"['Simone Teufel', 'Marc Moens']",introduction,"We believe that identifying the structure of scientific argumentation in articles can help in tasks such as automatic summarization or the automated construction of citation indexes. One particularly important aspect of this structure is the question of who a given scientific statement is attributed to: other researchers, the field in general, or the authors themselves.We present the algorithm and a systematic evaluation of a system which can recognize the most salient textual properties that contribute to the global argumentative structure of a text. In this paper we concentrate on two particular features, namely the occurrences of prototypical agents and their actions in scientific text.","The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #AUTHOR_TAG ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #AUTHOR_TAG ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering .', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orasan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #AUTHOR_TAG ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering .', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'For a variety of reasons, medicine is an interesting domain of research.', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"
CC1588,W06-3309,Generative content models for structural analysis of medical abstracts,the htk book,"['Steve Young', 'Gunnar Evermann', 'Thomas Hain', 'Dan Kershaw', 'Gareth Moore', 'Julian Odell', 'Dave Ollason', 'Dan Povey', 'Valtcho Valtchev', 'Phil Woodland']",method,,"Using the section labels , the HMM was trained using the HTK toolkit ( #AUTHOR_TAG ) , which efficiently performs the forward-backward algorithm and BaumWelch estimation .","['We then built a four-state Hidden Markov Model that outputs these four-dimensional vectors.', 'The transition probability matrix of the HMM was initialized with uniform probabilities over a fully connected graph.', 'The output probabilities were modeled as four-dimensional Gaussians mixtures with diagonal covariance matrices.', 'Using the section labels , the HMM was trained using the HTK toolkit ( #AUTHOR_TAG ) , which efficiently performs the forward-backward algorithm and BaumWelch estimation .', 'For testing, we performed a Viterbi (maximum likelihood) estimation of the label of each test sentence/vector (also using the HTK toolkit).']",5,"['We then built a four-state Hidden Markov Model that outputs these four-dimensional vectors.', 'The output probabilities were modeled as four-dimensional Gaussians mixtures with diagonal covariance matrices.', 'Using the section labels , the HMM was trained using the HTK toolkit ( #AUTHOR_TAG ) , which efficiently performs the forward-backward algorithm and BaumWelch estimation .']"
CC1589,W06-3309,Generative content models for structural analysis of medical abstracts,genre analysis english inacademic and research settings,['John M Swales'],introduction,"In recent years the concept of 'register' has been increasingly replaced by emphasis on the analysis of genre, which relates work in sociolinguistics, text linguistics and discourse analysis to the study of specialist areas of language. This book is a clear, authoritative guide to this complex area. He provides a survey of approaches to varieties of language, and considers these in relation to communication and task-based language learning. Swales outlines an approach to the analysis of genre, and then proceeds to consider examples of different genres and how they can be made accessible through genre analysis. This is important reading for all those working in teaching English for academic purposes and also of interest to those working in post-secondary writing and composition due to relevant issues in writing across the curriculum.","As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction""  , ""methods""  , ""results""  , and ""conclusions""  ( SalangerMeyer , 1990 ; #AUTHOR_TAG ; OrË\x98asan , 2001 ) .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction""  , ""methods""  , ""results""  , and ""conclusions""  ( SalangerMeyer , 1990 ; #AUTHOR_TAG ; OrË\x98asan , 2001 ) .', 'The ability to explicitly identify these sections in un-structured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'Demner-Fushman et al. (2005) found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.']",0,"['As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction""  , ""methods""  , ""results""  , and ""conclusions""  ( SalangerMeyer , 1990 ; #AUTHOR_TAG ; OrË\\x98asan , 2001 ) .']"
CC1590,W06-3309,Generative content models for structural analysis of medical abstracts,catching the drift probabilistic content models with applications to generation and summarization,"['Regina Barzilay', 'Lillian Lee']",conclusion,"We consider the problem of modeling the content structure of texts within a specic domain, in terms of the topics the texts address and the order in which these topics appear. We rst present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.","An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #AUTHOR_TAG ) .","['An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #AUTHOR_TAG ) .', 'This technique provides two important advantages.', 'First, Gaussian modeling adds an extra degree of freedom during training, by capturing second-order statistics.', 'This is not possible when modeling word sequences, where only the probability of a sentence is actually used in the HMM training.', 'Second, using continuous distributions allows us to leverage a variety of tools (e.g., LDA) that have been shown to be successful in other fields, such as speech recognition (Evermann et al., 2004).']",1,"['An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #AUTHOR_TAG ) .']"
CC1591,W06-3309,Generative content models for structural analysis of medical abstracts,text categorization with support vector machines learning with many relevant features,['Thorsten Joachims'],introduction,"Abstract. This paper explores the use of Support Vector Machines (SVMs) for learning text classi ers from examples. It analyzes the particular properties of learning with text data and identi es why SVMs are appropriate for this task. Empirical results support the theoretical ndings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly overavariety of di erent learning tasks. Furthermore, they are fully automatic, eliminating the need for manual parameter tuning.","Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( #AUTHOR_TAG ; Ng and Jordan , 2001 ) .","['Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( #AUTHOR_TAG ; Ng and Jordan , 2001 ) .', 'However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing.', 'Under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'Since HMMs are very wellsuited to modeling sequences, our discourse modeling task lends itself naturally to this particular generative approach.', 'In fact, we demonstrate that HMMs are competitive with SVMs, with the added advantage of lower computational complexity.', 'In addition, generative models can be directly applied to tackle certain classes of problems, such as sentence ordering, in ways that discriminative approaches cannot readily.', 'In the context of machine learning, we see our work as contributing to the ongoing debate between generative and discriminative approacheswe provide a case study in an interesting domain that begins to explore some of these tradeoffs.']",0,"['Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( #AUTHOR_TAG ; Ng and Jordan , 2001 ) .']"
CC1592,W06-3309,Generative content models for structural analysis of medical abstracts,effective mapping of biomedical text to the umls metathesaurus the metamap program,['Alan R Aronson'],introduction,"The UMLS (r) Metathesaurus(r), the largest thesaurus in the biomedical domain, provides a representation of biomedical knowledge consisting of concepts classi-fied by semantic type and both hierarchical and non-hierarchical relationships among the concepts. This knowledge has proved useful for many applications including decision support systems, management of patient records, information retrieval (IR) and data mining. Gaining effective access to the knowledge is critical to the success of these applications. This paper describes MetaMap, a program developed at the National Library of Medicine (NLM) to map bio-medical text to the Metathesaurus or, equivalently, to discover Metathesaurus concepts referred to in text. MetaMap uses a knowledge intensive approach based on symbolic, natural language processing (NLP) and computational linguistic techniques. Besides being applied for both IR and data mining applications, MetaMap is one of the foundations of NLM's Indexing Initiative System which is being applied to both semi-automatic and fully automatic indexing of the biomed-ical literature at the library","Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #AUTHOR_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #AUTHOR_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #AUTHOR_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .']"
CC1593,W06-3309,Generative content models for structural analysis of medical abstracts,an unsupervised approach to recognizing discourse relations,"['Daniel Marcu', 'Abdessamad Echihabi']",related work,"We present an unsupervised approach to recognizing discourse relations of CON-TRAST, EXPLANATION-EVIDENCE, CON-DITION and ELABORATION that hold be-tween arbitrary spans of texts. We show that discourse relation classifiers trained on examples that are automatically ex-tracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not ex-plicitly marked by cue phrases.","Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( McKeown , 1985 ; #AUTHOR_TAG ) .","['Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( McKeown , 1985 ; #AUTHOR_TAG ) .', 'Our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts.']",1,"['Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( McKeown , 1985 ; #AUTHOR_TAG ) .']"
CC1594,W06-3309,Generative content models for structural analysis of medical abstracts,answering physicians’ clinical questions obstacles and potential solutions,"['John W Ely', 'Jerome A Osheroff', 'M Lee Chambliss', 'Mark H Ebell', 'Marcy E Rosenbaum']",introduction,"To identify the most frequent obstacles preventing physicians from answering their patient-care questions and the most requested improvements to clinical information resources.Qualitative analysis of questions asked by 48 randomly selected generalist physicians during ambulatory care.Frequency of reported obstacles to answering patient-care questions and recommendations from physicians for improving clinical information resources.The physicians asked 1,062 questions but pursued answers to only 585 (55%). The most commonly reported obstacle to the pursuit of an answer was the physician's doubt that an answer existed (52 questions, 11%). Among pursued questions, the most common obstacle was the failure of the selected resource to provide an answer (153 questions, 26%). During audiotaped interviews, physicians made 80 recommendations for improving clinical information resources. For example, they requested comprehensive resources that answer questions likely to occur in practice with emphasis on treatment and bottom-line advice. They asked for help in locating information quickly by using lists, tables, bolded subheadings, and algorithms and by avoiding lengthy, uninterrupted prose.Physicians do not seek answers to many of their questions, often suspecting a lack of usable information. When they do seek answers, they often cannot find the information they need. Clinical resource developers could use the recommendations made by practicing physicians to provide resources that are more useful for answering clinical questions.","The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ) .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ) .', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ) .']"
CC1595,W06-3309,Generative content models for structural analysis of medical abstracts,what’s yours and what’s mine determining intellectual attribution in scientific text,"['Simone Teufel', 'Marc Moens']",related work,"We believe that identifying the structure of scientific argumentation in articles can help in tasks such as automatic summarization or the automated construction of citation indexes. One particularly important aspect of this structure is the question of who a given scientific statement is attributed to: other researchers, the field in general, or the authors themselves.We present the algorithm and a systematic evaluation of a system which can recognize the most salient textual properties that contribute to the global argumentative structure of a text. In this paper we concentrate on two particular features, namely the occurrences of prototypical agents and their actions in scientific text.","Our task is closer to the work of #AUTHOR_TAG , who looked at the problem of intellectual attribution in scientific texts .","['Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements (McKeown, 1985;Marcu and Echihabi, 2002).', 'Our task is closer to the work of #AUTHOR_TAG , who looked at the problem of intellectual attribution in scientific texts .']",1,"['Our task is closer to the work of #AUTHOR_TAG , who looked at the problem of intellectual attribution in scientific texts .']"
CC1596,W06-3309,Generative content models for structural analysis of medical abstracts,leveraging a common representation for personalized search and summarization in a medical digital library,"['Kathleen McKeown', 'Noemie Elhadad', 'Vasileios Hatzivassiloglou']",conclusion,"Despite the large amount of online medical literature, it can be difficult for clinicians to find relevant information at the point of patient care. In this paper, we present techniques to personalize the results of search, making use of the online patient record as a sophisticated, pre-existing user model. Our work in PERSIVAL, a medical digital library, includes methods for re-ranking the results of search to prioritize those that better match the patient record. It also generates summaries of the re-ranked results which highlight information that is relevant to the patient under the physician's care. We focus on the use of a common representation for the articles returned by search and the patient record which facilitates both the re-ranking and the summarization tasks. This common approach to both tasks has a strong positive effect on the ability to personalize information","Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system ( #AUTHOR_TAG ) .","['Although exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process.', 'In truth, our crossvalidation experiments do not correspond to any meaningful naturally-occurring task-structured abstracts are, after all, already appropriately labeled.', 'The true utility of content models is to structure abstracts that have no structure to begin with.', 'Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance.', 'Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system ( #AUTHOR_TAG ) .', 'We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured.']",3,"['The true utility of content models is to structure abstracts that have no structure to begin with.', 'Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system ( #AUTHOR_TAG ) .']"
CC1597,W06-3309,Generative content models for structural analysis of medical abstracts,categorization of sentence types in medical abstracts,"['Larry McKnight', 'Padmini Srinivasan']",introduction,"This study evaluated the use of machine learning techniques in the classification of sentence type. 7253 structured abstracts and 204 unstructured abstracts of Randomized Controlled Trials from MedLINE were parsed into sentences and each sentence was labeled as one of four types (Introduction, Method, Result, or Conclusion). Support Vector Machine (SVM) and Linear Classifier models were generated and evaluated on cross-validated data. Treating sentences as a simple ""bag of words"", the SVM model had an average ROC area of 0.92. Adding a feature of relative sentence location improved performance markedly for some models and overall increasing the average ROC to 0.95. Linear classifier performance was significantly worse than the SVM in all datasets. Using the SVM model trained on structured abstracts to predict unstructured abstracts yielded performance similar to that of models trained with unstructured abstracts in 3 of the 4 types. We conclude that classification of sentence type seems feasible within the domain of RCT's. Identification of sentence types may be helpful for providing context to end users or other text summarization techniques.",Table (b) again reproduces the results from #AUTHOR_TAG (2003) for a comparable task on a different subset of 206 unstructured abstracts.,['Table (b) again reproduces the results from #AUTHOR_TAG (2003) for a comparable task on a different subset of 206 unstructured abstracts.'],1,['Table (b) again reproduces the results from #AUTHOR_TAG (2003) for a comparable task on a different subset of 206 unstructured abstracts.']
CC1598,W06-3309,Generative content models for structural analysis of medical abstracts,zone analysis in biology articles as a basis for information extraction,"['Yoko Mizuta', 'Anna Korhonen', 'Tony Mullen', 'Nigel Collier']",introduction,"In the field of biomedicine, an overwhelming amount of experimental data has become available as a result of the high throughput of research in this domain. The amount of results reported has now grown beyond the limits of what can be managed by manual means. This makes it increasingly difficult for the researchers in this area to keep up with the latest developments. Information extraction (IE) in the biological domain aims to provide an effective automatic means to dynamically manage the information contained in archived journal articles and abstract collections and thus help researchers in their work. However, while considerable advances have been made in certain areas of IE, pinpointing and organizing factual information (such as experimental results) remains a challenge. In this paper we propose tackling this task by incorporating into IE information about rhetorical zones, i.e. classification of spans of text in terms of argumentation and intellectual attribution. As the first step towards this goal, we introduce a scheme for annotating biological texts for rhetorical zones and provide a qualitative and quantitative analysis of the data annotated according to this scheme. We also discuss our preliminary research on automatic zone analysis, and its incorporation into our IE framework.","The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( #AUTHOR_TAG ) , and question answering .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( #AUTHOR_TAG ) , and question answering .', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orasan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( #AUTHOR_TAG ) , and question answering .', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.']"
CC1599,W06-3309,Generative content models for structural analysis of medical abstracts,information needs in office practice are they being met,"['David G Covell', 'Gwen C Uman', 'Phil R Manning']",introduction,"We studied the self-reported information needs of 47 physicians during a half day of typical office practice. The physicians raised 269 questions about patient management. Questions related to all medical specialties and were highly specific to the individual patient's problem. Subspecialists most frequently asked questions related to other subspecialties. Only 30% of physicians' information needs were met during the patient visit, usually by another physician or other health professional. Reasons print sources were not used included the age of textbooks in the office, poor organization of journal articles, inadequate indexing of books and drug information sources, lack of knowledge of an appropriate source, and the time required to find the desired information. Better methods are needed to provide answers to questions that arise in office practice.","The need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ) .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ) .', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['The need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ) .']"
CC1600,W06-3309,Generative content models for structural analysis of medical abstracts,using argumentation to retrieve articles with similar citations an inquiry into improving related articles search in the medline digital library,"['Imad Tbahriti', 'Christine Chichester', 'Fr´ed´erique Lisacek', 'Patrick Ruch']",introduction,"The aim of this study is to investigate the relationships between citations and the scientific argumentation found abstracts. We design a related article search task and observe how the argumentation can affect the search results. We extracted citation lists from a set of 3200 full-text papers originating from a narrow domain. In parallel, we recovered the corresponding MEDLINE records for analysis of the argumentative moves. Our argumentative model is founded on four classes: PURPOSE, METHODS, RESULTS and CONCLUSION. A Bayesian classifier trained on explicitly structured MEDLINE abstracts generates these argumentative categories. The categories are used to generate four different argumentative indexes. A fifth index contains the complete abstract, together with the title and the list of Medical Subject Headings (MeSH) terms. To appraise the relationship of the moves to the citations, the citation lists were used as the criteria for determining relatedness of articles, establishing a benchmark; it means that two articles are considered as ""related"" if they share a significant set of co-citations. Our results show that the average precision of queries with the PURPOSE and CONCLUSION features is the highest, while the precision of the RESULTS and METHODS features was relatively low. A linear weighting combination of the moves is proposed, which significantly improves retrieval of related articles.","The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( #AUTHOR_TAG ) , information extraction ( Mizuta et al. , 2005 ) , and question answering .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( #AUTHOR_TAG ) , information extraction ( Mizuta et al. , 2005 ) , and question answering .', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orasan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( #AUTHOR_TAG ) , information extraction ( Mizuta et al. , 2005 ) , and question answering .', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.']"
CC1601,W06-3309,Generative content models for structural analysis of medical abstracts,categorization of sentence types in medical abstracts,"['Larry McKnight', 'Padmini Srinivasan']",experiments,"This study evaluated the use of machine learning techniques in the classification of sentence type. 7253 structured abstracts and 204 unstructured abstracts of Randomized Controlled Trials from MedLINE were parsed into sentences and each sentence was labeled as one of four types (Introduction, Method, Result, or Conclusion). Support Vector Machine (SVM) and Linear Classifier models were generated and evaluated on cross-validated data. Treating sentences as a simple ""bag of words"", the SVM model had an average ROC area of 0.92. Adding a feature of relative sentence location improved performance markedly for some models and overall increasing the average ROC to 0.95. Linear classifier performance was significantly worse than the SVM in all datasets. Using the SVM model trained on structured abstracts to predict unstructured abstracts yielded performance similar to that of models trained with unstructured abstracts in 3 of the 4 types. We conclude that classification of sentence type seems feasible within the domain of RCT's. Identification of sentence types may be helpful for providing context to end users or other text summarization techniques.","The table also presents the closest comparable experimental results reported by #AUTHOR_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 .","['The results of our second set of experiments (with RCTs only) are shown in Tables 2(a) and 2(b). Table 2(a) reports the multi-way classification error rate; once again, applying the Markov assumption to model discourse transitions improves performance, and using LDA further reduces error rate.', 'Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table).', 'The table also presents the closest comparable experimental results reported by #AUTHOR_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 .', 'This collection has significantly more training examples than our corpus of 27,075 abstracts, which could be a source of performance differences.', 'Furthermore, details regarding their procedure for mapping structured abstract headings to one of the four general labels was not discussed in their paper.', 'Nevertheless, our HMM- based approach is at least competitive with SVMs, perhaps better in some cases.']",1,"['The table also presents the closest comparable experimental results reported by #AUTHOR_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 .']"
CC1602,W06-3309,Generative content models for structural analysis of medical abstracts,text generation using discourse strategies and focus constraints to generate natural language text,['Kathleen R McKeown'],related work,Preface Introduction 2. Discourse structure 3. Focusing in discourse 4. TEXT system implementation 5. Discourse history 6. Related generation research 7. Summary and conclusions Appendices Bibliography Index.,"Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ) .","['Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ) .', 'Our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts.']",1,"['Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ) .']"
CC1603,W06-3309,Generative content models for structural analysis of medical abstracts,catching the drift probabilistic content models with applications to generation and summarization,"['Regina Barzilay', 'Lillian Lee']",introduction,"We consider the problem of modeling the content structure of texts within a specic domain, in terms of the topics the texts address and the order in which these topics appear. We rst present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.","Building on the work of Ruch et al. ( 2003 ) in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( #AUTHOR_TAG ) .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. ( 2003 ) in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( #AUTHOR_TAG ) .', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'Building on the work of Ruch et al. ( 2003 ) in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( #AUTHOR_TAG ) .', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"
CC1604,W06-3309,Generative content models for structural analysis of medical abstracts,discoursal movements in medical english abstracts and their linguistic exponents a genre analysis study,['Franc¸oise Salanger-Meyer'],introduction,,"This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #AUTHOR_TAG ) .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #AUTHOR_TAG ) .', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orasan, 2001).', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #AUTHOR_TAG ) .']"
CC1605,W06-3309,Generative content models for structural analysis of medical abstracts,can primary care physicians’ questions be answered using the medical journal literature,"['Paul N Gorman', 'Joan S Ash', 'Leslie W Wykoff']",introduction,"Medical librarians and informatics professionals believe the medical journal literature can be useful in clinical practice, but evidence suggests that practicing physicians do not share this belief. The authors designed a study to determine whether a random sample of ""native"" questions asked by primary care practitioners could be answered using the journal literature. Participants included forty-nine active, nonacademic primary care physicians providing ambulatory care in rural and nonrural Oregon, and seven medical librarians. The study was conducted in three stages: (1) office interviews with physicians to record clinical questions; (2) online searches to locate answers to selected questions; and (3) clinician feedback regarding the relevance and usefulness of the information retrieved. Of 295 questions recorded during forty-nine interviews, 60 questions were selected at random for searches. The average total time spent searching for and selecting articles for each question was forty-three minutes. The average cost per question searched was $27.37. Clinician feedback was received for 48 of 56 questions (four physicians could not be located, so their questions were not used in tabulating the results). For 28 questions (56%), clinicians judged the material relevant; for 22 questions (46%) the information provided a ""clear answer"" to their question. They expected the information would have had an impact on their patient in nineteen (40%) cases, and an impact on themselves or their practice in twenty-four (51%) cases. If the results can be generalized, and if the time and cost of performing searches can be reduced, increased use of the journal literature could significantly improve the extent to which primary care physicians' information needs are met.","The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ) .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ) .', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ) .', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments.""]"
CC1606,W06-3309,Generative content models for structural analysis of medical abstracts,the interaction of domain knowledge and linguistic structure in natural language processing interpreting hypernymic propositions in biomedical text,"['Thomas C Rindflesch', 'Marcelo Fiszman']",introduction,"Interpretation of semantic propositions in free-text documents such as MEDLINE citations would provide valuable support for biomedical applications, and several approaches to semantic interpretation are being pursued in the biomedical informatics community. In this paper, we describe a methodology for interpreting linguistic structures that encode hypernymic propositions, in which a more specific concept is in a taxonomic relationship with a more general concept. In order to effectively process these constructions, we exploit underspecified syntactic analysis and structured domain knowledge from the Unified Medical Language System (UMLS). After introducing the syntactic processing on which our system depends, we focus on the UMLS knowledge that supports interpretation of hypernymic propositions. We first use semantic groups from the Semantic Network to ensure that the two concepts involved are compatible; hierarchical information in the Metathesaurus then determines which concept is more general and which more specific. A preliminary evaluation of a sample based on the semantic group Chemicals and Drugs provides 83% precision. An error analysis was conducted and potential solutions to the problems encountered are presented. The research discussed here serves as a paradigm for investigating the interaction between domain knowledge and linguistic structure in natural language processing, and could also make a contribution to research on automatic processing of discourse structure. Additional implications of the system we present include its integration in advanced semantic interpretation processors for biomedical text and its use for information extraction in specific domains. The approach has the potential to support a range of applications, including information retrieval and ontology engineering.","Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .']"
CC1607,W06-3309,Generative content models for structural analysis of medical abstracts,catching the drift probabilistic content models with applications to generation and summarization,"['Regina Barzilay', 'Lillian Lee']",related work,"We consider the problem of modeling the content structure of texts within a specic domain, in terms of the topics the texts address and the order in which these topics appear. We rst present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.","Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .","['Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .', 'However, our study differs in several important respects.', 'Barzilay and Lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.', 'In contrast, because the discourse structure of medical abstracts is welldefined and training data is relatively easy to obtain, we were able to apply a supervised approach.', 'Whereas Barzilay and Lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task.', 'Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here.']",1,"['Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .']"
CC1608,W06-3309,Generative content models for structural analysis of medical abstracts,cuhtk conversational telephone speech transcription system,"['Gunnar Evermann', 'H Y Chan', 'Mark J F Gales', 'Thomas Hain', 'Xunying Liu', 'David Mrva', 'Lan Wang', 'Phil Woodland']",conclusion,,"Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .","['An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004).', 'This technique provides two important advantages.', 'First, Gaussian modeling adds an extra degree of freedom during training, by capturing second-order statistics.', 'This is not possible when modeling word sequences, where only the probability of a sentence is actually used in the HMM training.', 'Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .']",0,"['Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .']"
CC1609,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,junior science book of,['Nancy Larrick'],experiments,"This study aimed to describe the validity of SETS-oriented science book for junior high school students in environmental pollutions materials. In this research, researcher uses a quantitative descriptive method and uses validation sheet in the form of questionnaire as data-collection techniques. The questionnaire contains the criteria of material feasibility, language, and presentation. The result of validation gave the material feasibility score 3.9, for the language is 3.6, and the presentation criteria scored 3.9. So, it can be concluded that a science book with SETS-oriented is very well to be used as a learning facility according to the assessment of the validator","We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .","['We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .', 'The text gradually introduces concepts related to precipitation, and explains them.', 'Its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'The system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text.']",5,"['We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .', 'The text gradually introduces concepts related to precipitation, and explains them.', 'Its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'The system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text.']"
CC1610,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,semantic interpretation of nominalizations,"['Richard D Hull', 'Fernando Gomez']",related work,"A computational approach to the semantic interpretation of nominalizations is described. Interpretation of nominalizations involves three tasks: deciding whether the nominalization is being used in a verbal or non-verbal sense; disambiguating the nominalized verb when a verbal sense is used; and determining the fillers of the thematic roles of the verbal concept or predicate of the nominalization. A verbal sense can be recognized by the presence of modifiers that represent the arguments of the verbal concept. It is these same modifiers which provide the semantic clues to disambiguate the nominalized verb. In the absence of explicit modifiers, heuristics are used to discriminate between verbal and nonverbal senses. A correspondence between verbs and their nominalizations is exploited so that only a small amount of additional knowledge is needed to handle the nominal form. These methods are tested in the domain of encyclopedic texts and the results are shown.","Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .","['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']",0,"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .']"
CC1611,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,pattern matching for case analysis a computational definition of closeness,"['Sylvain Delisle', 'Terry Copeck', 'Stan Szpakowicz', 'Ken Barker']",,Proposes a conceptually and technically neat method to identify known semantic patterns close to a novel pattern. This occurs in the context of a system to acquire knowledge incrementally from systematically processed expository technical text. This semi-automatic system requires the user to respond to specific multiple-choice questions about the current sentence. The questions are prepared from linguistic elements previously encountered in the text similar to elements in the new sentence. We present a metric to characterize the similarity between semantic case patterns. The computation is based on syntactic indicators of semantic relations and is defined in terms of symbolic pattern matching.>,"This idea was inspired by #AUTHOR_TAG , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) .","['To process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P .', ""This idea was inspired by #AUTHOR_TAG , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) ."", 'In recent approaches, syntactic information is translated into features which, together with information from FrameNet, WordNet or VerbNet, will be used with ML tools to make predictions for each example in the test set (Carreras and Marquez, 2004;Carreras and Marquez, 2005).']",4,"['To process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P .', ""This idea was inspired by #AUTHOR_TAG , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) ."", 'In recent approaches, syntactic information is translated into features which, together with information from FrameNet, WordNet or VerbNet, will be used with ML tools to make predictions for each example in the test set (Carreras and Marquez, 2004;Carreras and Marquez, 2005).']"
CC1612,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,classbased construction of a verb lexicon in,"['Karin Kipper', 'Hoa Trang Dang', 'Martha Palmer']",related work,,"Most approaches rely on VerbNet ( #AUTHOR_TAG ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 ) .","['In current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet ( #AUTHOR_TAG ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 ) .']",0,"['Most approaches rely on VerbNet ( #AUTHOR_TAG ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 ) .']"
CC1613,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,systematic construction of a versatile case system,"['Ken Barker', 'Terry Copeck', 'Sylvain Delisle', 'Stan Szpakowicz']",introduction,"Case systems abound in natural language processing. Almost any attempt to recognize and uniformly represent relationships within a clause - a unit at the centre of any linguistic system that goes beyond word level statistics - must be based on semantic roles drawn from a small, closed set. The set of roles describing relationships between a verb and its arguments within a clause is a case system. What is required of such a case system? How does a natural language practitioner build a system that is complete and detailed yet practical and natural? This paper chronicles the construction of a case system from its origin in English marker words to its successful application in the analysis of English text.","The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent ( #AUTHOR_TAGa ) .","['We work with sentences, clauses, phrases and words, using syntactic structures generated by a parser.', 'Our system incrementally processes a text, and extracts pairs of text units: two clauses, a verb and each of its arguments, a noun and each of its modifiers.', 'For each pair of units, the system builds a syntactic graph surrounding the main element (main clause, head verb, head noun).', 'It then tries to find among the previously processed instances another main element with a matching syntactic graph.', 'If such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph.', 'We have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases.', 'The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent ( #AUTHOR_TAGa ) .']",4,"['The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent ( #AUTHOR_TAGa ) .']"
CC1614,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,classifying the semantic relations in nouncompounds via a domain specific hierarchy,"['Barbara Rosario', 'Marti Hearst']",related work,"We are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems). In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds. We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves.","In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #AUTHOR_TAG ) or the system ( Gomez , 1998 ) .","['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #AUTHOR_TAG ) or the system ( Gomez , 1998 ) .', 'Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)).', 'Lists of semantic relations are designed to capture salient domain information.']",0,"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #AUTHOR_TAG ) or the system ( Gomez , 1998 ) .', 'Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)).', 'Lists of semantic relations are designed to capture salient domain information.']"
CC1615,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,automatic labeling of semantic roles,"['Daniel Gildea', 'Daniel Jurafsky']",related work,"We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data.","Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .","['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']",0,"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']"
CC1616,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,building concept reprezentations from reusable components,"['Peter Clark', 'Bruce Porter']",related work,"Our goal is to build knowledge-based systems capable of answering a wide variety of questions, including questions that are unanticipated when the knowledge base is built. For systems to achieve this level of competence and generality, they require the ability to dynamically construct new concept representations, and to do so in response to the questions arLd tasks posed to them. Our approach to meeting this requirement is to build knowledge bases of generalized, representational components, and to develop methods for automatically composing components on demand. This work extends the normal inheritance approach used in frame-based systems, and imports ideas from several different areas of AI, in particular compositional modeling, terminological reasoning, and ontological engineering. The contribution of this work is a novel integration of these methods that improves the efficiency of building knowledge bases and the robustness of using them.","It helps them build complex knowledge bases by combining components : events , entities and modifiers ( #AUTHOR_TAG ) .","['In the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts.', 'It helps them build complex knowledge bases by combining components : events , entities and modifiers ( #AUTHOR_TAG ) .', ""The system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary.""]",0,"['It helps them build complex knowledge bases by combining components : events , entities and modifiers ( #AUTHOR_TAG ) .']"
CC1617,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,a representation of complex events and processes for the acquisition of knowledge from text,['Fernando Gomez'],related work,,"In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( #AUTHOR_TAG ) .","['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( #AUTHOR_TAG ) .', 'Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)).', 'Lists of semantic relations are designed to capture salient domain information.']",0,"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( #AUTHOR_TAG ) .', 'Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)).', 'Lists of semantic relations are designed to capture salient domain information.']"
CC1618,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,the descriptive technique of panini,['Vidya Niwas Misra'],introduction,,He was a grammarian who analysed Sanskrit ( #AUTHOR_TAG ) .,"['When analysing texts, it is essential to see how elements of meaning are interconnected.', 'This is an old idea.', 'The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1 .', 'He was a grammarian who analysed Sanskrit ( #AUTHOR_TAG ) .', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965; Fill- more, 1968).', 'Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998; Kipper et al., 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al., 2001; Shi and Mihalcea, 2005).']",0,"['When analysing texts, it is essential to see how elements of meaning are interconnected.', 'This is an old idea.', 'The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1 .', 'He was a grammarian who analysed Sanskrit ( #AUTHOR_TAG ) .', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965; Fill- more, 1968).', 'Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998; Kipper et al., 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al., 2001; Shi and Mihalcea, 2005).']"
CC1619,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,semantic role labelling using different syntactic views,"['Sameer Pradhan', 'Wayne Ward', 'Kadri Hacioglu', 'James H Martin', 'Daniel Jurafsky']",related work,"Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels. In this paper we present a state-of-the-art baseline semantic role labeling system based on Support Vector Machine classifiers. We show improvements on this system by: i) adding new features including features extracted from dependency parses, ii) performing feature selection and calibration and iii) combining parses obtained from semantic parsers trained using different syntactic views. Error analysis of the baseline system showed that approximately half of the argument identification errors resulted from parse errors in which there was no syntactic constituent that aligned with the correct argument. In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements.","Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 ) .","['In current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 ) .']",0,"['In current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 ) .']"
CC1620,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,remarks on nominalizations,['Noam Chomsky'],introduction,This article deals with French nouns derived from non-stative verbs which nevertheless systematically exhibit a stative interpretation in some of their uses e.g. emprisonement 'action of putting sdy in jail' vs. 'state of being jailed,"This idea was expanded to include nouns and their modifiers through verb nominalizations ( #AUTHOR_TAG ; Quirk et al. , 1985 ) .","['In this work we pursue a well-known and often tacitly assumed line of thinking: connections at the syntactic level reflect connections at the semantic level (in other words, syntax carries meaning).', 'Anecdotal support for this stance comes from the fact that the grammatical notion of case is the basis for semantic relations (Misra, 1966;Gruber, 1965;Fillmore, 1968).', 'Tesnière (1959), who proposes a grouping of verb arguments into actants and circumstances, gives a set of rules to connect specific types of actants -for example, agent or instrument -to such grammatical elements as subject, direct object, indirect object.', 'This idea was expanded to include nouns and their modifiers through verb nominalizations ( #AUTHOR_TAG ; Quirk et al. , 1985 ) .']",0,"['This idea was expanded to include nouns and their modifiers through verb nominalizations ( #AUTHOR_TAG ; Quirk et al. , 1985 ) .']"
CC1621,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,putting pieces together combining framenet verbnet and wordnet for robust semantic parsing,"['Lei Shi', 'Rada Mihalcea']",related work,"This paper describes our work in integrating three different lexical resources: FrameNet, VerbNet, and WordNet, into a unified, richer knowledge-base, to the end of enabling more robust semantic parsing. The construction of each of these lexical resources has required many years of laborious human effort, and they all have their strengths and shortcomings. By linking them together, we build an improved resource in which (1) the coverage of FrameNet is extended, (2) the VerbNet lexicon is augmented with frame semantics, and (3) selectional restrictions are implemented using WordNet semantic classes. The synergistic exploitation of various lexical resources is crucial for many complex language processing applications, and we prove it once again effective in building a robust semantic parser.","Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG ) .","['In current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG ) .']",0,"['Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG ) .']"
CC1622,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,realistic parsing practical solutions of difficult problems,"['Sylvain Delisle', 'Stan Szpakowicz']",experiments,"This paper describes work on the linguistic analysis of texts within a project devoted to knowledge acquisition from text. We focus on syntactic processing and present some key elements of the project's parser that allow it to deal successfully with technical texts. The parser is fully implemented and tested on a variety of real texts; improvements and enhancements are in progress. Because our knowledge acquisition method assumes no a priori model of the domain of the source text, the parser relies as much as possible on lexical and syntactic clues. That is why it strives for full syntactic analysis rather than some form of text skimming. We present a practical approach to four acknowledged difficult problems which to date have no generally acceptable answers: phrase attachment; time constraints for problematic input (how to avoid long and unproductive computation); parsing conjoined structures (how to preserve broad coverage without losing control of the parsing process); and the treatment of fragmentary input or fragments that are a by-product of a fallback parsing strategy. We review recent related work and conclude by listing several future work items.","The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT ( #AUTHOR_TAG ) .","['The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT ( #AUTHOR_TAG ) .', 'The parser, written in Prolog, implements a classic constituency English grammar from Quirk et al. (1985).', 'Pairs of syntactic units connected by grammatical relations are extracted from the parse trees.', 'A dependency parser would produce a similar output, but DIPETT also provides verb subcategorization information (such as, for example, subject-verb-object or subject-verb-objectindirect object), which we use to select the (best) matching syntactic structures.']",5,"['The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT ( #AUTHOR_TAG ) .']"
CC1623,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,the descent of hierarchy and selection in relational semantics,"['Barbara Rosario', 'Marti Hearst', 'Charles Fillmore']",related work,"In many types of technical texts, meaning is embedded in noun compounds. A language understanding program needs to be able to interpret these in order to ascertain sentence meaning. We explore the possibility of using an existing lexical hierarchy for the purpose of placing words from a noun compound into categories, and then using this category membership to determine the relation that holds between the nouns. In this paper we present the results of an analysis of this method on two-word noun compounds from the biomedical domain, obtaining classification accuracy of approximately 90%. Since lexical hierarchies are not necessarily ideally suited for this task, we also pose the question: how far down the hierarchy must the algorithm descend before all the terms within the subhierarchy behave uniformly with respect to the semantic relation in question? We find that the topmost levels of the hierarchy yield an accurate classification, thus providing an economic way of assigning relations to noun compounds.","Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ) .","['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']",0,"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']"
CC1624,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,studies in lexical relations,['Jeffrey Gruber'],introduction,,"The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ) .","['When analysing texts, it is essential to see how elements of meaning are interconnected.', 'This is an old idea.', 'The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5 th century B.C. and the work of Panini 1 .', 'He was a grammarian who analysed Sanskrit (Misra, 1966).', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ) .', 'Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005).']",0,"['The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ) .']"
CC1625,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,the case for case,['Charles Fillmore'],introduction,"This article puts forward an economic case for Scotland staying in the union. There have been many debates regarding the economic consequences of independence. It has been argued that Scotland would be better off. Independence, however, is an uncertain business; a new state might gain new freedoms but would lose present sources of stability, and some questions about independence are simply unanswerable in advance. It is nevertheless possible to draw some conclusions about its possible economic effects","The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ) .","['When analysing texts, it is essential to see how elements of meaning are interconnected.', 'This is an old idea.', 'The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5 th century B.C. and the work of Panini 1 .', 'He was a grammarian who analysed Sanskrit (Misra, 1966).', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ) .', 'Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005).']",0,"['This is an old idea.', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ) .']"
CC1626,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,systematic construction of a versatile case system,"['Ken Barker', 'Terry Copeck', 'Sylvain Delisle', 'Stan Szpakowicz']",,"Case systems abound in natural language processing. Almost any attempt to recognize and uniformly represent relationships within a clause - a unit at the centre of any linguistic system that goes beyond word level statistics - must be based on semantic roles drawn from a small, closed set. The set of roles describing relationships between a verb and its arguments within a clause is a case system. What is required of such a case system? How does a natural language practitioner build a system that is complete and detailed yet practical and natural? This paper chronicles the construction of a case system from its origin in English marker words to its successful application in the analysis of English text.",This design idea was adopted from TANKA ( #AUTHOR_TAGb ) .,"['Our system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text.', 'This design idea was adopted from TANKA ( #AUTHOR_TAGb ) .', 'The only manually encoded knowledge is a dictionary of markers (subordinators, coordinators, prepositions).', 'This resource does not affect the syntacticsemantic graph-matching heuristic.']",5,"['Our system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text.', 'This design idea was adopted from TANKA ( #AUTHOR_TAGb ) .', 'This resource does not affect the syntacticsemantic graph-matching heuristic.']"
CC1627,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,framenet and lexicographic relevance,"['Charles Fillmore', 'Beryl T Atkins']",related work,,"Such systems extract information from some types of syntactic units ( clauses in ( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .","['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']",0,"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']"
CC1628,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,the berkeley framenet project,"['Collin F Baker', 'Charles J Fillmore', 'John B Lowe']",related work,"FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year #NSF IRI-9618838, #Tools for Lexicon Building&quot;#. The project&apos;s key features are #a# a commitment to corpus evidence for semantic and syntactic generalizations, and #b# the representation of the valences of its target words #mostly nouns, adjectives, and verbs# in which the semantic portion makes use of frame semantics. The resulting database will contain #a# descriptions of the semantic frames underlying the meanings of the words described, and #b# the valence representation #semantic and syntactic# of several thousand words and phrases, each accompanied by #c# a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between #frame elements&quot; and their syntactic realizations #e.g. grammatical function, phrase type, and other syntactic traits#. This report will present the project&apos;s goals and workflow, and information about the computational tools that have been adapted or created in-house for this work",Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #AUTHOR_TAG ) .,"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #AUTHOR_TAG ) .', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)).', 'Lists of semantic relations are designed to capture salient domain information.']",0,"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #AUTHOR_TAG ) .', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)).', 'Lists of semantic relations are designed to capture salient domain information.']"
CC1629,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,systematic construction of a versatile case system,"['Ken Barker', 'Terry Copeck', 'Sylvain Delisle', 'Stan Szpakowicz']",experiments,"Case systems abound in natural language processing. Almost any attempt to recognize and uniformly represent relationships within a clause - a unit at the centre of any linguistic system that goes beyond word level statistics - must be based on semantic roles drawn from a small, closed set. The set of roles describing relationships between a verb and its arguments within a clause is a case system. What is required of such a case system? How does a natural language practitioner build a system that is complete and detailed yet practical and natural? This paper chronicles the construction of a case system from its origin in English marker words to its successful application in the analysis of English text.",The list of semantic relations with which we work is based on extensive literature study ( #AUTHOR_TAGa ) .,"['The list of semantic relations with which we work is based on extensive literature study ( #AUTHOR_TAGa ) .', 'Three lists of relations for three syntactic levels -inter-clause, intra-clause (case) and nounmodifier relations -were next combined based on syntactic and semantic phenomena.', 'The resulting list is the one used in the experiments we present in this paper.', 'The relations are grouped by general similarity into 6 relation classes (H denotes the head of a base NP, M denotes the modifier).', 'There is no consensus in the literature on a list of semantic relations that would work in all situations.', 'This is, no doubt, because a general list of relations such as the one we use would not be appropriate for the semantic analysis of texts in a specific domain, such as for example medical texts.', 'All the relations in the list we use were necessary, and sufficient, for the analysis of the input text.']",5,"['The list of semantic relations with which we work is based on extensive literature study ( #AUTHOR_TAGa ) .', 'The resulting list is the one used in the experiments we present in this paper.', 'The relations are grouped by general similarity into 6 relation classes (H denotes the head of a base NP, M denotes the modifier).', 'There is no consensus in the literature on a list of semantic relations that would work in all situations.', 'All the relations in the list we use were necessary, and sufficient, for the analysis of the input text.']"
CC1630,W10-1710,Anaphora Models and Reordering for Phrase-Based SMT,chunkbased verb reordering in vso sentences for arabicenglish statistical machine translation,"['Arianna Bisazza', 'Marcello Federico']",conclusion,"In Arabic-to-English phrase-based statistical machine translation, a large number of syntactic disfluencies are due to wrong long-range reordering of the verb in VSO sentences, where the verb is anticipated with respect to the English word order. In this paper, we propose a chunk-based reordering technique to automatically detect and displace clause-initial verbs in the Arabic side of a word-aligned parallel corpus. This method is applied to preprocess the training data, and to collect statistics about verb movements. From this analysis, specific verb reordering lattices are then built on the test sentences before decoding them. The application of our reordering methods on the training and test sets results in consistent BLEU score improvements on the NIST-MT 2009 Arabic-English benchmark.","Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #AUTHOR_TAG ) , we tried to adapt the same approach to the German-English language pair .","['Word reordering between German and English is a complex problem.', 'Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #AUTHOR_TAG ) , we tried to adapt the same approach to the German-English language pair .', 'It turned out that there is a larger variety of long reordering patterns in this case.', 'Nevertheless, some experiments performed after the official evaluation showed promising results.', 'We plan to pursue this work in several directions: Defining a lattice weighting scheme that distinguishes between original word order and reordering paths could help the decoder select the more promising path through the lattice.', 'Applying similar reordering rules to the training corpus would reduce the mismatch between the training data and the reordered input sentences.', 'Finally, it would be useful to explore the impact of different distortion limits on the decoding of reordering lattices in order to find an optimal trade-off between decoderdriven short-range and lattice-driven long-range reordering.']",4,"['Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #AUTHOR_TAG ) , we tried to adapt the same approach to the German-English language pair .']"
CC1631,W10-3814,New Parameterizations and Features for PSCFG-Based Machine Translation,a discriminative latent variable model for statistical machine translation,"['Phil Blunsom', 'Trevor Cohn', 'Miles Osborne']",conclusion,"Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems.We argue that a principle reason for this failure is not dealing with multiple, equivalent translations. We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised. Results show that accounting for multiple derivations does indeed improve performance. Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions. (c) 2008 Association for Computational Linguistics","Future research should apply the work of #AUTHOR_TAG and Blunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .","['Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.', 'This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension.', 'Future research should apply the work of #AUTHOR_TAG and Blunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']",3,"['Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.', 'This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension.', 'Future research should apply the work of #AUTHOR_TAG and Blunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']"
CC1632,W10-3814,New Parameterizations and Features for PSCFG-Based Machine Translation,probabilistic inference for machine translation,"['Phil Blunsom', 'Miles Osborne']",conclusion,"Recent advances in statistical machine translation (SMT) have used dynamic programming  (DP) based beam search methods for approximate inference within probabilistic  translation models. Despite their success, these methods compromise the probabilistic  interpretation of the underlying model thus limiting the application of probabilistically  defined decision rules during training and decoding.  As an alternative, in this thesis, we propose a novel Monte Carlo sampling approach  for theoretically sound approximate probabilistic inference within these models. The  distribution we are interested in is the conditional distribution of a log-linear translation  model; however, often, there is no tractable way of computing the normalisation term  of the model. Instead, a Gibbs sampling approach for phrase-based machine translation  models is developed which obviates the need of computing this term yet produces  samples from the required distribution.  We establish that the sampler effectively explores the distribution defined by a  phrase-based models by showing that it converges in a reasonable amount of time to  the desired distribution, irrespective of initialisation. Empirical evidence is provided to  confirm that the sampler can provide accurate estimates of expectations of functions of  interest. The mix of high probability and low probability derivations obtained through  sampling is shown to provide a more accurate estimate of expectations than merely  using the n-most highly probable derivations.  Subsequently, we show that the sampler provides a tractable solution for finding the  maximum probability translation in the model. We also present a unified approach to  approximating two additional intractable problems: minimum risk training and minimum  Bayes risk decoding. Key to our approach is the use of the sampler which  allows us to explore the entire probability distribution and maintain a strict probabilistic  formulation through the translation pipeline. For these tasks, sampling allies  the simplicity of n-best list approaches with the extended view of the distribution that  lattice-based approaches benefit from, while avoiding the biases associated with beam  search. Our approach is theoretically well-motivated and can give better and more  stable results than current state of the art methods","Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .","['Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.', 'This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension.', 'Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']",3,"['Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.', 'Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']"
CC1633,W11-1402,Understanding Differences in Perceived Peer-Review Helpfulness using Natural Language Processing,automatically predicting peerreview helpfulness,"['Wenting Xiong', 'Diane Litman']",introduction,"Identifying peer-review helpfulness is an important task for improving the quality of feedback that students receive from their peers. As a first step towards enhancing existing peer-review systems with new functionality based on helpfulness detection, we examine whether standard product review analysis techniques also apply to our new context of peer reviews. In addition, we investigate the utility of incorporating additional specialized features tailored to peer review. Our preliminary results show that the structural features, review uni-grams and meta-data combined are useful in modeling the helpfulness of both peer reviews and product reviews, while peer-review specific auxiliary features can further improve helpfulness prediction.","In our prior work ( #AUTHOR_TAG ) , we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review .","['In our prior work ( #AUTHOR_TAG ) , we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review .', 'While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness 1 , there are other types of helpfulness rating (e.g.', 'author perceived helpfulness) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model.', 'In fact, we observe that peer-review helpfulness seems to differ not only between students and experts (example 1), but also between types of experts (example 2).']",2,"['In our prior work ( #AUTHOR_TAG ) , we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review .', 'While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness 1 , there are other types of helpfulness rating (e.g.']"
CC1634,W11-1402,Understanding Differences in Perceived Peer-Review Helpfulness using Natural Language Processing,automatically predicting peerreview helpfulness,"['Wenting Xiong', 'Diane Litman']",experiments,"Identifying peer-review helpfulness is an important task for improving the quality of feedback that students receive from their peers. As a first step towards enhancing existing peer-review systems with new functionality based on helpfulness detection, we examine whether standard product review analysis techniques also apply to our new context of peer reviews. In addition, we investigate the utility of incorporating additional specialized features tailored to peer review. Our preliminary results show that the structural features, review uni-grams and meta-data combined are useful in modeling the helpfulness of both peer reviews and product reviews, while peer-review specific auxiliary features can further improve helpfulness prediction.",( Details of how the average-expert model performs can be found in our prior work ( #AUTHOR_TAG ) . ),"['10 Both algorithms are provided by Weka (http://www.cs.waikato.ac.nz/ml/weka/).', 'provide complementary perspectives.', 'While the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features.', 'To compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below.', 'Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings.', '( Details of how the average-expert model performs can be found in our prior work ( #AUTHOR_TAG ) . )']",2,"['To compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below.', 'Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings.', '( Details of how the average-expert model performs can be found in our prior work ( #AUTHOR_TAG ) . )']"
CC1635,W11-1410,"Review of Leacock, Chodorow, Gamon & Tetreault (2014): Automated Grammatical Error Detection for Language Learners",building a korean web corpus for analyzing learner language,"['Markus Dickinson', 'Ross Israel', 'Sun-Hee Lee']",,"Post-positional particles are a significant source of errors for learners of Korean. Following methodology that has proven effective in handling English preposition errors, we are beginning the process of building a machine learner for particle error detection in L2 Korean writing. As a first step, however, we must acquire data, and thus we present a methodology for constructing large-scale corpora of Korean from the Web, exploring the feasibility of building corpora appropriate for a given topic and grammatical construction.","We follow our previous work ( #AUTHOR_TAG ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ) .","['In selecting features for Korean, we have to ac- count for relatively free word order (Chung et al., 2010).', 'We follow our previous work ( #AUTHOR_TAG ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ) .', 'Each word is broken down into: stem, affixes, stem POS, and affixes POS.', 'We also have features for the preceding and following noun and verb, thereby approximating relevant selectional properties.', 'Although these are relatively shallow features, they provide enough lexical and grammatical context to help select better or worse training data (section 3) and to provide a basis for a preliminary system (section 4).']",2,"['In selecting features for Korean, we have to ac- count for relatively free word order (Chung et al., 2010).', 'We follow our previous work ( #AUTHOR_TAG ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ) .']"
CC1636,W11-2020,Novel Approaches to Pattern-based Interaction Quality Modeling,on nomatchs noinputs and bargeins do nonacoustic features support anger detection,"['Alexander Schmitt', 'Tobias Heinroth', 'Jackson Liscombe']",,"Most studies on speech-based emotion recognition are based on prosodic and acoustic features, only employing artificial acted corpora where the results cannot be generalized to telephone-based speech applications. In contrast, we present an approach based on utterances from 1,911 calls from a deployed telephone-based speech application, taking advantage of additional dialogue features, NLU features and ASR features that are incorporated into the emotion recognition process. Depending on the task, non-acoustic features add 2.3% in classification accuracy compared to using only acoustic features.","The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #AUTHOR_TAG ) .","['The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #AUTHOR_TAG ) .', '(Schmitt et al., 2009).', 'From all 4,832 user turns, 68.5% were non-angry, 14.3% slightly angry, 5.0% very angry and 12.2% contained garbage, i.e. nonspeech events.', 'In total, the number of interaction parameters servings as input variables for the model amounts to 52.']",2,"['The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #AUTHOR_TAG ) .']"
CC1637,W14-1609,Lexicon Infused Phrase Embeddings for Named Entity Resolution,design challenges and misconceptions in named entity recognition,"['Lev Ratinov', 'Dan Roth']",introduction,"We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.","This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ) .","['One simple language model that discovers useful embeddings is known as Brown clustering (Brown et al., 1992).', 'A Brown clustering is a class-based bigram model in which (1) the probability of a document is the product of the probabilities of its bigrams, (2) the probability of each bigram is the product of the probability of a bigram model over latent classes and the probability of each class generating the actual word types in the bigram, and (3) each word type has non-zero probability only on a single class.', 'Given a one-toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus.', 'The Brown clustering algorithm works by starting with an initial assignment of word types to classes (which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus), and then iteratively selecting the pair of classes to merge that would lead to the highest post-merge log-likelihood, doing so until all classes have been merged.', 'This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ) .', 'There are other similar models of distributional clustering of English words which can be similarly effective (Pereira et al., 1993).']",4,"['This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ) .']"
CC1638,W14-1609,Lexicon Infused Phrase Embeddings for Named Entity Resolution,design challenges and misconceptions in named entity recognition,"['Lev Ratinov', 'Dan Roth']",introduction,"We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.","Following #AUTHOR_TAG , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document .","['Following #AUTHOR_TAG , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document .', 'Since, as seen in section 2.1, Brown clusters are hierarchical, we use features corresponding to prefixes of the path from the root to the leaf for each word type.']",5,"['Following #AUTHOR_TAG , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document .']"
CC1639,W14-1609,Lexicon Infused Phrase Embeddings for Named Entity Resolution,design challenges and misconceptions in named entity recognition,"['Lev Ratinov', 'Dan Roth']",introduction,"We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.",It is inspired by the system described in #AUTHOR_TAG .,"['In this section we describe in detail the baseline NER system we use.', 'It is inspired by the system described in #AUTHOR_TAG .', 'Because NER annotations are commonly not nested (for example, in the text ""the US Army"", ""US Army"" is treated as a single entity, instead of the location ""US"" and the organization ""US Army"") it is possible to treat NER as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity.', 'Following Ratinov and Roth (2009) we use the BILOU encoding, where each token can either BEGIN an entity, be INSIDE an entity, be the LAST token in an entity, be OUTSIDE an entity, or be the single UNIQUE token in an entity.']",4,['It is inspired by the system described in #AUTHOR_TAG .']
CC1640,W14-1619,Probabilistic Modeling of Joint-context in Distributional Similarity,learning syntactic categories using paradigmatic representations of word context,"['Mehmet Ali Yatbaz', 'Enis Sert', 'Deniz Yuret']",introduction,We investigate paradigmatic representations of word context in the domain of unsupervised syntactic category acquisition. Paradigmatic representations of word context are based on potential substitutes of a word in contrast to syntagmatic representations based on properties of neighboring words. We compare a bigram based baseline model with several paradigmatic models and demonstrate significant gains in accuracy. Our best model based on Euclidean co-occurrence embedding combines the paradigmatic context representation with morphological and orthographic features and achieves 80% many-to-one accuracy on a 45-tag 1M word corpus.,"This choice is inspired by recent work on learning syntactic categories ( #AUTHOR_TAG ) , which successfully utilized such language models to represent word window contexts of target words .","['We hypothesize that an underlying model that could capture syntagmatic patterns in large word contexts, yet is flexible enough to deal with data sparseness, is desired.', 'It is generally accepted that the semantics of verbs in particular are correlated with their syntagmatic properties (Levin, 1993;Hanks, 2013).', 'This provides grounds to expect that such model has the potential to excel for verbs.', 'To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme.', 'This choice is inspired by recent work on learning syntactic categories ( #AUTHOR_TAG ) , which successfully utilized such language models to represent word window contexts of target words .', 'However, we note that other richer types of language models, such as class-based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be seamlessly integrated into our scheme.']",4,"['This choice is inspired by recent work on learning syntactic categories ( #AUTHOR_TAG ) , which successfully utilized such language models to represent word window contexts of target words .']"
CC1641,W14-1815,Natural Language Generation with Vocabulary Constraints,learning to freestyle hip hop challengeresponse induction via transduction rule segmentation,"['Dekai Wu', 'Karteek Addanki', 'Markus Saers', 'Meriem Beloucif']",related work,"We present a novel model, Freestyle, that learns to improvise rhyming and fluent responses upon being challenged with a line of hip hop lyrics, by combining both bottomup token based rule induction and top-down rule segmentation strategies to learn a stochastic transduction grammar that simultaneously learns both phrasing and rhyming associations. In this attack on the woefully under-explored natural language genre of music lyrics, we exploit a strictly unsupervised transduction grammar induction approach. Our task is particularly ambitious in that no use of any a priori linguistic or phonetic information is allowed, even though the domain of hip hop lyrics is particularly noisy and unstructured. We evaluate the performance of the learned model against a model learned only using the more conventional bottom-up token based rule induction, and demonstrate the superiority of our combined token based and rule segmentation induction method toward generating higher quality improvised responses, measured on fluency and rhyming criteria as judged by human evaluators. To highlight some of the inherent challenges in adapting other algorithms to this novel task, we also compare the quality of the responses generated by our model to those generated by an out-ofthe-box phrase based SMT system. We tackle the challenge of selecting appropriate training data for our task via a dedicated rhyme scheme detection module, which is also acquired via unsupervised learning and report improved quality of the generated responses. Finally, we report results with Maghrebi French hip hop lyrics indicating that our model performs surprisingly well with no special adaptation to other languages.","Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( #AUTHOR_TAG ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .","['The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( #AUTHOR_TAG ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .', 'In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric.']",1,"['The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( #AUTHOR_TAG ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .']"
CC1642,W14-1815,Natural Language Generation with Vocabulary Constraints,generating diagnostic multiple choice comprehension cloze questions,"['Jack Mostow', 'Hyeju Jang']",related work,"This paper describes and evaluates DQGen, which automatically generates multiple choice cloze questions to test a child's comprehension while reading a given text. Unlike previous methods, it generates different types of distracters designed to diagnose different types of comprehension failure, and tests comprehension not only of an individual sentence but of the context that precedes it. We evaluate the quality of the overall questions and the individual distracters, according to 8 human judges blind to the correct answers and intended distracter types. The results, errors, and judges' comments reveal limitations and suggest how to address some of them.","Our motivation for generation of material for language education exists in work such as Sumita et al. ( 2005 ) and #AUTHOR_TAG , which deal with automatic generation of classic fill in the blank questions .","['Our motivation for generation of material for language education exists in work such as Sumita et al. ( 2005 ) and #AUTHOR_TAG , which deal with automatic generation of classic fill in the blank questions .', 'Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences.']",4,"['Our motivation for generation of material for language education exists in work such as Sumita et al. ( 2005 ) and #AUTHOR_TAG , which deal with automatic generation of classic fill in the blank questions .']"
CC1643,W14-1815,Natural Language Generation with Vocabulary Constraints,automatic analysis of rhythmic poetry with applications to generation and translation,"['Erica Greene', 'Tugba Bodrumlu', 'Kevin Knight']",related work,"We employ statistical methods to analyze, generate, and translate rhythmic poetry. We first apply unsupervised learning to reveal word-stress patterns in a corpus of raw poetry. We then use these word-stress patterns, in addition to rhyme and discourse models, to generate English love poetry. Finally, we translate Italian poetry into English, choosing target realizations that conform to desired rhythmic patterns.","Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #AUTHOR_TAG ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .","['The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #AUTHOR_TAG ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .', 'In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric.']",1,"['Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #AUTHOR_TAG ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .']"
CC1644,W14-1815,Natural Language Generation with Vocabulary Constraints,automatic generation of tamil lyrics for melodies,"['Ananth Ramakrishnan A', 'Sankar Kuppan', 'Sobha Lalitha Devi']",related work,"This paper presents our on-going work to automatically generate lyrics for a given melody, for phonetic languages such as Tamil. We approach the task of identifying the required syllable pattern for the lyric as a sequence labeling problem and hence use the popular CRF++ toolkit for learning. A corpus comprising of 10 melodies was used to train the system to understand the syllable patterns. The trained model is then used to guess the syllabic pattern for a new melody to produce an optimal sequence of syllables. This sequence is presented to the Sentence Generation module which uses the Dijkstra's shortest path algorithm to come up with a meaningful phrase matching the syllabic pattern.","Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan #AUTHOR_TAG ) , where specified meter or rhyme schemes are enforced .","['The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan #AUTHOR_TAG ) , where specified meter or rhyme schemes are enforced .', 'In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric.']",1,"['Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan #AUTHOR_TAG ) , where specified meter or rhyme schemes are enforced .']"
CC1645,W14-3902,Code Mixing: A Challenge for Language Identification in the Language of Social Media,dcusymantec at the wmt 2013 quality estimation shared task,"['Raphael Rubino', 'Joachim Wagner', 'Jennifer Foster', 'Johann Roturier', 'Rasoul Samad Zadeh Kaljahi', 'Fred Hollowood']",experiments,,"3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) .","['1. Char-n-grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers.', 'Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments.', '2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments.', '3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) .', 'We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized.']",2,"['Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments.', '3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) .']"
CC1646,W14-3902,Code Mixing: A Challenge for Language Identification in the Language of Social Media,dcu aspectbased polarity classification for semeval task 4,"['Joachim Wagner', 'Piyush Arora', 'Santiago Cortes', 'Utsab Barman', 'Dasha Bogdanova', 'Jennifer Foster', 'Lamia Tounsi']",experiments,"We describe the work carried out by DCU on the Aspect Based Sentiment Analysis task at SemEval 2014. Our team submitted one constrained run for the restaurant domain and one for the laptop domain for sub-task B (aspect term polarity prediction), ranking highest out of 36 systems on the restaurant test set and joint highest out of 32 systems on the laptop test set.","raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( J48 ) .","['1. Char-n-grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers.', 'Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments.', '2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments.', 'raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( J48 ) .', 'We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized.']",2,"['1. Char-n-grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers.', 'Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments.', 'raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( J48 ) .', '4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized.']"
CC1647,W98-1124,The Theory and Practice of Discourse Parsing and Summarization,from discourse structures to text summaries,['Daniel Marcu'],,We describe experiments that show that the concepts of rhetorical analysts and nucleanty can be used effectively for deternumng the most nnportant umts m a text We show how these concepts can be xmplemented and we discuss results that we obtained with a chscourse-based summanzatmn program,"The only disambiguation metric that we used in our previous work ( #AUTHOR_TAGb ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right .","['The shape-based metric.', ""The only disambiguation metric that we used in our previous work ( #AUTHOR_TAGb ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right ."", 'The explanation for this metric is that text processing is, essentially, a left-to-right process.', 'In many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text levels.', 'I The more text writers add, the more they elaborate on the text that went before: as a consequence, incremental discourse building consists mostly of expansion of the tight branches.', 'According to the shape-based metric, we consider that a discourse tree A is ""better"" than another discourse tree B if A is more skewed to the right than B (see Marcu (1997c) for a mathematical formulation of the notion of skewedness).']",2,"[""The only disambiguation metric that we used in our previous work ( #AUTHOR_TAGb ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right ."", 'The explanation for this metric is that text processing is, essentially, a left-to-right process.', 'In many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text levels.', 'I The more text writers add, the more they elaborate on the text that went before: as a consequence, incremental discourse building consists mostly of expansion of the tight branches.', 'According to the shape-based metric, we consider that a discourse tree A is ""better"" than another discourse tree B if A is more skewed to the right than B (see Marcu (1997c) for a mathematical formulation of the notion of skewedness).']"
CCT1,A00-1004,Automatic construction of parallel English-Chinese corpus for cross-language information retrieval,a program for aligning sentences in bilingual corpora,"['William A Gale', 'Kenneth W Church']",method,"Researchers in both machine translation (e.g., Brown et al. 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann 1990) have recently become interested in studying bilingual corpora, bodies of text such as the Canadian Hansards (parliamentary proceedings), which are available in multiple languages (such as French and English). One useful step is to align the sentences, that is, to identify correspondences between sentences in one language and sentences in the other language.This paper will describe a method and a program (align) for aligning sentences based on a simple statistical model of character lengths. The program uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences. A probabilistic score is assigned to each proposed correspondence of sentences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference. This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences.It is remarkable that such a simple approach works as well as it does. An evaluation was performed based on a trilingual corpus of economic reports issued by the Union Bank of Switzerland (UBS) in English, French, and German. The method correctly aligned all but 4% of the sentences. Moreover, it is possible to extract a large subcorpus that has a much smaller error rate. By selecting the best-scoring 80% of the alignments, the error rate is reduced from 4% to 0.7%. There were more errors on the English-French subcorpus than on the English-German subcorpus, showing that error rates will depend on the corpus considered; however, both were small enough to hope that the method will be useful for many language pairs.To further research on bilingual corpora, a much larger sample of Canadian Hansards (approximately 90 million words, half in English and and half in French) has been aligned with the align program and will be available through the Data Collection Initiative of the Association for Computational Linguistics (ACL/DCI). In addition, in order to facilitate replication of the align program, an appendix is provided with detailed c-code of the more difficult core of the align program.","A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .","['The parallel Web pages we collected from various sites are not all of the same quality.', 'Some are highly parallel and easy to align while others can be very noisy.', 'Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', 'The method we adopted is that of Simard et al. (1992).', 'Because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length-based methods.', 'Cognates are identical sequences of characters in corresponding words in two languages.', 'They are commonly found in English and French.', 'In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the HTML markup in both texts are taken as cognates.', 'Because the HTML structures of parallel pages are normally similar, the markup was found to be helpful for alignment.']",0,"['Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', 'In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the HTML markup in both texts are taken as cognates.']"
CCT2,A00-1004,Automatic construction of parallel English-Chinese corpus for cross-language information retrieval,aligning a parallel englishchinese corpus statistically with lexical criteria,['Dekai Wu'],introduction,We describe our experience with automatic alignment of sentences in parallel English-Chinese texts. Our report concerns three related topics: (1) progress on the HKUST English-Chinese Parallel Bilingual Corpus; (2) experiments addressing the applicability of Gale & Church's (1991) length-based statistical method to the task of alignment involving a non-Indo-European language; and (3) an improved statistical method that also incorporates domain-specific lexical cues.,"Only a few such corpora exist , including the Hansard English-French corpus and the HKUST EnglishChinese corpus ( #AUTHOR_TAG ) .","['However, a major obstacle to this approach is the lack of parallel corpora for model training.', 'Only a few such corpora exist , including the Hansard English-French corpus and the HKUST EnglishChinese corpus ( #AUTHOR_TAG ) .', 'In this paper, we will describe a method which automatically searches for parallel texts on the Web.', ""We will discuss the text mining algorithm we adopted, some issues in translation model training using the generated parallel corpus, and finally the translation model's performance in CLIR.""]",0,"['Only a few such corpora exist , including the Hansard English-French corpus and the HKUST EnglishChinese corpus ( #AUTHOR_TAG ) .']"
CCT3,A00-1004,Automatic construction of parallel English-Chinese corpus for cross-language information retrieval,aligning sentences in bilingual corpora using lexical information,['S F Chen'],method,"In this paper, we describe a fast algorithm for aligning sentences with their translations in a bilingual corpus. Existing efficient algorithms ignore word identities and only consider sentence length (Brown et al., 1991b; Gale and Church, 1991). Our algorithm constructs a simple statistical word-to-word translation model on the fly during alignment. We find the alignment that maximizes the probability of generating the corpus with this translation model. We have achieved an error rate of approximately 0.4% on Canadian Hansard data, which is a significant improvement over previous results. The algorithm is language independent.","A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ) .","['The parallel Web pages we collected from various sites are not all of the same quality.', 'Some are highly parallel and easy to align while others can be very noisy.', 'Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ) .', 'The method we adopted is that of Simard et al. (1992).', 'Because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length-based methods.', 'Cognates are identical sequences of characters in corresponding words in two languages.', 'They are commonly found in English and French.', 'In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the HTML markup in both texts are taken as cognates.', 'Because the HTML structures of parallel pages are normally similar, the markup was found to be helpful for alignment.']",0,"['Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ) .', 'In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the HTML markup in both texts are taken as cognates.']"
CCT4,A00-1004,Automatic construction of parallel English-Chinese corpus for cross-language information retrieval,aligning a parallel englishchinese corpus statistically with lexical criteria,['Dekai Wu'],method,We describe our experience with automatic alignment of sentences in parallel English-Chinese texts. Our report concerns three related topics: (1) progress on the HKUST English-Chinese Parallel Bilingual Corpus; (2) experiments addressing the applicability of Gale & Church's (1991) length-based statistical method to the task of alignment involving a non-Indo-European language; and (3) an improved statistical method that also incorporates domain-specific lexical cues.,"For example , it would be helpful to consider strong correspondence between certain English and Chinese words , as in ( #AUTHOR_TAG ) .","['Beside HTML markups, other criteria may also be incorporated.', 'For example , it would be helpful to consider strong correspondence between certain English and Chinese words , as in ( #AUTHOR_TAG ) .', 'We hope to implement such correspondences in our future research.']",3,"['For example , it would be helpful to consider strong correspondence between certain English and Chinese words , as in ( #AUTHOR_TAG ) .']"
CCT5,A00-1004,Automatic construction of parallel English-Chinese corpus for cross-language information retrieval,aligning sentences in parallel corpora,"['P F Brown', 'J C Lai', 'R L Mercer']",method,"In this paper we describe a statistical tech-nique for aligning sentences with their translations in two parallel corpora. In addition to certain anchor points that are available in our da.ta, the only information about the sentences that we use for calculating alignments i the number of tokens that they contain. Because we make no use of the lexical details of the sentence, the alignment com-putation is fast and therefore practical for appli-cation to very large collections of text. We have used this technique to align several million sen-tences in the English-French Hans~trd corpora nd have achieved an accuracy in excess of 99 % in a random selected set of 1000 sentence pairs that we checked by hand. We show that even without he benefit of anchor points the correlation between the lengths of aligned sentences i strong enough that we should expect o achieve an accuracy of between 96 % and 97%. Thus, the technique may be applicable to a wider variety of texts than we have yet tried","A number of alignment techniques have been proposed , varying from statistical methods ( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .","['The parallel Web pages we collected from various sites are not all of the same quality.', 'Some are highly parallel and easy to align while others can be very noisy.', 'Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'A number of alignment techniques have been proposed , varying from statistical methods ( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', 'The method we adopted is that of Simard et al. (1992).', 'Because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length-based methods.', 'Cognates are identical sequences of characters in corresponding words in two languages.', 'They are commonly found in English and French.', 'In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the HTML markup in both texts are taken as cognates.', 'Because the HTML structures of parallel pages are normally similar, the markup was found to be helpful for alignment.']",0,"['Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'A number of alignment techniques have been proposed , varying from statistical methods ( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', 'In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the HTML markup in both texts are taken as cognates.']"
CCT6,A00-1008,Plan-based dialogue management in a physics tutor,spelling correction using context,"['M A Elmi', 'M W Evens']",introduction,"In computing, spell checking is the process of detecting and sometimes providing spelling suggestions for incorrectly spelled words in a text. Basically, a spell checker is a computer program that uses a dictionary of words to perform spell checking. The bigger the dictionary is, the higher is the error detection rate. The fact that spell checkers are based on regular dictionaries, they suffer from data sparseness problem as they cannot capture large vocabulary of words including proper names, domain-specific terms, technical jargons, special acronyms, and terminologies. As a result, they exhibit low error detection rate and often fail to catch major errors in the text. This paper proposes a new context-sensitive spelling correction method for detecting and correcting non-word and real-word errors in digital text documents. The approach hinges around data statistics from Google Web 1T 5-gram data set which consists of a big volume of n-gram word sequences, extracted from the World Wide Web. Fundamentally, the proposed method comprises an error detector that detects misspellings, a candidate spellings generator based on a character 2-gram model that generates correction suggestions, and an error corrector that performs contextual error correction. Experiments conducted on a set of text documents from different domains and containing misspellings, showed an outstanding spelling error correction rate and a drastic reduction of both non-word and real-word errors. In a further study, the proposed algorithm is to be parallelized so as to lower the computational cost of the error detection and correction processes.Comment: LACSC - Lebanese Association for Computational Sciences -   http://www.lacsc.or",Robust natural language understanding in Atlas-Andes is provided by RosÃ© 's CARMEL system ( RosÃ© 2000 ) ; it uses the spelling correction algorithm devised by #AUTHOR_TAG .,"['The first system we have implemented with APE is a prototype Atlas-Andes system that replaces the hints usually given for an incorrect acceleration vector by a choice of generated subdialogues.', 'Figure 4 shows the architecture of Atlas-Andes; any other system built with APE would look similar.', ""Robust natural language understanding in Atlas-Andes is provided by RosÃ© 's CARMEL system ( RosÃ© 2000 ) ; it uses the spelling correction algorithm devised by #AUTHOR_TAG .""]",5,"[""Robust natural language understanding in Atlas-Andes is provided by RosÃ© 's CARMEL system ( RosÃ© 2000 ) ; it uses the spelling correction algorithm devised by #AUTHOR_TAG .""]"
CCT7,A00-1014,MIMIC,using dialogue representations for concepttospeech generation,"['Christine H Nakatani', 'Jennifer Chu-Carroll']",,"We present an implemented concept-to-speech (CTS) system that offers original proposals for certain couplings of dialogue computation with prosodic computation. Specifically, the semantic interpretation, task modeling and dialogue strategy modules in a working spoken dialogue system are used to generate prosodic features to better convey the meaning of system replies. The new CTS system embodies and extends theoretical work on intonational meaning in a more general, robust and rigorous way than earlier approaches, by reflecting compositional aspects of both dialogue and intonation interepretation in an original computational framework for prosodic generation.",2 See ( #AUTHOR_TAG ) for how MIMIC 's dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation .,"[""2 See ( #AUTHOR_TAG ) for how MIMIC 's dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation .""]",0,"[""2 See ( #AUTHOR_TAG ) for how MIMIC 's dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation .""]"
CCT8,A00-1014,MIMIC,the commandtalk spoken dialogue system,"['Amanda Stent', 'John Dowding', 'Jean Mark Gawron', 'Elizabeth Owen Bratt', 'Robert Moore']",,"CommandTalk (Moore et al., 1997) is a spokenlanguage interface to the ModSAF battlefield simulator that allows simulation operators to generate and execute military exercises by creating forces and control measures, assigning missions to forces, and controlling the display (Ceranowicz, 1994). CommandTalk consists of independent, cooperating agents interacting through SRI's Open Agent Architecture (OAA) (Martin et al., 1998). This architecture allows components to be developed independently, and then flexibly and dynamically combined to support distributed computation. Most of the agents that compose CommandTalk have been described elsewhere !for more detail, see (Moore et al., 1997)). This paper describes extensions to CommandTalk to support spoken dialogue. While we make no theoretical claims about the nature and structure of dialogue, we are influenced by the theoretical work of (Grosz and Sidner, 1986) and will use terminology from that tradition when appropriate. We also follow (Chu-Carroll and Brown, 1997) in distinguishing task initiative and dialogue initiative. Section 2 demonstrates the dialogue capabilities of CommandTalk by way of an extended example. Section 3 describes how language in CommandTalk is modeled for understanding and generation. Section 4 describes the architecture of the dialogue manager in detail. Section 5 compares CommandTalk with other spo-","The strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e.g. , ( Bennacef et at. , 1996 ; #AUTHOR_TAG ) ) .","['The strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e.g. , ( Bennacef et at. , 1996 ; #AUTHOR_TAG ) ) .', 'To instantiate an attribute, MIMIC adopts the lnfoSeek dialogue act to solicit the missing information.', 'In contrast, when MIMIC has both initiatives, it plays a more active role by presenting the user with additional information comprising valid instantiations of the attribute (GiveOptions).', 'Given an invalid query, MIMIC notifies the user of the failed query and provides an openended prompt when it only has dialogue initiative.', ""When MIMIC has both initiatives, however, in addition to No-tifyFailure, it suggests an alternative close to the user's original query and provides a limited prompt."", 'Finally, when MIMIC has neither initiative, it simply adopts No-tifyFailure, allowing the user to determine the next discourse goal.']",1,"['The strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e.g. , ( Bennacef et at. , 1996 ; #AUTHOR_TAG ) ) .']"
CCT9,A00-1014,MIMIC,mixed initiative in dialogue an investigation into discourse segmentation,"['Marilyn Walker', 'Steve Whittaker']",,"Conversation between two people is usually of mixed-initiative, with control over the conversation being transferred from one person to another. We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns. The application of the control rules lets us derive domain-independent discourse structures. The derived structures indicate that initiative plays a role in the structuring of discourse. In order to explore the relationship of control and initiative to discourse processes like centering, we analyze the distribution of four different classes of anaphora for two data sets. This distribution indicates that some control segments are hierarchically related to others. The analysis suggests that discourse participants often mutually agree to a change of topic. We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types. These differences can be explained in terms of collaborative planning principles.Comment: 8 pages, late","Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ) .","['Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ) .', 'Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'Since task initiative models contribution to domain/problemsolving goals, while dialogue initiative affects the cur-5An alternative strategy to step ( 4) is to perform a database lookup based on the ambiguous query and summarize the results (Litman et al., 1998), which we leave for future work.', 'rent discourse goal, we developed alternative strategies for achieving the goals in Figure 4 based on initiative distribution, as shown in Table 1.']",0,"['Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ) .', 'Since task initiative models contribution to domain/problemsolving goals, while dialogue initiative affects the cur-5An alternative strategy to step ( 4) is to perform a database lookup based on the ambiguous query and summarize the results (Litman et al., 1998), which we leave for future work.', 'rent discourse goal, we developed alternative strategies for achieving the goals in Figure 4 based on initiative distribution, as shown in Table 1.']"
CCT10,A00-1014,MIMIC,cues and control in expertclient dialogues,"['Steve Whittaker', 'Phil Stenton']",,,"Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ) .","['Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ) .', 'Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'Since task initiative models contribution to domain/problemsolving goals, while dialogue initiative affects the cur-5An alternative strategy to step ( 4) is to perform a database lookup based on the ambiguous query and summarize the results (Litman et al., 1998), which we leave for future work.', 'rent discourse goal, we developed alternative strategies for achieving the goals in Figure 4 based on initiative distribution, as shown in Table 1.']",0,"['Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ) .']"
CCT11,A00-1014,MIMIC,evaluating automatic dialogue strategy adaptation for a spoken dialogue system,"['Jennifer Chu-Carroll', 'Jill S Nickerson']",experiments,"In this paper, we describe an empirical evaluation of an adaptive mixed initiative spoken dialogue system. We conducted two sets of experiments to evaluate the mixed initiative and automatic adaptation aspects of the system, and analyzed the resulting dialogues along three dimensions: performance factors, discourse features, and initiative distribution. Our results show that 1) both the mixed initiative and automatic adaptation aspects led to better system performance in terms of user satisfaction and dialogue efficiency, and 2) the system's adaptation behavior better matched user expectations, more efficiently resolved dialogue anomalies, and resulted in higher overall dialogue quality.",A companion paper describes the evaluation process and results in further detail ( #AUTHOR_TAG ) .,"[""We conducted two experiments to evaluate MIMIC's automatic adaptation capabilities."", 'We compared MIMIC with two control systems: MIMIC-SI, a system-initiative version of MIMIC in which the system retains both initiatives throughout the dialogue, and MIMIC-MI, a nonadaptive mixed-initiative version of MIMIC that resembles the behavior of many existing dialogue systems.', 'In this section we summarize these experiments and their results.', 'A companion paper describes the evaluation process and results in further detail ( #AUTHOR_TAG ) .', 'Each experiment involved eight users interacting with MIMIC and MIMIC-SI or MIMIC-MI to perform a set of tasks, each requiring the user to obtain specific movie information.', 'User satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'Furthermore, a number of performance features, largely based on the PARADISE dialogue evaluation scheme (Walker et al., 1997), were automatically logged, derived, or manually annotated.', 'In addition, we logged the cues automatically detected in each user utterance, as well as the initiative distribution for each turn and the dialogue acts selected to generate each system response.']",2,"[""We conducted two experiments to evaluate MIMIC's automatic adaptation capabilities."", 'We compared MIMIC with two control systems: MIMIC-SI, a system-initiative version of MIMIC in which the system retains both initiatives throughout the dialogue, and MIMIC-MI, a nonadaptive mixed-initiative version of MIMIC that resembles the behavior of many existing dialogue systems.', 'In this section we summarize these experiments and their results.', 'A companion paper describes the evaluation process and results in further detail ( #AUTHOR_TAG ) .', 'Furthermore, a number of performance features, largely based on the PARADISE dialogue evaluation scheme (Walker et al., 1997), were automatically logged, derived, or manually annotated.', 'In addition, we logged the cues automatically detected in each user utterance, as well as the initiative distribution for each turn and the dialogue acts selected to generate each system response.']"
CCT12,A00-1014,MIMIC,evaluating response strategies in a webbased spoken dialogue agent,"['Diane J Litman', 'Shimei Pan', 'Marilyn A Walker']",,,"5An alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( #AUTHOR_TAG ) , which we leave for future work .","['5An alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( #AUTHOR_TAG ) , which we leave for future work .']",3,"['5An alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( #AUTHOR_TAG ) , which we leave for future work .']"
CCT13,A00-1014,MIMIC,paradise a framework for evaluating spoken dialogue agents,"['Marilyn A Walker', 'Diane J Litman', 'Candance A Kamm', 'Alicia Abella']",experiments,"This paper presents PARADISE (PARAdigm for DIalogue System Evaluation), a general framework for evaluating spoken dialogue agents. The framework decouples task requirements from an agent's dialogue behaviors, supports comparisons among dialogue strategies, enables the calculation of performance over subdialogues and whole dialogues, specifies the relative contribution of various factors to performance, and makes it possible to compare agents performing different tasks by normalizing for task complexity.Comment: 10 pages, uses aclap, psfig, lingmacros, time","Furthermore , a number of performance features , largely based on the PARADISE dialogue evaluation scheme ( #AUTHOR_TAG ) , were automatically logged , derived , or manually annotated .","[""We conducted two experiments to evaluate MIMIC's automatic adaptation capabilities."", 'We compared MIMIC with two control systems: MIMIC-SI, a system-initiative version of MIMIC in which the system retains both initiatives throughout the dialogue, and MIMIC-MI, a nonadaptive mixed-initiative version of MIMIC that resembles the behavior of many existing dialogue systems.', 'In this section we summarize these experiments and their results.', 'A companion paper describes the evaluation process and results in further detail (Chu-Carroll and Nickerson, 2000).', 'Each experiment involved eight users interacting with MIMIC and MIMIC-SI or MIMIC-MI to perform a set of tasks, each requiring the user to obtain specific movie information.', 'User satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'Furthermore , a number of performance features , largely based on the PARADISE dialogue evaluation scheme ( #AUTHOR_TAG ) , were automatically logged , derived , or manually annotated .', 'In addition, we logged the cues automatically detected in each user utterance, as well as the initiative distribution for each turn and the dialogue acts selected to generate each system response.']",5,"['Furthermore , a number of performance features , largely based on the PARADISE dialogue evaluation scheme ( #AUTHOR_TAG ) , were automatically logged , derived , or manually annotated .']"
CCT14,A00-1015,Javox,natural language access to software applications,"['Paul Schmidt', 'Sibylle Rieder', 'Axel Theofilidis', 'Marius Groenendijk', 'Peter Phelan', 'Henrik Schulz', 'Thierry Declerck', 'Andrew Brenenkamp']",,"This paper reports on the ESPRIT project MELISSA (Methods and Tools for Natural-Language Interfacing with Standard Software Applications)1. MELISSA aims at developing the technology and tools enabling end users to interface with computer applications, using natural-language (NL), and to obtain a precompetitive product validated in selected enduser applications. This paper gives an overview of the approach to solving (NL) interfacing problem and outlines some of the methods and software components developed in the project.",The goal of the JAVOX toolkit is to speech-enable traditional desktop applications -- this is similar to the goals of the MELISSA project ( #AUTHOR_TAG ) .,"['Previous systems to assist in the development of spoken-langnage systems (SLSs) have focused on building stand-alone, customized applications, such as (Sutton et al., 1996) and (Pargellis et al., 1999).', 'The goal of the JAVOX toolkit is to speech-enable traditional desktop applications -- this is similar to the goals of the MELISSA project ( #AUTHOR_TAG ) .', 'It is intended to both speed the development of SLSs and to localize the speech-specific code within the application.', 'JAVOX allows developers to add speech interfaces to applications at the end of the development process; SLSs no longer need to be built from the ground up.']",1,"['The goal of the JAVOX toolkit is to speech-enable traditional desktop applications -- this is similar to the goals of the MELISSA project ( #AUTHOR_TAG ) .', 'JAVOX allows developers to add speech interfaces to applications at the end of the development process; SLSs no longer need to be built from the ground up.']"
CCT15,A00-1019,Unit completion for a computer-aided translation typing system,a corpusbased approach to automatic compound extraction,"['Keh-Yih Su', 'Ming-Wen Wu', 'Jing-Shin Chang']",method,,"It is also possible to focus on non-compositional compounds , a key point in bilingual applications ( #AUTHOR_TAG ; Melamed , 1997 ; Lin , 99 ) .","['One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints (Ganssier, 1995;Kupiec, 1993;hua Chen and Chen, 94;Fung, 1995;Evans and Zhai, 1996).', 'It is also possible to focus on non-compositional compounds , a key point in bilingual applications ( #AUTHOR_TAG ; Melamed , 1997 ; Lin , 99 ) .', 'Another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995;Furuse and Iida, 96).', 'In this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part-of-speech tags.', ""An excerpt of the association probabilities of a unit model trained considering only the NP-sequences is given in table 3. Applying this filter (referred to as JrNp in the following) to the 39,093 english sequences still surviving after previous filters ~'1 and ~'2 removes 35,939 of them (92%)."", '4: Completion results of several translation models, spared: theoretical proportion of characters saved; ok: number of target units accepted by the user; good: number of target units that matched the expected whether they were proposed or not; nu: number of sentences for which no target unit was found by the translation model; u: number of sentences for which at least one helpful unit has been found by the model, but not necessarily proposed.']",0,"['It is also possible to focus on non-compositional compounds , a key point in bilingual applications ( #AUTHOR_TAG ; Melamed , 1997 ; Lin , 99 ) .', 'Another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995;Furuse and Iida, 96).', 'In this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part-of-speech tags.']"
CCT16,A00-1019,Unit completion for a computer-aided translation typing system,retrieving collocations by cooccurrences and word order constraints,"['Sayori Shimohata', 'Toshiyuki Sugio', 'Junji Nagata']",method,,"This method allows the efficient retrieval of arbitrary length n-grams ( Nagao and Mori , 94 ; Haruno et al. , 96 ; Ikehaxa et al. , 96 ; #AUTHOR_TAG ; Russell , 1998 ) .","['Finding relevant units in a text has been explored in many areas of natural language processing.', 'Our approach relies on distributional and frequency statistics computed on each sequence of words found in a training corpus.', 'For sake of efficiency, we used the suffix array technique to get a compact representation of our training corpus.', 'This method allows the efficient retrieval of arbitrary length n-grams ( Nagao and Mori , 94 ; Haruno et al. , 96 ; Ikehaxa et al. , 96 ; #AUTHOR_TAG ; Russell , 1998 ) .']",0,"['Finding relevant units in a text has been explored in many areas of natural language processing.', 'Our approach relies on distributional and frequency statistics computed on each sequence of words found in a training corpus.', 'This method allows the efficient retrieval of arbitrary length n-grams ( Nagao and Mori , 94 ; Haruno et al. , 96 ; Ikehaxa et al. , 96 ; #AUTHOR_TAG ; Russell , 1998 ) .']"
CCT17,A00-1019,Unit completion for a computer-aided translation typing system,an algorithm for finding noun phrase correspondences in bilingual corpora,['Julian Kupiec'],method,"The paper describes an algorithm that employs English and French text taggers to associate noun phrases in an aligned bilingual corpus. The taggers provide part-of-speech categories which are used by finite-state recognizers to extract simple noun phrases for both languages. Noun phrases are then mapped to each other using an iterative re-estimation algorithm that bears similarities to the Baum-Welch algorithm which is used for training the taggers. The algorithm provides an alternative to other approaches for finding word correspondences, with the advantage that linguistic structure is incorporated. Improvements to the basic algorithm are described, which enable context to be accounted for when constructing the noun phrase mappings.","One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints ( Gaussier , 1995 ; #AUTHOR_TAG ; hua Chen and Chen , 94 ; Fung , 1995 ; Evans and Zhai , 1996 ) .","['One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints ( Gaussier , 1995 ; #AUTHOR_TAG ; hua Chen and Chen , 94 ; Fung , 1995 ; Evans and Zhai , 1996 ) .', 'It is also possible to focus on non-compositional compounds, a key point in bilingual applications (Su et al., 1994;Melamed, 1997;Lin, 99).', 'Another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995;Furuse and Iida, 96).', 'In this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part-of-speech tags.', ""An excerpt of the association probabilities of a unit model trained considering only the NP-sequences is given in table 3. Applying this filter (referred to as JrNp in the following) to the 39,093 english sequences still surviving after previous filters ~'1 and ~'2 removes 35,939 of them (92%)."", '4: Completion results of several translation models, spared: theoretical proportion of characters saved; ok: number of target units accepted by the user; good: number of target units that matched the expected whether they were proposed or not; nu: number of sentences for which no target unit was found by the translation model; u: number of sentences for which at least one helpful unit has been found by the model, but not necessarily proposed.']",5,"['One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints ( Gaussier , 1995 ; #AUTHOR_TAG ; hua Chen and Chen , 94 ; Fung , 1995 ; Evans and Zhai , 1996 ) .', 'It is also possible to focus on non-compositional compounds, a key point in bilingual applications (Su et al., 1994;Melamed, 1997;Lin, 99).']"
CCT18,A00-1016,A compact architecture for dialogue management based on scripts and meta-outputs,commandtalk a spokenlanguage interface for battlefield simulations,"['R Moore', 'J Dowding', 'H Bratt', 'J Gawron']",experiments,,"The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .","['The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .', 'The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998)).', 'Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000).', 'Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993), using a domain~independent unification grammar and a domain-specific lexicon.', 'The language processing grammar is compiled into a recognition grarnm~kr using the methods of (Moore et al., 1997); the net result is that only grammatically wellformed utterances can be recognized.', 'Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992), and passed in that form to the dialogue manager.', 'We refer to these as linguistic level representations.']",5,"['The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .']"
CCT19,A00-1016,A compact architecture for dialogue management based on scripts and meta-outputs,commandtalk a spokenlanguage interface for battlefield simulations,"['R Moore', 'J Dowding', 'H Bratt', 'J Gawron']",introduction,,"CornmandTalk ( #AUTHOR_TAG ) , Circuit Fix-It Shop ( Smith , 1997 ) and TRAINS-96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents .","['The basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system.', 'As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem.', 'More recent work on spoken language interfaces to semi-antonomous robots include SRrs Flakey robot (Konolige et al., 1993) and NCARArs InterBOT project (Perzanowski et al., 1998;Perzanowski et al., 1999).', 'A number of other systems have addressed part of the task.', 'CornmandTalk ( #AUTHOR_TAG ) , Circuit Fix-It Shop ( Smith , 1997 ) and TRAINS-96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents .', ""Jack's MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous."", 'Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995;Pyre et al., 1995).', 'In most of this and other related work the treatment is some variant of the following.', 'If there is a speech interface, the input speech signal is converted into text.', ""Text either from the recognizer or directly input by the user is then converted into some kind of logical formula, which abstractly represents the user's intended command; this formula is then fed into a command interpreter, which executes the command.""]",0,"['The basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system.', 'More recent work on spoken language interfaces to semi-antonomous robots include SRrs Flakey robot (Konolige et al., 1993) and NCARArs InterBOT project (Perzanowski et al., 1998;Perzanowski et al., 1999).', 'CornmandTalk ( #AUTHOR_TAG ) , Circuit Fix-It Shop ( Smith , 1997 ) and TRAINS-96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents .', ""Jack's MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous."", 'If there is a speech interface, the input speech signal is converted into text.', ""Text either from the recognizer or directly input by the user is then converted into some kind of logical formula, which abstractly represents the user's intended command; this formula is then fed into a command interpreter, which executes the command.""]"
CCT20,A00-1018,An automatic reviser,tagging english text with a probabilistic model,['B Merialdo'],,"In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence. The main novelty of these experiments is the use of untagged text in the training of the model. We have used a simple triclass Markov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided. Two approaches in particular are compared and combined:using text that has been tagged by hand and computing relative frequency counts,using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle.Experminents show that the best training is obtained by using as much tagged text as possible. They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy. In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available.",We do this with a first-order HMM part-ofspeech tagger ( Merialdo #AUTHOR_TAG ) .,"['1.', 'An aligner.', 'After identification of word and sentence boundaries the text is processed into a bi-text by an alignment program.', 'This alignment is done on the basis of both length (Gale and Church [7]) and a notion of cognateness (Simard [161).', '2. Transducers.', 'In order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'This is done with transducers (Kaplan and Kay, [10]).', '3. Part-of-speech tagger.', 'Misleading similarities in graphical form can sometime induce translation mistakes (deceptive cognates).', '~ These forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'We do this with a first-order HMM part-ofspeech tagger ( Merialdo #AUTHOR_TAG ) .', 'I In the rest of the paper, we will use deceptive cognate very Iosely often to refer to normative usage of word in general.']",5,"['3. Part-of-speech tagger.', 'We do this with a first-order HMM part-ofspeech tagger ( Merialdo #AUTHOR_TAG ) .']"
CCT21,A00-1018,An automatic reviser,a program for aligning sentences in bilingual corpora,"['W Gale', 'K Church']",,"Researchers in both machine translation (e.g., Brown et al. 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann 1990) have recently become interested in studying bilingual corpora, bodies of text such as the Canadian Hansards (parliamentary proceedings), which are available in multiple languages (such as French and English). One useful step is to align the sentences, that is, to identify correspondences between sentences in one language and sentences in the other language.This paper will describe a method and a program (align) for aligning sentences based on a simple statistical model of character lengths. The program uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences. A probabilistic score is assigned to each proposed correspondence of sentences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference. This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences.It is remarkable that such a simple approach works as well as it does. An evaluation was performed based on a trilingual corpus of economic reports issued by the Union Bank of Switzerland (UBS) in English, French, and German. The method correctly aligned all but 4% of the sentences. Moreover, it is possible to extract a large subcorpus that has a much smaller error rate. By selecting the best-scoring 80% of the alignments, the error rate is reduced from 4% to 0.7%. There were more errors on the English-French subcorpus than on the English-German subcorpus, showing that error rates will depend on the corpus considered; however, both were small enough to hope that the method will be useful for many language pairs.To further research on bilingual corpora, a much larger sample of Canadian Hansards (approximately 90 million words, half in English and and half in French) has been aligned with the align program and will be available through the Data Collection Initiative of the Association for Computational Linguistics (ACL/DCI). In addition, in order to facilitate replication of the align program, an appendix is provided with detailed c-code of the more difficult core of the align program.",This alignment is done on the basis of both length ( Gale and Church #AUTHOR_TAG ) and a notion of cognateness ( Simard [ 16 ] ) .,"['1.', 'An aligner.', 'After identification of word and sentence boundaries the text is processed into a bi-text by an alignment program.', 'This alignment is done on the basis of both length ( Gale and Church #AUTHOR_TAG ) and a notion of cognateness ( Simard [ 16 ] ) .', '2. Transducers.', 'In order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'This is done with transducers (Kaplan and Kay, [10]).', '3. Part-of-speech tagger.', 'Misleading similarities in graphical form can sometime induce translation mistakes (deceptive cognates).', '~ These forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'We do this with a first-order HMM part-ofspeech tagger (Merialdo [13]).', 'I In the rest of the paper, we will use deceptive cognate very Iosely often to refer to normative usage of word in general.']",5,"['An aligner.', 'After identification of word and sentence boundaries the text is processed into a bi-text by an alignment program.', 'This alignment is done on the basis of both length ( Gale and Church #AUTHOR_TAG ) and a notion of cognateness ( Simard [ 16 ] ) .', 'In order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', '3. Part-of-speech tagger.', 'Misleading similarities in graphical form can sometime induce translation mistakes (deceptive cognates).', '~ These forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.']"
CCT22,A00-1021,Ranking suspected answers to natural language questions using predictive annotation,disambiguation of proper names in text,"['Nina Wacholder', 'Yael Ravin', 'Misook Choi']",experiments,"Identifying the occurrences of proper names in text and the entities they refer to can be a difficult task because of the many-to-many mapping between names and their referents. We analyze the types of ambiguity --- structural and semantic --- that make the discovery of proper names difficult in text, and describe the heuristics used to disambiguate names in Nominator, a fully-implemented module for proper name recognition developed at the IBM T. J. Watson Research Center.","â¢ Before indexing the text , we process it with Textract ( Byrd and Ravin , 1998 ; #AUTHOR_TAG ) , which performs lemmatization , and discovers proper names and technical terms .","['â\x80¢ Before indexing the text , we process it with Textract ( Byrd and Ravin , 1998 ; #AUTHOR_TAG ) , which performs lemmatization , and discovers proper names and technical terms .', 'We added a new module (Resporator) which annotates text segments with QA-Tokens using pattern matching.', 'Thus the text ""for 5 centuries"" matches the DURATIONS pattern ""for :CARDINAL _timeperiod"", where :CAR-DINAL is the label for cardinal numbers, and _timeperiod marks a time expression.']",5,"['â\x80¢ Before indexing the text , we process it with Textract ( Byrd and Ravin , 1998 ; #AUTHOR_TAG ) , which performs lemmatization , and discovers proper names and technical terms .', 'We added a new module (Resporator) which annotates text segments with QA-Tokens using pattern matching.']"
CCT23,A00-1022,Message classification in the call center,an information extraction core system for real world german text processing,"['Giinter Neumann', 'Rolf Backofen', 'Judith Baur', 'Markus Becker', 'Christian Braun']",experiments,"This paper describes SMES, an information extraction core system for real world German text processing. The basic design criterion of the system is of providing a set of basic powerful, robust, and efficient natural language components and generic linguistic knowledge sources which can easily be customized for processing different tasks in a flexible manner.Comment: 9 pages; in Proc. of 5th ANLP, 199",100000 word stems of German ( #AUTHOR_TAG ) .,"['MorphAna: Morphological Analysis provided by sines yields the word stems of nouns, verbs and adjectives, as well as the full forms of unknown words.', 'We are using a lexicon of approx.', '100000 word stems of German ( #AUTHOR_TAG ) .']",5,['100000 word stems of German ( #AUTHOR_TAG ) .']
CCT24,A00-1022,Message classification in the call center,an information extraction core system for real world german text processing,"['Giinter Neumann', 'Rolf Backofen', 'Judith Baur', 'Markus Becker', 'Christian Braun']",,"This paper describes SMES, an information extraction core system for real world German text processing. The basic design criterion of the system is of providing a set of basic powerful, robust, and efficient natural language components and generic linguistic knowledge sources which can easily be customized for processing different tasks in a flexible manner.Comment: 9 pages; in Proc. of 5th ANLP, 199","Linguistic preprocessing of text documents is carried out by re-using smes , an information extraction core system for real-world German text processing ( #AUTHOR_TAG ) .","['Linguistic preprocessing of text documents is carried out by re-using smes , an information extraction core system for real-world German text processing ( #AUTHOR_TAG ) .', 'The fundamental design criterion of sines is to provide a set of basic, powerful, robust, and efficient STP components and 4Almost all tools we examined build a single multicategorizer except for SVM-Light, which builds multiple binary classifiers.', 'generic linguistic knowledge sources that can easily be customized to deal with different tasks in a flexible manner, sines includes a text tokenizer, a lexical processor and a chunk parser.', 'The chunk parser itself is subdivided into three components.', 'In the first step, phrasal fragments like general nominal expressions and verb groups are recognized.', 'Next, the dependency-based structure of the fragments of each sentence is computed using a set of specific sentence patterns.', 'Third, the grammatical functions are determined for each dependency-based structure on the basis of a large subcategorization lexicon.', 'The present application benefits from the high modularity of the usage of the components.', 'Thus, it is possible to run only a subset of the components and to tailor their output.', 'The experiments described in Section 4 make use of this feature.']",5,"['Linguistic preprocessing of text documents is carried out by re-using smes , an information extraction core system for real-world German text processing ( #AUTHOR_TAG ) .']"
CCT25,A00-1025,Examining the role of statistical and linguistic knowledge sources in a general-knowledge question-answering system,errordriven pruning of treebank grammars for base noun phrase identification,"['C Cardie', 'D Pierce']",,"Finding simple, non-recursive, base noun phrases is an important subtask for many natural language processing applications. While previous empirical methods for base NP identification have been rather complex, this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task. In particular, we present a corpus-based approach for finding base NPs by matching part-of-speech tag sequences. The training phase of the algorithm is based on two successful techniques: first the base NP grammar is read from a ""treebank"" corpus; then the grammar is improved by selecting rules with high ""benefit"" scores. Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on the Penn Treebank Wall Street Journal.","Our implementation of the NP-based QA system uses the Empire noun phrase finder , which is described in detail in #AUTHOR_TAG .","['The NP-based QA System.', 'Our implementation of the NP-based QA system uses the Empire noun phrase finder , which is described in detail in #AUTHOR_TAG .', 'Empire identifies base NPs --non-recursive noun phrases --using a very simple algorithm that matches part-of-speech tag sequences based on a learned noun phrase grammar.', 'The approach is able to achieve 94% precision and recall for base NPs derived from the Penn Treebank Wall Street Journal (Marcus et al., 1993).', 'In the experiments below, the NP filter follows the application of the document retrieval and text summarization components.', 'Pronoun answer hypotheses are discarded, and the NPs are assembled into 50-byte chunks.']",5,"['Our implementation of the NP-based QA system uses the Empire noun phrase finder , which is described in detail in #AUTHOR_TAG .', 'Empire identifies base NPs --non-recursive noun phrases --using a very simple algorithm that matches part-of-speech tag sequences based on a learned noun phrase grammar.', 'The approach is able to achieve 94% precision and recall for base NPs derived from the Penn Treebank Wall Street Journal (Marcus et al., 1993).', 'Pronoun answer hypotheses are discarded, and the NPs are assembled into 50-byte chunks.']"
CCT26,A00-1025,Examining the role of statistical and linguistic knowledge sources in a general-knowledge question-answering system,the tipster summac text summarization evaluation,"['I Mani', 'T Firmin', 'D House', 'G Klein', 'B Sundheim', 'L Hirschman']",,"All too frequently, a software cost estimate is required in the early stages of the life-cycle when requirements and design specifications are immature. To produce a cost estimate under these conditions requires extensive use of expert judgment and addressing significant estimatio","Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings ( #AUTHOR_TAG ) , we again propose the use of vector space methods from IR , which can be easily extended to the summarization task ( Salton et al. , 1994 ) :","['We next hypothesize that query-dependent text summarization algorithms will improve the performance of the QA system by focusing the system on the most relevant portions of the retrieved documents.', 'The goal for query-dependent summarization algorithms is to provide a short summary of a document with respect to a specific query.', 'Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings ( #AUTHOR_TAG ) , we again propose the use of vector space methods from IR , which can be easily extended to the summarization task ( Salton et al. , 1994 ) :']",1,"['Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings ( #AUTHOR_TAG ) , we again propose the use of vector space methods from IR , which can be easily extended to the summarization task ( Salton et al. , 1994 ) :']"
CCT27,A00-1031,TnT,a maximum entropy model for partofspeech tagging,['Adwait Ratnaparkhi'],introduction,,The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in ( #AUTHOR_TAG ) .,"['The aim of this paper is to give a detailed account of the techniques used in TnT.', 'Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993).', 'The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in ( #AUTHOR_TAG ) .', 'For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999).']",1,"['Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993).', 'The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in ( #AUTHOR_TAG ) .', 'For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999).']"
CCT28,A00-1031,TnT,a maximum entropy model for partofspeech tagging,['Adwait Ratnaparkhi'],conclusion,,"According to current tagger comparisons ( van Halteren et al. , 1998 ; Zavrel and Daelemans , 1999 ) , and according to a comparsion of the results presented here with those in ( #AUTHOR_TAG ) , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here .","['We have shown that a tagger based on Markov models yields state-of-the-art results, despite contrary claims found in the literature.', 'For example, the Markov model tagger used in the comparison of (van Halteren et al., 1998) yielded worse results than all other taggers.', 'In our opinion, a reason for the wrong claim is that the basic algorithms leave several decisions to the implementor.', 'The rather large amount of freedom was not handled in detail in previous publications: handling of start-and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.', 'Note that the decisions we made yield good results for both the German and the English Corpus.', 'They do so for several other corpora as well.', 'The architecture remains applicable to a large variety of languages.', 'According to current tagger comparisons ( van Halteren et al. , 1998 ; Zavrel and Daelemans , 1999 ) , and according to a comparsion of the results presented here with those in ( #AUTHOR_TAG ) , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here .', 'It is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both.']",1,"['According to current tagger comparisons ( van Halteren et al. , 1998 ; Zavrel and Daelemans , 1999 ) , and according to a comparsion of the results presented here with those in ( #AUTHOR_TAG ) , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here .']"
CCT29,A00-2004,TV program segmentation using text-visual analysis,text segmentation based on similarity between words,['Hideki Kozima'],,"This paper proposes a new indicator of text structure, called the lexical cohesion profile (LCP), which locates segment boundaries in a text. A text segment is a coherent scene; the words in a segment are linked together via lexical cohesion relations. LCP records mutual similarity of words in a sequence of text. The similarity of words, which represents their cohesiveness, is computed using a semantic network. Comparison with the text segments marked by a number of subjects shows that LCP closely correlates with the human judgments. LCP may provide valuable information for resolving anaphora and ellipsis.","This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .","['Four similarity measures were examined.', 'The cosine coefficient (R98(s,co,)) and dot density measure (R98(m,(lot)) yield similar results.', 'Our spread activation based semantic measure (R98( .....,)) improved a.ccura(:y.', ""This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .""]",1,"[""This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .""]"
CCT30,A00-2004,TV program segmentation using text-visual analysis,text segmentation based on similarity between words,['Hideki Kozima'],,"This paper proposes a new indicator of text structure, called the lexical cohesion profile (LCP), which locates segment boundaries in a text. A text segment is a coherent scene; the words in a segment are linked together via lexical cohesion relations. LCP records mutual similarity of words in a sequence of text. The similarity of words, which represents their cohesiveness, is computed using a semantic network. Comparison with the text segments marked by a number of subjects shows that LCP closely correlates with the human judgments. LCP may provide valuable information for resolving anaphora and ellipsis.","R98 ( , , , , â ) uses a variant of Kozima 's semantic similarity measure ( #AUTHOR_TAG ) to compute block similarity .","[""R98 ( , , , , â\x80\x9e ) uses a variant of Kozima 's semantic similarity measure ( #AUTHOR_TAG ) to compute block similarity ."", 'Word similarity is a function of word co- occurrence statistics in the given document.', 'Words that belong to the same sentence are considered to be related.', 'Given the co-occurrence frequencies f(wi, wj), the transition probability matrix t is computed by equation 10.', 'Equation 11 defines our spread activation scheme, s denotes the word similarity matrix, x is the number of activation steps and norm(y) converts a matrix y into a transition matrix, x = 5 was used in the experiment.']",2,"[""R98 ( , , , , â\x80\x9e ) uses a variant of Kozima 's semantic similarity measure ( #AUTHOR_TAG ) to compute block similarity ."", 'Word similarity is a function of word co- occurrence statistics in the given document.', 'Equation 11 defines our spread activation scheme, s denotes the word similarity matrix, x is the number of activation steps and norm(y) converts a matrix y into a transition matrix, x = 5 was used in the experiment.']"
CCT31,A00-2022,Ambiguity Packing in Constraint-based Parsing Practical Results,a bag of useful techniques for efficient and robust parsing,"['B Kiefer', 'H-U Krieger', 'J Carroll', 'R Malouf']",conclusion,This paper describes new and improved techniques which help a unification-based parser to process input efficiently and robustly. We show that combining these methods leads to a speed-up in parsing time of more than an order of magnitude. The methods are correct in the sense that none of them rule out legal rule applications,"Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .","['In related work, Miyao (1999) describes an ap- proach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments.', 'Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .', 'Unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison.']",1,"['Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .', 'Unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison.']"
CCT32,A00-2036,Splittability of Bilexical Context-Free Grammars is Undecidable,a linear observed time statistical parser based on maximum entropy models,['A Ratnaparkhi'],conclusion,"This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.","We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( #AUTHOR_TAG ) and ( Chelba and Jelinek , 1998 ) .","['In this paper we have provided an original mathematical argument in favour of this thesis.', 'Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1).', 'We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( #AUTHOR_TAG ) and ( Chelba and Jelinek , 1998 ) .', 'We leave this for future work.']",3,"['Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1).', 'We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( #AUTHOR_TAG ) and ( Chelba and Jelinek , 1998 ) .']"
CCT33,A00-2036,Splittability of Bilexical Context-Free Grammars is Undecidable,exploiting syntactic structure for language modeling,"['C Chelba', 'F Jelinek']",conclusion,"It is commonly believed that knowledge of syntactic structure should improve language modeling. However, effectively and computationally efficiently incorporating syntactic structure into neural language models has been a challenging topic. In this paper, we make use of a multi-task objective, i.e., the models simultaneously predict words as well as ground truth parse trees in a form called ""syntactic distances"", where information between these two separate objectives shares the same intermediate representation. Experimental results on the Penn Treebank and Chinese Treebank datasets show that when ground truth parse trees are provided as additional training signals, the model is able to achieve lower perplexity and induce trees with better quality.Comment: ACL2","We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( Ratnaparkhi , 1997 ) and ( #AUTHOR_TAG ) .","['In this paper we have provided an original mathematical argument in favour of this thesis.', 'Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1).', 'We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( Ratnaparkhi , 1997 ) and ( #AUTHOR_TAG ) .', 'We leave this for future work.']",3,"['Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1).', 'We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( Ratnaparkhi , 1997 ) and ( #AUTHOR_TAG ) .']"
CCT34,D08-1001,Revealing the structure of medical dictations with conditional random fields,a critique and improvement of an evaluation metric for text segmentation,"['Lev Pevzner', 'Marti Hearst']",experiments,"The Pk evaluation metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms. However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size distribution. We propose a simple modification to the Pk metric that remedies these problems. This new metriccalled Window Diffmoves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.","Accuracy is not the best measure to assess segmentation quality , therefore we also conducted experiments using the WindowDiff measure as proposed by #AUTHOR_TAG .","['Accuracy is not the best measure to assess segmentation quality , therefore we also conducted experiments using the WindowDiff measure as proposed by #AUTHOR_TAG .', 'WindowDiff returns 0 in case of a perfect segmentation; 1 is the worst possible score.', 'However, it only takes into account segment boundaries and disregards segment types.', 'In section 5.2, we mentioned that loopy BP is not guaranteed to converge in a finite number of iterations.', 'Since we optimize pseudolikelihood for parameter estimation, we are not affected by this limitation in the training phase.', 'However, we use loopy BP with a TRP schedule during testing, so we must expect to encounter non-convergence for some examples.', 'Theoretical results on this topic are discussed by Heskes (2004).', 'We give here an empirical observation of convergence behaviour of loopy BP in our setting; the maximum number of iterations of the TRP schedule was restricted to 1,000.', 'Table 4 shows the percentage of examples converging within this limit and the average number of iterations required by the converging examples, broken down by the different corpora.', 'From these results, we conclude that there is a connection between the quality of the annotation and the convergence behaviour of loopy BP.', ""In practice, even though loopy BP didn't converge for some examples, the solutions after 1,000 iterations where satisfactory.""]",5,"['Accuracy is not the best measure to assess segmentation quality , therefore we also conducted experiments using the WindowDiff measure as proposed by #AUTHOR_TAG .']"
CCT35,D08-1002,"It's a contradiction---no, it's not",natural logic for textual inference,"['B MacCartney', 'C D Manning']",experiments,"This paper presents the first use of a computational model of natural logic---a system of logical inference which operates over natural language---for textual inference. Most current approaches to the PASCAL RTE textual inference task achieve robustness by sacrificing semantic precision; while broadly effective, they are easily confounded by ubiquitous inferences involving monotonicity. At the other extreme, systems which rely on first-order logic and theorem proving are precise, but excessively brittle. This work aims at a middle way. Our system finds a low-cost edit sequence which transforms the premise into the hypothesis; learns to classify entailment relations across atomic edits; and composes atomic entailments into a top-level entailment judgment. We provide the first reported results for any system on the FraCaS test suite. We also evaluate on RTE3 data, and show that hybridizing an existing RTE system with our natural logic system yields significant performance gains.","Although this is only true in cases where y occurs in an upward monotone context ( #AUTHOR_TAG ) , in practice genuine contradictions between y-values sharing a meronym relationship are extremely rare .","['Meronyms: For some relations, there is no contradiction when y 1 and y 2 share a meronym, i.e. ""part of"" relation.', 'For example, in the set born in(Mozart,•) there is no contradiction between the y values ""Salzburg"" and ""Austria"", but ""Salzburg"" conflicts with ""Vienna"".', 'Although this is only true in cases where y occurs in an upward monotone context ( #AUTHOR_TAG ) , in practice genuine contradictions between y-values sharing a meronym relationship are extremely rare .', 'We therefore simply assigned contradictions between meronyms a probability close to zero.', 'We used the Tipster Gazetteer 4 and WordNet to identify meronyms, both of which have high precision but low coverage.']",4,"['Meronyms: For some relations, there is no contradiction when y 1 and y 2 share a meronym, i.e. ""part of"" relation.', 'Although this is only true in cases where y occurs in an upward monotone context ( #AUTHOR_TAG ) , in practice genuine contradictions between y-values sharing a meronym relationship are extremely rare .', 'We therefore simply assigned contradictions between meronyms a probability close to zero.', 'We used the Tipster Gazetteer 4 and WordNet to identify meronyms, both of which have high precision but low coverage.']"
CCT36,D08-1004,Modeling annotators,headdriven statistical models for natural language parsing,['Michael Collins'],,"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.",We parse each sentence with the Collins parser ( #AUTHOR_TAG ) .,"['To train our model, we use L-BFGS to locally maximize the log of the objective function (1): 15 These are the function words with count ≥ 40 in a random sample of 100 documents, and which were associated with the O-I tag transition at more than twice the average rate.', 'We do not use any other lexical φ-features that reference x, for fear that they would enable the learner to explain the rationales without changing θ as desired (see the end of section 5.3). 14', 'We parse each sentence with the Collins parser ( #AUTHOR_TAG ) .', 'Then the document has one big parse tree, whose root is DOC, with each sentence being a child of DOC.']",5,['We parse each sentence with the Collins parser ( #AUTHOR_TAG ) .']
CCT37,D08-1004,Modeling annotators,a sentimental education sentiment analysis using subjectivity summarization based on minimum cuts,"['B Pang', 'L Lee']",experiments,"Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as ""thumbs up"" or ""thumbs down"". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.","It is based on the dataset of #AUTHOR_TAG ,9 which consists of 1000 positive and 1000 negative movie reviews , tokenized and divided into 10 folds ( F0 -- F9 ) .","['In Zaidan et al. (2007), we introduced the �Movie Review Polarity Dataset Enriched with Annotator Rationales.�8', 'It is based on the dataset of #AUTHOR_TAG ,9 which consists of 1000 positive and 1000 negative movie reviews , tokenized and divided into 10 folds ( F0 -- F9 ) .', 'All our experiments use F9 as their final blind test set.']",2,"['In Zaidan et al. (2007), we introduced the Movie Review Polarity Dataset Enriched with Annotator Rationales.8', 'It is based on the dataset of #AUTHOR_TAG ,9 which consists of 1000 positive and 1000 negative movie reviews , tokenized and divided into 10 folds ( F0 -- F9 ) .']"
CCT38,D08-1004,Modeling annotators,a sentimental education sentiment analysis using subjectivity summarization based on minimum cuts,"['B Pang', 'L Lee']",,"Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as ""thumbs up"" or ""thumbs down"". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.",We collect substring rationales for a sentiment classification task ( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator .,"['A human annotator can provide hints to a machine learner by highlighting contextual ""rationales"" for each of his or her annotations (Zaidan et al., 2007).', 'How can one exploit this side information to better learn the desired parameters θ?', 'We present a generative model of how a given annotator, knowing the true θ, stochastically chooses rationales.', 'Thus, observing the rationales helps us infer the true θ.', 'We collect substring rationales for a sentiment classification task ( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator .', 'Our new generative approach exploits the rationales more effectively than our previous ""masking SVM"" approach.', 'It is also more principled, and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks.']",5,['We collect substring rationales for a sentiment classification task ( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator .']
CCT39,D08-1004,Modeling annotators,a sentimental education sentiment analysis using subjectivity summarization based on minimum cuts,"['B Pang', 'L Lee']",method,"Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as ""thumbs up"" or ""thumbs down"". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.","We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .","['where f (•) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and Z θ (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .', 'Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count ≥ 4 in the full 2000-document corpus.', 'Define f h (x, y) to be y if v h appears at least once in x, and 0 otherwise.', 'Thus θ ∈ R 17744 , and positive weights in θ favor class label y = +1 and equally discourage y = −1, while negative weights do the opposite.', 'This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.', 'Future work should consider more complex features and how they are signaled by rationales, as discussed in section 3.2.']",5,"['where f (*) extracts a feature vector from a classified document, th are the corresponding weights of those features, and Z th (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .', 'Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count >= 4 in the full 2000-document corpus.', 'This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.']"
CCT40,D08-1007,Discriminative learning of selectional preference from unlabeled text,automatic retrieval and clustering of similar words,['Dekang Lin'],experiments,Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.,"We gather similar words using #AUTHOR_TAGa ) , mining similar verbs from a comparable-sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text .4 We also use Keller and Lapata ( 2003 ) 's approach to obtaining web-counts .","['We first evaluate D S P on disambiguating positives from pseudo-negatives, comparing to recently-proposed systems that also require no manually- compiled resources like WordNet.', 'We convert Da- gan et al. (1999)�s similarity-smoothed probability to MI by replacing the empirical Pr n|v in Equa- tion (2) with the smoothed PrSIM from Equation (1).', 'We also test an MI model inspired by Erk (2007): MISIM n;v =log X Simn_;n Prv;n_ n__SIMS n', ""We gather similar words using #AUTHOR_TAGa ) , mining similar verbs from a comparable-sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text .4 We also use Keller and Lapata ( 2003 ) 's approach to obtaining web-counts .""]",5,"[""We gather similar words using #AUTHOR_TAGa ) , mining similar verbs from a comparable-sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text .4 We also use Keller and Lapata ( 2003 ) 's approach to obtaining web-counts .""]"
CCT41,D08-1007,Discriminative learning of selectional preference from unlabeled text,offline strategies for online question answering answering questions before they are asked,"['Michael Fleischman', 'Eduard Hovy', 'Abdessamad Echihabi']",method,,"We also made use of the person-name/instance pairs automatically extracted by #AUTHOR_TAG .2This data provides counts for pairs such as ""Edwin Moses , hurdler"" and ""William Farley , industrialist.","['We also made use of the person-name/instance pairs automatically extracted by #AUTHOR_TAG .2This data provides counts for pairs such as ""Edwin Moses , hurdler"" and ""William Farley , industrialist.', 'We have features for all concepts and therefore learn their association with each verb.']",5,"['We also made use of the person-name/instance pairs automatically extracted by #AUTHOR_TAG .2This data provides counts for pairs such as ""Edwin Moses , hurdler"" and ""William Farley , industrialist.']"
CCT42,D08-1007,Discriminative learning of selectional preference from unlabeled text,using the web to obtain frequencies for unseen bigrams,"['Frank Keller', 'Mirella Lapata']",experiments,"This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task.","We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) .","['Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990;Kehler et al., 2004).', ""We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) ."", 'pronoun is the direct object of a verb predicate, v.', ""A pronoun's antecedent must obey v's selectional preferences."", 'If we have a better model of SP, we should be able to better select pronoun antecedents.']",1,"[""We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) .""]"
CCT43,D08-1007,Discriminative learning of selectional preference from unlabeled text,automatic retrieval and clustering of similar words,['Dekang Lin'],experiments,Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.,"#AUTHOR_TAGa ) 's similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ) , because these have similar subjects to eat .","['It is interesting to inspect the feature weights returned by our system.', 'In particular, the weights on the verb co-occurrence features (Section 3.3.1)', 'provide a high-quality, argument-specific similarityranking of other verb contexts.', 'The DSP parameters for eat, for example, place high weight on features like Pr(n|braise), Pr(n|ration), and Pr(n|garnish).', ""#AUTHOR_TAGa ) 's similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ) , because these have similar subjects to eat ."", 'Discriminative, context-specific training seems to yield a better set of similar predicates, e.g. the highest-ranked contexts for DSP cooc on the verb join, 3 lead 1.42, rejoin 1.39, form 1.34, belong to 1.31, found 1.31, quit 1.29, guide 1.19, induct 1.19, launch (subj) 1.18, work at 1.14 give a better SIMS(join) for Equation (1) than the top similarities returned by (Lin, 1998a Other features are also weighted intuitively.', 'Note that case is a strong indicator for some arguments, for example the weight on being lower-case is high for become (0.972) and eat (0.505), but highly negative for accuse (-0.675) and embroil (-0.573) which often take names of people and organizations.']",0,"['It is interesting to inspect the feature weights returned by our system.', 'In particular, the weights on the verb co-occurrence features (Section 3.3.1)', 'provide a high-quality, argument-specific similarityranking of other verb contexts.', 'The DSP parameters for eat, for example, place high weight on features like Pr(n|braise), Pr(n|ration), and Pr(n|garnish).', ""#AUTHOR_TAGa ) 's similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ) , because these have similar subjects to eat ."", 'Discriminative, context-specific training seems to yield a better set of similar predicates, e.g. the highest-ranked contexts for DSP cooc on the verb join, 3 lead 1.42, rejoin 1.39, form 1.34, belong to 1.31, found 1.31, quit 1.29, guide 1.19, induct 1.19, launch (subj) 1.18, work at 1.14 give a better SIMS(join) for Equation (1) than the top similarities returned by (Lin, 1998a Other features are also weighted intuitively.']"
CCT44,D08-1007,Discriminative learning of selectional preference from unlabeled text,using the web to obtain frequencies for unseen bigrams,"['Frank Keller', 'Mirella Lapata']",experiments,"This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task.","Also , the #AUTHOR_TAG approach will be undefined if the pair is unobserved on the web .","['5 Available from the LDC as LDC2006T13.', 'This collection was generated from approximately 1 trillion tokens of online text.', 'Unfortunately, tokens appearing less than 200 times have been mapped to the UNK symbol, and only N-grams appearing more than 40 times are included.', 'Unlike results from search engines, however, experiments with this corpus are replicable.', 'not be able to provide a score for each example.', 'The similarity-smoothed examples will be undefined if SIMS(w) is empty.', 'Also , the #AUTHOR_TAG approach will be undefined if the pair is unobserved on the web .', 'As a reasonable default for these cases, we assign them a negative decision.']",5,"['5 Available from the LDC as LDC2006T13.', 'This collection was generated from approximately 1 trillion tokens of online text.', 'Unfortunately, tokens appearing less than 200 times have been mapped to the UNK symbol, and only N-grams appearing more than 40 times are included.', 'Unlike results from search engines, however, experiments with this corpus are replicable.', 'not be able to provide a score for each example.', 'Also , the #AUTHOR_TAG approach will be undefined if the pair is unobserved on the web .']"
CCT45,D08-1007,Discriminative learning of selectional preference from unlabeled text,inducing a semantically annotated lexicon via embased clustering,"['Mats Rooth', 'Stefan Riezler', 'Detlef Prescher', 'Glenn Carroll', 'Franz Beil']",method,"We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evaluated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.","Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .","['Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .', 'This data consists of triples (v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed.', 'The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones.', 'We refer to this as Pairwise Disambiguation.', 'Unlike this task, we classify each predicate-argument pair independently as plausible/implausible.', 'We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not simply the result of parser error or noise. 1']",1,"['Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .']"
CCT46,D08-1007,Discriminative learning of selectional preference from unlabeled text,cooccurrence retrieval a flexible framework for lexical distributional similarity,"['Julie Weeds', 'David Weir']",method,"Techniques that exploit knowledge of distributional similarity between words have been proposed in many areas of Natural Language Processing. For example, in language modeling, the sparse data problem can be alleviated by estimating the probabilities of unseen co-occurrences of events from the probabilities of seen co-occurrences of similar events. In other applications, distributional similarity is taken to be an approximation to semantic similarity. However, due to the wide range of potential applications and the lack of a strict definition of the concept of distributional similarity, many methods of calculating distributional similarity have been proposed or adopted. In this work, a flexible, parameterized framework for calculating distributional similarity is proposed. Within this framework, the problem of finding distributionally similar words is cast as one of co-occurrence retrieval (CR) for which precision and recall can be measured by analogy with the way they are measured in document retrieval. As will be shown, a number of popular existing measures of distributional similarity are simulated with parameter settings within the CR framework. In this article, the CR framework is then used to systematically investigate three fundamental questions concerning distributional similarity. First, is the relationship of lexical similarity necessarily symmetric, or are there advantages to be gained from considering it as an asymmetric relationship? Second, are some co-occurrences inherently more salient than others in the calculation of distributional similarity? Third, is it necessary to consider the difference in the extent to which each word occurs in each co-occurrence type? Two application-based tasks are used for evaluation: automatic thesaurus generation and pseudo-disambiguation. It is possible to achieve significantly better results on both these tasks by varying the parameters within the CR framework rather than using other existing distributional similarity measures; it will also be shown that any single unparameterized measure is unlikely to be able to do better on both tasks. This is due to an inherent asymmetry in lexical substitutability and therefore also in lexical distributional similarity.",The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .,"['The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .', 'They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', 'Our approach, on the other hand, discriminatively sets millions of individual similarity values.', 'Like Weeds and Weir (2005), our similarity values are asymmetric.']",1,"['The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .', 'Our approach, on the other hand, discriminatively sets millions of individual similarity values.', 'Like Weeds and Weir (2005), our similarity values are asymmetric.']"
CCT47,D08-1007,Discriminative learning of selectional preference from unlabeled text,isp learning inferential selectional preferences,"['Patrick Pantel', 'Rahul Bhagat', 'Bonaventura Coppola', 'Timothy Chklovski', 'Eduard Hovy']",method,"Semantic inference is a key component for advanced natural language understanding. However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering. This paper presents ISP, a collection of methods for automatically learning admissible argument values to which an inference rule can be applied, which we call inferential selectional preferences, and methods for filtering out incorrect inferences. We evaluate ISP and present empirical evidence of its effectiveness.",MI was also recently used for inference-rule SPs by #AUTHOR_TAG .,"['. That is, every feature is an intersection of the occurrence of a particular predicate,v, and some feature of the argument f (n).', 'For example, a feature for a verb-object pair might be, ""the verb is eat and the object is lower-case.""', 'In this representation, features for one predicate will be completely independent from those for every other predicate.', ""Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a For a fixed verb, MI is proportional to Keller and Lapata (2003)'s conditional probability scores for pseudodisambiguation of (v, n, n ′ ) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f (v, n)."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.', 'MI was also recently used for inference-rule SPs by #AUTHOR_TAG .']",0,['MI was also recently used for inference-rule SPs by #AUTHOR_TAG .']
CCT48,D08-1007,Discriminative learning of selectional preference from unlabeled text,using the web to obtain frequencies for unseen bigrams,"['Frank Keller', 'Mirella Lapata']",method,"This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task.","Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .","['Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .', 'This data consists of triples (v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed.', 'The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones.', 'We refer to this as Pairwise Disambiguation.', 'Unlike this task, we classify each predicate-argument pair independently as plausible/implausible.', 'We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not simply the result of parser error or noise. 1']",1,"['Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .', 'We refer to this as Pairwise Disambiguation.', 'We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not simply the result of parser error or noise. 1']"
CCT49,D08-1007,Discriminative learning of selectional preference from unlabeled text,inducing a semantically annotated lexicon via embased clustering,"['Mats Rooth', 'Stefan Riezler', 'Detlef Prescher', 'Glenn Carroll', 'Franz Beil']",related work,"We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evaluated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.","Usually , the classes are from WordNet ( Miller et al. , 1990 ) , although they can also be inferred from clustering ( #AUTHOR_TAG ) .","['Most approaches to SPs generalize from observed predicate-argument pairs to semantically similar ones by modeling the semantic class of the argument, following Resnik (1996).', 'For example, we might have a class Mexican Food and learn that the entire class is suitable for eating.', 'Usually , the classes are from WordNet ( Miller et al. , 1990 ) , although they can also be inferred from clustering ( #AUTHOR_TAG ) .', 'Brockmann and Lapata (2003) compare a number of WordNet-based approaches, including Resnik (1996), Li and Abe (1998), and Clark and Weir (2002), and found that the more sophisticated class-based approaches do not always outperform simple frequency-based models.']",0,"['Usually , the classes are from WordNet ( Miller et al. , 1990 ) , although they can also be inferred from clustering ( #AUTHOR_TAG ) .']"
CCT50,D08-1007,Discriminative learning of selectional preference from unlabeled text,inducing a semantically annotated lexicon via embased clustering,"['Mats Rooth', 'Stefan Riezler', 'Detlef Prescher', 'Glenn Carroll', 'Franz Beil']",experiments,"We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evaluated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.","Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .","['Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .', 'Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'We wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (and therefore have empty cooccurrence features (Section 3.3.1)).', 'We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise.', 'Next, we omit verb co-occurrence features for nouns that occur less than 10 times, and instead fire a low-count feature.', 'When we move to a new corpus, previouslyunseen nouns are treated like these low-count training nouns.']",1,"['Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .', 'We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise.', 'When we move to a new corpus, previouslyunseen nouns are treated like these low-count training nouns.']"
CCT51,D08-1007,Discriminative learning of selectional preference from unlabeled text,isp learning inferential selectional preferences,"['Patrick Pantel', 'Rahul Bhagat', 'Bonaventura Coppola', 'Timothy Chklovski', 'Eduard Hovy']",related work,"Semantic inference is a key component for advanced natural language understanding. However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering. This paper presents ISP, a collection of methods for automatically learning admissible argument values to which an inference rule can be applied, which we call inferential selectional preferences, and methods for filtering out incorrect inferences. We evaluate ISP and present empirical evidence of its effectiveness.","Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( #AUTHOR_TAG ; Roberto et al. , 2007 ) .","['Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( #AUTHOR_TAG ; Roberto et al. , 2007 ) .', 'Inferences such as ""[X wins Y] ⇒ [X plays Y]"" are only valid for certain argu-ments X and Y.', 'We follow Pantel et al. (2007) in using automatically-extracted semantic classes to help characterize plausible arguments.']",0,"['Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( #AUTHOR_TAG ; Roberto et al. , 2007 ) .']"
CCT52,D08-1007,Discriminative learning of selectional preference from unlabeled text,automatic retrieval and clustering of similar words,['Dekang Lin'],related work,Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.,Erk ( 2007 ) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and #AUTHOR_TAGa ) 's information-theoretic metric work best .,"['where Sim(v ′ , v) returns a real-valued similarity between two verbs v ′ and v (normalized over all pair similarities in the sum).', 'In contrast, Erk (2007) generalizes by substituting similar arguments, while Wang et al. (2005) use the cross-product of similar pairs.', 'One key issue is how to define the set of similar words, SIMS(w).', ""Erk ( 2007 ) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and #AUTHOR_TAGa ) 's information-theoretic metric work best ."", 'Similarity-smoothed models are simple to compute, potentially adaptable to new domains, and require no manually-compiled resources such as WordNet.']",0,"[""Erk ( 2007 ) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and #AUTHOR_TAGa ) 's information-theoretic metric work best .""]"
CCT53,D08-1007,Discriminative learning of selectional preference from unlabeled text,automatic retrieval and clustering of similar words,['Dekang Lin'],experiments,Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.,"We parsed the 3 GB AQUAINT corpus ( Voorhees , 2002 ) using Minipar ( #AUTHOR_TAGb ) , and collected verb-object and verb-subject frequencies , building an empirical MI model from this data .","['We parsed the 3 GB AQUAINT corpus ( Voorhees , 2002 ) using Minipar ( #AUTHOR_TAGb ) , and collected verb-object and verb-subject frequencies , building an empirical MI model from this data .', 'Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved.', 'Passive subjects (the car was bought) were converted to objects (bought car).', 'We set the MI-threshold, τ , to be 0, and the negative-to-positive ratio, K, to be 2.']",5,"['We parsed the 3 GB AQUAINT corpus ( Voorhees , 2002 ) using Minipar ( #AUTHOR_TAGb ) , and collected verb-object and verb-subject frequencies , building an empirical MI model from this data .']"
CCT54,D08-1007,Discriminative learning of selectional preference from unlabeled text,using the web to obtain frequencies for unseen bigrams,"['Frank Keller', 'Mirella Lapata']",experiments,"This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task.","Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .","['Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .', 'Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'We wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (and therefore have empty cooccurrence features (Section 3.3.1)).', 'We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise.', 'Next, we omit verb co-occurrence features for nouns that occur less than 10 times, and instead fire a low-count feature.', 'When we move to a new corpus, previouslyunseen nouns are treated like these low-count training nouns.']",1,"['Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .', 'We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise.', 'Next, we omit verb co-occurrence features for nouns that occur less than 10 times, and instead fire a low-count feature.', 'When we move to a new corpus, previouslyunseen nouns are treated like these low-count training nouns.']"
CCT55,D08-1007,Discriminative learning of selectional preference from unlabeled text,word association norms mutual information and lexicography,"['Kenneth Ward Church', 'Patrick Hanks']",method,"The term word association is used in a very particular sense in the p!ycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word &quot;nurse&quot; if it follows a highly associated word such as &quot;doctor.&quot;) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic rehtions of the doctor/nurse type (content word/content word) to lexico-syntactlc co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, makin it possible to estimate norms for tens of thousands of words",We measure this association using pointwise Mutual Information ( MI ) ( #AUTHOR_TAG ) .,"['To learn a discriminative model of selectional preference, we create positive and negative training examples automatically from raw text.', 'To create the positives, we automatically parse a large corpus, and then extract the predicate-argument pairs that have a statistical association in this data.', 'We measure this association using pointwise Mutual Information ( MI ) ( #AUTHOR_TAG ) .', 'The MI between a verb predicate, v, and its object argument, n, is:']",5,['We measure this association using pointwise Mutual Information ( MI ) ( #AUTHOR_TAG ) .']
CCT56,D08-1007,Discriminative learning of selectional preference from unlabeled text,using the web to obtain frequencies for unseen bigrams,"['Frank Keller', 'Mirella Lapata']",method,"This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task.","Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n â² ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) .","['. That is, every feature is an intersection of the occurrence of a particular predicate,v, and some feature of the argument f (n).', 'For example, a feature for a verb-object pair might be, ""the verb is eat and the object is lower-case.""', 'In this representation, features for one predicate will be completely independent from those for every other predicate.', ""Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n â\x80² ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) ."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.', 'MI was also recently used for inference-rule SPs by Pantel et al. (2007).']",4,"[""Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n â\x80² ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) .""]"
CCT57,D08-1009,Scaling textual inference to the web,natural logic for textual inference,"['B MacCartney', 'C D Manning']",introduction,"This paper presents the first use of a computational model of natural logic---a system of logical inference which operates over natural language---for textual inference. Most current approaches to the PASCAL RTE textual inference task achieve robustness by sacrificing semantic precision; while broadly effective, they are easily confounded by ubiquitous inferences involving monotonicity. At the other extreme, systems which rely on first-order logic and theorem proving are precise, but excessively brittle. This work aims at a middle way. Our system finds a low-cost edit sequence which transforms the premise into the hypothesis; learns to classify entailment relations across atomic edits; and composes atomic entailments into a top-level entailment judgment. We provide the first reported results for any system on the FraCaS test suite. We also evaluate on RTE3 data, and show that hybridizing an existing RTE system with our natural logic system yields significant performance gains.","HOLMES is given the following set of six domainindependent rules , which are similar to the upward monotone rules introduced by ( #AUTHOR_TAG ) .","['HOLMES is given the following set of six domainindependent rules , which are similar to the upward monotone rules introduced by ( #AUTHOR_TAG ) .']",1,"['HOLMES is given the following set of six domainindependent rules , which are similar to the upward monotone rules introduced by ( #AUTHOR_TAG ) .']"
CCT58,D08-1009,Scaling textual inference to the web,natural logic for textual inference,"['B MacCartney', 'C D Manning']",related work,"This paper presents the first use of a computational model of natural logic---a system of logical inference which operates over natural language---for textual inference. Most current approaches to the PASCAL RTE textual inference task achieve robustness by sacrificing semantic precision; while broadly effective, they are easily confounded by ubiquitous inferences involving monotonicity. At the other extreme, systems which rely on first-order logic and theorem proving are precise, but excessively brittle. This work aims at a middle way. Our system finds a low-cost edit sequence which transforms the premise into the hypothesis; learns to classify entailment relations across atomic edits; and composes atomic entailments into a top-level entailment judgment. We provide the first reported results for any system on the FraCaS test suite. We also evaluate on RTE3 data, and show that hybridizing an existing RTE system with our natural logic system yields significant performance gains.","While many approaches have addressed this problem , our work is most closely related to that of ( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ) , which convert the inputs into logical forms and then attempt to ` prove ' H from T plus a set of axioms .","['Textual Entailment systems are given two textual fragments, text T and hypothesis H, and attempt to decide if the meaning of H can be inferred from the meaning of T (Dagan et al., 2005).', ""While many approaches have addressed this problem , our work is most closely related to that of ( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ) , which convert the inputs into logical forms and then attempt to ` prove ' H from T plus a set of axioms ."", 'For instance, (Braz et al., 2005) represents T , H , and a set of rewrite rules in a description logic framework, and determines entailment by solving an integer linear program derived from that representation.']",1,"['Textual Entailment systems are given two textual fragments, text T and hypothesis H, and attempt to decide if the meaning of H can be inferred from the meaning of T (Dagan et al., 2005).', ""While many approaches have addressed this problem , our work is most closely related to that of ( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ) , which convert the inputs into logical forms and then attempt to ` prove ' H from T plus a set of axioms .""]"
CCT59,D08-1036,A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers,a fully bayesian approach to unsupervised partofspeech tagging,"['Sharon Goldwater', 'Tom Griffiths']",,,The studies presented by #AUTHOR_TAG and Johnson ( 2007 ) differed in the number of states that they used .,"['The studies presented by #AUTHOR_TAG and Johnson ( 2007 ) differed in the number of states that they used .', 'evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the full Penn Treebank tag set.', 'We ran all our estimators in both conditions here (thanks to Noah Smith for supplying us with his tag set).']",1,"['The studies presented by #AUTHOR_TAG and Johnson ( 2007 ) differed in the number of states that they used .', 'evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the full Penn Treebank tag set.', 'We ran all our estimators in both conditions here (thanks to Noah Smith for supplying us with his tag set).']"
CCT60,D08-1036,A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers,a fully bayesian approach to unsupervised partofspeech tagging,"['Sharon Goldwater', 'Tom Griffiths']",conclusion,,"On small data sets all of the Bayesian estimators strongly outperform EM ( and , to a lesser extent , VB ) with respect to all of our evaluation measures , confirming the results reported in #AUTHOR_TAG .","['As might be expected, our evaluation measures disagree somewhat, but the following broad tendancies seem clear.', 'On small data sets all of the Bayesian estimators strongly outperform EM ( and , to a lesser extent , VB ) with respect to all of our evaluation measures , confirming the results reported in #AUTHOR_TAG .', 'This is perhaps not too surprising, as the Bayesian prior plays a comparatively stronger role with a smaller training corpus (which makes the likelihood term smaller) and the approximation used by Variational Bayes is likely to be less accurate on smaller data sets.']",1,"['On small data sets all of the Bayesian estimators strongly outperform EM ( and , to a lesser extent , VB ) with respect to all of our evaluation measures , confirming the results reported in #AUTHOR_TAG .', 'This is perhaps not too surprising, as the Bayesian prior plays a comparatively stronger role with a smaller training corpus (which makes the likelihood term smaller) and the approximation used by Variational Bayes is likely to be less accurate on smaller data sets.']"
CCT61,D08-1039,Triplet lexicon models for statistical machine translation,word triggers and the em algorithm,"['Christoph Tillmann', 'Hermann Ney']",method,"In this paper, we study the use of so-called word trigger pairs to improve an existing language model, which is typically a trigram model in combination with a cache component. A word trigger pair is defined as a long-distance word pair. We present two methods to select the most significant single word trigger pairs. The selected trigger pairs are used in a combined model where the interpolation parameters and trigger interaction parameters are trained by the EM algorithm.","The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .","['The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .']",1,"['The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .']"
CCT62,D08-1042,A dependency-based word subsequence kernel,kernel methods for relation extraction,"['D Zelenko', 'C Aone', 'A Richardella']",related work,"We present an application of kernel methods to extracting relations from unstructured natural language sources. We introduce kernels defined over shallow parse representations of text, and design efficient algorithms for computing the kernels. We use the devised kernels in conjunction with Support Vector Machine and Voted Perceptron learning algorithms for the task of extracting person-affiliation and organization-location relations from text. We experimentally evaluate the proposed methods and compare them with feature-based learning algorithms, with promising results.","Their kernel is also very time consuming and in their more general sparse setting it requires O ( mn3 ) time and O ( mn2 ) space , where m and n are the number of nodes of the two trees ( m > = n ) ( #AUTHOR_TAG ) .","['A few kernels based on dependency trees have also been proposed.', 'Zelenko et al. (2003) proposed a tree kernel over shallow parse tree representations of sentences.', 'This tree kernel was slightly generalized by Culotta and Sorensen (2004) to compute similarity between two dependency trees.', 'In addition to the words, this kernel also incorporates word classes into the kernel.', 'The kernel is based on counting matching subsequences of children of matching nodes.', 'But as was also noted in (Bunescu and Mooney, 2005a), this kernel is opaque i.e. it is not obvious what the implicit features are and the authors do not describe it either.', 'In contrast, our dependency-based word subsequence kernel, which also computes similarity between two dependency trees, is very transparent with the implicit features being simply the dependency paths.', 'Their kernel is also very time consuming and in their more general sparse setting it requires O ( mn3 ) time and O ( mn2 ) space , where m and n are the number of nodes of the two trees ( m > = n ) ( #AUTHOR_TAG ) .', 'Bunescu and Mooney (2005a) give a shortest path dependency kernel for relation extraction.', 'Their kernel, however, does not find similarity between two sentences but between the shortest dependency paths connecting the two entities of interests in the sentences.', 'This kernel uses general dependency graphs but if the graph is a tree then the shortest path is the only path between the entities.', 'Their kernel also uses word classes in addition to the words themselves.']",3,"['A few kernels based on dependency trees have also been proposed.', 'This tree kernel was slightly generalized by Culotta and Sorensen (2004) to compute similarity between two dependency trees.', 'In addition to the words, this kernel also incorporates word classes into the kernel.', 'Their kernel is also very time consuming and in their more general sparse setting it requires O ( mn3 ) time and O ( mn2 ) space , where m and n are the number of nodes of the two trees ( m > = n ) ( #AUTHOR_TAG ) .', 'Bunescu and Mooney (2005a) give a shortest path dependency kernel for relation extraction.', 'Their kernel, however, does not find similarity between two sentences but between the shortest dependency paths connecting the two entities of interests in the sentences.', 'This kernel uses general dependency graphs but if the graph is a tree then the shortest path is the only path between the entities.', 'Their kernel also uses word classes in addition to the words themselves.']"
CCT63,D08-1066,Phrase Translation Probabilities with ITG Priors and Smoothing as Learning Objective,a discriminative latent variable model for statistical machine translation,"['P Blunsom', 'T Cohn', 'M Osborne']",conclusion,"Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems.We argue that a principle reason for this failure is not dealing with multiple, equivalent translations. We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised. Results show that accounting for multiple derivations does indeed improve performance. Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions. (c) 2008 Association for Computational Linguistics","Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .","['There are various strands of future research.', 'Firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .', 'We plan to build our own decoder (based on ITG) where different ideas can be tested including tractable ways for achieving a marginalization effect.', 'Apart from a new decoder, it will be worthwhile adapting the prior probability in our model to allow for consistent estimation.', 'Finally, it would be interesting to study properties of the penalized Deleted Estimation used in this paper.']",3,"['Firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .', 'We plan to build our own decoder (based on ITG) where different ideas can be tested including tractable ways for achieving a marginalization effect.', 'Apart from a new decoder, it will be worthwhile adapting the prior probability in our model to allow for consistent estimation.']"
CCT64,D08-1066,Phrase Translation Probabilities with ITG Priors and Smoothing as Learning Objective,a family of additive online algorithms for category ranking”,"['K Crammer', 'Y Singer']",related work,"We describe a new family of topic-ranking algorithms for multi-labeled documents. The motivation for the algorithms stem from recent advances in online learning algorithms. The algorithms are simple to implement and are also time and memory efficient. We provide a unified analysis of the family of algorithms in the mistake bound model. We then discuss experiments with the proposed family of topic-ranking algorithms on the Reuters-21578 corpus and the new corpus released by Reuters in 2000. On both corpora, the algorithms we present achieve state-of-the-art results and outperforms topic-ranking adaptations of Rocchio's algorithm and of the Perceptron algorithm.","More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .","['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .']",0,"['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .']"
CCT65,D08-1066,Phrase Translation Probabilities with ITG Priors and Smoothing as Learning Objective,an iterativelytrained segmentationfree phrase translation model for statistical machine translation,['Quirk'],conclusion,"Attempts to estimate phrase translation probablities for statistical machine translation using iteratively-trained models have repeatedly failed to produce translations as good as those obtained by estimating phrase translation probablities from surface statistics of bilingual word alignments as described by Koehn, et al. (2003). We propose a new iteratively-trained phrase translation model that produces translations of quality equal to or better than those produced by Koehn, et al.'s model. Moreover, with the new model, translation quality degrades much more slowly as pruning is tightend to reduce translation time.",Based on this advise ( Moore and #AUTHOR_TAG ) exclude the latent segmentation variables and opt for a heuristic training procedure .,"['The most similar efforts to ours, mainly (DeNero et al., 2006), conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.', 'Based on this advise ( Moore and #AUTHOR_TAG ) exclude the latent segmentation variables and opt for a heuristic training procedure .', 'In this work we also start out from a generative model with latent segmentation variables.', 'However, we find out that concentrating the learning effort on smoothing is crucial for good performance.', 'For this, we devise ITG-based priors over segmentations and employ a penalized version of Deleted Estimation working with EM at its core.', 'The fact that our results (at least) match the heuristic estimates on a reasonably sized data set (947k parallel sentence pairs) is rather encouraging.']",1,"['The most similar efforts to ours, mainly (DeNero et al., 2006), conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.', 'Based on this advise ( Moore and #AUTHOR_TAG ) exclude the latent segmentation variables and opt for a heuristic training procedure .']"
CCT66,D08-1113,Latent-variable modeling of string transductions with finite-state methods,learning structured models for phone recognition,"['Slav Petrov', 'Adam Pauls', 'Dan Klein']",conclusion,"We present a maximally streamlined approach to learning HMM-based acoustic models for automatic speech recognition. In our approach, an initial monophone HMM is iteratively refined using a split-merge EM procedure which makes no assumptions about subphone structure or context-dependent structure, and which uses only a single Gaussian per HMM state. Despite the much simplified training process, our acoustic model achieves state-of-the-art results on phone classification (where it outperforms almost all other methods) and competitive performance on phone recognition (where it outperforms standard CD triphone / subphone / GMM approaches). We also present an analysis of what is and is not learned by our system.",Latent variables we wish to consider are an increased number of word classes ; more flexible regions -- see #AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition -- and phonological features and syllable boundaries .,"['In future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string-transduction tasks.', 'We would like to use features that look at wide context on the input side, which is inexpensive (Jiampojamarn et al., 2007).', 'Latent variables we wish to consider are an increased number of word classes ; more flexible regions -- see #AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition -- and phonological features and syllable boundaries .', 'Indeed, our local log-linear features over several aligned latent strings closely resemble the soft constraints used by phonologists (Eisner, 1997).', 'Finally, rather than define a fixed set of feature templates as in Fig. 2, we would like to refine empirically useful features during training, resulting in language-specific backoff patterns and adaptively sized n-gram windows.', 'Many of these enhancements will increase the computational burden, and we are interested in strategies to mitigate this, including approximation methods.']",0,['Latent variables we wish to consider are an increased number of word classes ; more flexible regions -- see #AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition -- and phonological features and syllable boundaries .']
CCT67,D08-1113,Latent-variable modeling of string transductions with finite-state methods,applying manytomany alignments and hidden markov models to lettertophoneme conversion,"['Sittichai Jiampojamarn', 'Grzegorz Kondrak', 'Tarek Sherif']",conclusion,,"We would like to use features that look at wide context on the input side , which is inexpensive ( #AUTHOR_TAG ) .","['In future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string-transduction tasks.', 'We would like to use features that look at wide context on the input side , which is inexpensive ( #AUTHOR_TAG ) .', 'Latent variables we wish to consider are an increased number of word classes; more flexible regions-see Petrov et al. (2007) on learning a state transition diagram for acoustic regions in phone recognition-and phonological features and syllable boundaries.', 'Indeed, our local log-linear features over several aligned latent strings closely resemble the soft constraints used by phonologists (Eisner, 1997).', 'Finally, rather than define a fixed set of feature templates as in Fig. 2, we would like to refine empirically useful features during training, resulting in language-specific backoff patterns and adaptively sized n-gram windows.', 'Many of these enhancements will increase the computational burden, and we are interested in strategies to mitigate this, including approximation methods.']",3,"['In future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string-transduction tasks.', 'We would like to use features that look at wide context on the input side , which is inexpensive ( #AUTHOR_TAG ) .', 'Indeed, our local log-linear features over several aligned latent strings closely resemble the soft constraints used by phonologists (Eisner, 1997).']"
CCT68,D09-1003,Semi-supervised semantic role labeling using the latent words language model,automatic retrieval and clustering of similar words,['Dekang Lin'],conclusion,Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.,#AUTHOR_TAG for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples .,"['We have presented the Latent Words Language Model and showed how it learns, from unlabeled texts, latent words that capture the meaning of a certain word, depending on the context.', 'We then experimented with different methods to incorporate the latent words for Semantic Role Labeling, and tested different methods on the PropBank dataset.', 'Our best performing method showed a significant improvement over the supervised model and over methods previously proposed in the literature.', 'On the full training set the best method performed 2.33% better than the fully supervised model, which is a 10.91% error reduction.', 'Using only 5% of the training data the best semi-supervised model still achieved 60.29%, compared to 40.49% by the supervised model, which is an error reduction of 33.27%.', 'These results demonstrate that the latent words learned by the LWLM help for this complex information extraction task.', 'Furthermore we have shown that the latent words are simple to incorporate in an existing classifier by adding additional features.', 'We would like to perform experiments on employing this model in other information extraction tasks, such as Word Sense Disambiguation or Named Entity Recognition.', 'The current model uses the context in a very straightforward way, i.e. the two words left and right of the current word, but in the future we would like to explore more advanced methods to improve the similarity estimates.', '#AUTHOR_TAG for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples .']",0,"['We have presented the Latent Words Language Model and showed how it learns, from unlabeled texts, latent words that capture the meaning of a certain word, depending on the context.', 'We then experimented with different methods to incorporate the latent words for Semantic Role Labeling, and tested different methods on the PropBank dataset.', 'Our best performing method showed a significant improvement over the supervised model and over methods previously proposed in the literature.', 'On the full training set the best method performed 2.33% better than the fully supervised model, which is a 10.91% error reduction.', 'Using only 5% of the training data the best semi-supervised model still achieved 60.29%, compared to 40.49% by the supervised model, which is an error reduction of 33.27%.', 'These results demonstrate that the latent words learned by the LWLM help for this complex information extraction task.', 'Furthermore we have shown that the latent words are simple to incorporate in an existing classifier by adding additional features.', 'We would like to perform experiments on employing this model in other information extraction tasks, such as Word Sense Disambiguation or Named Entity Recognition.', 'The current model uses the context in a very straightforward way, i.e. the two words left and right of the current word, but in the future we would like to explore more advanced methods to improve the similarity estimates.', '#AUTHOR_TAG for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples .']"
CCT69,D09-1053,Model adaptation via model interpolation and boosting for web search ranking,language model adaptation with map estimation and the perceptron algorithm,"['M Bacchiani', 'B Roark', 'M Saraclar']",conclusion,"In this paper, we contrast two language model adaptation approaches: MAP estimation and the perceptron algorithm. Used in isolation, we show that MAP estimation outperforms the latter approach, for reasons which argue for combining the two approaches. When combined, the resulting system provides a 0.7 percent absolute reduction in word error rate over MAP estimation alone. In addition, we demonstrate that, in a multi-pass recognition scenario, it is better to use the perceptron algorithm on early pass word lattices, since the improved error rate improves acoustic model adaptation.","In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .","['In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .']",0,"['In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .']"
CCT70,D09-1144,Automatic acquisition of theargument-predicaterelations from a frame-annotated corpus,a probabilistic account of logical metonymy,"['Mirella Lapata', 'Alex Lascarides']",conclusion,"In this article we investigate logical metonymy, that is, constructions in which the argument of a word in syntax appears to be different from that argument in logical form (e.g., enjoy the book means enjoy reading the book, and easy problem means a problem that is easy to solve). The systematic variation in the interpretation of such constructions suggests a rich and complex theory of composition on the syntax/semantics interface. Linguistic accounts of logical metonymy typically fail to describe exhaustively all the possible interpretations, or they don&apos;t rank those interpretations in terms of their likelihood. In view of this, we acquire the meanings of metonymic verbs and adjectives from a large corpus and propose a probabilistic model that provides a ranking on the set of possible interpretations. We identify the interpretations automatically by exploiting the consistent correspondences between surface syntactic cues and meaning. We evaluate our results against paraphrase judgments elicited experimentally from humans and show that the model&apos;s ranking of meanings correlates reliably with human intuitions","As already mentioned in the literature , see for example ( #AUTHOR_TAG ) , knowledge about implicit predicates could be potentially useful for a variety of NLP tasks such as language generation , information extraction , question answering or machine translation .","['As already mentioned in the literature , see for example ( #AUTHOR_TAG ) , knowledge about implicit predicates could be potentially useful for a variety of NLP tasks such as language generation , information extraction , question answering or machine translation .', 'Many applications of semantic relations in NLP are connected to paraphrasing or query expansion, see for example (Voorhees, 1994).', ""Suppose that a search engine or a question answering system receives the query schnelle Bombe 'quick bomb'."", 'Probably, in this case the user is interested in finding information about bombs that explode quickly rather then about bombs in general.', ""Knowledge about predicates associated with the noun Bombe 'bomb' could be used for predicting a set of probable implicit predicates."", 'For generation of the semantically and syntactically correct paraphrases it is sometimes not enough to guess the most probable argument-predicate pairs.', 'Information about types of an argument-predicate relation could be helpful, i.e. which semantic and syntactic position does the argument fill in the argument structure of the predicate.', ""For example, compare eine Bombe explodiert schnell 'a bomb explodes quickly' for schnelle Bombe with ein Buch schnell lesen/schreiben 'to read/write a book quickly' for schnelles Buch 'quick book'."", 'In the first case the argument Bombe fills the subject position, while in the second case Buch fills the object position.', 'Since FrameNet contains information about syntactic realization patterns for frame elements, representation of argument-predicate relations in terms of frames directly supports generation of semantically and syntactically correct paraphrases.']",0,"['As already mentioned in the literature , see for example ( #AUTHOR_TAG ) , knowledge about implicit predicates could be potentially useful for a variety of NLP tasks such as language generation , information extraction , question answering or machine translation .', 'Many applications of semantic relations in NLP are connected to paraphrasing or query expansion, see for example (Voorhees, 1994).']"
CCT71,D10-1002,Self-training PCFG grammars with latent annotations across languages,kbest combination of syntactic parsers,"['Hui Zhang', 'Min Zhang', 'Chew Lim Tan', 'Haizhou Li']",conclusion,,Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( #AUTHOR_TAG ) .,"['We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'We demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'Two primary factors appear to be determining the efficacy of our self-training approach.', 'First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self-trained grammars.', 'Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010), despite being a single generative PCFG.', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( #AUTHOR_TAG ) .', 'In future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'One possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.', 'It would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.', 'A simple but computationally expensive way to do this would be to parse the data with an SM7 product model.']",1,"['We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'We demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self-trained grammars.', 'Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010), despite being a single generative PCFG.', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( #AUTHOR_TAG ) .', 'A simple but computationally expensive way to do this would be to parse the data with an SM7 product model.']"
CCT72,D10-1002,Self-training PCFG grammars with latent annotations across languages,coarsetofine nbest parsing and maxent discriminative reranking,"['Eugene Charniak', 'Mark Johnson']",conclusion,,"Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( #AUTHOR_TAG ) and products of latent variable grammars ( Petrov , 2010 ) , despite being a single generative PCFG .","['We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'We demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'Two primary factors appear to be determining the efficacy of our self-training approach.', 'First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self-trained grammars.', 'Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( #AUTHOR_TAG ) and products of latent variable grammars ( Petrov , 2010 ) , despite being a single generative PCFG .', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'In future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'One possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.', 'It would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.', 'A simple but computationally expensive way to do this would be to parse the data with an SM7 product model.']",1,"['Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( #AUTHOR_TAG ) and products of latent variable grammars ( Petrov , 2010 ) , despite being a single generative PCFG .', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).']"
CCT73,D10-1002,Self-training PCFG grammars with latent annotations across languages,products of random latent variable grammars,['Slav Petrov'],conclusion,"We show that the automatically induced latent variable grammars of Petrov et al. (2006) vary widely in their underlying representations, depending on their EM initialization point. We use this to our advantage, combining multiple automatically learned grammars into an unweighted product model, which gives significantly improved performance over state-of-the-art individual grammars. In our model, the probability of a constituent is estimated as a product of posteriors obtained from multiple grammars that differ only in the random seed used for initialization, without any learning or tuning of combination weights. Despite its simplicity, a product of eight automatically learned grammars improves parsing accuracy from 90.2% to 91.8% on English, and from 80.3% to 84.5% on German.","Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .","['We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'We demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'Two primary factors appear to be determining the efficacy of our self-training approach.', 'First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self-trained grammars.', 'Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'In future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'One possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.', 'It would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.', 'A simple but computationally expensive way to do this would be to parse the data with an SM7 product model.']",1,"['We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .']"
CCT74,D10-1038,Topic Segmentation and Labeling in Asynchronous Conversations,you talking to me a corpus and algorithm for conversation disentanglement,"['Micha Elsner', 'Eugene Charniak']",conclusion,"When multiple conversations occur simultaneously, a listener must decide which conversation each utterance is part of in order to interpret and respond to it appropriately. We refer to this task as disentanglement. We present a corpus of Internet Relay Chat (IRC) dialogue in which the various conversations have been manually disentangled, and evaluate annotator reliability. This is, to our knowledge, the first such corpus for internet chat. We propose a graph-theoretic model for disentanglement, using discourse-based features which have not been previously applied to this task. The model's predicted disentanglements are highly correlated with manual annotations.",In multi-party discussion people usually mention each other 's name for the purpose of disentanglement ( #AUTHOR_TAG ) .,"[""Another possibly critical feature is the 'mention of names'."", ""In multi-party discussion people usually mention each other 's name for the purpose of disentanglement ( #AUTHOR_TAG ) ."", ""In our corpus we found 175 instances where a participant mentions other participant's name."", ""In addition to these, 'Subject of the email', 'topic-shift cue words' can also be beneficial for a model."", 'As a next step for this research, we will investigate how to exploit these features in our methods.', 'We are also interested in the near future to transfer our approach to other similar domains by hierarchical Bayesian multi-task learning and other domain adaptation methods.', 'We plan to work on both synchronous (e.g., chats, meetings) and asynchronous (e.g., blogs) domains.']",0,"[""In multi-party discussion people usually mention each other 's name for the purpose of disentanglement ( #AUTHOR_TAG ) ."", ""In our corpus we found 175 instances where a participant mentions other participant's name."", 'We plan to work on both synchronous (e.g., chats, meetings) and asynchronous (e.g., blogs) domains.']"
CCT75,D10-1056,Two Decades of Unsupervised POS induction: How far have we come?,classbased ngram models of natural language,"['Peter F Brown', 'Vincent J Della Pietra', 'Peter V Desouza', 'Jennifer C Lai', 'Robert L Mercer']",conclusion,,"We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .","['Finally, we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .', 'These promising results suggest a new direction for future research: improving POS induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set.']",0,"['We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .']"
CCT76,D10-1123,Discovery of Semantic Relations between Web Services,scaling textual inference to the web,"['S Schoenmackers', 'O Etzioni', 'D Weld']",conclusion,"Most Web-based Q/A systems work by finding pages that contain an explicit answer to a question. These systems are helpless if the answer has to be inferred from multiple sentences, possibly on different pages. To solve this problem, we introduce the Holmes system, which utilizes textual inference (TI) over tuples extracted from text.    Whereas previous work on TI (e.g., the literature on textual entailment) has been applied to paragraph-sized texts, Holmes utilizes knowledge-based model construction to scale TI to a corpus of 117 million Web pages. Given only a few minutes, Holmes doubles recall for example queries in three disparate domains (geography, business, and nutrition). Importantly, Holmes's runtime is linear in the size of its input corpus due to a surprising property of many textual relations in the Web corpus---they are ""approximately"" functional in a well-defined sense.","Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .","['We run our techniques on a large set of relations to output a first repository of typed functional relations.', 'We release this list for further use by the research community. 2', 'Future Work: Functionality is one of the several properties a relation can possess.', 'Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .', 'These properties are very useful in increasing our understanding about these Open IE relation strings.', 'We believe that the general principles developed in this work, for example, connecting the Open IE knowledge with an existing knowledge resource, will come in very handy in identifying these other properties.']",0,"['We run our techniques on a large set of relations to output a first repository of typed functional relations.', 'We release this list for further use by the research community. 2', 'Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .']"
CCT77,D12-1027,Source Language Adaptation Approaches for Resource-Poor Machine Translation,improved statistical machine translation for resourcepoor languages using related resourcerich languages,"['Preslav Nakov', 'Hwee Tou Ng']",method,"We propose a novel language-independent approach for improving statistical machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. More precisely, we improve the translation from a resource-poor source language X1 into a resource-rich language Y given a bi-text containing a limited number of parallel sentences for X1-Y and a larger bi-text for X2-Y for some resource-rich language X2 that is closely related to X1. The evaluation for Indonesian-English (using Malay) and Spanish-English (using Portuguese and pretending Spanish is resource-poor) shows an absolute gain of up to 1.35 and 3.37 Bleu points, respectively, which is an improvement over the rivaling approaches, while using much less additional data.","Finally , we experiment with a method for combining phrase tables proposed in ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) .","['Sophisticated phrase table combination.', 'Finally , we experiment with a method for combining phrase tables proposed in ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) .', 'The first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one copy of the Indonesian-English bi-text.', 'The second table is built from the simple concatenation.', 'The two tables are then merged as follows: all phrase pairs from the first one are retained, and to them are added those phrase pairs from the second one that are not present in the first one.', 'Each phrase pair retains its original scores, which are further augmented with 1-3 additional feature scores indicating its origin: the first/second/third feature is 1 if the pair came from the first/second/both table(s), and 0 otherwise.', 'We experiment using all three, the first two, or the first feature only; we also try setting the features to 0.5 instead of 0. This makes the following six combinations (0, 00, 000, .5, .5.5, .5.5.5); on testing, we use the one that achieves the highest BLEU score on the development set.']",5,"['Sophisticated phrase table combination.', 'Finally , we experiment with a method for combining phrase tables proposed in ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) .', 'The first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one copy of the Indonesian-English bi-text.', 'The two tables are then merged as follows: all phrase pairs from the first one are retained, and to them are added those phrase pairs from the second one that are not present in the first one.', 'Each phrase pair retains its original scores, which are further augmented with 1-3 additional feature scores indicating its origin: the first/second/third feature is 1 if the pair came from the first/second/both table(s), and 0 otherwise.']"
CCT78,D12-1027,Source Language Adaptation Approaches for Resource-Poor Machine Translation,improved statistical machine translation for resourcepoor languages using related resourcerich languages,"['Preslav Nakov', 'Hwee Tou Ng']",related work,"We propose a novel language-independent approach for improving statistical machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. More precisely, we improve the translation from a resource-poor source language X1 into a resource-rich language Y given a bi-text containing a limited number of parallel sentences for X1-Y and a larger bi-text for X2-Y for some resource-rich language X2 that is closely related to X1. The evaluation for Indonesian-English (using Malay) and Spanish-English (using Portuguese and pretending Spanish is resource-poor) shows an absolute gain of up to 1.35 and 3.37 Bleu points, respectively, which is an improvement over the rivaling approaches, while using much less additional data.","For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .","['A third relevant line of research is on reusing bitexts between related languages without or with very little adaptation, which works well for very closely related languages.', 'For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .', 'However, our previous work did not attempt language adaptation, except for very simple transliteration for Portuguese-Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay-Indonesian, which use unified spelling.', 'Still, once we have language-adapted the large bi-text, it makes sense to try to combine it further with the small bi-text; thus, below we will directly compare and combine these two approaches.']",1,"['For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .', 'However, our previous work did not attempt language adaptation, except for very simple transliteration for Portuguese-Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay-Indonesian, which use unified spelling.', 'Still, once we have language-adapted the large bi-text, it makes sense to try to combine it further with the small bi-text; thus, below we will directly compare and combine these two approaches.']"
CCT79,D12-1084,Automated Discourse Analysis of Narrations by Adolescents with Autistic Spectrum Disorder,paraphrase fragment extraction from monolingual comparable corpora,"['Rui Wang', 'Chris Callison-Burch']",experiments,"We present a novel paraphrase fragment pair extraction method that uses a monolingual comparable corpus containing different arti-cles about the same topics or events. The pro-cedure consists of document pair extraction, sentence pair extraction, and fragment pair ex-traction. At each stage, we evaluate the in-termediate results manually, and tune the later stages accordingly. With this minimally su-pervised approach, we achieve 62 % of accu-racy on the paraphrase fragment pairs we col-lected and 67 % extracted from the MSR cor-pus. The results look promising, given the minimal supervision of the approach, which can be further scaled up.","We found the same number using our previous approach ( #AUTHOR_TAG ) , which is roughly equivalent to our core module .","['We mainly compute precision for this task, as the recall of paraphrase fragments is difficult to define.', ""However, we do include a measure we call productivity to indicate the algorithm's completeness."", 'It is defined as the ratio between the number of resulting fragment pairs and the number of sentence pairs used as input.', 'Tab. 2 shows the evaluation results.', 'We reach our best precision by using the VP-fragment heuristics, which is still more productive than the LCS method.', 'The grammatical filter gives us a higher precision compared to the purely alignment-based approaches.', 'Enhancing the system with coreference resolution raises the score even further.', 'We cannot directly compare this performance to other systems, as all other approaches have different data sources.', 'However, precision is usually manually evaluated, so the figures are at least indicative for a comparison with previous work: One state-of-theart system introduced by Zhao et al. (2008) extracts paraphrase fragments from bilingual parallel corpora and reaches a precision of 0.67.', 'We found the same number using our previous approach ( #AUTHOR_TAG ) , which is roughly equivalent to our core module .', 'Our approach outperforms both by 17% with similar estimated productivity.']",2,"['We mainly compute precision for this task, as the recall of paraphrase fragments is difficult to define.', 'It is defined as the ratio between the number of resulting fragment pairs and the number of sentence pairs used as input.', 'Tab. 2 shows the evaluation results.', 'We reach our best precision by using the VP-fragment heuristics, which is still more productive than the LCS method.', 'The grammatical filter gives us a higher precision compared to the purely alignment-based approaches.', 'Enhancing the system with coreference resolution raises the score even further.', 'However, precision is usually manually evaluated, so the figures are at least indicative for a comparison with previous work: One state-of-theart system introduced by Zhao et al. (2008) extracts paraphrase fragments from bilingual parallel corpora and reaches a precision of 0.67.', 'We found the same number using our previous approach ( #AUTHOR_TAG ) , which is roughly equivalent to our core module .']"
CCT80,D12-1084,Automated Discourse Analysis of Narrations by Adolescents with Autistic Spectrum Disorder,learning script knowledge with web experiments,"['Michaela Regneri', 'Alexander Koller', 'Manfred Pinkal']",related work,"We describe a novel approach to unsupervised learning of the events that make up a script, along with constraints on their temporal ordering. We collect natural-language descriptions of script-specific event sequences from volunteers over the Internet. Then we compute a graph representation of the script's temporal structure using a multiple sequence alignment algorithm. The evaluation of our system shows that we outperform two informed baselines.",We take some core ideas from our previous work on mining script information ( #AUTHOR_TAG ) .,"['We take some core ideas from our previous work on mining script information ( #AUTHOR_TAG ) .', 'In this earlier work, we focused on event structures and their possible realizations in natural language.', 'The corpus used in those experiments were short crowd-sourced descriptions of everyday tasks written in bullet point style.', 'We aligned them with a hand-crafted similarity measure that was specifically designed for this text type.', 'In this current work, we target the general task of extracting paraphrases for events rather than the much more specific scriptrelated task.', 'The current approach uses a domainindependent similarity measure instead of a specific hand-crafted similarity score and is thus applicable to standard texts.']",2,"['We take some core ideas from our previous work on mining script information ( #AUTHOR_TAG ) .', 'The corpus used in those experiments were short crowd-sourced descriptions of everyday tasks written in bullet point style.', 'In this current work, we target the general task of extracting paraphrases for events rather than the much more specific scriptrelated task.', 'The current approach uses a domainindependent similarity measure instead of a specific hand-crafted similarity score and is thus applicable to standard texts.']"
CCT81,D12-1084,Automated Discourse Analysis of Narrations by Adolescents with Autistic Spectrum Disorder,paraphrase fragment extraction from monolingual comparable corpora,"['Rui Wang', 'Chris Callison-Burch']",related work,"We present a novel paraphrase fragment pair extraction method that uses a monolingual comparable corpus containing different arti-cles about the same topics or events. The pro-cedure consists of document pair extraction, sentence pair extraction, and fragment pair ex-traction. At each stage, we evaluate the in-termediate results manually, and tune the later stages accordingly. With this minimally su-pervised approach, we achieve 62 % of accu-racy on the paraphrase fragment pairs we col-lected and 67 % extracted from the MSR cor-pus. The results look promising, given the minimal supervision of the approach, which can be further scaled up.",Our own work ( #AUTHOR_TAG ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora .,"['From an applicational point of view, sentential paraphrases are difficult to use in other NLP tasks.', 'At the phrasal level, interchangeable patterns (Shinyama et al., 2002;Shinyama and Sekine, 2003) or inference rules (Lin and Pantel, 2001) are extracted.', 'In both cases, each pattern or rule contains one or several slots, which are restricted to certain type of words, e.g., named entities (NE) or content words.', 'They are quite successful in NE-centered tasks, like information extraction, but their level of generalization or coverage is insufficient for applications like Recognizing Textual Entailment (Dinu and Wang, 2009).', 'The research on general paraphrase fragment extraction at the sub-sentential level is mainly based on phrase pair extraction techniques from the MT literature.', 'Munteanu and Marcu (2006) extract subsentential translation pairs from comparable corpora using the log-likelihood-ratio of word translation probability.', 'Quirk et al. (2007) extract fragments using a generative model of noisy translations.', 'Our own work ( #AUTHOR_TAG ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora .', 'Our current approach also uses word-word alignment, however, we use syntactic dependency trees to compute grammatical fragments.', 'Our use of dependency trees is inspired by the constituent-tree-based experiments of Callison-Burch (2008).']",2,['Our own work ( #AUTHOR_TAG ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora .']
CCT82,D12-1084,Automated Discourse Analysis of Narrations by Adolescents with Autistic Spectrum Disorder,paraphrase fragment extraction from monolingual comparable corpora,"['Rui Wang', 'Chris Callison-Burch']",,"We present a novel paraphrase fragment pair extraction method that uses a monolingual comparable corpus containing different arti-cles about the same topics or events. The pro-cedure consists of document pair extraction, sentence pair extraction, and fragment pair ex-traction. At each stage, we evaluate the in-termediate results manually, and tune the later stages accordingly. With this minimally su-pervised approach, we achieve 62 % of accu-racy on the paraphrase fragment pairs we col-lected and 67 % extracted from the MSR cor-pus. The results look promising, given the minimal supervision of the approach, which can be further scaled up.","Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .","['Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .', 'We extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (Sec.', '5.3).', 'Finally, we filter out trivial fragment pairs, such as identical or the original sentence pairs.']",2,"['Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .']"
CCT83,D14-1076,Improving Multi-documents Summarization by Sentence Compression based on Expanded Constituent Parse Trees,document summarization via guided sentence compression,"['Chen Li', 'Fei Liu', 'Fuliang Weng', 'Yang Liu']",experiments,"Joint compression and summarization has been used recently to generate high quality summaries. However, such word-based joint optimization is computationally expensive. In this paper we adopt the 'sentence compression + sentence selection ' pipeline approach for compressive summarization, but propose to perform summary guided compression, rather than generic sentence-based compression. To create an annotated corpus, the human anno-tators were asked to compress sentences while explicitly given the important summary words in the sentences. Using this corpus, we train a supervised sentence compression model us-ing a set of word-, syntax-, and document-level features. During summarization, we use multiple compressed sentences in the inte-ger linear programming framework to select salient summary sentences. Our results on the TAC 2008 and 2011 summarization data sets show that by incorporating the guided sen-tence compression model, our summarization system can yield significant performance gain as compared to the state-of-the-art.","Similar to ( #AUTHOR_TAGa ) , our summarization system is , which consists of three key components : an initial sentence pre-selection module to select some important sentence candidates ; the above compression model to generate n-best compressions for each sentence ; and then an ILP summarization method to select the best summary sentences from the multiple compressed sentences .","['Similar to ( #AUTHOR_TAGa ) , our summarization system is , which consists of three key components : an initial sentence pre-selection module to select some important sentence candidates ; the above compression model to generate n-best compressions for each sentence ; and then an ILP summarization method to select the best summary sentences from the multiple compressed sentences .']",1,"['Similar to ( #AUTHOR_TAGa ) , our summarization system is , which consists of three key components : an initial sentence pre-selection module to select some important sentence candidates ; the above compression model to generate n-best compressions for each sentence ; and then an ILP summarization method to select the best summary sentences from the multiple compressed sentences .']"
CCT84,D15-1148,Detecting Content-Heavy Sentences: A Cross-Language Case Study,assessing the discourse factors that influence the quality of machine translation,"['Junyi Jessy Li', 'Marine Carpuat', 'Ani Nenkova']",related work,"We present a study of aspects of discourse structure -- specifically discourse devices used to organize information in a sen-tence -- that significantly impact the qual-ity of machine translation. Our analysis is based on manual evaluations of trans-lations of news from Chinese and Ara-bic to English. We find that there is a particularly strong mismatch in the no-tion of what constitutes a sentence in Chi-nese and English, which occurs often and is associated with significant degradation in translation quality. Also related to lower translation quality is the need to em-ploy multiple explicit discourse connec-tives (because, but, etc.), as well as the presence of ambiguous discourse connec-tives in the English translation. Further-more, the mismatches between discourse expressions across languages significantly impact translation quality.",Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .,"['Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .', 'Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences.', 'This prior work was carried over a dataset containing a single reference translation for each Chinese sentence.', 'In the work presented in this paper, we strengthen our findings by examining multiple reference translations for each Chinese sentence.', 'We define heavy sentences based on agreement of translator choices and reader preferences.']",1,['Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .']
CCT85,E03-1004,Czech-English Dependency-based Machine Translation,automatic procedures in tectogrammatical tagging,['Alena Bohmova'],experiments,"This paper describes a specific part of the Prague Dependency Treebank annotation, the step from the surface dependency structure towards the underlying representation of the sentence. The first section explains the theoretical basis of the project. In Section 2 all the procedure of conversion to the tectogrammatical structure is summarized and Section 3 presents in detail the present stage of the automated part of the conversion procedure.",These automatic transformations are based on linguistic rules ( #AUTHOR_TAG ) .,"['During the tectogrammatical parsing of Czech, the analytical tree structure is converted into the tectogrammatical one.', 'These automatic transformations are based on linguistic rules ( #AUTHOR_TAG ) .', 'Subsequently, tectogrammatical functors are assigned by the C4.5 classifier (2abokrtsk9 et al., 2002).']",5,"['During the tectogrammatical parsing of Czech, the analytical tree structure is converted into the tectogrammatical one.', 'These automatic transformations are based on linguistic rules ( #AUTHOR_TAG ) .', 'Subsequently, tectogrammatical functors are assigned by the C4.5 classifier (2abokrtsk9 et al., 2002).']"
CCT86,E03-1004,Czech-English Dependency-based Machine Translation,a maximumentropyinspired parser,['Eugene Charniak'],experiments,,"We carried out two parallel experiments with two parsers available for Czech , parser I ( Hajie et al. , 1998 ) and parser II ( #AUTHOR_TAG ) .","['The analytical parsing of Czech runs in two steps: the statistical dependency parser, which creates the structure of a dependency tree, and a classifier assigning analytical functors.', 'We carried out two parallel experiments with two parsers available for Czech , parser I ( Hajie et al. , 1998 ) and parser II ( #AUTHOR_TAG ) .', 'In the second step, we used a module for automatic analytical functor assignment (2abokrtskyT et al., 2002).']",5,"['We carried out two parallel experiments with two parsers available for Czech , parser I ( Hajie et al. , 1998 ) and parser II ( #AUTHOR_TAG ) .']"
CCT87,E03-1004,Czech-English Dependency-based Machine Translation,improved statistical alignment models,"['F J Och', 'H Ney']",,"In this paper, we describe improved alignment models for statistical machine translation. The statistical translation approach uses two types of information: a translation model and a language model. The language model used is a bigram or general m-gram model. The translation model is decomposed into a lexical and an alignment model. We describe two different approaches for statistical translation and present experimental results. The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignmen","To make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic CzechEnglish dictionary by running GIZA + + training ( translation models 1-4 , see #AUTHOR_TAG ) on the training part of the English-Czech WSJ parallel corpus extended by the parallel corpus of entry/translation pairs from the manual dictionary.","['To make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic CzechEnglish dictionary by running GIZA + + training ( translation models 1-4 , see #AUTHOR_TAG ) on the training part of the English-Czech WSJ parallel corpus extended by the parallel corpus of entry/translation pairs from the manual dictionary.', 'As a result, the entry/translation pairs seen in the parallel corpus of WSJ become more probable.', 'For entry/translation pairs not seen in the parallel text, the probability distribution among translations is uniform.', 'The translation is ""GIZA++ se- lected"" if its probability is higher than a threshold, which is in our case set to 0.10.']",5,"['To make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic CzechEnglish dictionary by running GIZA + + training ( translation models 1-4 , see #AUTHOR_TAG ) on the training part of the English-Czech WSJ parallel corpus extended by the parallel corpus of entry/translation pairs from the manual dictionary.', 'For entry/translation pairs not seen in the parallel text, the probability distribution among translations is uniform.']"
CCT88,E03-1004,Czech-English Dependency-based Machine Translation,bleu a method for automatic evaluation of machine translation,"['Kishore Papineni', 'Salim Roukos', 'Todd Ward', 'WeiJing Zhu']",introduction,"Human evaluations of machine translation are extensive but expensive. Human eval-uations can take months to finish and in-volve human labor that can not be reused. We propose a method of automatic ma-chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu-ation, and that has little marginal cost per run. We present this method as an auto-mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.",For the evaluation of the results we use the BLEU score ( #AUTHOR_TAG ) .,"['First, we summarize resources available for the experiments (Section 2).', 'Section 3 describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of Czech input.', 'In Section 4 we describe the process of the filtering of dictionaries used in the transfer procedure (for its characterization, see Section 5).', 'The generation process consisting mainly of word reordering and lexical insertions is explained in Section 6, an example illustrating the generation steps is presented in Sec-tion 7.', 'For the evaluation of the results we use the BLEU score ( #AUTHOR_TAG ) .', 'Section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'We also compare the results with the output generated by the statistical translation system GIZA++/ISI ReWrite Decoder (Al-Onaizan et al., 1999;Och and Ney, 2000;Germann et al., 2001), trained on the same parallel corpus.']",5,['For the evaluation of the results we use the BLEU score ( #AUTHOR_TAG ) .']
CCT89,E03-1004,Czech-English Dependency-based Machine Translation,improved statistical alignment models,"['F J Och', 'H Ney']",introduction,"In this paper, we describe improved alignment models for statistical machine translation. The statistical translation approach uses two types of information: a translation model and a language model. The language model used is a bigram or general m-gram model. The translation model is decomposed into a lexical and an alignment model. We describe two different approaches for statistical translation and present experimental results. The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignmen","We also compare the results with the output generated by the statistical translation system GIZA + + / ISI ReWrite Decoder ( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ) , trained on the same parallel corpus .","['First, we summarize resources available for the experiments (Section 2).', 'Section 3 describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of Czech input.', 'In Section 4 we describe the process of the filtering of dictionaries used in the transfer procedure (for its characterization, see Section 5).', 'The generation process consisting mainly of word reordering and lexical insertions is explained in Section 6, an example illustrating the generation steps is presented in Sec-tion 7.', 'For the evaluation of the results we use the BLEU score (Papineni et al., 2001).', 'Section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'We also compare the results with the output generated by the statistical translation system GIZA + + / ISI ReWrite Decoder ( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ) , trained on the same parallel corpus .']",1,"['First, we summarize resources available for the experiments (Section 2).', 'Section 3 describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of Czech input.', 'The generation process consisting mainly of word reordering and lexical insertions is explained in Section 6, an example illustrating the generation steps is presented in Sec-tion 7.', 'For the evaluation of the results we use the BLEU score (Papineni et al., 2001).', 'Section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'We also compare the results with the output generated by the statistical translation system GIZA + + / ISI ReWrite Decoder ( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ) , trained on the same parallel corpus .']"
CCT90,E03-1004,Czech-English Dependency-based Machine Translation,bleu a method for automatic evaluation of machine translation,"['Kishore Papineni', 'Salim Roukos', 'Todd Ward', 'WeiJing Zhu']",experiments,"Human evaluations of machine translation are extensive but expensive. Human eval-uations can take months to finish and in-volve human labor that can not be reused. We propose a method of automatic ma-chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu-ation, and that has little marginal cost per run. We present this method as an auto-mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.","We evaluated our translations with IBM 's BLEU evaluation metric ( #AUTHOR_TAG ) , using the same evaluation method and reference retranslations that were used for evaluation at HLT Workshop 2002 at CLSP ( Haji 6 et al. , 2002 ) .","[""We evaluated our translations with IBM 's BLEU evaluation metric ( #AUTHOR_TAG ) , using the same evaluation method and reference retranslations that were used for evaluation at HLT Workshop 2002 at CLSP ( Haji 6 et al. , 2002 ) ."", 'We used four reference retranslations of 490 sentences selected from the WSJ sections 22, 23, and 24, which were themselves used as the fifth reference.', 'The evaluation method used is to hold out each reference in turn and evaluate it against the remaining four, averaging the five BLEU scores.']",5,"[""We evaluated our translations with IBM 's BLEU evaluation metric ( #AUTHOR_TAG ) , using the same evaluation method and reference retranslations that were used for evaluation at HLT Workshop 2002 at CLSP ( Haji 6 et al. , 2002 ) .""]"
CCT91,E03-1007,Using POS information for statistical machine translation into morphologically rich languages,the mathematics of statistical machine translation parameter estimation,"['P F Brown', 'S A Della Pietra', 'V J Della Pietra', 'R L Mercer']",experiments,"We describe a series o,f ive statistical models o,f the translation process and give algorithms,for estimating the parameters o,f these models given a set o,f pairs o,f sentences that are translations o,f one another. We define a concept o,f word-by-word alignment between such pairs o,f sentences. For any given pair of such sentences each o,f our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable o,f these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair o,f sentences. We have a great deal o,f data in French and English from the proceedings o,f the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we,feel that because our algorithms have minimal inguistic content hey would work well on other pairs o,f languages. We also,feel, again because of the minimal inguistic content o,f our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus. 1",We performed translation experiments with an implementation of the IBM-4 translation model ( #AUTHOR_TAG ) .,"['We compared the two statistical lexica obtained from the baseline system and from the maximum entropy training on the transformed corpus.', 'For the baseline lexicon, we observed an average of 5.82 Catalan translation candidates per English word and 6.16 Spanish translation candidates.', 'These numbers are significantly reduced in the lexicon which was trained on the transformed corpus using maximum entropy: there, we have an average of 4.20 for Catalan and 4.46 for Spanish.', 'Especially for (nominative) English pronouns (which have many verbs as translation candidates in the baseline lexicon), the number of translation candidates was substantially scaled down by a factor around 4. This shows that our method was successful in producing a more focused lexicon probability distribution.', 'We performed translation experiments with an implementation of the IBM-4 translation model ( #AUTHOR_TAG ) .', 'A description of the system can be found in (Tillmann and Ney, 2002).', 'Table 5 presents an assessment of translation quality for both the language pairs English-Catalan and English-Spanish.', 'We see that there is a significant decrease in error rate for the translation into Catalan.', 'This change is consistent across both error rates, the WER and 100-BLEU.', 'For translations from English into Spanish, the improvement is less substantial.', 'A reason for this might be that the Spanish vocabulary contains more entries and the ratio between fullforms and baseforms is higher: 1.57 for Spanish versus 1.53 for Catalan4 .', 'This makes it more difficult for the system to choose the correct inflection when generating a Spanish sentence.', 'We assume that the extension of our approach to other word classes than verbs will yield a quality gain for translations into Spanish.']",5,['We performed translation experiments with an implementation of the IBM-4 translation model ( #AUTHOR_TAG ) .']
CCT92,E03-1007,Using POS information for statistical machine translation into morphologically rich languages,a maximum entropy approach to natural language processing,"['A L Berger', 'S A Della Pietra', 'V J Della Pietra']",,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.",The maximum entropy approach ( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources .,"['The maximum entropy approach ( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources .', 'This principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy.', 'The distribution is required to satisfy constraints, which represent facts known from the data.', 'These constraints are expressed on the basis of feature functions hu,(s,t),']",5,['The maximum entropy approach ( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources .']
CCT93,E03-1007,Using POS information for statistical machine translation into morphologically rich languages,improved alignment models for statistical machine translation,"['F J Och', 'C Tillmann', 'H Ney']",,"In this paper, we describe improved alignment models for statistical machine translation. The statistical translation approach uses two types of information: a translation model and a language model. The language model used is a bigram or general m-gram model. The translation model is decomposed into a lexical and an alignment model. We describe two different approaches for statistical translation and present experimental results. The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words. We present results using the Verbmobil task (German-English, 6000word vocabulary) which is a limited-domain spoken-language task. The experimental tests were performed on both the text transcription and the speech recognizer output. 1 S t a t i s t i c a l M a c h i n e T r a n s l a t i o n The goal of machine translation is the translation of a text given in some source language into a target language. We are given a source string f / = fl...fj...fJ, which is to be translated into a target string e{ = el...ei...ex. Among all possible target strings, we will choose the string with the highest probability: = argmax {Pr(ezIlflJ)}","For descriptions of SMT systems see for example ( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 ) .","['The input string can be preprocessed before being passed to the search algorithm.', 'If necessary, the inverse of these transformations will be applied to the generated output string.', 'In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology.', 'For descriptions of SMT systems see for example ( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 ) .']",0,"['The input string can be preprocessed before being passed to the search algorithm.', 'If necessary, the inverse of these transformations will be applied to the generated output string.', 'In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology.', 'For descriptions of SMT systems see for example ( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 ) .']"
CCT94,E03-1007,Using POS information for statistical machine translation into morphologically rich languages,a maximum entropy approach to natural language processing,"['A L Berger', 'S A Della Pietra', 'V J Della Pietra']",,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.","For an introduction to maximum entropy modeling and training procedures , the reader is referred to the corresponding literature , for instance ( #AUTHOR_TAG ) or ( Ratnaparkhi , 1997 ) .","['where A = {Am } is the set of model parameters with one weight A, for each feature function hm .', 'For an introduction to maximum entropy modeling and training procedures , the reader is referred to the corresponding literature , for instance ( #AUTHOR_TAG ) or ( Ratnaparkhi , 1997 ) .']",0,"['For an introduction to maximum entropy modeling and training procedures , the reader is referred to the corresponding literature , for instance ( #AUTHOR_TAG ) or ( Ratnaparkhi , 1997 ) .']"
CCT95,E03-1007,Using POS information for statistical machine translation into morphologically rich languages,fast decoding and optimal decoding for machine translation,"['U Germann', 'M Jahr', 'K Knight', 'D Marcu', 'K Yamada']",,"A good decoding algorithm is critical to the success of any statistical machine translation system. The decoder's job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them). Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions. In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem.","For descriptions of SMT systems see for example ( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 ) .","['The input string can be preprocessed before being passed to the search algorithm.', 'If necessary, the inverse of these transformations will be applied to the generated output string.', 'In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology.', 'For descriptions of SMT systems see for example ( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 ) .']",0,"['The input string can be preprocessed before being passed to the search algorithm.', 'If necessary, the inverse of these transformations will be applied to the generated output string.', 'In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology.', 'For descriptions of SMT systems see for example ( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 ) .']"
CCT96,E09-1100,Character-level dependencies in Chinese,exploiting unlabeled text with different unsupervised segmentation criteria for chinese word segmentation,"['Hai Zhao', 'Chunyu Kit']",method,This paper presents a novel approach to improve Chinese word seg- mentation (CWS) that attempts to utilize unlabeled data such as training and test data without annotation for further enhancement of the state-of-the-art perfor- mance of supervised learning. The lexical information plays the role of infor- mation transformation from unlabeled text to supervised learning model. Four types of unsupervised segmentation criteria are used for word candidate extrac- tion and the corresponding word likelihood computation. The information output by unsupervised segmentation criteria as features therefore is integrated into su- pervised learning model to strengthen the learning for the matching subsequence. The effectiveness of the proposed method is verified in data sets from the latest in- ternational CWS evaluation. Our experimental results show that character-based conditional random fields framework can effectively make use of such informa- tion from unlabeled data for performance enhancement on top of the best existing results.,"Due to using a global model like CRFs , our previous work in ( Zhao et al. , 2006 ; #AUTHOR_TAGc ) reported the best results over the evaluated corpora of Bakeoff-2 until now7 .","['Due to using a global model like CRFs , our previous work in ( Zhao et al. , 2006 ; #AUTHOR_TAGc ) reported the best results over the evaluated corpora of Bakeoff-2 until now7 .', 'Though those results are slightly better than the results here, we still see that the results of character-level dependency parsing approach (Scheme E) are comparable to those state-of-the-art ones on each evaluated corpus.']",1,"['Due to using a global model like CRFs , our previous work in ( Zhao et al. , 2006 ; #AUTHOR_TAGc ) reported the best results over the evaluated corpora of Bakeoff-2 until now7 .']"
CCT97,J00-1003,Practical Experiments with Regular Approximation of Context-Free Languages,dynamic compilation of weighted contextfree grammars,"['Mehryar Mohri', 'Fernando C N Pereira']",,"Weighted context-free grammars are a convenient formalism for representing grammatical constructions and their likelihoods in a variety of language-processing applications. In particular, speech understanding applications require appropriate grammars both to constrain speech recognition and to help extract the meaning of utterances. In many of those applications, the actual languages described are regular, but context-free representations are much more concise and easier to create. We describe an efficient algorithm for compiling into weighted finite automata an interesting class of weighted context-free grammars that represent regular languages. The resulting automata can then be combined with other speech recognition components. Our method allows the recognizer to dynamically activate or deactivate grammar rules and substitute a new regular language for some terminal symbols, depending on previously recognized inputs, all without recompilation. We also report experimental results showing the practicality of the approach.",1 The representation in #AUTHOR_TAG is even more compact than ours for grammars that are not self-embedding .,"['1 The representation in #AUTHOR_TAG is even more compact than ours for grammars that are not self-embedding .', 'However, in this paper we use our representation as an intermediate result in approximating an unrestricted context-free grammar, with the final objective of obtaining a single minimal deterministic automaton.', ""For this purpose, Mohri and Pereira's representation offers little advantage.""]",1,"['1 The representation in #AUTHOR_TAG is even more compact than ours for grammars that are not self-embedding .', 'However, in this paper we use our representation as an intermediate result in approximating an unrestricted context-free grammar, with the final objective of obtaining a single minimal deterministic automaton.']"
CCT98,J00-1003,Practical Experiments with Regular Approximation of Context-Free Languages,finitestate approximation of constraintbased grammars using leftcorner grammar transforms,['Mark Johnson'],,,"This idea was proposed by Krauwer and des Tombe ( 1981 ) , Langendoen and Langsam ( 1987 ) , and Pulman ( 1986 ) , and was rediscovered by Black ( 1989 ) and recently by #AUTHOR_TAG .","['By restricting the height of the stack of a pushdown automaton, one obstructs recognition of a set of strings in the context-free language, and therefore a subset approximation results.', 'This idea was proposed by Krauwer and des Tombe ( 1981 ) , Langendoen and Langsam ( 1987 ) , and Pulman ( 1986 ) , and was rediscovered by Black ( 1989 ) and recently by #AUTHOR_TAG .', 'Since the latest publication in this area is more explicit in its presentation, we will base our treatment on this, instead of going to the historical roots of the method.']",0,"['By restricting the height of the stack of a pushdown automaton, one obstructs recognition of a set of strings in the context-free language, and therefore a subset approximation results.', 'This idea was proposed by Krauwer and des Tombe ( 1981 ) , Langendoen and Langsam ( 1987 ) , and Pulman ( 1986 ) , and was rediscovered by Black ( 1989 ) and recently by #AUTHOR_TAG .']"
CCT99,J00-1003,Practical Experiments with Regular Approximation of Context-Free Languages,approximating contextfree grammars with a finitestate calculus,['Edmund Grimley-Evans'],,,"We rephrase the method of #AUTHOR_TAG as follows : First , we construct the approximating finite automaton according to the unparameterized RTN method above .","['We rephrase the method of #AUTHOR_TAG as follows : First , we construct the approximating finite automaton according to the unparameterized RTN method above .', 'Then an additional mechanism is introduced that ensures for each rule A --~ X1 • .. Xm separately that the list of visits to the states qo,.. • • qm satisfies some reasonable criteria: a visit to qi, with 0 < i < m, should be followed by one to qi+l or q0.', 'The latter option amounts to a nested incarnation of the rule.', 'There is a complementary condition for what should precede a visit to qi, with 0 < i < m.']",5,"['We rephrase the method of #AUTHOR_TAG as follows : First , we construct the approximating finite automaton according to the unparameterized RTN method above .']"
CCT100,J00-1003,Practical Experiments with Regular Approximation of Context-Free Languages,precise ngram probabilities from stochastic contextfree grammars,"['Andreas Stolcke', 'Jonathan Segal']",,"We present an algorithm for computing n-gram probabilities from stochastic context-free grammars, a procedure that can alleviate some of the standard problems associated with n-grams (estimation from sparse data, lack of linguistic structure, among others). The method operates via the computation of substring expectations, which in turn is accomplished by solving systems of linear equations derived from the grammar. The procedure is fully implemented and has proved viable and useful in practice.","This method can be generalized , inspired by #AUTHOR_TAG , who derive N-gram probabilities from stochastic context-free grammars .","['This method can be generalized , inspired by #AUTHOR_TAG , who derive N-gram probabilities from stochastic context-free grammars .', 'By ignoring the probabilities, each N = 1, 2, 3 .... gives rise to a superset approximation that can be described as follows: The set of strings derivable from a nonterminal A is approximated by the set of strings al ... an such that • for each substring v = ai+l ... ai+N (0 < i < n --N) we have A --+* wvy, for some w and y,']",0,"['This method can be generalized , inspired by #AUTHOR_TAG , who derive N-gram probabilities from stochastic context-free grammars .']"
CCT101,J00-1003,Practical Experiments with Regular Approximation of Context-Free Languages,contextfree parsing through regular approximation,['Mark-Jan Nederhof'],,"We show that context-free parsing can be realised by a 2-phase process, relying on an approximated context-free grammar. In the first phase a finite transducer performs parsing according to the approximation. In the second phase, the approximated parses are refined according to the original grammar.",See #AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata .,['See #AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata .'],0,['See #AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata .']
CCT102,J00-1003,Practical Experiments with Regular Approximation of Context-Free Languages,contextfree parsing through regular approximation,['Mark-Jan Nederhof'],,"We show that context-free parsing can be realised by a 2-phase process, relying on an approximated context-free grammar. In the first phase a finite transducer performs parsing according to the approximation. In the second phase, the approximated parses are refined according to the original grammar.","A very similar formulation , for another grammar transformation , is given in #AUTHOR_TAG .","['Figure 10 (b) is no longer possible, since no nonterminal in the transformed grammar would contain 1 in its superscript.Because of the demonstrated increase of the counter f, this transformation is guaranteed to remove self-embedding from the grammar.', 'However, it is not as selective as the transformation we saw before, in the sense that it may also block subderivations that are not of the form A --** ~Afl.', 'Consider for example the subderivation from Figure10, but replacing the lower occurrence of S by any other nonterminal C that is mutually recursive with S, A, and B. Such a subderivation S ---** b c C d a would also be blocked by choosing d = 0.', 'In general, increasing d allows more of such derivations that are not of the form A ~"" o~Afl but also allows more derivations that are of that form.The reason for considering this transformation rather than any other that eliminates self-embedding is purely pragmatic: of the many variants we have tried that yield nontrivial subset approximations, this transformation has the lowest complexity in terms of the sizes of intermediate structures and of the resulting finite automata.In the actual implementation, we have integrated the grammar transformation and the construction of the finite automaton, which avoids reanalysis of the grammar to determine the partition of mutually recursive nonterminals after transformation.', 'This integration makes use, for example, of the fact that for fixed Ni and fixed f, the set of nonterminals of the form A,f, with A c Ni, is (potentially) mutually right-recursive.A set of such nonterminals can therefore be treated as the corresponding case from Figure2, assuming the value right.The full formulation of the integrated grammar transformation and construction of the finite automaton is rather long and is therefore not given here.', 'A very similar formulation , for another grammar transformation , is given in #AUTHOR_TAG .']",1,"['This integration makes use, for example, of the fact that for fixed Ni and fixed f, the set of nonterminals of the form A,f, with A c Ni, is (potentially) mutually right-recursive.A set of such nonterminals can therefore be treated as the corresponding case from Figure2, assuming the value right.The full formulation of the integrated grammar transformation and construction of the finite automaton is rather long and is therefore not given here.', 'A very similar formulation , for another grammar transformation , is given in #AUTHOR_TAG .']"
CCT103,J00-2003,A Multistrategy Approach to Improving Pronunciation by Analogy,algorithms for graphemephoneme translation for english and french applications for database searches and speech synthesis,"['Michel Divay', 'Anthony J Vitale']",introduction,"Letter-to-sound rules, also known as grapheme-to-phoneme rules, are important computational tools and have been used for a variety of purposes including word or name lookups for database searches and speech synthesis.These rules are especially useful when integrated into database searches on names and addresses, since they can complement orthographic search algorithms that make use of permutation, deletion, and insertion by allowing for a comparison with the phonetic equivalent. In databases, phonetics can help retrieve a word or a proper name without the user needing to know the correct spelling. A phonetic index is built with the vocabulary of the application. This could be an entire dictionary, or a list of proper names. The searched word is then converted into phonetics and retrieved with its information, if the word is in the phonetic index. This phonetic lookup can be used to retrieve a misspelled word in a dictionary or a database, or in a text editor to suggest corrections.Such rules are also necessary to formalize grapheme-phoneme correspondences in speech synthesis architecture. In text-to-speech systems, these rules are typically used to create phonemes from computer text. These phonemic symbols, in turn, are used to feed lower-level phonetic modules (such as timing, intonation, vowel formant trajectories, etc.) which, in turn, feed a vocal tract model and finally output a waveform and, via a digital-analogue converter, synthesized speech. Such rules are a necessary and integral part of a text-to-speech system since a database lookup (dictionary search) is not sufficient to handle derived forms, new words, nonce forms, proper nouns, low-frequency technical jargon, and the like; such forms typically are not included in the database. And while the use of a dictionary is more important now that denser and faster memory is available to smaller systems, letter-to-sound still plays a crucial and central role in speech synthesis technology.Grapheme-to-phoneme technology is also useful in speech recognition, as a way of generating pronunciations for new words that may be available in grapheme form, or for naive users to add new words more easily. In that case, the system must generate the multiple variations of the word.While there are different problems in languages that use non-alphabetic writing systems (syllabaries, as in Japanese, or logographic systems, as in Chinese) (DeFrancis 1984), all alphabetic systems have a structured set of correspondences. These range from the trivial in languages like Spanish or Swahili, to extremely complex in languages such as English and French. This paper will outline some of the previous attempts to construct such rule sets and will describe new and successful approaches to the construction of letter-to-sound rules for English and French.","Typical letter-to-sound rule sets are those described by Ainsworth ( 1973 ) , McIlroy ( 1973 ) , Elovitz et al. ( 1976 ) , Hurmicutt ( 1976 ) , and #AUTHOR_TAG .","['which states that the letter substring B with left context A and right context C receives the pronunciation (i.e., phoneme substring) D. Such rules can also be straightforwardly cast in the IF... THEN form commonly featured in high-level programming languages and employed in expert, knowledge-based systems technology.', 'They also constitute a formal model of universal computation (Post 1943).', 'Conventionally, these rules are specified by an expert linguist, conversant with the sound and spelling systems of the language of concern.', 'Typical letter-to-sound rule sets are those described by Ainsworth ( 1973 ) , McIlroy ( 1973 ) , Elovitz et al. ( 1976 ) , Hurmicutt ( 1976 ) , and #AUTHOR_TAG .']",0,"['which states that the letter substring B with left context A and right context C receives the pronunciation (i.e., phoneme substring) D. Such rules can also be straightforwardly cast in the IF... THEN form commonly featured in high-level programming languages and employed in expert, knowledge-based systems technology.', 'Conventionally, these rules are specified by an expert linguist, conversant with the sound and spelling systems of the language of concern.', 'Typical letter-to-sound rule sets are those described by Ainsworth ( 1973 ) , McIlroy ( 1973 ) , Elovitz et al. ( 1976 ) , Hurmicutt ( 1976 ) , and #AUTHOR_TAG .']"
CCT104,J00-2003,A Multistrategy Approach to Improving Pronunciation by Analogy,algorithms for graphemephoneme translation for english and french applications for database searches and speech synthesis,"['Michel Divay', 'Anthony J Vitale']",introduction,"Letter-to-sound rules, also known as grapheme-to-phoneme rules, are important computational tools and have been used for a variety of purposes including word or name lookups for database searches and speech synthesis.These rules are especially useful when integrated into database searches on names and addresses, since they can complement orthographic search algorithms that make use of permutation, deletion, and insertion by allowing for a comparison with the phonetic equivalent. In databases, phonetics can help retrieve a word or a proper name without the user needing to know the correct spelling. A phonetic index is built with the vocabulary of the application. This could be an entire dictionary, or a list of proper names. The searched word is then converted into phonetics and retrieved with its information, if the word is in the phonetic index. This phonetic lookup can be used to retrieve a misspelled word in a dictionary or a database, or in a text editor to suggest corrections.Such rules are also necessary to formalize grapheme-phoneme correspondences in speech synthesis architecture. In text-to-speech systems, these rules are typically used to create phonemes from computer text. These phonemic symbols, in turn, are used to feed lower-level phonetic modules (such as timing, intonation, vowel formant trajectories, etc.) which, in turn, feed a vocal tract model and finally output a waveform and, via a digital-analogue converter, synthesized speech. Such rules are a necessary and integral part of a text-to-speech system since a database lookup (dictionary search) is not sufficient to handle derived forms, new words, nonce forms, proper nouns, low-frequency technical jargon, and the like; such forms typically are not included in the database. And while the use of a dictionary is more important now that denser and faster memory is available to smaller systems, letter-to-sound still plays a crucial and central role in speech synthesis technology.Grapheme-to-phoneme technology is also useful in speech recognition, as a way of generating pronunciations for new words that may be available in grapheme form, or for naive users to add new words more easily. In that case, the system must generate the multiple variations of the word.While there are different problems in languages that use non-alphabetic writing systems (syllabaries, as in Japanese, or logographic systems, as in Chinese) (DeFrancis 1984), all alphabetic systems have a structured set of correspondences. These range from the trivial in languages like Spanish or Swahili, to extremely complex in languages such as English and French. This paper will outline some of the previous attempts to construct such rule sets and will describe new and successful approaches to the construction of letter-to-sound rules for English and French.","For instance , #AUTHOR_TAG recently wrote : `` To our knowledge , learning algorithms , although promising , have not ( yet ) reached the level of rule sets developed by humans '' ( p. 520 ) .","['It is also conceivable that data-driven techniques can actually outperform traditional rules.', 'However, this possibility is not usually given much credence.', ""For instance , #AUTHOR_TAG recently wrote : `` To our knowledge , learning algorithms , although promising , have not ( yet ) reached the level of rule sets developed by humans '' ( p. 520 ) ."", '520).', 'Dutoit (1997) takes this further, stating ""such training-based strategies are often assumed to exhibit much more intelligence than they do in practice, as revealed by their poor transcription scores"" (p.', '115, note 14).']",0,"['It is also conceivable that data-driven techniques can actually outperform traditional rules.', ""For instance , #AUTHOR_TAG recently wrote : `` To our knowledge , learning algorithms , although promising , have not ( yet ) reached the level of rule sets developed by humans '' ( p. 520 ) ."", '520).', 'Dutoit (1997) takes this further, stating ""such training-based strategies are often assumed to exhibit much more intelligence than they do in practice, as revealed by their poor transcription scores"" (p.']"
CCT105,J00-2003,A Multistrategy Approach to Improving Pronunciation by Analogy,using an online dictionary to find rhyming words and pronunciations for unknown words,"['Roy J Byrd', 'Martin S Chodorow']",introduction,"Humans know a great deal about relationships among words. This paper discusses relationships among word pronunciations. We describe a computer system which models human judgement of rhyme by assigning specific roles to the location of primary stress, the similarity of phonetic segments, and other factors. By using the model as an experimental tool, we expect to improve our understanding of rhyme. A related computer model will attempt to generate pronunciations for unknown words by analogy with those for known words. The analogical processes involve techniques for segmenting and matching word spellings, and for mapping spelling to sound in known words. As in the case of rhyme, the computer model will be an important tool for improving our understanding of these processes. Both models serve as the basis for functions in the WordSmith automated dictionary system.","See also the work of #AUTHOR_TAG , which considers computer-based pronunciation by analogy but does not mention the possible application to text-to-speech synthesis .","['Pronunciation by analogy (PbA) is a data-driven technique for the automatic phonemization of text, originally proposed as a model of reading, e.g., by Glushko (1979) and Kay and Marcel (1981).', 'It was first proposed for TTS applications over a decade ago by Dedina andNusbaum (1986, 1991).', 'See also the work of #AUTHOR_TAG , which considers computer-based pronunciation by analogy but does not mention the possible application to text-to-speech synthesis .', 'As detailed by Damper (1995) and Damper and Eastmond (1997), PbA shares many similarities with the artificial intelligence paradigms variously called case-based, memory-based, or instance-based reasoning as applied to letter-to-phoneme conversion (Stanfill and Waltz 1986;Lehnert 1987;Stanfill 1987Stanfill , 1988Golding 1991;Golding and Rosenbloom 1991;van den Bosch and Daelemans 1993).']",0,"['Pronunciation by analogy (PbA) is a data-driven technique for the automatic phonemization of text, originally proposed as a model of reading, e.g., by Glushko (1979) and Kay and Marcel (1981).', 'See also the work of #AUTHOR_TAG , which considers computer-based pronunciation by analogy but does not mention the possible application to text-to-speech synthesis .']"
CCT106,J00-2003,A Multistrategy Approach to Improving Pronunciation by Analogy,classifier combination for improved lexical disambiguation,"['Eric Brill', 'Jun Wu']",experiments,"One of the most exciting recent directions in machine learning is the discovery that the combination of multiple classifiers often results in significantly better performance than what can be achieved with a single classifier. In this paper, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary. Next, we show how this complementary behavior can be used to our advantage. By using contextual cues to guide tagger combination, we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers. Introduction Part of speech tagging has been a central problem in natural language processing for many years. Since the advent of manually tagged corpora such as the Brown Corpus and the Penn Treebank (Francis(1982), Marcus(1993)), the efficacy of machine learning for training a tagger has been demonstrated using a wide array of techniques, including: Markov models, decision trees..","Consequently , fusion has been applied to a wide variety of pattern recognition and decision theoretic problems -- using a plethora of theories , techniques , and tools -- including some applications in computational linguistics ( e.g. , #AUTHOR_TAG ; van Halteren , Zavrel , and Daelemans 1998 ) and speech technology ( e.g. , Bowles and Damper 1989 ; Romary and Pierre11989 ) .","['Clearly, the above characterization is very wide ranging.', 'Consequently , fusion has been applied to a wide variety of pattern recognition and decision theoretic problems -- using a plethora of theories , techniques , and tools -- including some applications in computational linguistics ( e.g. , #AUTHOR_TAG ; van Halteren , Zavrel , and Daelemans 1998 ) and speech technology ( e.g. , Bowles and Damper 1989 ; Romary and Pierre11989 ) .', 'According to Abbott (1999, 290), ""While the reasons [that] combining models works so well are not rigorously understood, there is ample evidence that improvements over single models are typical....', 'A strong case can be made for combining models across algorithm families as a means of providing uncorrelated output estimates.""', 'Our purpose in this paper is to study and exploit such fusion by model (or strategy) combination as a way of achieving performance gains in PbA.']",0,"['Consequently , fusion has been applied to a wide variety of pattern recognition and decision theoretic problems -- using a plethora of theories , techniques , and tools -- including some applications in computational linguistics ( e.g. , #AUTHOR_TAG ; van Halteren , Zavrel , and Daelemans 1998 ) and speech technology ( e.g. , Bowles and Damper 1989 ; Romary and Pierre11989 ) .']"
CCT107,J00-2004,Models of Translational Equivalence among Words,automatic evaluation and uniform filter cascades for inducing nbest translation lexicons,['I Dan Melamed'],,"This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources. The knowledge sources are cast as filters, so that any subset of them can be cascaded in a uniform framework. A new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades. The best filter cascades improve lexicon quality by up to 137% over the plain vanilla statistical method, and approach human performance. Drastically reducing the size of the training corpus has a much smaller impact on lexicon quality when these knowledge sources are used. This makes it practical to train on small hand-built corpora for language pairs where large bilingual corpora are unavailable. Moreover, three of the four filters prove useful even when used with large training corpora.","Many researchers have proposed greedy algorithms for estimating nonprobabilistic word-to-word translation models, also known as translation lexicons (e.g., Catizone, Russell, and Warwick 1989; Gale and Church 1991; Fung 1995; Kumano and Hirakawa 1994; #AUTHOR_TAG 1995; Wu and Xia 1994).","['Many researchers have proposed greedy algorithms for estimating nonprobabilistic word-to-word translation models, also known as translation lexicons (e.g., Catizone, Russell, and Warwick 1989; Gale and Church 1991; Fung 1995; Kumano and Hirakawa 1994; #AUTHOR_TAG 1995; Wu and Xia 1994).', 'Most of these algorithms can be summarized as follows:']",0,"['Many researchers have proposed greedy algorithms for estimating nonprobabilistic word-to-word translation models, also known as translation lexicons (e.g., Catizone, Russell, and Warwick 1989; Gale and Church 1991; Fung 1995; Kumano and Hirakawa 1994; #AUTHOR_TAG 1995; Wu and Xia 1994).']"
CCT108,J00-2004,Models of Translational Equivalence among Words,but dictionaries are data too,"['Peter F Brown', 'Stephen A Della Pietra', 'Vincent J Della Pietra', 'Meredith J Goldsmith', 'Jan Hajic', 'Robert L Mercer', 'Surya Mohanty']",related work,"Although empiricist approaches tomachine translation depend vitally on data in the form of large bilingual cor-pora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum iike-fihood estimates of the parameters ofa probabilistic model from this combined ata and we show how these param-eters are affected by inclusion of the dictionary for some sample words. There is a sharp dichotomy today between ratio",Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by #AUTHOR_TAGb ) .,"['Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by #AUTHOR_TAGb ) .', 'These models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution.', 'I shall review these models using the notation in Table 1.']",0,['Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by #AUTHOR_TAGb ) .']
CCT109,J00-2004,Models of Translational Equivalence among Words,but dictionaries are data too,"['Peter F Brown', 'Stephen A Della Pietra', 'Vincent J Della Pietra', 'Meredith J Goldsmith', 'Jan Hajic', 'Robert L Mercer', 'Surya Mohanty']",method,"Although empiricist approaches tomachine translation depend vitally on data in the form of large bilingual cor-pora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum iike-fihood estimates of the parameters ofa probabilistic model from this combined ata and we show how these param-eters are affected by inclusion of the dictionary for some sample words. There is a sharp dichotomy today between ratio","Unlike the models proposed by #AUTHOR_TAGb ) , this model is symmetric , because both word bags are generated together from a joint probability distribution .","['The probability distribution trans (.1, ~) is a word-to-word translation model.', 'Unlike the models proposed by #AUTHOR_TAGb ) , this model is symmetric , because both word bags are generated together from a joint probability distribution .', ""Brown and his colleagues' models, reviewed in Section 4.3, generate one half of the bitext given the other hall so they are represented by conditional probability distributions."", 'A sequenceto-sequence translation model can be obtained from a word-to-word translation model by combining Equation 11 with order information as in Equation 8.']",1,"['The probability distribution trans (.1, ~) is a word-to-word translation model.', 'Unlike the models proposed by #AUTHOR_TAGb ) , this model is symmetric , because both word bags are generated together from a joint probability distribution .']"
CCT110,J00-2004,Models of Translational Equivalence among Words,a perspective on word sense disambiguation methods and their evaluation,"['Philip Resnik', 'David Yarowsky']",,"In this position paper, we make several observations about the state of the art in automatic word sense disambiguation. Motivated by these observations, we offer several specific proposals to the community regarding improved evaluation criteria, common training and testing resources, and the definition of sense inventories.","If each word 's translation is treated as a sense tag ( #AUTHOR_TAG ) , then `` translational '' collocations have the unique property that the collocate and the word sense are one and the same !","['in bitext space is another kind of collocation.', ""If each word 's translation is treated as a sense tag ( #AUTHOR_TAG ) , then `` translational '' collocations have the unique property that the collocate and the word sense are one and the same !"", 'Method B exploits this property under the hypothesis that ""one sense per collocation"" holds for translational collocations.', 'This hypothesis implies that if u and v are possible mutual translations, and a token u co-occurs with a token v in the bitext, then with very high probability the pair (u, v) was generated from the same concept and should be linked.', 'To test this hypothesis, I ran one iteration of Method A on 300,000 aligned sentence pairs from the Canadian Hansards bitext.', 'I then plotted the links (u,v) ratio ~ for several values of cooc (u, v) in Figure 2. The curves show that the ratio links (u,v) cooc (u,v) tends to be either very high or very low.', 'This bimodality is not an artifact of the competitive linking process, because in the first iteration, linking decisions are based only on the initial similarity metric.']",5,"[""If each word 's translation is treated as a sense tag ( #AUTHOR_TAG ) , then `` translational '' collocations have the unique property that the collocate and the word sense are one and the same !"", 'Method B exploits this property under the hypothesis that ""one sense per collocation"" holds for translational collocations.']"
CCT111,J00-2004,Models of Translational Equivalence among Words,automatic evaluation and uniform filter cascades for inducing nbest translation lexicons,['I Dan Melamed'],,"This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources. The knowledge sources are cast as filters, so that any subset of them can be cascaded in a uniform framework. A new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades. The best filter cascades improve lexicon quality by up to 137% over the plain vanilla statistical method, and approach human performance. Drastically reducing the size of the training corpus has a much smaller impact on lexicon quality when these knowledge sources are used. This makes it practical to train on small hand-built corpora for language pairs where large bilingual corpora are unavailable. Moreover, three of the four filters prove useful even when used with large training corpora.","In informal experiments described elsewhere ( #AUTHOR_TAG ) , I found that the G2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02 .","['In informal experiments described elsewhere ( #AUTHOR_TAG ) , I found that the G2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02 .', 'Let the cells of the contingency table be named as follows:']",2,"['In informal experiments described elsewhere ( #AUTHOR_TAG ) , I found that the G2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02 .']"
CCT112,J00-2004,Models of Translational Equivalence among Words,reading more into foreign languages,"['John Nerbonne', 'Lauri Karttunen', 'Elena Paskaleva', 'Gabor Proszeky', 'Tiit Roosmaa']",method,"GLOSSER is designed to support reading and learning to read in a foreign language. There are four language pairs currently supported by GLOSSER: English Bulgarian, English-Estonian, English Hungarian and French-Dutch. The program is operational on UNIX and Windows '95 platforms, and has undergone a pilot user-study. A demonstration (in UNIX) for Applied Natural Language Processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis (ICALL), including disambiguated morphological analysis and lemmatized indexing for an aligned bilingual corpus of word examples.","â¢ cross-language information retrieval ( e.g. , McCarley 1999 ) , â¢ multilingual document filtering ( e.g. , Oard 1997 ) , â¢ computer-assisted language learning ( e.g. , #AUTHOR_TAG ) , â¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,","['â\x80¢ cross-language information retrieval ( e.g. , McCarley 1999 ) , â\x80¢ multilingual document filtering ( e.g. , Oard 1997 ) , â\x80¢ computer-assisted language learning ( e.g. , #AUTHOR_TAG ) , â\x80¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â\x80¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']",0,"['â\x80¢ cross-language information retrieval ( e.g. , McCarley 1999 ) , â\x80¢ multilingual document filtering ( e.g. , Oard 1997 ) , â\x80¢ computer-assisted language learning ( e.g. , #AUTHOR_TAG ) , â\x80¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â\x80¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']"
CCT113,J00-2004,Models of Translational Equivalence among Words,but dictionaries are data too,"['Peter F Brown', 'Stephen A Della Pietra', 'Vincent J Della Pietra', 'Meredith J Goldsmith', 'Jan Hajic', 'Robert L Mercer', 'Surya Mohanty']",,"Although empiricist approaches tomachine translation depend vitally on data in the form of large bilingual cor-pora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum iike-fihood estimates of the parameters ofa probabilistic model from this combined ata and we show how these param-eters are affected by inclusion of the dictionary for some sample words. There is a sharp dichotomy today between ratio","In this situation , #AUTHOR_TAGb , 293 ) recommend `` evaluating the expectations using only a single , probable alignment . ''","[""In this situation , #AUTHOR_TAGb , 293 ) recommend `` evaluating the expectations using only a single , probable alignment . ''"", 'The single most probable assignment Ama~ is the maximum a posteriori (MAP) assignment:']",4,"[""In this situation , #AUTHOR_TAGb , 293 ) recommend `` evaluating the expectations using only a single , probable alignment . ''""]"
CCT114,J00-2004,Models of Translational Equivalence among Words,but dictionaries are data too,"['Peter F Brown', 'Stephen A Della Pietra', 'Vincent J Della Pietra', 'Meredith J Goldsmith', 'Jan Hajic', 'Robert L Mercer', 'Surya Mohanty']",,"Although empiricist approaches tomachine translation depend vitally on data in the form of large bilingual cor-pora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum iike-fihood estimates of the parameters ofa probabilistic model from this combined ata and we show how these param-eters are affected by inclusion of the dictionary for some sample words. There is a sharp dichotomy today between ratio","Just as easily , we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not ( cfXXX #AUTHOR_TAG ) .","['In Method B, the estimation of the auxiliary parameters A + and A-depends only on the overall distribution of co-occurrence counts and link frequencies.', 'All word pairs that co-occur the same number of times and are linked the same number of times are assigned the same score.', 'More accurate models can be induced by taking into account various features of the linked tokens.', 'For example, frequent words are translated less consistently than rare words (Catizone, Russell, and Warwick 1989).', 'To account for these differences, we can estimate separate values of A + and A-for different ranges of cooc (u, v).', 'Similarly, the auxiliary parameters can be conditioned on the linked parts of speech.', 'A kind of word order correlation bias can be effected by conditioning the auxiliary parameters on the relative positions of linked word tokens in their respective texts.', 'Just as easily , we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not ( cfXXX #AUTHOR_TAG ) .', 'Brown et al. 1993).', 'When the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class: B (links (u, v)[cooc(u, v), A +) scorec (u, vlZ = class(u, v)) = log B(links(u, v)[cooc(u, v), A z)"" (37) Section 6.1.1 describes the link classes used in the experiments below.']",5,"['Just as easily , we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not ( cfXXX #AUTHOR_TAG ) .']"
CCT115,J00-2004,Models of Translational Equivalence among Words,but dictionaries are data too,"['Peter F Brown', 'Stephen A Della Pietra', 'Vincent J Della Pietra', 'Meredith J Goldsmith', 'Jan Hajic', 'Robert L Mercer', 'Surya Mohanty']",,"Although empiricist approaches tomachine translation depend vitally on data in the form of large bilingual cor-pora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum iike-fihood estimates of the parameters ofa probabilistic model from this combined ata and we show how these param-eters are affected by inclusion of the dictionary for some sample words. There is a sharp dichotomy today between ratio","Until now , translation models have been evaluated either subjectively ( e.g. White and O'Connell 1993 ) or using relative metrics , such as perplexity with respect to other models ( #AUTHOR_TAGb ) .","[""Until now , translation models have been evaluated either subjectively ( e.g. White and O'Connell 1993 ) or using relative metrics , such as perplexity with respect to other models ( #AUTHOR_TAGb ) ."", 'Objective and more accurate tests can be carried out using a ""gold standard.""', 'I hired bilingual annotators to link roughly 16,000 corresponding words between on-line versions of the Bible in French and English.', 'This bitext was selected to facilitate widespread use and standardization (see Melamed [1998c] for details).', 'The entire Bible bitext comprised 29,614 verse pairs, of which 250 verse pairs were hand-linked using a specially developed annotation tool.', 'The annotation style guide (Melamed 1998b) was based on the intuitions of the annotators, so it was not biased towards any particular translation model.', 'The annotation was replicated five times by seven different annotators.']",1,"[""Until now , translation models have been evaluated either subjectively ( e.g. White and O'Connell 1993 ) or using relative metrics , such as perplexity with respect to other models ( #AUTHOR_TAGb ) ."", 'I hired bilingual annotators to link roughly 16,000 corresponding words between on-line versions of the Bible in French and English.', 'The entire Bible bitext comprised 29,614 verse pairs, of which 250 verse pairs were hand-linked using a specially developed annotation tool.', 'The annotation style guide (Melamed 1998b) was based on the intuitions of the annotators, so it was not biased towards any particular translation model.', 'The annotation was replicated five times by seven different annotators.']"
CCT116,J00-2004,Models of Translational Equivalence among Words,accurate methods for the statistics of surprise and coincidence,['Ted Dunning'],,"Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text.","In informal experiments described elsewhere ( Melamed 1995 ) , I found that the G2 statistic suggested by #AUTHOR_TAG slightly outperforms 02 .","['In informal experiments described elsewhere ( Melamed 1995 ) , I found that the G2 statistic suggested by #AUTHOR_TAG slightly outperforms 02 .', 'Let the cells of the contingency table be named as follows:']",0,"['In informal experiments described elsewhere ( Melamed 1995 ) , I found that the G2 statistic suggested by #AUTHOR_TAG slightly outperforms 02 .', 'Let the cells of the contingency table be named as follows:']"
CCT117,J00-2004,Models of Translational Equivalence among Words,building parallel ltag for french and italian,['Marie-Helene Candito'],method,"In this paper we view Lexicalized Tree Adjoining Grammars as the compilation of a more abstract and modular layer of linguistic description: the metagrammar (MG). MG provides a hierarchical representation of lexico-syntactic descriptions and principles that capture the well-formedness of lexicalized structures, expressed using syntactic functions. This makes it possible for a tool to compile an instance of MG into an LTAG, automatically performing the relevant combinations of linguistic phenomena. We then describe the instantiation of an MG for Italian and French. The work for French was performed starting with an existing LTAG, which has been augmented as a result. The work for Italian was performed by systematic contrast with the French MG. The automatic compilation gives two parallel LTAG, compatible for multilingual NLP applications.","There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .","['The above equation holds regardless of how we represent concepts.', 'There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .', 'Of course, for a representation to be used, a method must exist for estimating its distribution in data.']",0,"['There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .']"
CCT118,J00-2004,Models of Translational Equivalence among Words,should we translate the documents or the queries in crosslanguage information retrieval,['J Scott McCarley'],method,"Previous comparisons of document and query translation suffered difficulty due to differing quality of machine translation in these two opposite directions. We avoid this difficulty by training identical statistical translation models for both translation directions using the same training data. We investigate information retrieval between English and French, incorporating both translations directions into both document translation and query translation-based information retrieval, as well as into hybrid systems. We find that hybrids of document and query translation-based systems out-perform query translation systems, even human-quality query translation systems.","â¢ cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , â¢ multilingual document filtering ( e.g. , Oard 1997 ) , â¢ computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , â¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,","['â\x80¢ cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , â\x80¢ multilingual document filtering ( e.g. , Oard 1997 ) , â\x80¢ computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , â\x80¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â\x80¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']",0,"['â\x80¢ cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , â\x80¢ multilingual document filtering ( e.g. , Oard 1997 ) , â\x80¢ computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , â\x80¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â\x80¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']"
CCT119,J00-2005,Pipelines and Size Constraints,the linguistic basis of text generation,['Laurence Danlos'],introduction,"This study presents an original and penetrating analysis of the complex problems surrounding the automatic generation of natural language text. Laurence Danlos provides a valuable critical review of current research in this important and increasingly active field, and goes on to describe a new theoretical model that is thoroughly grounded in linguistic principles.The model emphasizes the semantic, syntactic and lexical constraints that must be dealt with when establishing a relationship between meaning and form, and it is consideration of such linguistic constraints that determines Danlos' generation algorithm. The book concludes with a description of a generation system based on this algorithm which produces texts in several domains and also a system for the synthesis of spoken messages from semantic representations.The book is a significant addition to the literature on text generation, and will be of particular interest to all computational linguists and AI researchers who have wrestled with the problem of vocabulary selection.",Many other such cases are described in Danlos 's book ( #AUTHOR_TAG ) .,"[""Many other such cases are described in Danlos 's book ( #AUTHOR_TAG ) ."", 'The common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints (such as unambiguous reference) or performing linguistic optimizations (such as using pronouns instead of longer referring expressions whenever possible) in cases where the constraints or optimizations depend on decisions made in multiple modules.', 'This is largely due to the fact that pipelined systems cannot perform general search over a decision space that includes decisions made in more than one module.']",0,"[""Many other such cases are described in Danlos 's book ( #AUTHOR_TAG ) .""]"
CCT120,J00-2005,Pipelines and Size Constraints,describing complex charts in natural language a caption generation system,"['Vibhu Mittal', 'Johanna Moore', 'Guiseppe Carenini', 'Steven Roth']",introduction,"Graphical presentations can be used to communicate information in relational data sets succinctly and effectively. However, novel graphical presentations that represent many attributes and relationships are often difficult to understand completely until explained. Automatically generated graphical presentations must therefore either be limited to generating simple, conventionalized graphical presentations, or risk incomprehensibility. A possible solution to this problem would be to extend automatic graphical presentation systems to generate explanatory captions in natural language, to enable users to understand the information expressed in the graphic. This paper presents a system to do so. It uses a text planner to determine the content and structure of the captions based on: (1) a representation of the structure of the graphical presentation and its mapping to the data it depicts, (2) a framework for identifying the perceptual complexity of graphical elements, and (3) the structure of the data expressed in the graphic. The output of the planner is further processed regarding issues such as ordering, aggregation, centering, generating referring expressions, and lexical choice. We discuss the architecture of our system and its strengths and limitations. Our implementation is currently limited to 2-D charts and maps, but, except for lexical information, it is completely domain independent. We illustrate our discussion with figures and generated captions about housing sales in Pittsburgh.","This may be because pipelines have many engineering advantages , and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems ( #AUTHOR_TAG ) .","['Despite these arguments, most applied NLG systems use a pipelined architecture; indeed, a pipeline was used in every one of the systems surveyed by Reiter (1994) and Paiva (1998).', 'This may be because pipelines have many engineering advantages , and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems ( #AUTHOR_TAG ) .']",0,"['This may be because pipelines have many engineering advantages , and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems ( #AUTHOR_TAG ) .']"
CCT121,J00-2005,Pipelines and Size Constraints,has a consensus nl generation architecture appeared and is it psycholinguistically plausible,['Ehud Reiter'],introduction,"I survey some recent applications-oriented  NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations  these modules perform, and the way the modules interact with each other. I also  compare this &apos;consensus architecture&apos; among  applied NLG systems with psycholinguistic  knowledge about how humans speak, and argue  that at least some aspects of the consensns  architecture seem to be in agreement  with what is known about human language production, despite the fact that psycholinguistic  plausibility was not in general a goal  of the developers of the surveyed systems","Despite these arguments , most applied NLG systems use a pipelined architecture ; indeed , a pipeline was used in every one of the systems surveyed by #AUTHOR_TAG and Paiva ( 1998 ) .","['Despite these arguments , most applied NLG systems use a pipelined architecture ; indeed , a pipeline was used in every one of the systems surveyed by #AUTHOR_TAG and Paiva ( 1998 ) .', 'This may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems (Mittal et al. 1998).']",0,"['Despite these arguments , most applied NLG systems use a pipelined architecture ; indeed , a pipeline was used in every one of the systems surveyed by #AUTHOR_TAG and Paiva ( 1998 ) .']"
CCT122,J00-2009,Book Reviews: Lexical Semantics and Knowledge Representation in Multilingual Text Generation,upper modeling organizing knowledge for natural language processing,['John A Bateman'],,"Abstract : A general, reusable computational resource has been developed within the Penman text generation project for organizing domain knowledge appropriately for linguistic realization. This resource, called the upper model, provides a domain- and task-independent classification system' that supports sophisticated natural language processing while significantly simplifying the interface between domain-specific knowledge and general linguistic resources. This paper presents the results of our experiences in designing and using the upper model in a variety of applications over the past 5 years. In particular, we present our conclusions concerning the appropriate organization of an upper model, its domain- independence, and the types of interrelationships that need to be supported between upper model and grammar and semantics.","The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG , the Upper Model ( #AUTHOR_TAG ) .","['At first I found Chapters 4 through 8 slightly overwhelming, as they introduce several levels of representation, each with its own terminology and acronyms.', 'However, at a second, more-careful, reading, everything falls into place.', ""The resulting approach has at its center a lexicon that partly implements current theories of lexical semantics such as Jackendoff's (1990) and Levin's (1993)."", 'The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG , the Upper Model ( #AUTHOR_TAG ) .', 'Although the idea of a two-level representation accommodating language-neutral and language-specific requirements is not new (see for example Nirenburg and Levin [1992], Dorr and Voss [1993], and Di Eugenio [1998]), Stede is among the few who make effective use of those two levels in a complex system.']",0,"['The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG , the Upper Model ( #AUTHOR_TAG ) .']"
CCT123,J00-2013,"Extended Finite State Models of Language András Kornai (editor) (BBN Technologies) Cambridge University Press (Studies in natural language processing), 1999, xii+278 pp and CD-ROM; hardbound, ISBN 0-521-63198-X, $59.95",regular models of phonological rule systems,"['Ronald M Kaplan', 'Martin Kay']",,This paper presents a set of mathematical and computational tools for manipulating and reasoning about regular languages and regular relations and argues that they provide a solid basis for computational phonology. It shows in detail how this framework applies to ordered sets of context-sensitive rewriting rules and also to grammars in Koskenniemi's two-level formalism. This analysis provides a common representation of phonological constraints that supports efficient generation and recognition by a single simple interpreter.,"Shortly after the publication of The Sound Pattern of English ( Chomsky and Halle 1968 ) , Kornai points out , `` Johnson ( 1970 ) demonstrated that the context-sensitive machinery of SPE ... [ could ] be replaced by a much simpler one , based on finite-state transducers ( FSTs ) ; the same conclusion was reached independently by Kaplan and Kay , whose work remained an underground classic until it was finally published in #AUTHOR_TAG . ''","[""Shortly after the publication of The Sound Pattern of English ( Chomsky and Halle 1968 ) , Kornai points out , `` Johnson ( 1970 ) demonstrated that the context-sensitive machinery of SPE ... [ could ] be replaced by a much simpler one , based on finite-state transducers ( FSTs ) ; the same conclusion was reached independently by Kaplan and Kay , whose work remained an underground classic until it was finally published in #AUTHOR_TAG . ''"", ""These works inspired Koskenniemi's two-level system, and the Xerox rule compiler (Dalrymple et al. 1987)."", 'Both are now dominant tools in the fields of computational phonology and morphology, as exemplified by Tateno et al. (Chapter 6), ""The Japanese lexical transducer based on stem-suffix style forms"" and Kim and Jang (Chapter 7), ""Acquiring rules for reducing morphological ambiguity from POS tagged corpus in Korean.""', 'The latter includes an algorithm for automatically inferring regular grammar rules for morphological relations directly from part-of-speech tagged corpora.']",0,"[""Shortly after the publication of The Sound Pattern of English ( Chomsky and Halle 1968 ) , Kornai points out , `` Johnson ( 1970 ) demonstrated that the context-sensitive machinery of SPE ... [ could ] be replaced by a much simpler one , based on finite-state transducers ( FSTs ) ; the same conclusion was reached independently by Kaplan and Kay , whose work remained an underground classic until it was finally published in #AUTHOR_TAG . ''"", 'Both are now dominant tools in the fields of computational phonology and morphology, as exemplified by Tateno et al. (Chapter 6), ""The Japanese lexical transducer based on stem-suffix style forms"" and Kim and Jang (Chapter 7), ""Acquiring rules for reducing morphological ambiguity from POS tagged corpus in Korean.""', 'The latter includes an algorithm for automatically inferring regular grammar rules for morphological relations directly from part-of-speech tagged corpora.']"
CCT124,J00-2014,Surface Opacity of Metrical Structure in Optimality Theory,efficient generation in primitive optimality theory,['Jason Eisner'],introduction,"This paper introduces primitive Optimality Theory (OTP), a linguistically motivated formalization of OT. OTP specifies the class of autosegmental representations, the universal generator Gen, and the two simple families of permissible constraints. In contrast to less restricted theories using Generalized Alignment, OTP's optimal surface forms can be generated with finite-state methods adapted from (Ellison, 1994). Unfortunately these methods take time exponential on the size of the grammar. Indeed the generation problem is shown NP-complete in this sense. However, techniques are discussed for making Ellison's approach fast in the typical case, including a simple trick that alone provides a 100-fold speedup on a grammar fragment of moderate size. One avenue for future improvements is a new finite-state notion, ""factored automata,"" where regular languages are represented compactly via formal intersections ki=1Ai of FSAs.","OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question .","['OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question .']",0,"['OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question .']"
CCT125,J00-2014,Surface Opacity of Metrical Structure in Optimality Theory,comparing a linguistic and a stochastic tagger,"['Christer Samuelsson', 'Atro Voutilainen']",,"Concerning different approaches to automatic PoS tagging: EngCG-2, a constraint-based morphological tagger, is compared in a double-blind test with a state-of-the-art statistical tagger on a common disambiguation task using a common tag set. The experiments show that for the same amount of remaining ambiguity, the error rate of the statistical tagger is one order of magnitude greater than that of the rule-based one. The two related issues of priming effects compromising the results and disagreement between human annotators are also addressed.Comment: 8 pages, LaTeX, 2 postscript figures. E-ACL'9","#AUTHOR_TAG report excellent part-of-speech tagging results using a handcrafted approach that is close to OT .3 More speculatively , imagine an OT grammar for stylistic revision of parsed sentences .","['Second, weights are an annoyance when writing grammars by hand.', 'In some cases rankings may work well enough.', '#AUTHOR_TAG report excellent part-of-speech tagging results using a handcrafted approach that is close to OT .3 More speculatively , imagine an OT grammar for stylistic revision of parsed sentences .', ""The tension between preserving the original author's text (faithfulness to the underlying form) and making it readable in various ways (well-formedness) is right up OT's alley."", 'The same applies to document layout: I have often wished I could write OT-style TeX macros~ Third, even in statistical corpus-based NLP, estimating a full Gibbs distribution is not always feasible.', 'Even if strict ranking is not quite accurate, sparse data or the complexity of parameter estimation may make it easier to learn a good OT grammar than a good arbitrary Gibbs model.', ""A well-known example is Yarowsky's (1996) work on word sense disambiguation using decision lists (a kind of OT grammar)."", 'Although decision lists are not very powerful because of their simple output space, they have the characteristic OT property that each generalization partially masks lower-ranked generalizations.']",0,"['#AUTHOR_TAG report excellent part-of-speech tagging results using a handcrafted approach that is close to OT .3 More speculatively , imagine an OT grammar for stylistic revision of parsed sentences .']"
CCT126,J00-2014,Surface Opacity of Metrical Structure in Optimality Theory,efficient generation in primitive optimality theory,['Jason Eisner'],,"This paper introduces primitive Optimality Theory (OTP), a linguistically motivated formalization of OT. OTP specifies the class of autosegmental representations, the universal generator Gen, and the two simple families of permissible constraints. In contrast to less restricted theories using Generalized Alignment, OTP's optimal surface forms can be generated with finite-state methods adapted from (Ellison, 1994). Unfortunately these methods take time exponential on the size of the grammar. Indeed the generation problem is shown NP-complete in this sense. However, techniques are discussed for making Ellison's approach fast in the typical case, including a simple trick that alone provides a 100-fold speedup on a grammar fragment of moderate size. One avenue for future improvements is a new finite-state notion, ""factored automata,"" where regular languages are represented compactly via formal intersections ki=1Ai of FSAs.",But typical OT grammars offer much richer finite-state models of left context ( #AUTHOR_TAGa ) than provided by the traditional HMM finite-state topologies .,"['For example, consider the relevance to hidden Markov models (HMMs), another restricted class of Gibbs distributions used in speech recognition or part-of-speech tagging.', ""Just like OT grammars, HMM Viterbi decoders are functions that pick the optimal output from ~', based on criteria of well-formedness (transition probabilities) and faithfulness to the input (emission probabilities)."", 'But typical OT grammars offer much richer finite-state models of left context ( #AUTHOR_TAGa ) than provided by the traditional HMM finite-state topologies .', 'Now, among methods that use a Gibbs distribution to choose among linguistic forms, OT generation is special in that the distribution ranks the features strictly, rather than weighting them in a gentler way that allows tradeoffs.', 'When is this appropriate?', 'It seems to me that there are three possible uses.']",0,"['For example, consider the relevance to hidden Markov models (HMMs), another restricted class of Gibbs distributions used in speech recognition or part-of-speech tagging.', ""Just like OT grammars, HMM Viterbi decoders are functions that pick the optimal output from ~', based on criteria of well-formedness (transition probabilities) and faithfulness to the input (emission probabilities)."", 'But typical OT grammars offer much richer finite-state models of left context ( #AUTHOR_TAGa ) than provided by the traditional HMM finite-state topologies .', 'Now, among methods that use a Gibbs distribution to choose among linguistic forms, OT generation is special in that the distribution ranks the features strictly, rather than weighting them in a gentler way that allows tradeoffs.', 'When is this appropriate?', 'It seems to me that there are three possible uses.']"
CCT127,J00-3001,Extracting the Lowest-Frequency Words: Pitfalls and Possibilities,word association norms mutual information and lexicography,"['Kenneth W Church', 'Patrick Hanks']",introduction,"The term word association is used in a very particular sense in the p!ycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word &quot;nurse&quot; if it follows a highly associated word such as &quot;doctor.&quot;) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic rehtions of the doctor/nurse type (content word/content word) to lexico-syntactlc co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, makin it possible to estimate norms for tens of thousands of words","#AUTHOR_TAG use mutual information to identify collocations , a method they claim is reasonably effective for words with a frequency of not less than five .","['Our original question concerned the extent to which recall and precision are influenced by the size of the window.', 'It turns out, however, that a preliminary question needs to be answered first, namely, how to gauge the significance of the large effect of the lowest-frequency words on recall, precision, and the number of words extracted as potentially relevant terms.', 'It is common practice in information retrieval to discard the lowest-frequency words a priori as nonsignificant (Rijsbergen 1979).', ""In Smadja's collocation algorithm Xtract, the lowest-frequency words are effectively discarded as well (Smadja 1993)."", '#AUTHOR_TAG use mutual information to identify collocations , a method they claim is reasonably effective for words with a frequency of not less than five .']",0,"['#AUTHOR_TAG use mutual information to identify collocations , a method they claim is reasonably effective for words with a frequency of not less than five .']"
CCT128,J00-3001,Extracting the Lowest-Frequency Words: Pitfalls and Possibilities,word association norms mutual information and lexicography,"['Kenneth W Church', 'Patrick Hanks']",conclusion,"The term word association is used in a very particular sense in the p!ycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word &quot;nurse&quot; if it follows a highly associated word such as &quot;doctor.&quot;) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic rehtions of the doctor/nurse type (content word/content word) to lexico-syntactlc co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, makin it possible to estimate norms for tens of thousands of words","While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :","[""Given that we want to retain dis legomena with a 2-0 distribution, we proceed to compute the corresponding significance levels for both G 2 and Fisher's exact test by Equations 1 and 2. The critical X 2 value for G 2 equals 3.65, the critical P for Fisher's exact test is 0.161."", 'The extraction results for both tests as measured by F are 0.31 and 0.33, respectively.', 'This procedure allows us to extract 64/139 = 46.0% of the lowfrequency words and 66/170 -~ 38.8% of the high-frequency words using G 2, and 64/139 = 46.0%', 'and 79/170 = 46.7%,', ""respectively, using Fisher's exact test."", ""Note that this technique is optimal for the extraction of the lowest-frequency words, leading to identical performance for G 2 and Fisher's exact test for these words."", ""For the higherfrequency words, Fisher's exact test leads to a slightly better recall with the same precision scores (0.31 for both tests)."", ""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :""]",0,"[""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :""]"
CCT129,J00-3001,Extracting the Lowest-Frequency Words: Pitfalls and Possibilities,word association norms mutual information and lexicography,"['Kenneth W Church', 'Patrick Hanks']",,"The term word association is used in a very particular sense in the p!ycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word &quot;nurse&quot; if it follows a highly associated word such as &quot;doctor.&quot;) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic rehtions of the doctor/nurse type (content word/content word) to lexico-syntactlc co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, makin it possible to estimate norms for tens of thousands of words","Given the definition of Mutual Information ( #AUTOR_TAG 1990), P(x, y) I(x, y) = log2 p(x)p(y), we consider the distribution of a window word according to the contingency table (a) in Table 4.","['Given the definition of Mutual Information ( #AUTOR_TAG 1990), P(x, y) I(x, y) = log2 p(x)p(y), we consider the distribution of a window word according to the contingency table (a) in Table 4.', 'P(x) is the relative frequency of the target word, P(y) is the relative frequency of the seed term, and P(x, y) is the frequency of the target word in the window.', 'In terms of the contingency table, we have: nu n++ I(x, y) = log2 n1+ s where S n++ n++ —u, we find that is the frequency of the seed.', 'Substituting nn = nl+ - n12, we find that /11+ -- F/12 I(x,y) = log 2 n++ nl+ S \' //++ //++ 1 = log 2 n++ //1+ S \' n++(nl+ -- nu) "" n++ = log2(n++) - log2(S) - log2(nl+) + log2(nl+ - n12).']",5,"['Given the definition of Mutual Information ( #AUTOR_TAG 1990), P(x, y) I(x, y) = log2 p(x)p(y), we consider the distribution of a window word according to the contingency table (a) in Table 4.']"
CCT130,J00-3006,Language Communication,chinese numbernames tree adjoining languages and mild contextsensitivity,['Daniel Radzinsky'],,,"For example , #AUTHOR_TAG proves that Chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao , for the number 5000000000000000005000000000000005000000000005000000005000 , are not context-free , which implies that Chinese is not a context-free language and thus might parse in exponential worst-case time .","['Second, the complexity of a grammar class is measured by the worst case: a grammar class has a complexity x if there exists some grammar in this class such that there exists an infinite series of long-enough sentences that parse in time x by this grammar.', 'However, what matters in engineering practice is the average case for a specific grammar.', 'Specific, since a specific grammar belonging to a high complexity class may well prove to parse much faster than the worst grammar of its class, even with the general algorithm, if the possible time-consuming behavior of the algorithm never happens for this grammar.', 'Average, since it can happen that the grammar does admit hard-toparse sentences that are not used (or at least not frequently used) in the real corpus.', 'For example , #AUTHOR_TAG proves that Chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao , for the number 5000000000000000005000000000000005000000000005000000005000 , are not context-free , which implies that Chinese is not a context-free language and thus might parse in exponential worst-case time .', 'Do such arguments--no doubt important for mathematical linguistics--have any direct consequences for an engineering linguistics?', 'Even if a Chinese grammar includes a non-context-flee rule for parsing such numerals, how frequently will it be activated?', 'Does it imply impossibility of processing real Chinese texts in reasonable time?', 'Clearly, the average time for a specific grammar cannot be calculated in such a mathematically elegant way as the worst-case complexity of a grammar class; for the time being, the only practical way to compare the complexity of natural language processing formalisms is the hard one--building real-world systems and comparing their efficiency and coverage.']",0,"['For example , #AUTHOR_TAG proves that Chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao , for the number 5000000000000000005000000000000005000000000005000000005000 , are not context-free , which implies that Chinese is not a context-free language and thus might parse in exponential worst-case time .', 'Even if a Chinese grammar includes a non-context-flee rule for parsing such numerals, how frequently will it be activated?']"
CCT131,J00-4001,Automatic Text Categorization in Terms of Genre and Author,robust text processing in automated information retrieval,['Tornek Strzalkowski'],method,"This paper outlines a prototype text retrieval system which uses relatively advanced natural language processing techniques in order to enhance the effectiveness of statistical document retrieval. The backbone of our system is a traditional retrieval engine which builds inverted index files from pre-processed docu- ments, and then searches and ranks the documents in response to user queries. Natural language processing is used to (1) preprocess the documents in order to extract contents-carrying terms, (2) discover interterm dependencies and build a conceptual hierarchy specific to the database domain, and (3) process user&apos;s natural language requests into effective search queries. The basic assumption of this design is that term-based representation of contents is in principle sufficient to build an effective if not optimal search query out of any users request. This has been confirmed by an experiment that compared effectiveness of expert-user prepared queries with those derived automatically from an initial narrative information request. In this paper we show that largescale natural language processing (hundreds of millions of words and more) is not only required for a better retrieval, but it is also doable, given appropriate resources. We report on selected preliminary restfits of experiments with 500 MByte database of Wall Street Journal articles, as well as some earlier restfits with a smaller document collection","For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .","['The particular analysis-level style markers can be calculated only when this specific computational tool is utilized.', 'However, the SCBD is a general-purpose tool and was not designed for providing stylistic information exclusively.', 'Thus, any NLP tool (e.g., part-of-speech taggers, parsers, etc.) can provide similar measures.', 'The appropriate analysis-level style markers have to be defined according to the methodology used by the tool in order to analyze the text.', 'For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .', 'This parser produces trees to represent the structure of the sentences that compose the text.', 'However, it is set to ""skip"" or surrender attempts to parse clauses after reaching a time-out threshold.', 'When the parser skips, it notes that in the parse tree.', 'The measures proposed by Karlgren (1999) as indicators of clausal complexity are the average parse tree depth and the number of parser skips per sentence, which in essence are analysis-level style markers.']",0,"['For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .']"
CCT132,J00-4001,Automatic Text Categorization in Terms of Genre and Author,using registerdiversified corpora for general language studies,['Douglas Biber'],,"The present study summarizes corpus-based research on linguistic characteristics from several different structural levels, in English as well as other languages, showing that register variation is inherent in natural language. It further argues that, due to the importance and systematicity of the linguistic differences among registers, diversified corpora representing a broad range of register variation are required as the basis for general language studies.First, the extent of cross-register differences are illustrated from consideration of individual grammatical and lexical features; these register differences are also important for probabilistic part-of-speech taggers and syntactic parsers, because the probabilities associated with grammatically ambiguous forms are often markedly different across registers. Then, corpus-based multidimensional analyses of English are summarized, showing that linguistic features from several structural levels function together as underlying dimensions of variation, with each dimension defining a different set of linguistic relations among registers. Finally, the paper discusses how such analyses, based on register-diversified corpora, can be used to address two current issues in computational linguistics: the automatic classification of texts into register categories and cross-linguistic comparisons of register variation.",Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .,"['where Cx is the covariance matrix of x.', 'Using this classification method we can also derive the probability that a case belongs to a particular group (i.e., posterior probabilities), which is roughly proportional to the Mahalanobis distance from that group centroid.', 'Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .']",0,['Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .']
CCT133,J06-2003,Building and Using a Lexical Knowledge Base of Near-Synonym Differences,unsupervised word sense disambiguation rivaling supervised methods,['David Yarowsky'],,"This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints -- that words tend to have one sense per discourse and one sense per collocation -- exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%",The algorithm we implemented is inspired by the work of #AUTHOR_TAG on word sense disambiguation .,"['The algorithm we implemented is inspired by the work of #AUTHOR_TAG on word sense disambiguation .', 'He classified the senses of a word on the basis of other words that the given word co-occurs with.', 'Collins and Singer (1999)']",4,['The algorithm we implemented is inspired by the work of #AUTHOR_TAG on word sense disambiguation .']
CCT134,J06-3002,The Notion of Argument in Prepositional Phrase Attachment,corpus based pp attachment ambiguity resolution with a semantic dictionary,"['Jiri Stetina', 'Makoto Nagao']",method,"This paper deals with two important ambiguities of natural language: prepositional phrase attachment and word sense ambiguity. We propose a new supervised learning method for PPattachment based on a semantically tagged corpus. Because any sufficiently big sense-tagged corpus does not exist, we also propose a new unsupervised context based word sense disambiguation algorithm which amends the training corpus for the PP attachment by word sense tags. We present the results of our approach and evaluate the achieved PP attachment accuracy in comparison with other methods.","The changes made were inspired by those described in #AUTHOR_TAG , page 75 ) .","['The automatic annotation of nouns and verbs in the corpus has been done by matching them with the WordNet database files.', 'Before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and WordNet.', 'The changes made were inspired by those described in #AUTHOR_TAG , page 75 ) .', 'To lemmatize the words we used �morpha,� a lemmatizer developed by John A. Carroll and freely available at the address: http://www.informatics.susx.ac.uk./research/nlp/carroll/morph.html.', 'Upon simple observation, it showed a better performance than the frequently used Porter Stemmer for this task.']",4,"['The automatic annotation of nouns and verbs in the corpus has been done by matching them with the WordNet database files.', 'Before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and WordNet.', 'The changes made were inspired by those described in #AUTHOR_TAG , page 75 ) .', 'To lemmatize the words we used morpha, a lemmatizer developed by John A. Carroll and freely available at the address: http://www.informatics.susx.ac.uk./research/nlp/carroll/morph.html.', 'Upon simple observation, it showed a better performance than the frequently used Porter Stemmer for this task.']"
CCT135,J11-1005,Syntactic Processing Using the Generalized Perceptron and Beam Search,joint word segmentation and pos tagging using a single perceptron,"['Yue Zhang', 'Stephen Clark']",introduction,"For Chinese POS tagging, word segmentation is a preliminary step. To avoid error propagation and improve segmentation by utilizing POS information, segmentation and tagging can be performed simultaneously. A challenge for this joint approach is the large combined search space, which makes efficient decoding very hard. Recent research has explored the integration of segmentation and POS tagging, by decoding under restricted versions of the full combined search space. In this paper, we propose a joint segmentation and POS tagging model that does not impose any hard constraints on the interaction between word and POS information. Fast decoding is achieved by using a novel multiple-beam search algorithm. The system uses a discriminative statistical model, trained using the generalized perceptron algorithm. The joint model gives an error reduction in segmentation accuracy of 14.6% and an error reduction in tagging accuracy of 12.2%, compared to the traditional pipeline approach.","For the joint segmentation and POS-tagging task , we present a novel solution using the framework in this article , and show that it gives comparable accuracies to our previous work ( #AUTHOR_TAGa ) , while being more than an order of magnitude faster .","['In Section 2 we describe our general framework of the generic beam-search algorithm and the generalized perceptron.', 'Then in the subsequent sections we describe each task in turn, based on conference papers including Zhang and Clark (2007, 2008a, 2008b, presented in our single coherent framework.', 'We give an updated set of results, plus a number of additional experiments which probe further into the advantages and disadvantages of our framework.', 'For the segmentation task, we also compare our beam-search framework with alternative decoding algorithms including an exact dynamic-programming method, showing that the beam-search method is significantly faster with comparable accuracy.', 'For the joint segmentation and POS-tagging task , we present a novel solution using the framework in this article , and show that it gives comparable accuracies to our previous work ( #AUTHOR_TAGa ) , while being more than an order of magnitude faster .']",1,"['For the joint segmentation and POS-tagging task , we present a novel solution using the framework in this article , and show that it gives comparable accuracies to our previous work ( #AUTHOR_TAGa ) , while being more than an order of magnitude faster .']"
CCT136,J12-4003,Semantic Role Labeling of Implicit Arguments for Nominal Predicates,beyond nombank a study of implicit arguments for nominal predicates,"['Matthew Gerber', 'Joyce Chai']",conclusion,"Despite its substantial coverage, NomBank does not account for all within-sentence arguments and ignores extra-sentential arguments altogether. These arguments, which we call implicit, are important to semantic processing, and their recovery could potentially benefit many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates. We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank. We demonstrate the feasibility of recovering implicit arguments with a supervised classification model. Our results and analyses provide a baseline for future work on this emerging task.","Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .","['Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .', 'In each test, the discriminative model was retrained and reevaluated without a particular group of features.', 'We summarize the findings of this study in this section.']",2,"['Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .']"
CCT137,J12-4003,Semantic Role Labeling of Implicit Arguments for Nominal Predicates,beyond nombank a study of implicit arguments for nominal predicates,"['Matthew Gerber', 'Joyce Chai']",experiments,"Despite its substantial coverage, NomBank does not account for all within-sentence arguments and ignores extra-sentential arguments altogether. These arguments, which we call implicit, are important to semantic processing, and their recovery could potentially benefit many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates. We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank. We demonstrate the feasibility of recovering implicit arguments with a supervised classification model. Our results and analyses provide a baseline for future work on this emerging task.","This evaluation set-up is an improvement versus the one we previously reported ( #AUTHOR_TAG ) , in which fixed partitions were used for training , development , and testing .","['In order to remove any confounding factors caused by specific documents, we first randomized the annotated predicate instances.', 'Following this, we split the predicate instances evenly into ten folds and used each fold as testing data for a model trained on the instances outside the fold.', 'This evaluation set-up is an improvement versus the one we previously reported ( #AUTHOR_TAG ) , in which fixed partitions were used for training , development , and testing .']",2,"['In order to remove any confounding factors caused by specific documents, we first randomized the annotated predicate instances.', 'Following this, we split the predicate instances evenly into ten folds and used each fold as testing data for a model trained on the instances outside the fold.', 'This evaluation set-up is an improvement versus the one we previously reported ( #AUTHOR_TAG ) , in which fixed partitions were used for training , development , and testing .']"
CCT138,J15-3005,Discriminative Syntax-Based Word Ordering for Text Generation,ccgbank a corpus of ccg derivations and dependency structures extracted from the penn treebank,"['Julia Hockenmaier', 'Mark Steedman']",introduction,"This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word-word dependencies. The resulting corpus, CCGbank, includes 99.4% of the sentences in the Penn Treebank. It is available from the Linguistic Data Consortium, and has been used to train wide-coverage statistical parsers that obtain state-of-the-art rates of dependency recovery. In order to obtain linguistically adequate CCG analyses, and to eliminate noise and inconsistencies in the original annotation, an extensive analysis of the constructions and annotations in the Penn Treebank was called for, and a substantial number of changes to the Treebank were necessary. We discuss the implications of our findings for the extraction of other linguistically expressive grammars from the Treebank, and for the design of future treebanks.",CCGBank ( #AUTHOR_TAG ) is used to train the model .,"['We now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder.', 'CCGBank ( #AUTHOR_TAG ) is used to train the model .', 'For each training sentence, the corresponding CCGBank derivation together with all its sub-derivations are treated as gold-standard hypotheses.', 'All other hypotheses that can be constructed from the same bag of words are non-gold hypotheses.', 'From the generation perspective this assumption is too strong, because sentences can have multiple orderings (with multiple derivations) that are both gram- matical and fluent.', 'Nevertheless, it is the most feasible choice given the training data available.']",5,"['CCGBank ( #AUTHOR_TAG ) is used to train the model .', 'For each training sentence, the corresponding CCGBank derivation together with all its sub-derivations are treated as gold-standard hypotheses.']"
CCT139,J15-3005,Discriminative Syntax-Based Word Ordering for Text Generation,syntactic processing using the generalized perceptron and beam search,"['Yue Zhang', 'Stephen Clark']",introduction,"We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model, trained by the generalized perceptron together with a generic beam-search decoder. We apply the framework to word segmentation, joint segmentation and POS-tagging, dependency parsing, and phrase-structure parsing. Both components of the framework are conceptually and computationally very simple. The beam-search decoder only requires the syntactic processing task to be broken into a sequence of decisions, such that, at each stage in the process, the decoder is able to consider the top-n candidates and generate all possibilities for the next stage. Once the decoder has been defined, it is applied to the training data, using trivial updates according to the generalized perceptron to induce a model. This simple framework performs surprisingly well, giving accuracy results competitive with the state-of-the-art on all the tasks we consider. The computational simplicity of the decoder and training algorithm leads to significantly higher test speeds and lower training times than their main alternatives, including log-linear and large-margin training algorithms and dynamic-programming for decoding. Moreover, the framework offers the freedom to define arbitrary features which can make alternative training and decoding algorithms prohibitively slow. We discuss how the general framework is applied to each of the problems studied in this article, making comparisons with alternative learning and decoding algorithms. We also show how the comparability of candidates considered by the beam is an important factor in the performance. We argue that the conceptual and computational simplicity of the framework, together with its language-independent nature, make it a competitive choice for a range of syntactic processing tasks and one that should be considered for comparison by developers of alternative approaches.","In our previous papers ( #AUTHOR_TAG ; Zhang , Blackwood , and Clark 2012 ) , we applied a set of beams to this structure , which makes it similar to the data structure used for phrase-based MT decoding ( Koehn 2010 ) .","['In our formulation of the word ordering problem, a hypothesis is a phrase or sentence together with its CCG derivation.', 'Hypotheses are constructed bottom-up: starting from single words, smaller phrases are combined into larger ones according to CCG rules.', 'To allow the combination of hypotheses, we use an additional structure to store a set of hypotheses that have been expanded, which we call accepted hypotheses.', 'When a hypothesis from the agenda is expanded, it is combined with all accepted hypotheses in all possible ways to produce new hypotheses.', 'The data structure for accepted hypotheses is similar to that used for best-first parsing (Caraballo and Charniak 1998), and we adopt the term chart for this structure.', 'However, note there are important differences to the parsing problem.', 'First, the parsing problem has a fixed word order and is considerably simpler than the word ordering problem we are tackling.', 'Second, although we use the term chart, the structure for accepted hypotheses is not a dynamic programming chart in the same way as for the parsing problem.', 'In our previous papers ( #AUTHOR_TAG ; Zhang , Blackwood , and Clark 2012 ) , we applied a set of beams to this structure , which makes it similar to the data structure used for phrase-based MT decoding ( Koehn 2010 ) .', 'However, we will show later that this structure is unnecessary when the model has more discriminative power, and a conceptually simpler single beam can be used.', 'We will also investigate the possibility of applying dynamic-programming-style pruning to the chart.']",1,"['Hypotheses are constructed bottom-up: starting from single words, smaller phrases are combined into larger ones according to CCG rules.', 'The data structure for accepted hypotheses is similar to that used for best-first parsing (Caraballo and Charniak 1998), and we adopt the term chart for this structure.', 'However, note there are important differences to the parsing problem.', 'First, the parsing problem has a fixed word order and is considerably simpler than the word ordering problem we are tackling.', 'Second, although we use the term chart, the structure for accepted hypotheses is not a dynamic programming chart in the same way as for the parsing problem.', 'In our previous papers ( #AUTHOR_TAG ; Zhang , Blackwood , and Clark 2012 ) , we applied a set of beams to this structure , which makes it similar to the data structure used for phrase-based MT decoding ( Koehn 2010 ) .']"
CCT140,K15-1001,A Coactive Learning View of Online Structured Prediction in Statistical Machine Translation,minimum error rate training in statistical machine translation,['Franz Josef Och'],experiments,"Minimum Error Rate Training (MERT) is an effective means to estimate the feature func-tion weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature func-tion its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and repre-senting the exact error surface of all trans-lations that are encoded in a phrase lattice. Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experi-ments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT.",This was done by MERT optimization ( #AUTHOR_TAG ) towards post-edits under the TER target metric .,"['this experiment is to confirm our theoretical analysis by showing convergence in regret for learning from weak and strong feedback.', 'We select feedback of varying grade by directly inspecting the optimal w * , thus this feedback is idealized.', 'However, the experiment also has a realistic background since we show that α-informative feedback corresponds to improvements under standard evaluation metrics such as lowercased and tokenized TER, and that learning from weak and strong feedback leads to convergence in TER on test data.', 'For this experiment, the post-edit data from the LIG corpus were randomly split into 3 subsets: PE-train (6,881 sentences), PE-dev, and PE-test (2,000 sentences each).', 'PE-train was used for our online learning experiments.', ""PE-test was held out for testing the algorithms' progress on unseen data."", 'PE-dev was used to obtain w * to define the utility model.', 'This was done by MERT optimization ( #AUTHOR_TAG ) towards post-edits under the TER target metric .', 'Note that the goal of our experi-  ments is not to improve SMT performance over any algorithm that has access to full information to compute w * .', 'Rather, we want to show that learning from weak feedback leads to convergence in regret with respect to the optimal model, albeit at a slower rate than learning from strong feedback.', 'The feedback data in this experiment were generated by searching the n-best list for translations that are α-informative at α ∈ {0.1, 0.5, 1.0} (with possible non-zero slack).', 'This is achieved by scanning the n-best list output for every input x t and returning the firstȳ t = y t that satisfies Equation (2). 5 This setting can be thought of as an idealized scenario where a user picks translations from the n-best list that are considered improvements under the optimal w * .']",5,"['this experiment is to confirm our theoretical analysis by showing convergence in regret for learning from weak and strong feedback.', 'We select feedback of varying grade by directly inspecting the optimal w * , thus this feedback is idealized.', 'However, the experiment also has a realistic background since we show that a-informative feedback corresponds to improvements under standard evaluation metrics such as lowercased and tokenized TER, and that learning from weak and strong feedback leads to convergence in TER on test data.', 'For this experiment, the post-edit data from the LIG corpus were randomly split into 3 subsets: PE-train (6,881 sentences), PE-dev, and PE-test (2,000 sentences each).', 'PE-train was used for our online learning experiments.', ""PE-test was held out for testing the algorithms' progress on unseen data."", 'PE-dev was used to obtain w * to define the utility model.', 'This was done by MERT optimization ( #AUTHOR_TAG ) towards post-edits under the TER target metric .', 'Note that the goal of our experi-  ments is not to improve SMT performance over any algorithm that has access to full information to compute w * .', 'Rather, we want to show that learning from weak feedback leads to convergence in regret with respect to the optimal model, albeit at a slower rate than learning from strong feedback.', 'The feedback data in this experiment were generated by searching the n-best list for translations that are a-informative at a  {0.1, 0.5, 1.0} (with possible non-zero slack).', 'This is achieved by scanning the n-best list output for every input x t and returning the firsty t = y t that satisfies Equation (2). 5 This setting can be thought of as an idealized scenario where a user picks translations from the n-best list that are considered improvements under the optimal w * .']"
CCT141,K15-1001,A Coactive Learning View of Online Structured Prediction in Statistical Machine Translation,discriminative training methods for hidden markov models theory and experiments with perceptron algorithms,['Michael Collins'],,"We describe new algorithms for train-ing tagging models, as an alternative to maximum-entropy models or condi-tional random elds (CRFs). The al-gorithms rely on Viterbi decoding of training examples, combined with sim-ple additive updates. We describe the-ory justifying the algorithms through a modication of the proof of conver-gence of the perceptron algorithm for classi cation problems. We give exper-imental results on part-of-speech tag-ging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.","In practice , perceptron-type algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; #AUTHOR_TAG ) .","['Generalization for Online-to-Batch Conversion.', 'In practice , perceptron-type algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; #AUTHOR_TAG ) .', 'The difference to the online learning scenario is that we treat the multi-epoch algorithm as an empirical risk minimizer that selects a final weight vector w T,K whose expected loss on unseen data we would like to bound.', 'We assume that the algorithm is fed with a sequence of examples x 1 , . . .', ', x T , and at each epoch k = 1, . . .', ', K it makes a prediction y t,k .', 'The correct label is y * t .', 'For k = 1, . . .', ', K and t = 1, . . .', ', T , let t,k = U (x t , y * t ) − U (x t , y t,k ), and denote by ∆ t,k and ξ t,k the distance at epoch k for example t, and the slack at epoch k for example t, respectively.', 'Finally, we denote by D T,K = T t=1 ∆ 2 t,K , and by w T,K the final weight vector returned after K epochs.', 'We state a condition of convergence :']",1,"['In practice , perceptron-type algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; #AUTHOR_TAG ) .']"
CCT142,K15-1001,A Coactive Learning View of Online Structured Prediction in Statistical Machine Translation,moses open source toolkit for statistical machine translation,"['Philipp Koehn', 'Hieu Hoang', 'Alexandra Birch', 'Chris Callison-Birch', 'Marcello Federico', 'Nicola Bertoldi', 'Brooke Cowan', 'Wade Shen']",experiments,"We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.","To prepare SMT outputs for post-editing , the creators of the corpus used their own WMT10 system ( Potet et al. , 2010 ) , based on the Moses phrase-based decoder ( #AUTHOR_TAG ) with dense features .","['We used the LIG corpus 3 which consists of 10,881 tuples of French-English post-edits (Potet et al., 2012).', 'The corpus is a subset of the newscommentary dataset provided at WMT 4 and contains input French sentences, MT outputs, postedited outputs and English references.', 'To prepare SMT outputs for post-editing , the creators of the corpus used their own WMT10 system ( Potet et al. , 2010 ) , based on the Moses phrase-based decoder ( #AUTHOR_TAG ) with dense features .', 'We replicated a similar Moses system using the same monolingual and parallel data: a 5-gram language model was estimated with the KenLM toolkit (Heafield, 2011) on news.en', 'data (48.65M', 'sentences, 1.13B tokens), pre-processed with the tools from the cdec toolkit (Dyer et al., 2010).', 'perceptron cycling theorem (Block and Levin, 1970;Gelfand et al., 2010) should suffice to show a similar bound.', 'Parallel data (europarl+news-comm, 1.64M sentences) were similarly pre-processed and aligned with fast align (Dyer et al., 2013).', 'In all experiments, training is started with the Moses default weights.', 'The size of the n-best list, where used, was set to 1,000.', 'Irrespective of the use of re-scaling in perceptron training, a constant learning rate of −5 was used for learning from simulated feedback, and 10 −4 for learning from surrogate translations.']",5,"['The corpus is a subset of the newscommentary dataset provided at WMT 4 and contains input French sentences, MT outputs, postedited outputs and English references.', 'To prepare SMT outputs for post-editing , the creators of the corpus used their own WMT10 system ( Potet et al. , 2010 ) , based on the Moses phrase-based decoder ( #AUTHOR_TAG ) with dense features .', 'We replicated a similar Moses system using the same monolingual and parallel data: a 5-gram language model was estimated with the KenLM toolkit (Heafield, 2011) on news.en', 'sentences, 1.13B tokens), pre-processed with the tools from the cdec toolkit (Dyer et al., 2010).', 'Parallel data (europarl+news-comm, 1.64M sentences) were similarly pre-processed and aligned with fast align (Dyer et al., 2013).']"
CCT143,N01-1001,Instance-Based Natural Language Generation,forestbased statistical sentence generation,['Irene Langkilde'],,"This paper presents a new approach to statistical sentence generation in which alternative phrases are represented as packed sets of trees, or forests, and then ranked statistically to choose the best one. This representation offers advantages in compactness and in the ability to represent syntactic information. It also facilitates more efficient statistical ranking than a previous approach to statistical generation. An efficient ranking algorithm is described, together with experimental results showing significant improvements over simple enumeration or a lattice-based approach.","In contrast , a single statistical model allows one to maintain a single table ( #AUTHOR_TAG ) .","['In contrast , a single statistical model allows one to maintain a single table ( #AUTHOR_TAG ) .']",0,"['In contrast , a single statistical model allows one to maintain a single table ( #AUTHOR_TAG ) .']"
CCT144,N01-1001,Instance-Based Natural Language Generation,chart generation,['Martin Kay'],experiments,"This paper presents a compilation procedure which determines internal and external indices for signs in a unification based grammar to be used in improving the computational efficiency of lexicalist chart generation. The procedure takes as input a grammar and a set of feature paths indicating the position of semantic indices in a sign, and calculates the fixed-point of a set of equations derived from the grammar. The result is a set of independent constraints stating which indices in a sign can be bound to other signs within a complete sentence. Based on these constraints, two tests are formulated which reduce the search space during generation.Comment: 8 pages, Latex; to appear in 7th International Conference on   Theoretical and Methodological Issues in Machine Translation (TMI-97",IGEN uses standard chart generation techniques ( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates .,['IGEN uses standard chart generation techniques ( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates .'],0,['IGEN uses standard chart generation techniques ( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates .']
CCT145,N01-1002,Corpus-based NP Modifier Generation,can nominal expressions achieve multiple goals an empirical study,['P Jordan'],,"While previous work suggests that multiple goals can be addressed by a nominal expression, there is no systematic work describing what goals in addition to identification might be relevant and how speakers can use nominal expressions to achieve them. In this paper, we first hypothesize a number of communicative goals that could be addressed by nominal expressions in task-oriented dialogues. We then describe the intentional influences model for nominal expression generation that attempts to simultaneously address the identification goal and these additional goals with a single nominal expression. Our evaluation results show that the intentional influences model fits the nominal expressions in the COCONUT corpus as well as previous accounts that focus solely on the identification goal.","Notice that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen , unless we know in advance all of the properties that can be attributed to a given object , as in the case of Jordan 's work on the COCONUT domain ( #AUTHOR_TAG ) .","[""Notice that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen , unless we know in advance all of the properties that can be attributed to a given object , as in the case of Jordan 's work on the COCONUT domain ( #AUTHOR_TAG ) .""]",0,"[""Notice that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen , unless we know in advance all of the properties that can be attributed to a given object , as in the case of Jordan 's work on the COCONUT domain ( #AUTHOR_TAG ) .""]"
CCT146,N01-1002,Corpus-based NP Modifier Generation,capturing the interaction between aggregation and text planning in two generation systems,"['H Cheng', 'C Mellish']",introduction,"In natural language generation, different generation tasks often interact with each other in a complex way. We think that how to resolve the complex interactions inside and between tasks is more important to the generation of a coherent text than how to model each individual factor. This paper focuses on the interaction between aggregation and text planning, and tries to explore what preferences exist among the features considered by the two tasks. The preferences are implemented in two generation systems, namely ILEX-TS and a text planner using a Genetic Algorithm. The evaluation emphasises the second implementation and shows that capturing these preferences properly can lead to coherent text.","It has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of ( Grosz and Sidner , 1986 ) ) ( #AUTHOR_TAGa ) .","['It has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of ( Grosz and Sidner , 1986 ) ) ( #AUTHOR_TAGa ) .']",0,"['It has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of ( Grosz and Sidner , 1986 ) ) ( #AUTHOR_TAGa ) .']"
CCT147,N01-1002,Corpus-based NP Modifier Generation,capturing the interaction between aggregation and text planning in two generation systems,"['H Cheng', 'C Mellish']",,"In natural language generation, different generation tasks often interact with each other in a complex way. We think that how to resolve the complex interactions inside and between tasks is more important to the generation of a coherent text than how to model each individual factor. This paper focuses on the interaction between aggregation and text planning, and tries to explore what preferences exist among the features considered by the two tasks. The preferences are implemented in two generation systems, namely ILEX-TS and a text planner using a Genetic Algorithm. The evaluation emphasises the second implementation and shows that capturing these preferences properly can lead to coherent text.","Using WordNet , annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2Some descriptions of int modifiers can be found in ( #AUTHOR_TAGb ) .","['Using WordNet , annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2Some descriptions of int modifiers can be found in ( #AUTHOR_TAGb ) .']",0,"['Using WordNet , annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2Some descriptions of int modifiers can be found in ( #AUTHOR_TAGb ) .']"
CCT148,N01-1002,Corpus-based NP Modifier Generation,can nominal expressions achieve multiple goals an empirical study,['P Jordan'],introduction,"While previous work suggests that multiple goals can be addressed by a nominal expression, there is no systematic work describing what goals in addition to identification might be relevant and how speakers can use nominal expressions to achieve them. In this paper, we first hypothesize a number of communicative goals that could be addressed by nominal expressions in task-oriented dialogues. We then describe the intentional influences model for nominal expression generation that attempts to simultaneously address the identification goal and these additional goals with a single nominal expression. Our evaluation results show that the intentional influences model fits the nominal expressions in the COCONUT corpus as well as previous accounts that focus solely on the identification goal.","In addition to a referring function , noun phrases ( NP ) can also serve communicative goals such as providing new information about the referent and expressing the speaker 's emotional attitude towards the referent ( Appelt , 1985 ; #AUTHOR_TAG ) .","[""In addition to a referring function , noun phrases ( NP ) can also serve communicative goals such as providing new information about the referent and expressing the speaker 's emotional attitude towards the referent ( Appelt , 1985 ; #AUTHOR_TAG ) .""]",0,"[""In addition to a referring function , noun phrases ( NP ) can also serve communicative goals such as providing new information about the referent and expressing the speaker 's emotional attitude towards the referent ( Appelt , 1985 ; #AUTHOR_TAG ) .""]"
CCT149,N01-1009,A corpus-based account of regular polysemy,the generative lexicon,['James Pustejovsky'],method,"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.",Table 1 gives the interpretations of eight adjective-noun combinations discussed in #AUTHOR_TAG and Vendler ( 1968 ) .,"['In what follows we explain the properties of the model by applying it to a small number of adjective-noun combinations taken from the lexical semantics literature.', 'Table 1 gives the interpretations of eight adjective-noun combinations discussed in #AUTHOR_TAG and Vendler ( 1968 ) .', 'Table 2 shows the five most likely interpretations for these combinations as derived by the model discussed in the previous sections (v is the most likely interpretation, v 2 is the second most likely interpretation, etc.).']",5,"['In what follows we explain the properties of the model by applying it to a small number of adjective-noun combinations taken from the lexical semantics literature.', 'Table 1 gives the interpretations of eight adjective-noun combinations discussed in #AUTHOR_TAG and Vendler ( 1968 ) .', 'Table 2 shows the five most likely interpretations for these combinations as derived by the model discussed in the previous sections (v is the most likely interpretation, v 2 is the second most likely interpretation, etc.).']"
CCT150,N01-1009,A corpus-based account of regular polysemy,the generative lexicon,['James Pustejovsky'],introduction,"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.","Adjectives , more than other categories , are a striking example of regular polysemy since they are able to take on different meanings depending on their context , viz. , the noun or noun class they modify ( see #AUTHOR_TAG and the references therein ) .","['Much recent work in lexical semantics has been concerned with accounting for regular polysemy, i.e., the regular and predictable sense alternations certain classes of words are subject to.', 'Adjectives , more than other categories , are a striking example of regular polysemy since they are able to take on different meanings depending on their context , viz. , the noun or noun class they modify ( see #AUTHOR_TAG and the references therein ) .']",0,"['Much recent work in lexical semantics has been concerned with accounting for regular polysemy, i.e., the regular and predictable sense alternations certain classes of words are subject to.', 'Adjectives , more than other categories , are a striking example of regular polysemy since they are able to take on different meanings depending on their context , viz. , the noun or noun class they modify ( see #AUTHOR_TAG and the references therein ) .']"
CCT151,N01-1009,A corpus-based account of regular polysemy,the generative lexicon,['James Pustejovsky'],experiments,"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.","We chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature ( Vendler , 1968 ; #AUTHOR_TAG ) .","['We chose nine adjectives according to a set of minimal criteria and paired each adjective with 10 nouns randomly selected from the BNC.', 'We chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature ( Vendler , 1968 ; #AUTHOR_TAG ) .', 'From these we randomly sampled nine adjectives (difficult, easy, fast, good, hard, right, safe, slow, wrong).', 'These adjectives had to be unambiguous with respect to their part-of-speech: each adjective was unambiguously tagged as ""adjective"" 98.6% of the time, measured as the number of different part-of-speech tags assigned to the word in the BNC.', 'We identified adjective-noun pairs using Gsearch (Corley et al., 2000), a chart parser which detects syntactic patterns in a tagged corpus by exploiting a userspecified context free grammar and a syntactic query.', 'Gsearch was run on a lemmatized version of the BNC so as to compile a comprehensive corpus count of all nouns occurring in a modifier-head relationship with each of the nine adjectives.', 'From the syntactic analysis provided by  We used the model outlined in Section 2 to derive meanings for the 90 adjective-noun combinations.', 'We employed no threshold on the frequencies f (a, v) and f (rel, v, n).', 'In order to obtain the frequency f (a, v) the adjective was mapped to its corresponding adverb.', 'In particular, good was mapped to good and well, fast to fast, easy to easily, hard to hard, right to rightly and right, safe to safely and safe, slow to slowly and slow and wrong to wrongly and wrong.', 'The adverbial function of the adjective difficult is expressed only periphrastically (i.e., in a difficult manner, with difficulty).', 'As a result, the frequency f (difficult, v) was estimated only on the basis of infinitival constructions (see ( 17)).', 'We estimated the probability P(a, n, v, rel) for each adjective-noun pair by varying both the terms v and rel.']",5,"['We chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature ( Vendler , 1968 ; #AUTHOR_TAG ) .', 'From these we randomly sampled nine adjectives (difficult, easy, fast, good, hard, right, safe, slow, wrong).', 'Gsearch was run on a lemmatized version of the BNC so as to compile a comprehensive corpus count of all nouns occurring in a modifier-head relationship with each of the nine adjectives.', 'From the syntactic analysis provided by  We used the model outlined in Section 2 to derive meanings for the 90 adjective-noun combinations.', 'We employed no threshold on the frequencies f (a, v) and f (rel, v, n).']"
CCT152,N01-1009,A corpus-based account of regular polysemy,the generative lexicon,['James Pustejovsky'],introduction,"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.",#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .,"['(2) a. easy problem b. difficult language c. good cook d. good soup Adjectives like fast have been extensively studied in the lexical semantics literature and their properties have been known at least since Vendler (1968).', 'The meaning of adjective-noun combinations like those in (1) and ( 2) are usually paraphrased with a verb modified by the adjective in question or its corresponding adverb.', 'For example, an easy problem is ""a problem that is easy to solve"" or ""a problem that one can solve easily"".', 'In order to account for the meaning of these combinations Vendler (1968, 92) points out that ""in most cases not one verb, but a family of verbs is needed"".', 'Vendler further observes that the noun figuring in an adjective-noun combination is usually the subject or object of the paraphrasing verb.', 'Although fast usually triggers a verb-subject interpretation (see (1)), easy and difficult trigger verb-object interpretations (see (2a,b)).', 'An easy problem is usually a problem that is easy to solve, whereas a difficult language is a language that is difficult to learn, speak, or write.', 'Adjectives like good allow either verb-subject or verb-object interpretations: a good cook is a cook who cooks well whereas good soup is soup that tastes good or soup that is good to eat.', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .', 'Pustejovsky treats nouns as having a qualia structure as part of their lexical entries, which among other things, specifies possible events associated with the entity.', 'For example, the telic (purpose) role of the qualia structure for problem has a value equivalent to solve.', 'When the adjective easy is combined with problem, it predicates over the telic role of problem and consequently the adjective-noun combination receives the interpretation a problem that is easy to solve.', 'Pustejovsky (1995) does not give an exhaustive list of the telic roles a given noun may have.', 'Furthermore, in cases where more than one interpretations are provided (see Vendler (1968)), no information is given with respect to the likelihood of these interpretations.', 'Outof context, the number of interpretations for fast scientist is virtually unlimited, yet some interpretations are more likely than others: fast scientist is more likely to be a scientist who performs experiments quickly or who publishes quickly than a scientist who draws or drinks quickly.']",0,"['(2) a. easy problem b. difficult language c. good cook d. good soup Adjectives like fast have been extensively studied in the lexical semantics literature and their properties have been known at least since Vendler (1968).', 'For example, an easy problem is ""a problem that is easy to solve"" or ""a problem that one can solve easily"".', 'In order to account for the meaning of these combinations Vendler (1968, 92) points out that ""in most cases not one verb, but a family of verbs is needed"".', 'Although fast usually triggers a verb-subject interpretation (see (1)), easy and difficult trigger verb-object interpretations (see (2a,b)).', 'An easy problem is usually a problem that is easy to solve, whereas a difficult language is a language that is difficult to learn, speak, or write.', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .', 'Pustejovsky treats nouns as having a qualia structure as part of their lexical entries, which among other things, specifies possible events associated with the entity.', 'Pustejovsky (1995) does not give an exhaustive list of the telic roles a given noun may have.', 'Furthermore, in cases where more than one interpretations are provided (see Vendler (1968)), no information is given with respect to the likelihood of these interpretations.']"
CCT153,N01-1011,A Decision Tree of Bigrams is an Accurate Predictor of Word Sense,a simple approach to building ensembles of naive bayesian classifiers for word sense disambiguation,['T Pedersen'],conclusion,"This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. Despite the simplicity of this approach, empirical results disambiguating the widely studied nouns line and interest show that such an ensemble achieves accuracy rivaling the best previously published results.","We have presented an ensemble approach to word sense disambiguation ( #AUTHOR_TAG ) where multiple Naive Bayesian classifiers , each based on co -- occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line .","['We have presented an ensemble approach to word sense disambiguation ( #AUTHOR_TAG ) where multiple Naive Bayesian classifiers , each based on co -- occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line .']",0,"['We have presented an ensemble approach to word sense disambiguation ( #AUTHOR_TAG ) where multiple Naive Bayesian classifiers , each based on co -- occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line .']"
CCT154,N01-1012,An Algorithm for Aspects of Semantic Interpretation Using an Enhanced WordNet,linking wordnet verb classes to semantic interpretation,['F Gomez'],,An analysis that defines predicates for Wordnet verb classes and links them to semantic interpretation is presented. The selectional restrictions for the thematic roles defining the predicates are WordNet ontological categories. Thematic roles are also linked to the syntactic relations that realize them. The paper illustrates the methodology by providing a detailed analysis of some major WordNet verb classes.,See ( #AUTHOR_TAG ) for a discussion .,['See ( #AUTHOR_TAG ) for a discussion .'],0,['See ( #AUTHOR_TAG ) for a discussion .']
CCT155,N01-1012,An Algorithm for Aspects of Semantic Interpretation Using an Enhanced WordNet,linking wordnet verb classes to semantic interpretation,['F Gomez'],,An analysis that defines predicates for Wordnet verb classes and links them to semantic interpretation is presented. The selectional restrictions for the thematic roles defining the predicates are WordNet ontological categories. Thematic roles are also linked to the syntactic relations that realize them. The paper illustrates the methodology by providing a detailed analysis of some major WordNet verb classes.,Other definitions of predicates may be found in ( #AUTHOR_TAG ) .,['Other definitions of predicates may be found in ( #AUTHOR_TAG ) .'],0,['Other definitions of predicates may be found in ( #AUTHOR_TAG ) .']
CCT156,N01-1013,Class-Based Probability Estimation Using a Semantic Hierarchy,generalizing case frames using a thesaurus and the mdl principle,"['H Li', 'N Abe']",,"A new method for automatically acquiring case frame patterns from large corpora is proposed. In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed. In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as ""cuts"" in the thesaurus tree, thus reducing the generalization problem to that of estimating a ""tree cut model"" of the thesaurus tree. An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL. Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity. Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods.","This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG .","['This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG .']",1,"['This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG .']"
CCT157,N01-1013,Class-Based Probability Estimation Using a Semantic Hierarchy,distributional clustering of english words,"['F Pereira', 'N Tishby', 'L Lee']",experiments,"We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical ""soft"" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.",The task we used to compare different generalisation techniques is similar to that used by #AUTHOR_TAG and Rooth et al. ( 1999 ) .,['The task we used to compare different generalisation techniques is similar to that used by #AUTHOR_TAG and Rooth et al. ( 1999 ) .'],1,['The task we used to compare different generalisation techniques is similar to that used by #AUTHOR_TAG and Rooth et al. ( 1999 ) .']
CCT158,N01-1013,Class-Based Probability Estimation Using a Semantic Hierarchy,prepositional phrase attachment through a backedoff model,"['M Collins', 'J Brooks']",,"Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity. Typically, ambiguous verb phrases of the form v np1 p np2 are resolved through a model which considers values of the four head words (v, n1, p and n2). This paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable. Results on Wall Street Journal data of 84.5% accuracy are obtained using this method. A surprising result is the importance of low-count events -- ignoring events which occur less than 5 times in training data reduces performance to 81.6%.","The problem with this approach is that any threshold is , to some extent , arbitrary , and there is evidence to suggest that , for some tasks , low counts are important ( #AUTHOR_TAG ) .","['The problem with this approach is that any threshold is , to some extent , arbitrary , and there is evidence to suggest that , for some tasks , low counts are important ( #AUTHOR_TAG ) .']",4,"['The problem with this approach is that any threshold is , to some extent , arbitrary , and there is evidence to suggest that , for some tasks , low counts are important ( #AUTHOR_TAG ) .']"
CCT159,N01-1013,Class-Based Probability Estimation Using a Semantic Hierarchy,accurate methods for the statistics of surprise and coincidence,['T Dunning'],Motivation,"Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text.","However , #AUTHOR_TAG claims that the log-likelihood chisquared statistic ( G2 ) is more appropriate for corpus-based NLP .","['However , #AUTHOR_TAG claims that the log-likelihood chisquared statistic ( G2 ) is more appropriate for corpus-based NLP .']",4,"['However , #AUTHOR_TAG claims that the log-likelihood chisquared statistic ( G2 ) is more appropriate for corpus-based NLP .']"
CCT160,N01-1013,Class-Based Probability Estimation Using a Semantic Hierarchy,inducing a semantically annotated lexicon via embased clustering,"['M Rooth', 'S Riezler', 'D Prescher', 'G Carroll', 'F Beil']",experiments,"We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evaluated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.",The task we used to compare different generalisation techniques is similar to that used by Pereira et al. ( 1993 ) and #AUTHOR_TAG .,['The task we used to compare different generalisation techniques is similar to that used by Pereira et al. ( 1993 ) and #AUTHOR_TAG .'],1,['The task we used to compare different generalisation techniques is similar to that used by Pereira et al. ( 1993 ) and #AUTHOR_TAG .']
CCT161,N01-1013,Class-Based Probability Estimation Using a Semantic Hierarchy,accurate methods for the statistics of surprise and coincidence,['T Dunning'],experiments,"Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text.","The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP .","['The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP .']",1,"['The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP .']"
CCT162,N04-2004,A computational framework for non-lexicalist semantics,derivational minimalism,['Edward Stabler'],,"International audienceMinimalist grammars (MGs) constitute a mildly context-sensitive formalism when being equipped with a particular locality condition (LC), the shortest move condition. In this format MGs define the same class of derivable string languages as multiple context-free grammars (MCFGs). Adding another LC to MGs, the specifier island condition (SPIC), results in a proper subclass of derivable languages. It is rather straightforward to see this class is embedded within the class of languages derivable by some well-nested MCFG (MCFG wn ). In this paper we show that the embedding is even proper. We partially do so adapting the methods used in [13] to characterize the separation of MCFG wn -languages from MCFG-languages by means of a ""simple copying"" theorem. The separation of strict derivational minimalism from well-nested MCFGs is then characterized by means of a ""simple reverse copying"" theorem. Since for MGs, well-nestedness seems to be a rather ad hoc restriction, whereas for MCFGs, this holds regarding the SPIC, our result may suggest we are concerned here with a structural difference between MGs and MCFGs which cannot immediately be overcome in a non-stipulated manner","The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 ) .","['My theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 ) .""]",1,"[""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 ) .""]"
CCT163,N04-2004,A computational framework for non-lexicalist semantics,on argument structure and the lexical expression of syntactic relations,"['Kenneth Hale', 'Samuel Jay Keyser']",,,"These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .","['These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .', 'They describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'This present framework builds on the work of Hale and Keyser, but in addition to advancing a more refined theory of verbal argument structure, I also describe a computational implementation.']",0,"['These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .']"
CCT164,N04-2004,A computational framework for non-lexicalist semantics,the event argument and the semantics of voice,['Angelika Kratzer'],,,"It projects a functional head , voice ( #AUTHOR_TAG ) , whose specifier is the external argument .","['The light verb v DO licenses an atelic non-inchoative event, and is compatible with verbal roots expressing activity.', 'It projects a functional head , voice ( #AUTHOR_TAG ) , whose specifier is the external argument .', 'Lexical entries in the system are minimally specified, each consisting of a phonetic form, a list of relevant features, and semantics in the form of a λ expression.']",0,"['It projects a functional head , voice ( #AUTHOR_TAG ) , whose specifier is the external argument .']"
CCT165,N04-2004,A computational framework for non-lexicalist semantics,a recognizer for minimalist grammars,['Henk Harkema'],,"Minimalist Grammars are a rigorous formalization of the sort of grammars proposed in the linguistic framework of Chomsky's Minimalist Program. One notable property of Minimalist Grammars is that they allow constituents to move during the derivation of a sentence, thus creating discontinuous constituents. In this paper we will present a bottom-up parsing method for Minimalist Grammars, prove its correctness, and discuss its complexity.","The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 ) .","['My theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 ) .""]",1,"[""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 ) .""]"
CCT166,N04-2004,A computational framework for non-lexicalist semantics,distributed morphology and the pieces of inflection,"['Morris Halle', 'Alec Marantz']",introduction,"The last few years have seen the emergence of several clearly articulated alternative approaches to morphology. One such approach rests on the notion that only stems of the so-called lexical categories (N, V, A) are morpheme &quot;pieces &quot; in the traditional sense--connections between (bun-dles of) meaning (features) and (bundles of) sound (features). What look like affixes on this view are merely the by-product of morphophonological rules called word formation rules (WFRs) that are sensitive to features associated with the lexical categories, called lexemes. Such an a-morphous or affixless theory, adumbrated by Beard (1966) and Aronoff (1976), has been articulated most notably by Anderson (1992) and in major ne","In this paper , I present a computational implementation of Distributed Morphology ( #AUTHOR_TAG ) , a non-lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation .","['In this paper , I present a computational implementation of Distributed Morphology ( #AUTHOR_TAG ) , a non-lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation .', 'This framework leads to finer-grained semantics capable of better capturing linguistic generalizations.']",5,"['In this paper , I present a computational implementation of Distributed Morphology ( #AUTHOR_TAG ) , a non-lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation .']"
CCT167,N04-2004,A computational framework for non-lexicalist semantics,adding semantic annotation to the penn treebank,"['Paul Kingsbury', 'Martha Palmer', 'Mitch Marcus']",introduction,"This paper presents our basic approach to creating Proposition Bank, which involves adding a layer of semantic annotation to the Penn English TreeBank. Without attempting to confirm or disconfirm any particular semantic theory, our goal is to provide consistent argument labeling that will facilitate the automatic extraction of relational data. An argument such asthe window in John broke the window and in The window brokewould receive the same label in both sentences. In order to ensure reliable human annotation, we provide our annotators with explicit guidelines for labeling all of the syntactic and semantic frames of each particular verb. We give several examples of these guidelines and discuss the inter-annotator agreement figures. We also discuss our current experiments on the automatic expansion of our verb guidelines based on verb class membership. Our current rate of progress and our consistency of annotation demonstrate the feasibility of the task.","This approach has its roots in Fillmore 's Case Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( Baker et al. , 1998 ) and PropBank ( #AUTHOR_TAG ) .","['A common lexical semantic representation in the computational linguistics literature is a frame-based model where syntactic arguments are associated with various semantic roles (essentially frame slots).', 'Verbs are viewed as simple predicates over their arguments.', ""This approach has its roots in Fillmore 's Case Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( Baker et al. , 1998 ) and PropBank ( #AUTHOR_TAG ) .""]",0,"[""This approach has its roots in Fillmore 's Case Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( Baker et al. , 1998 ) and PropBank ( #AUTHOR_TAG ) .""]"
CCT168,N04-2004,A computational framework for non-lexicalist semantics,event structure and the encoding of arguments the syntax of the english and mandarin verb phrase,['Jimmy Lin'],,"This work presents a theory of linguistic representation that attempts to capture the syntactic structure of verbs and their arguments. My framework is based on the assumption that the proper representation of argument structure is event structure. Furthermore, I develop the hypothesis that event structure is syntactic structure, and argue that verb meanings are compositionally derived in the syntax from verbalizing heads, functional elements that license eventive interpretations, and verbal roots, abstract concepts drawn from encyclopedic knowledge. The overall goal of the enterprise is to develop a theory that is able to transparently relate the structure and meaning of verbal arguments. By hypothesis, languages share the same inventory of primitive building blocks and are governed by the same set of constraints--all endowed by principles of Universal Grammar and subjected to parametric variations. Support for my theory is drawn from both Mandarin Chinese and English. In particular, the organization of the Mandarin verbal system provides strong evidence for the claim that activity and state are the only two primitive verb types in Chinese-- achievements and accomplishments are syntactically-derived complex categories. As a specific instance of complex event composition, I examine Mandarin resultative verb compounds and demonstrate that a broad range of variations can be perspicuously captured in my framework. I show that patterns of argument sharing in these verbal compounds can be analyzed as control, thus grounding argument structure in wellknown syntactic constraints such as the Minimum Distance Principle. Finally, I argue that cross-linguistic differences in the realization of verbal arguments can be reduced to variations in the way functional elements interact with verbal roots. Overall, my work not only contributes to our understanding of how events are syntactically represented, but also explicates interactions at the syntax-semantics interface, clarifying the relationship between surface form, syntactic structure, and logical form. A theory of argument structure grounded in independently-motivated syntactic constraints, on the one hand, and the semantic structure of events, on the other hand, is able to account for a wide range of empirical facts with few stipulations. Thesis Supervisor: Boris Katz Title: Principal Research Scientist","In ( #AUTHOR_TAG ) , I present evidence from Mandarin Chinese that this analysis is on the right track .","['In ( #AUTHOR_TAG ) , I present evidence from Mandarin Chinese that this analysis is on the right track .', 'The rest of this paper, however, will be concerned with the computational implementation of my theoretical framework.']",2,"['In ( #AUTHOR_TAG ) , I present evidence from Mandarin Chinese that this analysis is on the right track .', 'The rest of this paper, however, will be concerned with the computational implementation of my theoretical framework.']"
CCT169,N04-2004,A computational framework for non-lexicalist semantics,immediate head parsing for language models,['Eugene Charniak'],introduction,"We present two language models based upon an  immediate-head&quot; parser | our name for a parser that conditions all events below a constituent  c upon the head of c. While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology. The perplexity for both of these models signicantly improve upon the trigram model base-line as well as the best previous grammar based language model. For the better of our two models these improvements are 24% and 13% respectively. We also suggest that improvement of the underlying parser should signicantly improve the model&apos;s perplexity and that even in the near term there is a lot of porential for improvement in immediate-head language models.  1 Introduction  All of the most accurate statistical parsers [2,4, 6,7,10,12] are lexicalized in the sense that they condition probabilities on the lexical content of the sentences being parsed. Furthermore, all of these parsers are wh..","Due to advances in statistical syntactic parsing techniques ( Collins , 1997 ; #AUTHOR_TAG ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .","['The understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'Due to advances in statistical syntactic parsing techniques ( Collins , 1997 ; #AUTHOR_TAG ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .']",0,"['Due to advances in statistical syntactic parsing techniques ( Collins , 1997 ; #AUTHOR_TAG ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .']"
CCT170,N04-2004,A computational framework for non-lexicalist semantics,the generative lexicon,['James Pustejovsky'],,"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.","There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAGb ; Rappaport Hovav and Levin , 1998 ) .","['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAGb ; Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example:']",0,"['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAGb ; Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example:']"
CCT171,N04-2004,A computational framework for non-lexicalist semantics,on argument structure and the lexical expression of syntactic relations,"['Kenneth Hale', 'Samuel Jay Keyser']",,,"With a minimal set of features and a small number of lexical entries , Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by Levin ( 1993 ) using a #AUTHOR_TAG style analysis .","['The currently implemented system is still at the ""toy parser"" stage.', 'Although the effectiveness and coverage  of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'With a minimal set of features and a small number of lexical entries , Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by Levin ( 1993 ) using a #AUTHOR_TAG style analysis .', 'I believe that with a suitable lexicon (either hand crafted or automatically induced), my framework can be elaborated into a system whose performance is comparable to that of current statistical parsers, but with the added advantage of simultaneously providing a richer lexical semantic representation of the input sentence than flat predicate argument structures based on semantic roles.']",0,"['With a minimal set of features and a small number of lexical entries , Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by Levin ( 1993 ) using a #AUTHOR_TAG style analysis .']"
CCT172,N04-2004,A computational framework for non-lexicalist semantics,grammaticalizing aspect and affectedness,['Carol Tenny'],,"Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Linguistics and Philosophy, 1987.","#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration , which is often a question of granularity .","[""Activities know run believe walk Accomplishments Achievements paint a picture recognize make a chair find Under Vendler's classification, activities and states both depict situations that are inherently temporally unbounded (atelic); states denote static situations, whereas activities denote on-going dynamic situations."", 'Accomplishments and achievements both express a change of state, and hence are temporally bounded (telic); achievements are punctual, whereas accomplishments extend over a period of time.', '#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration , which is often a question of granularity .']",0,"[""Activities know run believe walk Accomplishments Achievements paint a picture recognize make a chair find Under Vendler's classification, activities and states both depict situations that are inherently temporally unbounded (atelic); states denote static situations, whereas activities denote on-going dynamic situations."", 'Accomplishments and achievements both express a change of state, and hence are temporally bounded (telic); achievements are punctual, whereas accomplishments extend over a period of time.', '#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration , which is often a question of granularity .']"
CCT173,N04-2004,A computational framework for non-lexicalist semantics,english verb classes and alternations a preliminary investigation,['Beth Levin'],,"In this rich reference work, Beth Levin classifies over 3,000 English verbs according to shared meaning and behavior. Levin starts with the hypothesis that a verb's meaning influences its syntactic behavior and develops it into a powerful tool for studying the English verb lexicon. She shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning. The first part of the book sets out alternate ways in which verbs can express their arguments. The second presents classes of verbs that share a kernel of meaning and explores in detail the behavior of each class, drawing on the alternations in the first part. Levin's discussion of each class and alternation includes lists of relevant verbs, illustrative examples, comments on noteworthy properties, and bibliographic references. The result is an original, systematic picture of the organization of the verb inventory. Easy to use, ""English Verb Classes and Alternations"" sets the stage for further explorations of the interface between lexical semantics and syntax. It will prove indispensable for theoretical and computational linguists, psycholinguists, cognitive scientists, lexicographers, and teachers of English as a second language. Beth Levin is associate professor of linguistics at Northwestern University.","With a minimal set of features and a small number of lexical entries , Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by #AUTHOR_TAG using a Hale and Keyser ( 1993 ) style analysis .","['The currently implemented system is still at the ""toy parser"" stage.', 'Although the effectiveness and coverage  of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'With a minimal set of features and a small number of lexical entries , Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by #AUTHOR_TAG using a Hale and Keyser ( 1993 ) style analysis .', 'I believe that with a suitable lexicon (either hand crafted or automatically induced), my framework can be elaborated into a system whose performance is comparable to that of current statistical parsers, but with the added advantage of simultaneously providing a richer lexical semantic representation of the input sentence than flat predicate argument structures based on semantic roles.']",0,"['With a minimal set of features and a small number of lexical entries , Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by #AUTHOR_TAG using a Hale and Keyser ( 1993 ) style analysis .']"
CCT174,N04-2004,A computational framework for non-lexicalist semantics,the generative lexicon,['James Pustejovsky'],,"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.","This framework , where the `` semantic load '' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content , is essentially the model advocated by #AUTHOR_TAGa ) , among many others .","['Considering that the only difference between flat.ADJ and flatten.V is the suffix -en, it must be the source of inchoativity and contribute the change of state reading that distinguishes the verb from the adjective.', 'Here, we have evidence that derivational affixes affect the semantic representation of lexical items, that is, fragments of event structure are directly associated with derivational morphemes.', 'We have the following situation: In this case, the complete event structure of a word can be compositionally derived from its component morphemes.', ""This framework , where the `` semantic load '' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content , is essentially the model advocated by #AUTHOR_TAGa ) , among many others ."", 'Note that such an approach is no longer lexicalist: each lexical item does not fully encode its associated syntactic and semantic structures.', 'Rather, meanings are composed from component morphemes.']",0,"[""This framework , where the `` semantic load '' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content , is essentially the model advocated by #AUTHOR_TAGa ) , among many others ."", 'Note that such an approach is no longer lexicalist: each lexical item does not fully encode its associated syntactic and semantic structures.']"
CCT175,N04-2004,A computational framework for non-lexicalist semantics,a minimalist implementation of verb subcategorization,['Sourabh Niyogi'],,"Traditional accounts of verb subcategorization, from the classic work of Fillmore on, require either a considerable number of syntactic rules to account for diverse sentence constructions, including crosslanguage variation, or else complex linking rules mapping the thematic roles of semantic event templates with possible syntactic forms. In this paper we exhibit a third approach: we implement, via an explicit parser and lexicon, the incorporation theory of Hale and Keyser (1993, 1998) to systematically cover most patterns in English Verb Classes and Alternations (Levin 1993), typically using only 1 or 2 lexical entries per verb to subsume a large number of syntactic constructions and also most information typically contained in semantic event templates, and, further, replacing the notion of ""thematic roles"" with precise structural configurations. The implemented parser uses the merge and move operations formalized by Stabler (1997) in the minimalist framework of Chomsky (2001). As a side benefit, we extend the minimalist recognizer of Harkema (2000) to a full parsing implementation. We summarize the current compactness and coverage of our account and provide this minimalist lexicon and parser online at http://web.mit.edu/niyogi/www/minimal.htm 1 The Problem of Verb Subcategorization Why do certain verbs undergo particular certain alternations and not others? On some accounts, e.g. Levin (1993), referred to hereafter as EVCA, alternations provide insight into verb subcategorization and hence hooks to parsing, cross-language variation, machine translation, and class based verb learning. However, fully implemented accounts of the phenomena remains an open problem, with at least three alternative models, shown in Figure 1. Accounts may be solely descriptive - for example, classifying verbs as having an intransitive, a transitive, and/or ditransitive form, as is familiar. Traditional computational accounts (see 1) map these forms into individual grammar rules, (perhaps by macro expansion-like techniques) adding as many rules as necessary to account for naturally' occurring constructions (wh-movement, passive forms, etc.) For each grammatical rule, a separate semantic decomposition is required, typically labeling component phrases with one of several ""thematic roles."" A richer account provided by lexical semantics (see 2), exemplified in Jackendoff (1983, 1990) and Rappaport Hovav and Levin (1998), is one that hypothesizes semantic templates, but requires linking rules mapping syntactic frames with semantic templates governed by a particular verb. Often these semantic templates are constructed in an ad hoc manner, and the corresponding linking rules are consquently a collection of difficult-toimplement heuristics. In this paper we implement a rather different formalism (Hale and Keyser's Incorporation theory, see 3), wherein fewer lexical entries govern syntactic and semantic behavior, with no appeal to thematic roles or complex linking rules. 0. Verb Subcategorization Phenomena * Bob put. Butter was put on the bread. * Bob put butter. What was put on the bread? Bob put butter on the bread. Where was the butter put? 1. Traditional Account VP - V0 NP PPloc V0 - put VP - was VPass VPass - V0 PPloc VP/NP - V0 NP/NP PPloc VP/NP - V0 NP PPloc/NP PPloc - Ploc NP Ploc - on | in | ... PPloc/NP - Ploc NP/NP Exhaustive modelling with a considerable number of grammatical rules. Semantics separate, otherwise unspecified. 2. Lexical Semantics Account   put V NPjPPk CAUSE ( [BOB]i , GO ( [BUTTER]j , TO ( [BREAD]k )))   Syntax handled by numerous argument-fusing ""linking rules"", typically difficult to formalize. Semantic templates mirror alternation patterns, but are ad-hocly constructed. 3. Minimalist/Incorporation Account /put/ =ploc =d vcause (l(=ploc) (l(=d) (=ploc =d))) /on/ =d +k ploc (l(=d) (l(x) ((go x) (path self =d)))) // >vcause +k =d pred (l(>vcause) (l(=d) ((cause >vcause) =d))) /-ed/ >pred ++k t (l(>pred) (tense >pred past)) Small number of lexical entries handle all syntactic phenomena. Semantics directly encoded in lexical entry. Entries structurally governed by small number of rules, specifying how N/A/P are related. Figure 1: Three Different Accounts of Verb Subcategorization 2 Incorporation Theory At the heart of our new contribution to modeling verb subcategorization is the marriage of Hale and Keyser's (1993, 1998) argument structure theory with Stabler's (1997) 'minimalist' structure building rules. In the Hale and Keyser's theory, using the terminology of X-bar syntax, a particular head (labeled X), may or may or may not take a complement (labeled Y) and may or may not project a specifier (labeled S), resulting in 4 possible structural configurations: X (c) H X Y X (c)(c) H H S X (c) H X Y a (c)(c) H H S a (c) H a X X (a) -subj, +comp (V) (b) +subj, +comp (P) (c) +subj, -comp (A) (d) -subj, -comp (N) Figure 2: Four fundamental primitives in Hale and Keyser's incorporation theory The combinatorial possibilities of incorporation with X=V, A, N, P heads, plus 'head movement', is designed to yield the space of possible syntactic argument structure configurations, presumably across all languages. Notions of agent, patient, instrument, theme, goal, etc. are not 'primitives', but are derived from positions in structural configurations. In English (but not necessarily in all languages), (a) the category V takes a complement but projects no specifier; (b) the category P takes both a complement and projects a specifier; (c) the category A takes no complement but projects a specifier; (d) the category N takes neither complement nor specifier. A particular verbal entry, being of category V, may incorporate one or more of these structures as its complement, as shown in Figure 3: * Nouns incorporated directly into a verbal entry yield structures such as (a): no subject is projected by the N. The phonetic material of the noun head incorporates (undergoes head movement) into the phonetic material of the verb head, which itself may undergo further movement. Verbs such as these are intransitive by nature, generating, e.g., /The light glow -ed/ but */Bob glow -ed the light/. This argument structure typifies purely internally caused processes.","The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG ) .","['My theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG ) .""]",1,"['My theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG ) .""]"
CCT176,N04-2004,A computational framework for non-lexicalist semantics,no escape from syntax don’t try morphological analysis in the privacy of your own lexicon,['Alec Marantz'],,"So Lexicalism claims that the syntax manipulates internally complex words, not unanalyzable atomic units. The leading idea of Lexicalism might be summarized as follows: Everyone agrees that there has to be a list of sound/meaning connections for the atomic building blocks of language (=the ""morphemes""). There also has to be a list of idiosyncratic properties associated with the building blocks. Perhaps the storage house of sound/meaning connections for building blocks and the storage house of idiosyncratic information associated with building blocks is the same house. Perhaps the distinction between this unified storage house and the computational system of syntax could be used to correlate and localize various other crucial distinctions: non-syntax vs. syntax, ""lexical"" phonological rules vs. phrasal and everywhere phonological rules, unpredictable composition vs. predictable composition ... Syntax is for the ruly, the lexicon for the unruly (see, e.g., DiSciullo and Williams 1987). The Lexicalist view of the computational lexicon may be pictured as in (3), where both the Lexicon and the Syntax connect sound and meaning by relating the sound and meaning of complex constituents systematically to the sounds and meanings of their constitutive parts.","Here , I adopt the model proposed by #AUTHOR_TAG and decompose lexical verbs into verbalizing heads and verbal roots .","['Following the non-lexicalist tradition, these primitives are argued to occupy functional projections in the syntactic structure, as so-called light verbs.', 'Here , I adopt the model proposed by #AUTHOR_TAG and decompose lexical verbs into verbalizing heads and verbal roots .', 'Verbalizing heads introduce relevant eventive interpretations in the syntax, and correspond to (assumed) universal primitives of the human cognitive system.', 'On the other hand, verbal roots represent abstract (categoryless) concepts and basically correspond to open-class items drawn from encyclopedic knowledge.', 'I assume an inventory of three verbalizing heads, each corresponding to an aforementioned primitive:']",5,"['Following the non-lexicalist tradition, these primitives are argued to occupy functional projections in the syntactic structure, as so-called light verbs.', 'Here , I adopt the model proposed by #AUTHOR_TAG and decompose lexical verbs into verbalizing heads and verbal roots .']"
CCT177,N04-2004,A computational framework for non-lexicalist semantics,the berkeley framenet project,"['Collin F Baker', 'Charles J Fillmore', 'John B Lowe']",introduction,"FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year #NSF IRI-9618838, #Tools for Lexicon Building&quot;#. The project&apos;s key features are #a# a commitment to corpus evidence for semantic and syntactic generalizations, and #b# the representation of the valences of its target words #mostly nouns, adjectives, and verbs# in which the semantic portion makes use of frame semantics. The resulting database will contain #a# descriptions of the semantic frames underlying the meanings of the words described, and #b# the valence representation #semantic and syntactic# of several thousand words and phrases, each accompanied by #c# a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between #frame elements&quot; and their syntactic realizations #e.g. grammatical function, phrase type, and other syntactic traits#. This report will present the project&apos;s goals and workflow, and information about the computational tools that have been adapted or created in-house for this work","This approach has its roots in Fillmore 's Case Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( #AUTHOR_TAG ) and PropBank ( Kingsbury et al. , 2002 ) .","['A common lexical semantic representation in the computational linguistics literature is a frame-based model where syntactic arguments are associated with various semantic roles (essentially frame slots).', 'Verbs are viewed as simple predicates over their arguments.', ""This approach has its roots in Fillmore 's Case Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( #AUTHOR_TAG ) and PropBank ( Kingsbury et al. , 2002 ) .""]",0,"['A common lexical semantic representation in the computational linguistics literature is a frame-based model where syntactic arguments are associated with various semantic roles (essentially frame slots).', 'Verbs are viewed as simple predicates over their arguments.', ""This approach has its roots in Fillmore 's Case Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( #AUTHOR_TAG ) and PropBank ( Kingsbury et al. , 2002 ) .""]"
CCT178,N04-2004,A computational framework for non-lexicalist semantics,word meaning and,['David Dowty'],,"Reading and listening involve complex psychological processes that recruit many brain areas. The anatomy of processing English words has been studied by a variety of imaging methods. Although there is widespread agreement on the general anatomical areas involved in comprehending words, there are still disputes about the computations that go on in these areas. Examination of the time relations (circuitry) among these anatomical areas can aid in under-standing their computations. In this paper we concentrate on tasks which involve obtaining the meaning of a word in isolation or in relation to a sentence. Our current data support a finding in the literature that frontal semantic areas are active well before posterior areas. We use the subjects attention to amplify relevant brain areas involved either in semantic classification or in judging the relation of the word to a sentence in order to test the hypothesis that frontal areas are concerned with lexical semantics while posterior areas are more involved in comprehension of propositions that involve several words","There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) .","['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example:']",0,"['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) .']"
CCT179,N04-2004,A computational framework for non-lexicalist semantics,three generative lexicalized models for statistical parsing,['Michael Collins'],introduction,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).","Due to advances in statistical syntactic parsing techniques ( #AUTHOR_TAG ; Charniak , 2001 ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .","['The understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'Due to advances in statistical syntactic parsing techniques ( #AUTHOR_TAG ; Charniak , 2001 ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .']",0,"['Due to advances in statistical syntactic parsing techniques ( #AUTHOR_TAG ; Charniak , 2001 ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .']"
CCT180,N04-2004,A computational framework for non-lexicalist semantics,semantics and cognition,['Ray Jackendoff'],,"A new perspective on cognition views cortical cell assemblies linking together knowledge about actions and perceptions not only as the vehicles of integrated action and perception processing but, furthermore, as a brain basis for a wide range of higher cortical functions, including attention, meaning and concepts, sequences, goals and intentions, and even communicative social interaction. This article explains mechanisms relevant to mechanistic action perception theory, points to concrete neuronal circuits in brains along with artificial neuronal network simulations, and summarizes recent brain imaging and other experimental data documenting the role of action perception circuits in cognition, language and communication","There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) .","['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example:']",0,"['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) .']"
CCT181,N04-2004,A computational framework for non-lexicalist semantics,a minimalist implementation of verb subcategorization,['Sourabh Niyogi'],,"Traditional accounts of verb subcategorization, from the classic work of Fillmore on, require either a considerable number of syntactic rules to account for diverse sentence constructions, including crosslanguage variation, or else complex linking rules mapping the thematic roles of semantic event templates with possible syntactic forms. In this paper we exhibit a third approach: we implement, via an explicit parser and lexicon, the incorporation theory of Hale and Keyser (1993, 1998) to systematically cover most patterns in English Verb Classes and Alternations (Levin 1993), typically using only 1 or 2 lexical entries per verb to subsume a large number of syntactic constructions and also most information typically contained in semantic event templates, and, further, replacing the notion of ""thematic roles"" with precise structural configurations. The implemented parser uses the merge and move operations formalized by Stabler (1997) in the minimalist framework of Chomsky (2001). As a side benefit, we extend the minimalist recognizer of Harkema (2000) to a full parsing implementation. We summarize the current compactness and coverage of our account and provide this minimalist lexicon and parser online at http://web.mit.edu/niyogi/www/minimal.htm 1 The Problem of Verb Subcategorization Why do certain verbs undergo particular certain alternations and not others? On some accounts, e.g. Levin (1993), referred to hereafter as EVCA, alternations provide insight into verb subcategorization and hence hooks to parsing, cross-language variation, machine translation, and class based verb learning. However, fully implemented accounts of the phenomena remains an open problem, with at least three alternative models, shown in Figure 1. Accounts may be solely descriptive - for example, classifying verbs as having an intransitive, a transitive, and/or ditransitive form, as is familiar. Traditional computational accounts (see 1) map these forms into individual grammar rules, (perhaps by macro expansion-like techniques) adding as many rules as necessary to account for naturally' occurring constructions (wh-movement, passive forms, etc.) For each grammatical rule, a separate semantic decomposition is required, typically labeling component phrases with one of several ""thematic roles."" A richer account provided by lexical semantics (see 2), exemplified in Jackendoff (1983, 1990) and Rappaport Hovav and Levin (1998), is one that hypothesizes semantic templates, but requires linking rules mapping syntactic frames with semantic templates governed by a particular verb. Often these semantic templates are constructed in an ad hoc manner, and the corresponding linking rules are consquently a collection of difficult-toimplement heuristics. In this paper we implement a rather different formalism (Hale and Keyser's Incorporation theory, see 3), wherein fewer lexical entries govern syntactic and semantic behavior, with no appeal to thematic roles or complex linking rules. 0. Verb Subcategorization Phenomena * Bob put. Butter was put on the bread. * Bob put butter. What was put on the bread? Bob put butter on the bread. Where was the butter put? 1. Traditional Account VP - V0 NP PPloc V0 - put VP - was VPass VPass - V0 PPloc VP/NP - V0 NP/NP PPloc VP/NP - V0 NP PPloc/NP PPloc - Ploc NP Ploc - on | in | ... PPloc/NP - Ploc NP/NP Exhaustive modelling with a considerable number of grammatical rules. Semantics separate, otherwise unspecified. 2. Lexical Semantics Account   put V NPjPPk CAUSE ( [BOB]i , GO ( [BUTTER]j , TO ( [BREAD]k )))   Syntax handled by numerous argument-fusing ""linking rules"", typically difficult to formalize. Semantic templates mirror alternation patterns, but are ad-hocly constructed. 3. Minimalist/Incorporation Account /put/ =ploc =d vcause (l(=ploc) (l(=d) (=ploc =d))) /on/ =d +k ploc (l(=d) (l(x) ((go x) (path self =d)))) // >vcause +k =d pred (l(>vcause) (l(=d) ((cause >vcause) =d))) /-ed/ >pred ++k t (l(>pred) (tense >pred past)) Small number of lexical entries handle all syntactic phenomena. Semantics directly encoded in lexical entry. Entries structurally governed by small number of rules, specifying how N/A/P are related. Figure 1: Three Different Accounts of Verb Subcategorization 2 Incorporation Theory At the heart of our new contribution to modeling verb subcategorization is the marriage of Hale and Keyser's (1993, 1998) argument structure theory with Stabler's (1997) 'minimalist' structure building rules. In the Hale and Keyser's theory, using the terminology of X-bar syntax, a particular head (labeled X), may or may or may not take a complement (labeled Y) and may or may not project a specifier (labeled S), resulting in 4 possible structural configurations: X (c) H X Y X (c)(c) H H S X (c) H X Y a (c)(c) H H S a (c) H a X X (a) -subj, +comp (V) (b) +subj, +comp (P) (c) +subj, -comp (A) (d) -subj, -comp (N) Figure 2: Four fundamental primitives in Hale and Keyser's incorporation theory The combinatorial possibilities of incorporation with X=V, A, N, P heads, plus 'head movement', is designed to yield the space of possible syntactic argument structure configurations, presumably across all languages. Notions of agent, patient, instrument, theme, goal, etc. are not 'primitives', but are derived from positions in structural configurations. In English (but not necessarily in all languages), (a) the category V takes a complement but projects no specifier; (b) the category P takes both a complement and projects a specifier; (c) the category A takes no complement but projects a specifier; (d) the category N takes neither complement nor specifier. A particular verbal entry, being of category V, may incorporate one or more of these structures as its complement, as shown in Figure 3: * Nouns incorporated directly into a verbal entry yield structures such as (a): no subject is projected by the N. The phonetic material of the noun head incorporates (undergoes head movement) into the phonetic material of the verb head, which itself may undergo further movement. Verbs such as these are intransitive by nature, generating, e.g., /The light glow -ed/ but */Bob glow -ed the light/. This argument structure typifies purely internally caused processes.",#AUTHOR_TAG has developed an agenda-driven chart parser for the feature-driven formalism described above ; please refer to his paper for a description of the parsing algorithm .,"['#AUTHOR_TAG has developed an agenda-driven chart parser for the feature-driven formalism described above ; please refer to his paper for a description of the parsing algorithm .', 'I have adapted it for my needs and developed grammar fragments that reflect my non-lexicalist semantic framework.', 'As an example, a simplified derivation of the sentence ""The tire flattened."" is shown in Figure 1.']",2,"['#AUTHOR_TAG has developed an agenda-driven chart parser for the feature-driven formalism described above ; please refer to his paper for a description of the parsing algorithm .', 'I have adapted it for my needs and developed grammar fragments that reflect my non-lexicalist semantic framework.', 'As an example, a simplified derivation of the sentence ""The tire flattened."" is shown in Figure 1.']"
CCT182,N04-2004,A computational framework for non-lexicalist semantics,english verb classes and alternations a preliminary investigation,['Beth Levin'],introduction,"In this rich reference work, Beth Levin classifies over 3,000 English verbs according to shared meaning and behavior. Levin starts with the hypothesis that a verb's meaning influences its syntactic behavior and develops it into a powerful tool for studying the English verb lexicon. She shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning. The first part of the book sets out alternate ways in which verbs can express their arguments. The second presents classes of verbs that share a kernel of meaning and explores in detail the behavior of each class, drawing on the alternations in the first part. Levin's discussion of each class and alternation includes lists of relevant verbs, illustrative examples, comments on noteworthy properties, and bibliographic references. The result is an original, systematic picture of the organization of the verb inventory. Easy to use, ""English Verb Classes and Alternations"" sets the stage for further explorations of the interface between lexical semantics and syntax. It will prove indispensable for theoretical and computational linguists, psycholinguists, cognitive scientists, lexicographers, and teachers of English as a second language. Beth Levin is associate professor of linguistics at Northwestern University.","The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .","['The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .', 'The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class.', 'In addition, lexical rules could be formulated to derive certain alternations from more basic forms.']",1,"['The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .', 'The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class.', 'In addition, lexical rules could be formulated to derive certain alternations from more basic forms.']"
CCT183,N04-2004,A computational framework for non-lexicalist semantics,building verb meanings,"['Malka Rappaport Hovav', 'Beth Levin']",,,"There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .","['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .', 'Consider the following example:']",0,"['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .']"
CCT184,N04-2004,A computational framework for non-lexicalist semantics,verbs and times,['Zeno Vendler'],,"T HE fact that verbs have tenses indicates that considerations involving the concept of time are relevant to their use. These considerations are not limited merely to the obvious discrimination between past, present, and future; there is another, a more subtle dependence on that concept: the use of a verb may also suggest the particular way in which that verb presupposes and involves the notion of time. In a number of recent publications some attention has been paid to these finer aspects, perhaps for the first time systematically. Distinctions have been made among verbs suggesting processes, states, dispositions, occurrences, tasks, achievements, and so on. Obviously these differences cannot be explained in terms of time alone: other factors, like the presence or absence of an object, conditions, intended states of affairs, also enter the picture. Nevertheless one feels that the time element remains crucial; at least it is important enough to warrant separate treatment. Indeed, as I intend to show, if we focus our attention primarily upon the time schemata presupposed by various verbs,"" we are able to throw light on some of the obscurities which still remain in these matters. These time schemata will appear as important constituents of the concepts that prompt us to use those terms the way we consistently do. There are a few such schemata of very wide application. Once they have been discovered in some typical examples, they may be used as models of comparison in exploring and clarifying the behavior of any verb whatever. In indicating these schemata, I do not claim that they represent all possible ways in which verbs can be used correctly with respect to time determination nor that a verb exhibiting a use fairly covered by one schema cannot have divergent uses, which","A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )","['Dowty breaks the event described by (2) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean.', ""A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )""]",0,"[""A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )""]"
CCT185,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,natural language processing for information assurance and security an overview and implementations,"['Mikhail J Atallah', 'Craig J McDonough', 'Victor Raskin', 'Sergei Nirenburg']",related work,"This research paper explores a promising interface between natural language processing (NLP) and information assurance and security (IAS). More specificall~ it is devoted to possible applications to, and further dedicated development of, the accumulated considerable resources in NLP for, IAS. The expected and partially accomplished result is in harnessing the weird, illogical ways natural languages encode meaning, the very ways that defy all the usual combinatorial approaches to mathematical--and computational--complexity and make NLP so hard, to enhance information security. The paper is of a mixed theoretical and empirical nature. Of the four possible venues of applications, (i) memorizing randomly generated passwords with the help of automatically generated funny jingles, (ii) natural language watermarking, (iii) using the available machine translation (MT) systems for (additional) encryption of text messages, and (iv) downgrading, or sanitizing classified information in networks, two venues, (i) and (iv), have been at least partially implemented and the remaining two (ii) and (iii) are being implemented to the proof-of-concept level. We must make it very clear, however, that we have done very little experimentation or evaluation at this point, though we are moving quickly in that direction. The merits of the paper, if any, are in its venture to make considerable progress achieved recently in NLE especially in knowledge representation and meaning analysis, useful for IAS needs. The NLP approach adopted here, ontological semantics, has been developed by two of the coauthors; watermarking is based on the pioneering research by another coauthor and his associates; most of the implementation of the password memorization software has been done by the fourth coauthor. All the four of us have agonized whether we should report this research now or wait till we have fully implemented all or at least some of the systems we are developing. At the end of the day, we have reached a consensus that it is important, even at this early stage, to review for the information security community what NLP can do for it and to invite feedback and further efforts and ideas on what seems likely to become a new paradigm in information security. To the body of the paper, we Mikhail J. Atallah, Craig J. McDonough, Victor Raskin Center for Education and Research in Information Assurance and Security (CERIAS, www.cerias.purdue.edu) Purdue University W. Lafayette, IN 47907 mja, raskin, mcdonoug@cerias.purdue.edu Sergei Nirenburg Computing Research Laboratory, New Mexico State University Las Cruces, NM 88003 sergei@crl.nmsu.edu have added two self-contained deliberately reference-free appendices on NLP and ontological semantics, respectively, primarily for the benefit of those IAS readers, who are interested in expanding their understanding of those fields and further exploring their possible fruitful interactions with IAS.",#AUTHOR_TAGb ) and Topkara et al. ( 2006a ) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method .,"['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. (2005), Meral et al. (2007, Murphy (2001), Murphy and Vogel (2007) and Topkara et al. (2006a) all belong to the syntactic transformation category.', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', '#AUTHOR_TAGb ) and Topkara et al. ( 2006a ) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method .']",0,"['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. (2005), Meral et al. (2007, Murphy (2001), Murphy and Vogel (2007) and Topkara et al. (2006a) all belong to the syntactic transformation category.', '#AUTHOR_TAGb ) and Topkara et al. ( 2006a ) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method .']"
CCT186,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,hiding the hidden a software system for concealing ciphertext as innocuous text,"['Mark Chapman', 'George I Davida']",related work,"In this paper we present a system for protecting the privacy of cryptograms to avoid detection by censors. The system transforms ciphertext into innocuous text which can be transformed back into the original ciphertext. The expandable set of tools allows experimentation with custom dictionaries, automatic simulation of writing style, and the use of Context-Free-Grammars to control text generation. The scope of this paper is to provide an overview of the basic transformation processes and to demonstrate the quality of the generated text.",The first lexical substitution method was proposed by #AUTHOR_TAG .,"['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by #AUTHOR_TAG .', 'Later works, such as Atallah et al. (2001a), Bolshakov (2004), Taskiran et al. (2006 and Topkara et al. (2006b), further made use of part-ofspeech taggers and electronic dictionaries, such as WordNet and VerbNet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']",0,['The first lexical substitution method was proposed by #AUTHOR_TAG .']
CCT187,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,the syntax of concealment reliable methods for plain text information hiding,"['Brian Murphy', 'Carl Vogel']",related work,"Many plain text information hiding techniques demand deep semantic processing, and so suffer in reliability. In contrast, syntactic processing is a more mature and reliable technology. Assuming a perfect parser, this paper evaluates a set of automated and reversible syntactic transforms that can hide information in plain text without changing the meaning or style of a document. A large representative collection of newspaper text is fed through a prototype system. In contrast to previous work, the output is subjected to human testing to verify that the text has not been significantly compromised by the information hiding procedure, yielding a success rate of 96% and bandwidth of 0.3 bits per sentence.","Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , Murphy ( 2001 ) , #AUTHOR_TAG and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .","['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , Murphy ( 2001 ) , #AUTHOR_TAG and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001b) and Topkara et al. (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.']",0,"['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , Murphy ( 2001 ) , #AUTHOR_TAG and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001b) and Topkara et al. (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.']"
CCT188,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,natural language processing for information assurance and security an overview and implementations,"['Mikhail J Atallah', 'Craig J McDonough', 'Victor Raskin', 'Sergei Nirenburg']",related work,"This research paper explores a promising interface between natural language processing (NLP) and information assurance and security (IAS). More specificall~ it is devoted to possible applications to, and further dedicated development of, the accumulated considerable resources in NLP for, IAS. The expected and partially accomplished result is in harnessing the weird, illogical ways natural languages encode meaning, the very ways that defy all the usual combinatorial approaches to mathematical--and computational--complexity and make NLP so hard, to enhance information security. The paper is of a mixed theoretical and empirical nature. Of the four possible venues of applications, (i) memorizing randomly generated passwords with the help of automatically generated funny jingles, (ii) natural language watermarking, (iii) using the available machine translation (MT) systems for (additional) encryption of text messages, and (iv) downgrading, or sanitizing classified information in networks, two venues, (i) and (iv), have been at least partially implemented and the remaining two (ii) and (iii) are being implemented to the proof-of-concept level. We must make it very clear, however, that we have done very little experimentation or evaluation at this point, though we are moving quickly in that direction. The merits of the paper, if any, are in its venture to make considerable progress achieved recently in NLE especially in knowledge representation and meaning analysis, useful for IAS needs. The NLP approach adopted here, ontological semantics, has been developed by two of the coauthors; watermarking is based on the pioneering research by another coauthor and his associates; most of the implementation of the password memorization software has been done by the fourth coauthor. All the four of us have agonized whether we should report this research now or wait till we have fully implemented all or at least some of the systems we are developing. At the end of the day, we have reached a consensus that it is important, even at this early stage, to review for the information security community what NLP can do for it and to invite feedback and further efforts and ideas on what seems likely to become a new paradigm in information security. To the body of the paper, we Mikhail J. Atallah, Craig J. McDonough, Victor Raskin Center for Education and Research in Information Assurance and Security (CERIAS, www.cerias.purdue.edu) Purdue University W. Lafayette, IN 47907 mja, raskin, mcdonoug@cerias.purdue.edu Sergei Nirenburg Computing Research Laboratory, New Mexico State University Las Cruces, NM 88003 sergei@crl.nmsu.edu have added two self-contained deliberately reference-free appendices on NLP and ontological semantics, respectively, primarily for the benefit of those IAS readers, who are interested in expanding their understanding of those fields and further exploring their possible fruitful interactions with IAS.","Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .","['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an n- gram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']",0,"['The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .']"
CCT189,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,words are not enough sentence level natural language watermarking,"['Mercan Topkara', 'Umut Topkara', 'Mikhail J Atallah']",related work,"Compared to other media, natural language text presents unique challenges for information hiding. These challenges require the design of a robust algorithm that can work under following constraints: (i) low embedding bandwidth, i.e., number of sentences is comparable with message length, (ii) not all transformations can be applied to a given sentence (iii) the number of alternative forms for a sentence is relatively small, a limitation governed by the grammar and vocabulary of the natural language, as well as the requirement to preserve the style and fluency of the document. The adversary can carry out all the transformations used for embedding to remove the embedded message. In addition, the adversary can also permute the sentences, select and use a subset of sentences, and insert new sentences. We give a scheme that overcomes these challenges, together with a partial implementation and its evaluation for the English language. The present application of this scheme works at the sentence level while also using a word-level watermarking technique that was recently designed and built into a fully automatic system (""Equimark""). Unlike Equimark, whose resilience relied on the introduction of ambiguities, the present paper's sentence-level technique is more tuned to situations where very little change to the text is allowable (i.e., when style is important). Secondarily, this paper shows how to use lower-level (in this case word-level) marking to improve the resilience and embedding properties of higher level (in this case sentence level) schemes. We achieve this by using the word-based methods as a separate channel from the sentence-based methods, thereby improving the results of either one alone. The sentence level watermarking technique we introduce is novel and powerful, as it relies on multiple features of each sentence and exploits the notion of orthogonality between features.","Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .","['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']",0,"['Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .']"
CCT190,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,syntactic information hiding in plain text masters thesis trinity college dublin,['Brian Murphy'],related work,,"Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , #AUTHOR_TAG , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .","['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , #AUTHOR_TAG , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001b) and Topkara et al. (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.']",0,"['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , #AUTHOR_TAG , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .']"
CCT191,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,a method of linguistic steganography based on coladdressallyverified synonym,['Igor A Bolshakov'],related work,,"Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .","['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']",0,"['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']"
CCT192,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,webscale ngram models for lexical disambiguation,"['Shane Bergsma', 'Dekang Lin', 'Randy Goebel']",experiments,,"The Google n-gram data was collected by Google Research for statistical language modelling , and has been used for many tasks such as lexical disambiguation ( #AUTHOR_TAG ) , and contains English n-grams and their observed frequency counts , for counts of at least 40 .","['The Google n-gram data was collected by Google Research for statistical language modelling , and has been used for many tasks such as lexical disambiguation ( #AUTHOR_TAG ) , and contains English n-grams and their observed frequency counts , for counts of at least 40 .', 'The striking feature of the n-gram corpus is the large number of n-grams and the size of the counts, since the counts were extracted from over 1 trillion word tokens of English text on publicly accessible Web pages collected in January 2006.', 'For example, the 5-gram phrase the part that you were has a count of 103.', 'The compressed data is around 24 GB on disk.']",0,"['The Google n-gram data was collected by Google Research for statistical language modelling , and has been used for many tasks such as lexical disambiguation ( #AUTHOR_TAG ) , and contains English n-grams and their observed frequency counts , for counts of at least 40 .']"
CCT193,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,steganography in digital media principles algorithms and applications,['Jessica Fridrich'],introduction,"Steganography, the art of hiding of information in apparently innocuous objects or images, is a field with a rich heritage, and an area of rapid current development. This clear, self-contained guide shows you how to understand the building blocks of covert communication in digital media files and how to apply the techniques in practice, including those of steganalysis, the detection of steganography. Assuming only a basic knowledge in calculus and statistics, the book blends the various strands of steganography, including information theory, coding, signal estimation and detection, and statistical signal processing. Experiments on real media files demonstrate the performance of the techniques in real life, and most techniques are supplied with pseudo-code, making it easy to implement the algorithms. The book is ideal for students taking courses on steganography and information hiding, and is also a useful reference for engineers and practitioners working in media security and information assurance.","Steganography is concerned with hiding information in some cover medium , by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer ( #AUTHOR_TAG ) .","['Steganography is concerned with hiding information in some cover medium , by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer ( #AUTHOR_TAG ) .', 'The covert communication is such that the very act of communication is to be kept secret from outside observers.', 'A related area is Watermarking, in which modifications are made to a cover medium in order to identify it, for example for the purposes of copyright.', 'Here the changes may be known to an observer, and the task is to make the changes in such a way that the watermark cannot easily be removed.']",0,"['Steganography is concerned with hiding information in some cover medium , by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer ( #AUTHOR_TAG ) .']"
CCT194,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,words are not enough sentence level natural language watermarking,"['Mercan Topkara', 'Umut Topkara', 'Mikhail J Atallah']",related work,"Compared to other media, natural language text presents unique challenges for information hiding. These challenges require the design of a robust algorithm that can work under following constraints: (i) low embedding bandwidth, i.e., number of sentences is comparable with message length, (ii) not all transformations can be applied to a given sentence (iii) the number of alternative forms for a sentence is relatively small, a limitation governed by the grammar and vocabulary of the natural language, as well as the requirement to preserve the style and fluency of the document. The adversary can carry out all the transformations used for embedding to remove the embedded message. In addition, the adversary can also permute the sentences, select and use a subset of sentences, and insert new sentences. We give a scheme that overcomes these challenges, together with a partial implementation and its evaluation for the English language. The present application of this scheme works at the sentence level while also using a word-level watermarking technique that was recently designed and built into a fully automatic system (""Equimark""). Unlike Equimark, whose resilience relied on the introduction of ambiguities, the present paper's sentence-level technique is more tuned to situations where very little change to the text is allowable (i.e., when style is important). Secondarily, this paper shows how to use lower-level (in this case word-level) marking to improve the resilience and embedding properties of higher level (in this case sentence level) schemes. We achieve this by using the word-based methods as a separate channel from the sentence-based methods, thereby improving the results of either one alone. The sentence level watermarking technique we introduce is novel and powerful, as it relies on multiple features of each sentence and exploits the notion of orthogonality between features.","Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and #AUTHOR_TAGa ) all belong to the syntactic transformation category .","['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and #AUTHOR_TAGa ) all belong to the syntactic transformation category .', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001b) and Topkara et al. (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.']",0,"['Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and #AUTHOR_TAGa ) all belong to the syntactic transformation category .']"
CCT195,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,a method of text watermarking using presuppositions,"['M Olga Vybornova', 'Benoit Macq']",related work,"We propose a method for watermarking texts of arbitrary length using natural-language semantic structures. For the key of our approach we use the linguistic semantic phenomenon of presuppositions. Presupposition is the implicit information considered as well-known or which readers of the text are supposed to treat as well-known; this information is a semantic component of certain linguistic expressions (lexical items and syntactical constructions called presupposition triggers). The same sentence can be used with or without presupposition, or with a different presupposition trigger, provided that all the relations between subjects, objects and other discourse referents are preserved - such transformations will not change the meaning of the sentence. We define the distinct rules for presupposition identification for each trigger and regular transformation rules for using/non-using the presupposition in a given sentence (one bit per sentence in this case). Isolated sentences can carry the proposed watermarks. However, the longer is the text, the more efficient is the watermark. The proposed approach is resilient to main types of random transformations, like passivization, topicalization, extraposition, preposing, etc. The web of resolved presupposed information in the text will hold the watermark of the text (e.g. integrity watermark, or prove of ownership), introducing ""secret ordering"" into the text structure to make it resilient to ""data loss"" attacks and ""data altering"" attacks.Anglai","#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition , with the idea that some presuppositional information can be removed without changing the meaning of a sentence .","['The semantic transformation method is the most sophisticated approach for linguistic steganography, and perhaps impractical given the current state-ofthe-art for NLP technology.', 'It requires some sophisticated tools and knowledge to model natural language semantics.', 'Atallah et al. (2002) used semantic transformations and embed information in textmeaning representation (TMR) trees of the text by either pruning, grafting or substituting the tree structure with information available from ontological semantic resources.', '#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition , with the idea that some presuppositional information can be removed without changing the meaning of a sentence .']",0,"['#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition , with the idea that some presuppositional information can be removed without changing the meaning of a sentence .']"
CCT196,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,extracting paraphrases from a parallel corpus,"['Regina Barzilay', 'Kathleen R McKeown']",experiments,"While paraphrasing is critical both for interpretation and generation of natural language, current systems use manual or semi-automatic methods to collect paraphrases. We present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple English translations of the same source text. Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases.",#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context .,"['Table 1 gives summary statistics of the paraphrase dictionary and its coverage on Section 00 of the Penn Treebank.', 'The length of the extracted n-gram phrases ranges from unigrams to five-grams.', 'The coverage figure gives the percentage of sentences which have at least one phrase in the dictionary.', 'The coverage is important for us because it determines the payload capacity of the embedding method described in Section 5.  Original phrase Paraphrases the end of this year later this year the end of the year year end a number of people some of my colleagues differences the European peoples party the PPE group dictionary is a mapping from phrases to sets of possible paraphrases.', 'Each paraphrase also has a probability, based on a statistical machine translation model, but we do not use that feature here.', 'The examples show that, while some of the paraphrases are of a high quality, some are not.', 'For example, differences is unlikely to be a suitable paraphrase for a number of people in any context.', 'Moreover, there are some phrase, paraphrase pairs which are only suitable in particular contexts.', 'For example, year end is an unsuitable paraphrase for the end of this year in the sentence The chart compares the gold price at the end of last year with the end of this year.', '#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context .', 'Section 4 describes our method for determining if a paraphrase is suitable in a given context.']",0,"['Table 1 gives summary statistics of the paraphrase dictionary and its coverage on Section 00 of the Penn Treebank.', 'Each paraphrase also has a probability, based on a statistical machine translation model, but we do not use that feature here.', 'The examples show that, while some of the paraphrases are of a high quality, some are not.', 'Moreover, there are some phrase, paraphrase pairs which are only suitable in particular contexts.', '#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context .', 'Section 4 describes our method for determining if a paraphrase is suitable in a given context.']"
CCT197,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,syntactic constraints on paraphrases extracted from parallel corpora,['Chris Callison-Burch'],experiments,"ccb cs jhu edu We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type. This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method.","The paraphrase dictionary that we use was generated for us by Chris Callison-Burch , using the technique described in #AUTHOR_TAG , which exploits a parallel corpus and methods developed for statistical machine translation .","['The cover text used for our experiments consists of newspaper sentences from Section 00 of the Penn Treebank (Marcus et al., 1993).', 'Hence we require possible paraphrases for phrases that occur in Section 00.', 'The paraphrase dictionary that we use was generated for us by Chris Callison-Burch , using the technique described in #AUTHOR_TAG , which exploits a parallel corpus and methods developed for statistical machine translation .']",5,"['Hence we require possible paraphrases for phrases that occur in Section 00.', 'The paraphrase dictionary that we use was generated for us by Chris Callison-Burch , using the technique described in #AUTHOR_TAG , which exploits a parallel corpus and methods developed for statistical machine translation .']"
CCT198,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,widecoverage efficient statistical parsing with ccg and loglinear models,"['Stephen Clark', 'James R Curran']",introduction,,"We also experiment with a CCG parser (Clark and Curran, 2007), requiring that the contexts surrounding the original phrase and paraphrase are assigned the same CCG lexical categories by the parser.","['In order to test the grammaticality and meaning preserving nature of a paraphrase, we employ a simple technique based on checking whether the contexts containing the paraphrase are in the Google ngram corpus.', 'This technique is based on the simple hypothesis that, if the paraphrase in context has been used many times before on the web, then it is an appropriate use.', 'We test our n-gram-based system against some human judgements of the grammaticality of paraphrases in context.', 'We find that using larger contexts leads to a high precision system (100% when using 5-grams), but at the cost of a reduced recall.', 'This precision-recall tradeoff reflects the inherent tradeoff between imperceptibility and payload in a Linguistic Steganography system.', 'We also experiment with a CCG parser (Clark and Curran, 2007), requiring that the contexts surrounding the original phrase and paraphrase are assigned the same CCG lexical categories by the parser.', 'This method increases the precision of the Google n-gram check with a slight loss in recall.']",5,"['We also experiment with a CCG parser (Clark and Curran, 2007), requiring that the contexts surrounding the original phrase and paraphrase are assigned the same CCG lexical categories by the parser.']"
CCT199,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,a natural language watermarking based on chinese syntax,"['Yuling Liu', 'Xingming Sun', 'Yong Wu']",related work,"A novel text watermarking algorithm is presented. It combines natural language watermarking and Chinese syntax based on BP neural networks. Since the watermarking signals are embedded into some Chinese syntactic structure rather than the appearance of text elements, the algorithm is totally based on the content that can prove to be very resilient. It will play an important role in protecting the security of Chinese documents over Internet.","#AUTHOR_TAG , Meral et al. ( 2007 ) , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .","['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', '#AUTHOR_TAG , Meral et al. ( 2007 ) , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001b) and Topkara et al. (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.']",0,"['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', '#AUTHOR_TAG , Meral et al. ( 2007 ) , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001b) and Topkara et al. (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.']"
CCT200,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,whitesteg a new scheme in information hiding using text steganography,"['Lip Y Por', 'Ang T Fong', 'B Delina']",introduction,"Abstract:- Sending encrypted messages frequently will draw the attention of third parties, i.e. crackers and hackers, perhaps causing attempts to break and reveal the original messages. In this digital world, steganography is introduced to hide the existence of the communication by concealing a secret message inside another unsuspicious message. The hidden message maybe plaintext, or any data that can be represented as a stream of bits. Steganography is often being used together with cryptography and offers an acceptable amount of privacy and security over the communication channel. This paper presents an overview of text steganography and a brief history of steganography along with various existing techniques of text steganography. Highlighted are some of the problems inherent in text steganography as well as issues with existing solutions. A new approach, named WhiteSteg is proposed in information hiding using inter-word spacing and inter-paragraph spacing as a hybrid method to reduce the visible detection of the embedded messages. WhiteSteg offers dynamic generated cover-text with six options of maximum capacity according to the length of the secret message. Besides, the advantage of exploiting whitespaces in information hiding is discussed. This paper also analyzes the significant drawbacks of each existing method and how WhiteSteg could be recommended as a solution","Note that we are concerned with transformations which are linguistic in nature , rather than dealing with superficial properties of the text , e.g. the amount of white space between words ( #AUTHOR_TAG ) .","['Section 2 describes some of the previous transformations used in Linguistic Steganography.', 'Note that we are concerned with transformations which are linguistic in nature , rather than dealing with superficial properties of the text , e.g. the amount of white space between words ( #AUTHOR_TAG ) .', 'Our proposed method is based on the automatically acquired paraphrase dictionary described in Callison-Burch (2008), in which the application of paraphrases from the dictionary encodes secret bits.', 'One advantage of the dictionary is that it has wide coverage, being automatically extracted; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', 'Since we require any changes to be imperceptible to a human observer, it is crucial to our system that any uses of paraphrasing are grammatical and retain the meaning of the original cover text.']",1,"['Section 2 describes some of the previous transformations used in Linguistic Steganography.', 'Note that we are concerned with transformations which are linguistic in nature , rather than dealing with superficial properties of the text , e.g. the amount of white space between words ( #AUTHOR_TAG ) .']"
CCT201,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,syntactic constraints on paraphrases extracted from parallel corpora,['Chris Callison-Burch'],,"ccb cs jhu edu We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type. This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method.","Our proposed method is based on the automatically acquired paraphrase dictionary described in #AUTHOR_TAG , in which the application of paraphrases from the dictionary encodes secret bits .","['Section 2 describes some of the previous transformations used in Linguistic Steganography.', 'Note that we are concerned with transformations which are linguistic in nature, rather than dealing with superficial properties of the text, e.g. the amount of white space between words (Por et al., 2008).', 'Our proposed method is based on the automatically acquired paraphrase dictionary described in #AUTHOR_TAG , in which the application of paraphrases from the dictionary encodes secret bits .', 'One advantage of the dictionary is that it has wide coverage, being automatically extracted; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', 'Since we require any changes to be imperceptible to a human observer, it is crucial to our system that any uses of paraphrasing are grammatical and retain the meaning of the original cover text.']",5,"['Our proposed method is based on the automatically acquired paraphrase dictionary described in #AUTHOR_TAG , in which the application of paraphrases from the dictionary encodes secret bits .', 'One advantage of the dictionary is that it has wide coverage, being automatically extracted; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', 'Since we require any changes to be imperceptible to a human observer, it is crucial to our system that any uses of paraphrasing are grammatical and retain the meaning of the original cover text.']"
CCT202,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,a comprehensive bibliography of linguistic steganography,['Richard Bergmair'],introduction,"In this paper, we will attempt to give a comprehensive bibliographic account of the work in linguistic steganography published up to date. As the field is still in its infancy there is no widely accepted publication venue. Relevant work on the subject is scattered throughout the literature on information security, information hiding, imaging and watermarking, cryptology, and natural language processing. Bibliographic references within the field are very sparse. This makes literature research on linguistic steganography a tedious task and a comprehensive bibliography a valuable aid to the researcher.","However , the literature on Linguistic Steganography , in which linguistic properties of a text are modified to hide information , is small compared with other media ( #AUTHOR_TAG ) .","['There is a large literature on image steganography and watermarking, in which images are modified to encode a hidden message or watermark.', 'Image stegosystems exploit the redundancy in an image representation together with limitations of the human visual system.', 'For example, a standard image stegosystem uses the least-significant-bit (LSB) substitution technique.', 'Since the difference between 11111111 and 11111110 in the value for red/green/blue intensity is likely to be undetectable by the human eye, the LSB can be used to hide information other than colour, without being perceptable by a human observer. 1', ' key question for any steganography system is the choice of cover medium.', 'Given the ubiquitous nature of natural languages and electronic text, text is an obvious medium to consider.', 'However , the literature on Linguistic Steganography , in which linguistic properties of a text are modified to hide information , is small compared with other media ( #AUTHOR_TAG ) .', 'The likely reason is that it is easier to make changes to images and other nonlinguistic media which are undetectable by an observer.', 'Language has the property that even small local changes to a text, e.g.', 'replacing a word by a word with similar meaning, may result in text which is anomalous at the document level, or anomalous with respect to the state of the world.', 'Hence finding linguistic transformations which can be applied reliably and often is a challenging problem for Linguistic Steganography.']",0,"['However , the literature on Linguistic Steganography , in which linguistic properties of a text are modified to hide information , is small compared with other media ( #AUTHOR_TAG ) .']"
CCT203,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,widecoverage efficient statistical parsing with ccg and loglinear models,"['Stephen Clark', 'James R Curran']",method,,We use the #AUTHOR_TAG CCG parser to analyse the sentence before and after paraphrasing .,"['In order to improve the grammaticality checking, we use a parser as an addition to the basic Google ngram method.', 'We use the #AUTHOR_TAG CCG parser to analyse the sentence before and after paraphrasing .', 'Combinatory Categorial Grammar (CCG) is a lexicalised grammar formalism, in which CCG lexical categories -typically expressing subcategorisation information -are assigned to each word in a sentence.', 'The grammatical check works by checking if the words in the sentence outside of the phrase and paraphrase receive the same lexical categories before and after paraphrasing.', 'If there is any change in lexical category assignment to these words then the paraphrase is judged ungrammatical.', 'Hence the grammar check is at the word, rather than derivation, level; however, CCG lexical categories contain a large amount of syntactic information which this method is able to exploit.']",5,"['In order to improve the grammaticality checking, we use a parser as an addition to the basic Google ngram method.', 'We use the #AUTHOR_TAG CCG parser to analyse the sentence before and after paraphrasing .', 'Combinatory Categorial Grammar (CCG) is a lexicalised grammar formalism, in which CCG lexical categories -typically expressing subcategorisation information -are assigned to each word in a sentence.', 'Hence the grammar check is at the word, rather than derivation, level; however, CCG lexical categories contain a large amount of syntactic information which this method is able to exploit.']"
CCT204,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,syntactic tools for text watermarking,"['Hasan M Meral', 'Emre Sevinc', 'Ersin Unkar', 'Bulent Sankur', 'A Sumru Ozsoy', 'Tunga Gungor']",related work,"This paper explores the morphosyntactic tools for text watermarking and develops a syntax-based natural language watermarking scheme. Turkish, an agglutinative language, provides a good ground for the syntax-based natural language watermarking with its relatively free word order possibilities and rich repertoire of morphosyntactic structures. The unmarked text is first transformed into a syntactic tree diagram in which the syntactic hierarchies and the functional dependencies are coded. The watermarking software then operates on the sentences in syntax tree format and executes binary changes under control of Wordnet to avoid semantic drops. The key-controlled randomization of morphosyntactic tool order and the insertion of void watermark provide a certain level of security. The embedding capacity is calculated statistically, and the imperceptibility is measured using edit hit counts.","Liu et al. ( 2005 ) , #AUTHOR_TAG , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .","['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ) , #AUTHOR_TAG , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001b) and Topkara et al. (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.']",0,"['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ) , #AUTHOR_TAG , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .']"
CCT205,N13-1036,Unsupervised Arabic dialect segmentation for machine translation,dialectal to standard arabic paraphrasing to improve arabicenglish statistical machine translation,"['Wael Salloum', 'Nizar Habash']",related work,"This paper is about improving the quality of Arabic-English statistical machine translation (SMT) on dialectal Arabic text using morphological knowledge. We present a light-weight rule-based approach to producing Modern Standard Arabic (MSA) paraphrases of dialectal Arabic out-of-vocabulary (OOV) words and low frequency words. Our approach extends an existing MSA analyzer with a small number of morphological clitics, and uses transfer rules to generate paraphrase lattices that are input to a state-of-the-art phrase-based SMT system. This approach improves BLEU scores on a blind test set by 0.56 absolute BLEU (or 1.5% relative). A manual error analysis of translated dialectal words shows that our system produces correct translations in 74% of the time for OOVs and 60% of the time for low frequency words.","In our previous work ( #AUTHOR_TAG ; Salloum and Habash , 2012 ) , we applied our approach to tokenized Arabic and our DA-MSA transfer component used feature transfer rules only .","['In our previous work ( #AUTHOR_TAG ; Salloum and Habash , 2012 ) , we applied our approach to tokenized Arabic and our DA-MSA transfer component used feature transfer rules only .', 'We did not use a language model to pick the best path; instead we kept the ambiguity in the lattice and passed it to our SMT system.', 'In contrast, in this paper, we run ELISSA on untokenized Arabic, we use feature, lemma, and surface form transfer rules, and we pick the best path of the generated MSA lattice through a language model.', ""Certain aspects of our approach are similar to Riesa and Yarowsky (2006)'s, in that we use morphological analysis for DA to help DA-English MT; but unlike them, we use a rule-based approach to model DA morphology.""]",1,"['In our previous work ( #AUTHOR_TAG ; Salloum and Habash , 2012 ) , we applied our approach to tokenized Arabic and our DA-MSA transfer component used feature transfer rules only .', 'In contrast, in this paper, we run ELISSA on untokenized Arabic, we use feature, lemma, and surface form transfer rules, and we pick the best path of the generated MSA lattice through a language model.']"
CCT206,N13-1036,Unsupervised Arabic dialect segmentation for machine translation,a systematic comparison of various statistical alignment models,"['F J Och', 'H Ney']",,"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.",The parallel corpus is word-aligned using GIZA + + ( #AUTHOR_TAG ) .,"['We use the open-source Moses toolkit (Koehn et al., 2007) to build a phrase-based SMT system trained on mostly MSA data (64M words on the Arabic side) obtained from several LDC corpora including some limited DA data.', 'Our system uses a standard phrase-based architecture.', 'The parallel corpus is word-aligned using GIZA + + ( #AUTHOR_TAG ) .', 'Phrase translations of up to 10 words are extracted in the Moses phrase table.', 'The language model for our system is trained on the English side of the bitext augmented with English Gigaword (Graff and Cieri, 2003).', 'We use a 5-gram language model with modified Kneser-Ney smoothing.', 'Feature weights are tuned to maximize BLEU on the NIST MTEval 2006 test set using Minimum Error Rate Training (Och, 2003).', 'This is only done on the baseline systems.', 'The English data is tokenized using simple punctuation-based rules.', 'The Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004) using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Habash and Rambow, 2005;Roth et al., 2008).', 'The Arabic text is also Alif/Ya normalized.', 'MADA-produced Arabic lemmas are used for word alignment.']",5,"['Our system uses a standard phrase-based architecture.', 'The parallel corpus is word-aligned using GIZA + + ( #AUTHOR_TAG ) .', 'Phrase translations of up to 10 words are extracted in the Moses phrase table.', 'We use a 5-gram language model with modified Kneser-Ney smoothing.', 'Feature weights are tuned to maximize BLEU on the NIST MTEval 2006 test set using Minimum Error Rate Training (Och, 2003).', 'This is only done on the baseline systems.', 'The English data is tokenized using simple punctuation-based rules.', 'The Arabic text is also Alif/Ya normalized.']"
CCT207,N13-1036,Unsupervised Arabic dialect segmentation for machine translation,dialectal to standard arabic paraphrasing to improve arabicenglish statistical machine translation,"['Wael Salloum', 'Nizar Habash']",,"This paper is about improving the quality of Arabic-English statistical machine translation (SMT) on dialectal Arabic text using morphological knowledge. We present a light-weight rule-based approach to producing Modern Standard Arabic (MSA) paraphrases of dialectal Arabic out-of-vocabulary (OOV) words and low frequency words. Our approach extends an existing MSA analyzer with a small number of morphological clitics, and uses transfer rules to generate paraphrase lattices that are input to a state-of-the-art phrase-based SMT system. This approach improves BLEU scores on a blind test set by 0.56 absolute BLEU (or 1.5% relative). A manual error analysis of translated dialectal words shows that our system produces correct translations in 74% of the time for OOVs and 60% of the time for low frequency words.",This is a similar conclusion to our previous work in #AUTHOR_TAG .,"['In the last system group, phrase+word-based selection, phrase-based selection is used to select phrases and add them on top of the best performers of the previous two groups.', 'Phrase-based trans-  lation is also added to word-based translation.', 'Results show that selecting and translating phrases improve the three best performers of word-based selection.', 'The best performer, shown in the last raw, suggests using phrase-based selection and restricted word-based selection.', 'The restriction is to include OOV words and selected low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries.', 'Comparing the best performer to the OOV selection mode system shows that translating low frequency in-vocabulary dialectal words and phrases to their MSA paraphrases can improve the English translation.', 'This is a similar conclusion to our previous work in #AUTHOR_TAG .']",1,"['In the last system group, phrase+word-based selection, phrase-based selection is used to select phrases and add them on top of the best performers of the previous two groups.', 'Phrase-based trans-  lation is also added to word-based translation.', 'Results show that selecting and translating phrases improve the three best performers of word-based selection.', 'The best performer, shown in the last raw, suggests using phrase-based selection and restricted word-based selection.', 'The restriction is to include OOV words and selected low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries.', 'Comparing the best performer to the OOV selection mode system shows that translating low frequency in-vocabulary dialectal words and phrases to their MSA paraphrases can improve the English translation.', 'This is a similar conclusion to our previous work in #AUTHOR_TAG .']"
CCT208,N13-1036,Unsupervised Arabic dialect segmentation for machine translation,moses open source toolkit for statistical machine translation,"['Philipp Koehn', 'Hieu Hoang', 'Alexandra Birch', 'Christopher Callison-Burch', 'Marcello Federico', 'Nicola Bertoldi', 'Brooke Cowan', 'Wade Shen', 'Christine Moran', 'Richard Zens', 'Christopher Dyer', 'Ondrej Bojar', 'Alexandra Constantin', 'Evan Herbst']",,"We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.",We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .,"['We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .', 'Our system uses a standard phrase-based architecture.', 'The parallel corpus is word-aligned using GIZA++ (Och and Ney, 2003).', 'Phrase translations of up to 10 words are extracted in the Moses phrase table.', 'The language model for our system is trained on the English side of the bitext augmented with English Gigaword (Graff and Cieri, 2003).', 'We use a 5-gram language model with modified Kneser-Ney smoothing.', 'Feature weights are tuned to maximize BLEU on the NIST MTEval 2006 test set using Minimum Error Rate Training (Och, 2003).', 'This is only done on the baseline systems.', 'The English data is tokenized using simple punctuation-based rules.', 'The Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004) using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Habash and Rambow, 2005;Roth et al., 2008).', 'The Arabic text is also Alif/Ya normalized.', 'MADA-produced Arabic lemmas are used for word alignment.']",5,['We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .']
CCT209,P00-1001,Processes that shape conversation and their implications for computational linguistics,deterministic parsing of syntactic nonfluencies,['D Hindle'],introduction,,"Few approaches to parsing have tried to handle disfluent utterances ( notable exceptions are Core & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ; Shriberg , Bear , & Dowding , 1992 ) .","[""The implicit assumptions of psychological and computational theories that ignore disfluencies must be either that people aren't disfluent, or that disfluencies make processing more difficult, and so theories of fluent speech processing should be developed before the research agenda turns to disfluent speech processing."", 'The first assumption is clearly false; disfluency rates in spontaneous speech are estimated by Fox Tree (1995) and by Bortfeld, Leon, Bloom, Schober, and Brennan (2000) to be about 6 disfluencies per 100 words, not including silent pauses.', 'The rate is lower for speech to machines (Oviatt, 1995;Shriberg, 1996), due in part to utterance length; that is, disfluency rates are higher in longer utterances, where planning is more difficult, and utterances addressed to machines tend to be shorter than those addressed to people, often because dialogue interfaces are designed to take on more initiative.', 'The average speaker may believe, quite rightly, that machines are imperfect speech processors, and plan their utterances to machines more carefully.', 'The good news is that speakers can adapt to machines; the bad news is that they do so by recruiting limited cognitive resources that could otherwise be focused on the task itself.', 'As for the second assumption, if the goal is to eventually process unrestricted, natural human speech, then committing to an early and exclusive focus on processing fluent utterances is risky.', 'In humans, speech production and speech processing are done incrementally, using contextual information from the earliest moments of processing (see, e.g., Tanenhaus et al. 1995).', 'This sort of processing requires quite a different architecture and different mechanisms for ambiguity resolution than one that begins processing only at the end of a complete and well-formed utterance.', 'Few approaches to parsing have tried to handle disfluent utterances ( notable exceptions are Core & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ; Shriberg , Bear , & Dowding , 1992 ) .']",0,"['Few approaches to parsing have tried to handle disfluent utterances ( notable exceptions are Core & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ; Shriberg , Bear , & Dowding , 1992 ) .']"
CCT210,P06-1012,Estimating class priors in domain adaptation for word sense disambiguation,an empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation,"['Yoong Keok Lee', 'Hwee Tou Ng']",experiments,"In this paper, we evaluate a variety  of knowledge sources and supervised  learning algorithms for word sense  disambiguation on SENSEVAL-2 and  SENSEVAL-1 data. Our knowledge  sources include the part-of-speech of  neighboring words, single words in the  surrounding context, local collocations,  and syntactic relations. The learning algorithms  evaluated include Support Vector  Machines (SVM), Naive Bayes, AdaBoost,  and decision tree algorithms. We  present empirical results showing the relative  contribution of the component knowledge  sources and the different learning  algorithms. In particular, using all of  these knowledge sources and SVM (i.e.,  a single learning algorithm) achieves accuracy  higher than the best official scores  on both SENSEVAL-2 and SENSEVAL-1  test data","Similar to our previous work ( Chan and Ng , 2005b ) , we used the supervised WSD approach described in ( #AUTHOR_TAG ) for our experiments , using the naive Bayes algorithm as our classifier .","['Similar to our previous work ( Chan and Ng , 2005b ) , we used the supervised WSD approach described in ( #AUTHOR_TAG ) for our experiments , using the naive Bayes algorithm as our classifier .', 'Knowledge sources used include partsof-speech, surrounding words, and local collocations.', 'This approach achieves state-of-the-art accuracy.', 'All accuracies reported in our experiments are micro-averages over all test examples.']",5,"['Similar to our previous work ( Chan and Ng , 2005b ) , we used the supervised WSD approach described in ( #AUTHOR_TAG ) for our experiments , using the naive Bayes algorithm as our classifier .', 'Knowledge sources used include partsof-speech, surrounding words, and local collocations.']"
CCT211,P08-1101,"Regularized Structured Perceptron: A Case Study on Chinese Word Segmentation, POS Tagging and Parsing",chinese partofspeech tagging oneatatime or allatonce wordbased or characterbased,"['Hwee Tou Ng', 'Jin Kiat Low']",experiments,,We chose to follow #AUTHOR_TAG and split the sentences evenly to facilitate further comparison .,"['The overall tagging accuracy of our joint model was comparable to but less than the joint model of Shi and Wang (2007).', 'Despite the higher accuracy improvement from the baseline, the joint system did not give higher overall accuracy.', 'One likely reason is that Shi and Wang (2007) included knowledge about special characters and semantic knowledge from web corpora (which may explain the higher baseline accuracy), while our system is completely data-driven.', 'However, the comparison is indirect because our partitions of the CTB corpus are different.', 'Shi and Wang (2007) also chunked the sentences before doing 10-fold cross validation, but used an uneven split.', 'We chose to follow #AUTHOR_TAG and split the sentences evenly to facilitate further comparison .']",5,"['The overall tagging accuracy of our joint model was comparable to but less than the joint model of Shi and Wang (2007).', 'One likely reason is that Shi and Wang (2007) included knowledge about special characters and semantic knowledge from web corpora (which may explain the higher baseline accuracy), while our system is completely data-driven.', 'We chose to follow #AUTHOR_TAG and split the sentences evenly to facilitate further comparison .']"
CCT212,P08-1101,"Regularized Structured Perceptron: A Case Study on Chinese Word Segmentation, POS Tagging and Parsing",chinese segmentation with a wordbased perceptron algorithm,"['Yue Zhang', 'Stephen Clark']",experiments,,"We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .","['We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .', 'We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).', 'The features used by the baseline segmentor are shown in Table 1.', 'The features used by the POS tagger, some of which are different to those from Collins (2002) and are specific to Chinese, are shown in Table 2.']",2,"['We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .']"
CCT213,P10-1143,Unsupervised Event Coreference Resolution with Rich Linguistic Features,unsupervised coreference resolution in a nonparametric bayesian model,"['Aria Haghighi', 'Dan Klein']",method,"We present an unsupervised, nonparametric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document. While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state. Despite being unsupervised, our system achieves a 70.3 MUC F1 measure on the MUC-6 test set, broadly in the range of some recent supervised results.",Our HDP extension is also inspired from the Bayesian model proposed by #AUTHOR_TAG .,"['We present an extension of the hierarchical Dirichlet process (HDP) model which is able to represent each observable object (i.e., event mention) by a finite number of feature types L.', 'Our HDP extension is also inspired from the Bayesian model proposed by #AUTHOR_TAG .', 'However, their model is strictly customized for entity coreference resolution, and therefore, extending it to include additional features for each observable object is a challenging task (Ng, 2008; Poon and Domingos, 2008).']",4,"['Our HDP extension is also inspired from the Bayesian model proposed by #AUTHOR_TAG .', 'However, their model is strictly customized for entity coreference resolution, and therefore, extending it to include additional features for each observable object is a challenging task (Ng, 2008; Poon and Domingos, 2008).']"
CCT214,P10-2002,A Joint Rule Selection Model for Hierarchical Phrase-based Translation *,soft syntactic constraints for hierarchical phrasedbased translation,"['Yuval Marton', 'Philip Resnik']",method,"In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic analysis, versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data. A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then nding appropriate ways to soften that commitment. We present an approach that explores the tradeoff from the other direction, starting with a context-free translation model learned directly from aligned parallel text, and then adding soft constituent-level constraints based on parses of the source language. We obtain substantial improvements in performance for translation from Chinese and Arabic to English.","These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .","['ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM.', 'These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .']",4,"['These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .']"
CCT215,P10-2002,A Joint Rule Selection Model for Hierarchical Phrase-based Translation *,rich sourceside context for statistical machine translation,"['Kevin Gimpel', 'Noah A Smith']",method,"We explore the augmentation of statistical machine translation models with features of the context of each phrase to be translated. This work extends several existing threads of research in statistical MT, including the use of context in example-based machine translation (Carl and Way, 2003) and the incorporation of word sense disambiguation into a translation model (Chan et al., 2007). The context features we consider use surrounding words and part-of-speech tags, local syntactic structure, and other properties of the source language sentence to help predict each phrase's translation. Our approach requires very little computation beyond the standard phrase extraction algorithm and scales well to large data scenarios. We report significant improvements in automatic evaluation scores for Chinese-to-English and English-to-German translation, and also describe our entry in the WMT-08 shared task based on this approach.","These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .","['ME approach has the merit of easily combining different features to predict the probability of each class.', 'We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM.', 'These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .']",4,"['We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM.', 'These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .']"
CCT216,P10-2005,Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages,confidence measure for word alignment,['Fei Huang'],introduction,"In this paper we present a confidence mea-sure for word alignment based on the posterior probability of alignment links. We introduce sentence alignment confi-dence measure and alignment link con-fidence measure. Based on these mea-sures, we improve the alignment qual-ity by selecting high confidence sentence alignments and alignment links from mul-tiple word alignments of the same sen-tence pair. Additionally, we remove low confidence alignment links from the word alignment of a bilingual training corpus, which increases the alignment F-score, improves Chinese-English and Arabic-English translation quality and sig-nificantly reduces the phrase translation table size.","More recently , an alignment selection approach was proposed in ( #AUTHOR_TAG ) , which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold .","['More recently , an alignment selection approach was proposed in ( #AUTHOR_TAG ) , which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold .', 'The alignments used in that work were generated from different aligners (HMM, block model, and maximum entropy model).', 'In this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function.', 'There is no need for a pre-determined threshold as used in (Huang, 2009).', 'Also, we utilize various knowledge sources to enrich the alignments instead of using different aligners.', 'Our strategy is to diversify and then combine in order to catch any complementary information captured in the word alignments for low-resource languages.']",1,"['More recently , an alignment selection approach was proposed in ( #AUTHOR_TAG ) , which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold .', 'Our strategy is to diversify and then combine in order to catch any complementary information captured in the word alignments for low-resource languages.']"
CCT217,P10-2019,Chinese semantic role labeling with shallow parsing,accurate unlexicalized parsing,"['Dan Klein', 'Christopher D Manning']",,"We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize.","Inspired by ( #AUTHOR_TAG ) , we split one phrase type into several subsymbols , which contain category information of current constituent 's parent .","['We introduce two types of chunks.', 'The first is simply the phrase type, such as NP, PP, of current chunk.', 'The column CHUNK 1 illustrates this kind of chunk type definition.', 'The second is more complicated.', ""Inspired by ( #AUTHOR_TAG ) , we split one phrase type into several subsymbols , which contain category information of current constituent 's parent ."", 'For example, an NP immediately dominated by a S, will be substituted by NPˆS.', 'This strategy severely increases the number of chunk types and make it hard to train chunking models.', 'To shrink this number, we linguistically use a cluster of CTB phrasal types, which was introduced in .', 'The column CHUNK 2 illustrates this definition.', 'E.g., NPˆS implicitly represents Subject while NPˆVP represents Object.']",4,"['We introduce two types of chunks.', ""Inspired by ( #AUTHOR_TAG ) , we split one phrase type into several subsymbols , which contain category information of current constituent 's parent ."", 'This strategy severely increases the number of chunk types and make it hard to train chunking models.', 'To shrink this number, we linguistically use a cluster of CTB phrasal types, which was introduced in .']"
CCT218,P10-2026,Learning Semantic Representations for Nonterminals in Hierarchical Phrase-Based Translation,comparison of extended lexicon models in search and rescoring for smt,"['Saˇsa Hasan', 'Hermann Ney']",related work,We show how the integration of an extended lexicon model into the decoder can improve translation performance. The model is based on lexical triggers that capture long-distance dependencies on the sentence level. The results are compared to variants of the model that are applied in reranking of n-best lists. We present how a combined application of these models in search and rescoring gives promising results. Experiments are reported on the GALE Chinese-English task with improvements of up to +0.9% BLEU and -1.5% TER absolute on a competitive baseline.,The feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach ( #AUTHOR_TAG ) .,"['The feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach ( #AUTHOR_TAG ) .', 'Hasan and Ney ( 2009) introduced a second word to trigger the target word without considering any linguistic information.', 'Furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved.', 'Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding.', 'However, as the size of the corpus increases, the maximum entropy model will become larger.', 'Similarly, In (Shen et al., 2009), context language model is proposed for better rule selection.', 'Taking the dependency edge as condition, our approach is very different from previous approaches of exploring context information.']",4,"['The feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach ( #AUTHOR_TAG ) .', 'Furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved.', 'Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding.', 'Similarly, In (Shen et al., 2009), context language model is proposed for better rule selection.']"
CCT219,W06-1639,Get out the vote,mining newsgroups using networks arising from social behavior,"['R Agrawal', 'S Rajagopalan', 'R Srikant', 'Y Xu']",related work,"Recent advances in information retrieval over hyperlinked corpora have convincingly demonstrated that links carry less noisy information than text. We investigate the feasibility of applying link-based methods in new applications domains. The specific application we consider is to partition authors into opposite camps within a given topic in the context of newsgroups. A typical newsgroup posting consists of one or more quoted lines from another posting followed by the opinion of the author. This social behavior gives rise to a network in which the vertices are individuals and the links represent ""responded-to "" relationships. An interesting characteristic of many newsgroups is that people more frequently respond to a message when they disagree than when they agree. This behavior is in sharp contrast to the WWW link graph, where linkage is an indicator of agreement or common interest. By analyzing the graph structure of the responses, we are able to effectively classify people into opposite camps. In contrast, methods based on statistical analysis of text yield low accuracy on such datasets because the vocabulary used by the two sides tends to be largely identical, and many newsgroup postings consist of relatively few words of text","Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .","['Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .']",0,"['Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .']"
CCT220,W06-1639,Get out the vote,extracting policy positions from political texts using words as data american political science review,"['M Laver', 'K Benoit', 'J Garry']",related work,,"There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ) .","['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ) .', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']",0,"['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ) .']"
CCT221,W06-1639,Get out the vote,maxmargin markov networks,"['B Taskar', 'C Guestrin', 'D Koller']",related work,"In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches.","Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .","['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']",0,"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']"
CCT222,W06-1639,Get out the vote,cultural orientation classifying subjective documents by cociation sic analysis,['M Efron'],related work,"This paper introduces a simple method for estimating cultural orientation, the affiliations of hypertext documents in a polarized field of discourse. Using a probabilistic model based on cocitation information, two experiments are reported. The first experiment tests the modeli? 1/2 s ability to discriminate between left- and right-wing documents about politics. In this context the model is tested on two sets of data, 695 partisan web documents, and 162 political weblogs. Accuracy above 90% is obtained from the cocitation model, outperforming lexically based classifiers at statistically significant levels. In the second experiment, the proposed method is used to classify the home pages of musical artists with respect to their mainstream or ""alternative"" appeal. For musical artists the model is tested on a set of 515 artist home pages, achieving 88% accuracy.","There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ) .","['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ) .', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']",0,"['There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ) .', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']"
CCT223,W06-1639,Get out the vote,transductive learning via spectral graph partitioning,['T Joachims'],related work,"We present a new method for transductive learning, which can be seen as a transductive version of the k nearest-neighbor classifier. Unlike for many other transductive learning methods, the training problem has a meaningful relaxation that can be solved globally optimally using spectral methods. We propose an algorithm that robustly achieves good generalization performance and that can be trained efficiently. A key advantage of the algorithm is that it does not require additional heuristics to avoid unbalanced splits. Furthermore, we show a connection to transductive Support Vector Machines, and that an effective Co-Training algorithm arises as a special case.","Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , Bansal et al. ( 2002 ) , Kondor and Lafferty ( 2002 ) , and #AUTHOR_TAG .","['Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , Bansal et al. ( 2002 ) , Kondor and Lafferty ( 2002 ) , and #AUTHOR_TAG .', 'Zhu (2005) maintains a survey of this area.']",0,"['Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , Bansal et al. ( 2002 ) , Kondor and Lafferty ( 2002 ) , and #AUTHOR_TAG .']"
CCT224,W06-1639,Get out the vote,iterative classification in relational data,"['J Neville', 'D Jensen']",related work,"Relational data offer a unique opportunity for improving the classification accuracy of statistical models. If two objects are related, inferring something about one object can aid inferences about the other. We present an iterative classification procedure that exploits this characteristic of relational data. This approach uses simple Bayesian classifiers in an iterative fashion, dynamically updating the attributes of some objects as inferences are made about related objects. Inferences made with high confidence in initial iterations are fed back into the data and are used to strengthen subsequent inferences about related objects. We evaluate the performance of iterative classification on a corporate dataset, using a binary classification task. Experiments indicate that iterative classification significantly increases accuracy when compared to a single-pass approach. 1","Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .","['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']",0,"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .']"
CCT225,W06-1639,Get out the vote,detection of agreement vs disagreement in meetings training with unlabeled data,"['D Hillard', 'M Ostendorf', 'E Shriberg']",related work,"To support summarization of automatically transcribed meetings, we introduce a classifier to recognize agreement or disagreement utterances, utilizing both word-based and prosodic cues. We show that hand-labeling efforts can be minimized by using unsupervised training on a large unlabeled data set combined with supervised training on a small amount of data. For ASR transcripts with over 45% WER, the system recovers nearly 80% of agree/disagree utterances with a confusion rate of only 3%.","More sophisticated approaches have been proposed ( #AUTHOR_TAG ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( Galley et al. , 2004 ) .","['More sophisticated approaches have been proposed ( #AUTHOR_TAG ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( Galley et al. , 2004 ) .', 'Also relevant is work on the gen- eral problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002).']",0,"['More sophisticated approaches have been proposed ( #AUTHOR_TAG ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( Galley et al. , 2004 ) .', 'Also relevant is work on the gen- eral problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002).']"
CCT226,W06-1639,Get out the vote,the theory and practice of discourse parsing and summarization,['D Marcu'],related work,"From the Publisher:  Until now, most discourse researchers have assumed that full semantic understanding is necessary to derive the discourse structure of texts. This book documents the first serious attempt to construct automatically and use nonsemantic computational structures for text summarization. Daniel Marcu develops a semantics-free theoretical framework that is both general enough to be applicable to naturally occurring texts and concise enough to facilitate an algorithmic approach to discourse analysis. He presents and evaluates two discourse parsing methods: one uses manually written rules that reflect common patterns of usage of cue phrases such as ""however"" and ""in addition to""; the other uses rules that are learned automatically from a corpus of discourse structures. By means of a psycholinguistic experiment, Marcu demonstrates how a discourse-based summarizer identifies the most important parts of texts at levels of performance that are close to those of humans.  Marcu also discusses how the automatic derivation of discourse structures may be used to improve the performance of current natural language generation, machine translation, summarization, question answering, and information retrieval systems.","Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( #AUTHOR_TAG ; Teufel and Moens , 2002 ) .","['We used a simple method to learn to identify cross-speaker references indicating agreement.', 'More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( #AUTHOR_TAG ; Teufel and Moens , 2002 ) .']",0,"['Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( #AUTHOR_TAG ; Teufel and Moens , 2002 ) .']"
CCT227,W06-1639,Get out the vote,multidimensional text analysis for erulemaking,"['N Kwon', 'S Shulman', 'E Hovy']",related work,"To support rule-writers, we are developing techniques to automatically analyze large number of public comments on proposed regulations. A document is analyzed in various ways including argument structure, topics, and opinions. The individual results are integrated into a unified output. The experiments reported here were performed on comments submitted to the Environmental Protection Agency in response to their proposed rule for mercury regulation.","Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ) .","['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']",0,"['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ) .']"
CCT228,W06-1639,Get out the vote,mining newsgroups using networks arising from social behavior,"['R Agrawal', 'S Rajagopalan', 'R Srikant', 'Y Xu']",introduction,"Recent advances in information retrieval over hyperlinked corpora have convincingly demonstrated that links carry less noisy information than text. We investigate the feasibility of applying link-based methods in new applications domains. The specific application we consider is to partition authors into opposite camps within a given topic in the context of newsgroups. A typical newsgroup posting consists of one or more quoted lines from another posting followed by the opinion of the author. This social behavior gives rise to a network in which the vertices are individuals and the links represent ""responded-to "" relationships. An interesting characteristic of many newsgroups is that people more frequently respond to a message when they disagree than when they agree. This behavior is in sharp contrast to the WWW link graph, where linkage is an indicator of agreement or common interest. By analyzing the graph structure of the responses, we are able to effectively classify people into opposite camps. In contrast, methods based on statistical analysis of text yield low accuracy on such datasets because the vocabulary used by the two sides tends to be largely identical, and many newsgroup postings consist of relatively few words of text","For example, we may find textual4 evidence of a high likelihood of agreement be-tween two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings (see Mullen and Malouf (2006) but cfXXX #AUTHOR_TAG.","['Most sentiment-polarity classifiers proposed in the recent literature categorize each document in- dependently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006).', 'Many interesting opinion-oriented docu- ments, however, can be linked through certain re- lationships that occur in the context of evaluative discussions.', 'For example, we may find textual4 evidence of a high likelihood of agreement be-tween two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings (see Mullen and Malouf (2006) but cfXXX #AUTHOR_TAG.', 'Agreement evidence can be a powerful aid in our classification task: for ex- ample, we can easily categorize a complicated (or overly terse) document if we find within it indica- tions of agreement with a clearly positive text.']",0,"['Most sentiment-polarity classifiers proposed in the recent literature categorize each document in- dependently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006).', 'Many interesting opinion-oriented docu- ments, however, can be linked through certain re- lationships that occur in the context of evaluative discussions.', 'For example, we may find textual4 evidence of a high likelihood of agreement be-tween two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings (see Mullen and Malouf (2006) but cfXXX #AUTHOR_TAG.', 'Agreement evidence can be a powerful aid in our classification task: for ex- ample, we can easily categorize a complicated (or overly terse) document if we find within it indica- tions of agreement with a clearly positive text.']"
CCT229,W06-1639,Get out the vote,a preliminary investigation into sentiment analysis of informal political discourse,"['T Mullen', 'R Malouf']",related work,"With the rise of weblogs and the increasing tendency of online publications to turn to message-board style reader feedback venues, informal political discourse is becoming an important feature of the intellectual landscape of the Internet, creating a challenging and worthwhile area for experimentation in techniques for sentiment analysis. We describe preliminary statistical tests on a new dataset of political discussion group postings which indicate that posts made in direct response to other posts in a thread have a strong tendency to represent an opposing political viewpoint to the original post. We conclude that traditional text classification methods will be inadequate to the task of sentiment analysis in this domain, and that progress is to be made by exploiting information about how posters interact with each","There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ) .","['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ) .', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']",0,"['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ) .']"
CCT230,W06-1639,Get out the vote,thumbs up sentiment classification using machine learning techniques,"['B Pang', 'L Lee', 'S Vaithyanathan']",method,"We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.","Fol- lowing standard practice in sentiment analysis ( #AUTHOR_TAG ) , the input to SVMlight con- sisted of normalized presence-of-feature (rather than frequency-of-feature) vectors.","['In our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features.5', 'Fol- lowing standard practice in sentiment analysis ( #AUTHOR_TAG ) , the input to SVMlight con- sisted of normalized presence-of-feature (rather than frequency-of-feature) vectors.', 'The ind value for each speech segment s was based on the signed distanceds fromthevectorrepresentingstothe trained SVM decision plane: \x0e � ds \x0e23�4s� d s �23�4s� ds ��23�4s def ds = \x0e+ 23�4s 2 ind s;Y where 3�4s is the standard deviation of d s over all speech segments s in the debate in question, and def ind s;N = \x0e�ind s;Y .']",5,"['Fol- lowing standard practice in sentiment analysis ( #AUTHOR_TAG ) , the input to SVMlight con- sisted of normalized presence-of-feature (rather than frequency-of-feature) vectors.']"
CCT231,W06-1639,Get out the vote,automated classification of congressional legislation,"['S Purpura', 'D Hillard']",related work,"For social science researchers, content analysis and classification of United States Congressional legislative activities have been time consuming and costly. The Library of Congress THOMAS system provides detailed information about bills and laws, but its classification system, the Legislative Indexing Vocabulary (LIV), is geared toward information retrieval instead of the pattern or historical trend recognition that social scientists value. The same event (a bill) may be coded with many subjects at the same time, with little indication of its primary emphasis. In addition, because the LIV system has not been applied to other activities, it cannot be used to compare (for example) legislative issue attention to executive, media, or public issue attention.This paper presents the Congressional Bills Project's (www.congressionalbills.org) automated classification system. This system applies a topic spotting classification algorithm to the task of coding legislative activities into one of 226 subtopic areas. The algorithm uses a traditional bag-of-words document representation, an extensive set of human coded examples, and an exhaustive topic coding system developed for use by the Congressional Bills Project and the Policy Agendas Project (www.policyagendas.org). Experimental results demonstrate that the automated system is about as effective as human assessors, but with significant time and cost savings. The paper concludes by discussing challenges to moving the system into operational use.","Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( Yang and Callan , 2005 ; #AUTHOR_TAG ) .","['Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( Yang and Callan , 2005 ; #AUTHOR_TAG ) .']",0,"['Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( Yang and Callan , 2005 ; #AUTHOR_TAG ) .']"
CCT232,W06-1639,Get out the vote,conditional models of identity uncertainty with application to noun coreference,"['A McCallum', 'B Wellner']",related work,"Coreference analysis, also known as record linkage or identity uncer-tainty, is a difficult and important problem in natural language process-ing, databases, citation matching and many other tasks. This paper intro-duces several discriminative, conditional-probability models for coref-erence analysis, all examples of undirected graphical models. Unlike many historical approaches to coreference, the models presented here are relational--they do not assume that pairwise coreference decisions should be made independently from each other. Unlike other relational models of coreference that are generative, the conditional model here can incorporate a great variety of features of the input without having to be concerned about their dependencies--paralleling the advantages of con-ditional random fields over hidden Markov models. We present positive results on noun phrase coreference in two standard text data sets.","Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ) .","['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']",0,"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ) .']"
CCT233,W06-1639,Get out the vote,thumbs up or thumbs down semantic orientation applied to unsupervised classification of reviews,['P Turney'],introduction,"This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., ""subtle nuances"") and a negative semantic orientation when it has bad associations (e.g., ""very cavalier""). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word ""excellent"" minus the mutual information between the given phrase and the word ""poor"". A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews.","In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 ) .","['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography).', ""In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 ) .""]",0,"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography).', ""In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 ) .""]"
CCT234,W06-1639,Get out the vote,sentiment analysis a new approach for effective use of linguistic knowledge and exploiting similarities in a set of documents to be classified,"['A Agarwal', 'P Bhattacharyya']",related work,"Sentiment Analysis aims at determining the overall polarity of a document, for instance, identifying whether a movie review appreciates or criticizes a movie. We present a machine learning based approach to this problem similar to text categorization. The technique is made more effective by incorporating linguistic knowledge gathered through Wordnet1 synonymy graphs. A method to improve the accuracy of classification over a set of test documents is finally given.","Previous sentiment-analysis work in different domains has considered inter-document similarity ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyper- links (Agrawal et al., 2003).","['Previous sentiment-analysis work in different domains has considered inter-document similarity ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyper- links (Agrawal et al., 2003).']",0,"['Previous sentiment-analysis work in different domains has considered inter-document similarity ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyper- links (Agrawal et al., 2003).']"
CCT235,W06-1639,Get out the vote,conditional random fields probabilistic models for segmenting and labeling sequence data,"['J Lafferty', 'A McCallum', 'F Pereira']",related work,"We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.","Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .","['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']",0,"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .']"
CCT236,W06-1639,Get out the vote,evaluation of machine learning methods for natural language processing tasks,"['W Daelemans', 'V Hoste']",method,"We show that the methodology currently in use for comparing symbolic supervised learning methods applied to human language technology tasks is unreliable. We show that the interaction between algorithm parameter settings and feature selection within a single algorithm often accounts for a higher variation in results than differences between different algorithms or information sources. We illustrate this with experiments on a number of linguistic datasets. The consequences of this phenomenon are far-reaching, and we discuss possible solutions to this methodological problem.","Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( #AUTHOR_TAG ; Munson et al. , 2005 ) .","['SVMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( #AUTHOR_TAG ; Munson et al. , 2005 ) .']",3,"['SVMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( #AUTHOR_TAG ; Munson et al. , 2005 ) .']"
CCT237,W06-1639,Get out the vote,analyzing research papers using citation sentences,"['W Lehnert', 'C Cardie', 'E Riloff']",related work,,"Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( #AUTHOR_TAG ) , and computational rhetorical analysis ( Marcu , 2000 ; Teufel and Moens , 2002 ) .","['We used a simple method to learn to identify cross-speaker references indicating agreement.', 'More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( #AUTHOR_TAG ) , and computational rhetorical analysis ( Marcu , 2000 ; Teufel and Moens , 2002 ) .']",0,"['Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( #AUTHOR_TAG ) , and computational rhetorical analysis ( Marcu , 2000 ; Teufel and Moens , 2002 ) .']"
CCT238,W06-1639,Get out the vote,seeing stars exploiting class relationships for sentiment categorization with respect to rating scales,"['B Pang', 'L Lee']",introduction,"We address the rating-inference problem, wherein rather than simply decide whether a review is ""thumbs up"" or ""thumbs down"", as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five ""stars""). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, ""three stars"" is intuitively closer to ""four stars"" than to ""one star"".We first evaluate human performance at the task. Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem.","A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) .","['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'For example, we may find textual 4 evidence of a high likelihood of agreement be- 4 Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information.', 'For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data.', ""Indeed, in other settings (e.g., a movie-discussion listserv) one may not be able to determine the participants' political leanings, and such information may not lead to significantly improved results even if it were available.""]",0,"['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.']"
CCT239,W06-1639,Get out the vote,sentiment analysis a new approach for effective use of linguistic knowledge and exploiting similarities in a set of documents to be classified,"['A Agarwal', 'P Bhattacharyya']",introduction,"Sentiment Analysis aims at determining the overall polarity of a document, for instance, identifying whether a movie review appreciates or criticizes a movie. We present a machine learning based approach to this problem similar to text categorization. The technique is made more effective by incorporating linguistic knowledge gathered through Wordnet1 synonymy graphs. A method to improve the accuracy of classification over a set of test documents is finally given.","A few others incorporate various measures of inter-document similarity between the texts to be labeled ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) .","['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'For example, we may find textual 4 evidence of a high likelihood of agreement be- 4 Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information.', 'For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data.', ""Indeed, in other settings (e.g., a movie-discussion listserv) one may not be able to determine the participants' political leanings, and such information may not lead to significantly improved results even if it were available.""]",0,"['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) .']"
CCT240,W06-1639,Get out the vote,seeing stars when there aren’t many stars graphbased semisupervised learning for sentiment categorization,"['A B Goldberg', 'J Zhu']",introduction,"We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference. Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., ""4 stars""), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text. In particular, we are interested in the situation where labeled data is scarce. We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve rating-inference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training.","A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) .","['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'For example, we may find textual 4 evidence of a high likelihood of agreement be- 4 Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information.', 'For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data.', ""Indeed, in other settings (e.g., a movie-discussion listserv) one may not be able to determine the participants' political leanings, and such information may not lead to significantly improved results even if it were available.""]",0,"['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) .']"
CCT241,W06-1639,Get out the vote,learning associative markov networks,"['B Taskar', 'V Chatalbashev', 'D Koller']",related work,"Markov networks are extensively used to model complex sequential, spatial, and relational interactions in fields as diverse as image processing, natural language analysis, and bioinformatics. However, inference and learning in general Markov networks is intractable. In this paper, we focus on learning a large subclass of such models (called associative Markov networks) that are tractable or closely approximable. This subclass contains networks of discrete variables with K labels each and clique potentials that favor the same labels for all variables in the clique. Such networks capture the ""guilt by association "" pattern of reasoning present in many domains, in which connected (""associated"") variables tend to have the same label. Our approach exploits a linear programming relaxation for the task of finding the best joint assignment in such networks, which provides an approximate quadratic program (QP) for the problem of learning a marginmaximizing Markov network. We show that for associative Markov network over binary-valued variables, this approximate QP is guaranteed to return an optimal parameterization for Markov networks of arbitrary topology. For the nonbinary case, optimality is not guaranteed, but the relaxation produces good solutions in practice. Experimental results with hypertext and newswire classification show significant advantages over standard approaches. 1","Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ) .","['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']",0,"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ) .']"
CCT242,W06-1639,Get out the vote,identifying agreement and disagreement in conversational speech use of bayesian networks to model pragmatic dependencies,"['M Galley', 'K McKeown', 'J Hirschberg', 'E Shriberg']",related work,"We describe a statistical approach for modeling agreements and disagreements in conversational interaction. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse. We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance. Our approach achieves 86.9% accuracy, a 4.9% increase over previous work.","More sophisticated approaches have been proposed ( Hillard et al. , 2003 ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( #AUTHOR_TAG ) .","['We used a simple method to learn to identify cross-speaker references indicating agreement.', 'More sophisticated approaches have been proposed ( Hillard et al. , 2003 ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( #AUTHOR_TAG ) .', 'Also relevant is work on the general problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000;Teufel and Moens, 2002).']",0,"['We used a simple method to learn to identify cross-speaker references indicating agreement.', 'More sophisticated approaches have been proposed ( Hillard et al. , 2003 ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( #AUTHOR_TAG ) .', 'Also relevant is work on the general problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000;Teufel and Moens, 2002).']"
CCT243,W06-1639,Get out the vote,a sentimental education sentiment analysis using subjectivity summarization based on minimum cuts,"['B Pang', 'L Lee']",method,"Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as ""thumbs up"" or ""thumbs down"". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.","As has been previously observed and exploited in the NLP literature ( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .","['As has been previously observed and exploited in the NLP literature ( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', 'In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision.']",1,"['As has been previously observed and exploited in the NLP literature ( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', 'In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision.']"
CCT244,W06-1639,Get out the vote,discriminative probabilistic models for relational data,"['B Taskar', 'P Abbeel', 'D Koller']",related work,"In many supervised learning tasks, the entities to be labeled are related to each other in complex ways and their labels are not independent. For example, in hypertext classification, the labels of linked pages are highly correlated. A standard approach is to classify each entity independently, ignoring the correlations between them. Recently, Probabilistic Relational Models, a relational version of Bayesian networks, were used to define a joint probabilistic model for a collection of related entities. In this paper, we present an alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach. First, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models. Second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy. We show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities. We provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies.","Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .","['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']",0,"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.']"
CCT245,W06-1639,Get out the vote,electronic rulemaking new frontiers in public participation prepared for the annual meeting of the american political science association,"['S Shulman', 'D Schlosberg']",introduction,,"In the United States , for example , governmental bodies are providing and soliciting political documents via the Internet , with lofty goals in mind : electronic rulemaking ( eRulemaking ) initiatives involving the `` electronic collection , distribution , synthesis , and analysis of public commentary in the regulatory rulemaking process '' , may `` [ alter ] the citizen-government relationship '' ( #AUTHOR_TAG ) .","[""In the United States , for example , governmental bodies are providing and soliciting political documents via the Internet , with lofty goals in mind : electronic rulemaking ( eRulemaking ) initiatives involving the `` electronic collection , distribution , synthesis , and analysis of public commentary in the regulatory rulemaking process '' , may `` [ alter ] the citizen-government relationship '' ( #AUTHOR_TAG ) ."", 'Additionally, much media attention has been focused recently on the potential impact that Internet sites may have on politics 2 , or at least on political journalism 3 .', 'Regardless of whether one views such claims as clear-sighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text, given the importance of enabling informed participation in the political process.']",0,"[""In the United States , for example , governmental bodies are providing and soliciting political documents via the Internet , with lofty goals in mind : electronic rulemaking ( eRulemaking ) initiatives involving the `` electronic collection , distribution , synthesis , and analysis of public commentary in the regulatory rulemaking process '' , may `` [ alter ] the citizen-government relationship '' ( #AUTHOR_TAG ) .""]"
CCT246,W06-1639,Get out the vote,yahoo for amazon extracting market sentiment from stock message boards,"['S Das', 'M Chen']",introduction,,"In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 ) .","['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography).', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 ) .']",0,"['In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 ) .']"
CCT247,W06-1639,Get out the vote,semisupervised learning literature survey computer sciences,['J Zhu'],related work,,#AUTHOR_TAG maintains a survey of this area .,"['Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor andLafferty (2002), andJoachims (2003).', '#AUTHOR_TAG maintains a survey of this area .']",0,"['Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor andLafferty (2002), andJoachims (2003).', '#AUTHOR_TAG maintains a survey of this area .']"
CCT248,W06-1639,Get out the vote,seeing stars exploiting class relationships for sentiment categorization with respect to rating scales,"['B Pang', 'L Lee']",related work,"We address the rating-inference problem, wherein rather than simply decide whether a review is ""thumbs up"" or ""thumbs down"", as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five ""stars""). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, ""three stars"" is intuitively closer to ""four stars"" than to ""one star"".We first evaluate human performance at the task. Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem.","Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).","['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']",0,"['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']"
CCT249,W06-1639,Get out the vote,on the computation of point of view,['W Sack'],introduction,"Previous work in AI story understanding has largely been used to build tools which can summarize stories and categorize them according to the events they describe (e.g., the technologies developed for the Message Understanding Conferences). These sorts of technologies are built around the assumptions that (1) events reported as facts in news stories should be ""understood"" as facts; (2) the style of a story, i.e., the way in which a story is told, is not of interest; and, (3) the source of a story should not influence its analysis. These assumptions are obviously unrealistic. Everyone knows that one should not believe everything in the news. But, by making these simplifying assumptions most existing story understanding systems function as gullible ""readers."" The focus of my current research is to build a less gullible story understander by encoding in it a means to recognize point of view. The techniques that I am developing will be useful, not only for information retrieval tasks which demand a search for credible stories, but also in future entertainment technologies which will be capable of finding and then assembling together into a unified presentation a set of texts or video clips to tell a story from an ensemble of points of view.","Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , #AUTHOR_TAG , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .","['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , #AUTHOR_TAG , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']",0,"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , #AUTHOR_TAG , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .']"
CCT250,W06-1639,Get out the vote,sentiment analysis a new approach for effective use of linguistic knowledge and exploiting similarities in a set of documents to be classified,"['A Agarwal', 'P Bhattacharyya']",method,"Sentiment Analysis aims at determining the overall polarity of a document, for instance, identifying whether a movie review appreciates or criticizes a movie. We present a machine learning based approach to this problem similar to text categorization. The technique is made more effective by incorporating linguistic knowledge gathered through Wordnet1 synonymy graphs. A method to improve the accuracy of classification over a set of test documents is finally given.","As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .","['As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', 'In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision.']",1,"['As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', 'In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision.']"
CCT251,W06-1639,Get out the vote,learning from labeled and unlabeled data using graph mincuts,"['A Blum', 'S Chawla']",method,"Many application domains suffer from not having enough labeled training data for learning. However, large amounts of unlabeled examples can often be gathered cheaply. As a result, there has been a great deal of work in recent years on how unlabeled data can be used to aid classification. We consider an algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data.","Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .","['The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation.', 'As discussed in Section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate.', 'Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .', 'In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships.']",5,"['The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation.', 'Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .']"
CCT252,W06-1639,Get out the vote,nearduplicate detection for erulemaking,"['H Yang', 'J Callan']",related work,,"Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( #AUTHOR_TAG ; Purpura and Hillard , 2006 ) .","['Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( #AUTHOR_TAG ; Purpura and Hillard , 2006 ) .']",0,"['Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( #AUTHOR_TAG ; Purpura and Hillard , 2006 ) .']"
CCT253,W06-1639,Get out the vote,tracking point of view in narrative,['J M Wiebe'],introduction,"Third-person fictional narrative text is composed not only of passages that objectively narrate events, but also of passages that present characters' thoughts, perceptions, and inner states. Such passages take a character's psychological point of view. A language understander must determine the current psychological point of view in order to distinguish the beliefs of the characters from the facts of the story, to correctly attribute beliefs and other attitudes to their sources, and to understand the discourse relations among sentences. Tracking the psychological point of view is not a trivial problem, because many sentences are not explicitly marked for point of view, and whether the point of view of a sentence is objective or that of a character (and if the latter, which character it is) often depends on the context in which the sentence appears. Tracking the psychological point of view is the problem addressed in this work. The approach is to seek, by extensive examinations of naturally occurring narrative, regularities in the ways that authors manipulate point of view, and to develop an algorithm that tracks point of view on the basis of the regularities found. This paper presents this algorithm, gives demonstrations of an implemented system, and describes the results of some preliminary empirical studies, which lend support to the algorithm.","Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , Sack ( 1994 ) , and #AUTHOR_TAG ; see Esuli ( 2006 ) for an active bibliography ) .","['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , Sack ( 1994 ) , and #AUTHOR_TAG ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']",0,"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , Sack ( 1994 ) , and #AUTHOR_TAG ; see Esuli ( 2006 ) for an active bibliography ) .']"
CCT254,W06-1639,Get out the vote,learning from labeled and unlabeled data using graph mincuts,"['A Blum', 'S Chawla']",related work,"Many application domains suffer from not having enough labeled training data for learning. However, large amounts of unlabeled examples can often be gathered cheaply. As a result, there has been a great deal of work in recent years on how unlabeled data can be used to aid classification. We consider an algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data.","Notable early papers on graph-based semisupervised learning include #AUTHOR_TAG , Bansal et al. ( 2002 ) , Kondor and Lafferty ( 2002 ) , and Joachims ( 2003 ) .","['Notable early papers on graph-based semisupervised learning include #AUTHOR_TAG , Bansal et al. ( 2002 ) , Kondor and Lafferty ( 2002 ) , and Joachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area.']",0,"['Notable early papers on graph-based semisupervised learning include #AUTHOR_TAG , Bansal et al. ( 2002 ) , Kondor and Lafferty ( 2002 ) , and Joachims ( 2003 ) .']"
CCT255,W06-1639,Get out the vote,on the collective classification of email “speech acts”,"['V Carvalho', 'W W Cohen']",related work,"We consider classification of email messages as to whether or not they contain certain ""email acts"", such as a request or a commitment. We show that exploiting the sequential correlation among email messages in the same thread can improve email-act classification. More specifically, we describe a new text-classification algorithm based on a dependency-network based collective classification method, in which the local classifiers are maximum entropy models based on words and certain relational features. We show that statistically significant improvements over a bag-of-words baseline classifier can be obtained for some, but not all, email-act classes. Performance improvements obtained by collective classification appears to be consistent across many email acts suggested by prior speech-act theory.","Relationships between the unlabeled items #AUTHOR_TAG consider sequential relations between different types of emails ( e.g. , between requests and satisfactions thereof ) to classify messages , and thus also explicitly exploit the structure of conversations .","['We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work.', 'Relationships between the unlabeled items #AUTHOR_TAG consider sequential relations between different types of emails ( e.g. , between requests and satisfactions thereof ) to classify messages , and thus also explicitly exploit the structure of conversations .']",0,"['Relationships between the unlabeled items #AUTHOR_TAG consider sequential relations between different types of emails ( e.g. , between requests and satisfactions thereof ) to classify messages , and thus also explicitly exploit the structure of conversations .']"
CCT256,W06-1639,Get out the vote,the american congress,"['S S Smith', 'J M Roberts', 'R J Vander Wielen']",introduction,,"People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion , given the dense nature of legislative language and the fact that ( U.S. ) bills often reach several hundred pages in length ( #AUTHOR_TAG ) .","[""Evaluative and persuasive documents, such as a politician's speech regarding a bill or a blogger's commentary on a legislative proposal, form a particularly interesting type of politically oriented text."", 'People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion , given the dense nature of legislative language and the fact that ( U.S. ) bills often reach several hundred pages in length ( #AUTHOR_TAG ) .', 'Moreover, political opinions are exsional bills and related data was launched in January 1995, when Mosaic was not quite two years old and Altavista did not yet exist.']",0,"['People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion , given the dense nature of legislative language and the fact that ( U.S. ) bills often reach several hundred pages in length ( #AUTHOR_TAG ) .']"
CCT257,W06-1639,Get out the vote,collective content selection for concepttotext generation,"['R Barzilay', 'M Lapata']",method,"A content selection component determines which information should be conveyed in the output of a natural language generation system. We present an efficient method for automatically learning content selection rules from a corpus and its related database. Our modeling framework treats content selection as a collective classification problem, thus allowing us to capture contextual dependencies between input items. Experiments in a sports domain demonstrate that this approach achieves a substantial improvement over context-agnostic methods.","As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .","['As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', 'In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision.']",1,"['As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', 'In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision.']"
CCT258,W06-1639,Get out the vote,thumbs up sentiment classification using machine learning techniques,"['B Pang', 'L Lee', 'S Vaithyanathan']",introduction,"We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.","In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 ) .","['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography).', ""In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 ) .""]",0,"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography).', ""In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 ) .""]"
CCT259,W06-1639,Get out the vote,correlation clustering,"['N Bansal', 'A Blum', 'S Chawla']",related work,"In this paper, we introduce and study the Robust-Correlation-Clustering problem: given a graph G = (V,E) where every edge is either labeled + or - (denoting similar or dissimilar pairs of vertices), and a parameter m, the goal is to delete a set D of m vertices, and partition the remaining vertices V  D into clusters to minimize the cost of the clustering, which is the sum of the number of + edges with end-points in different clusters and the number of - edges with end-points in the same cluster. This generalizes the classical Correlation-Clustering problem which is the special case when m = 0. Correlation clustering is useful when we have (only) qualitative information about the similarity or dissimilarity of pairs of points, and Robust-Correlation-Clustering equips this model with the capability to handle noise in datasets. In this work, we present a constant-factor bi-criteria algorithm for Robust-Correlation-Clustering on complete graphs (where our solution is O(1)-approximate w.r.t the cost while however discarding O(1) m points as outliers), and also complement this by showing that no finite approximation is possible if we do not violate the outlier budget. Our algorithm is very simple in that it first does a simple LP-based pre-processing to delete O(m) vertices, and subsequently runs a particular Correlation-Clustering algorithm ACNAlg [Ailon et al., 2005] on the residual instance. We then consider general graphs, and show (O(log n), O(log^2 n)) bi-criteria algorithms while also showing a hardness of alpha_MC on both the cost and the outlier violation, where alpha_MC is the lower bound for the Minimum-Multicut problem","Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , #AUTHOR_TAG , Kondor and Lafferty ( 2002 ) , and Joachims ( 2003 ) .","['Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , #AUTHOR_TAG , Kondor and Lafferty ( 2002 ) , and Joachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area.']",0,"['Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , #AUTHOR_TAG , Kondor and Lafferty ( 2002 ) , and Joachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area.']"
CCT260,W06-1639,Get out the vote,mining the peanut gallery opinion extraction and semantic classification of product reviews,"['K Dave', 'S Lawrence', 'D M Pennock']",introduction,"The web contains a wealth of product reviews, but sifting through them is a daunting task. Ideally, an opinion mining tool would process a set of search results for a given item, generating a list of product attributes (quality, features, etc.) and aggregating opinions about each of them (poor, mixed, good). We begin by identifying the unique properties of this problem and develop a method for automatically distinguishing between positive and negative reviews. Our classifier draws on information retrieval techniques for feature extraction and scoring, and the results for various metrics and heuristics vary depending on the testing situation. The best methods work as well as or better than traditional machine learning. When operating on individual sentences collected from web searches, performance is limited due to noise and ambiguity. But in the context of a complete web-based tool and aided by a simple method for grouping sentences into attributes, the results are qualitatively quite useful.","In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG ) .","['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography).', ""In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG ) .""]",0,"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography).', ""In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG ) .""]"
CCT261,W06-1639,Get out the vote,using natural language processing to improve erulemaking,"['C Cardie', 'C Farina', 'T Bruce', 'E Wagner']",related work,"This paper describes in brief Cornell's interdisciplinary eRulemaking project that was recently funded (December, 2005) by the National Science Foundation.","Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ) .","['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']",0,"['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ) .']"
CCT262,W06-1639,Get out the vote,a computational theory of perspective and reference in narrative,"['J M Wiebe', 'W J Rapaport']",introduction,"William J. end Shapiro, Stuart C. (1984), &quot;Quasi-lndexical Reference in Propositional Semantic Networks, &quot; Proceedings of the loth International Conference on Computational Linguistics ( COLING-84 ; Stanford Univ.) (Morristown, NJ: Assoc. for Computational Linguistics): 65-70.  Rapaport, William J. (1986), &quot;Logical Foundations for Belief Representation,&quot; Cognitiv e Science 10: 371-422.  Reiser, Brian J. (1981), &quot;Character Tracking and the Understanding of Narrative,&quot; Proceedings of the 7th International  Joint Conference on Artificial Intelligence (IJCAI-81; Van. couver) (Los Altos, CA: Morgen Kanhmmn): 209-211.  Roach, Eleanor and Lloyd, B.B. (1978). Cognition and Categorization (Hillsdale, NJ: Lawrence Erlbaum Associ- ates).  Shapiro, Smart C. (1979). &quot;The SNePS Sementic Network Processing System,&quot; in N.V. Findlet (ed.), Associative Network. v (New York: Academic): 179-203.  Shapiro, Stuart C. end Rapaport, William J. (1987). &quot;SNePS Considered as a Fully Intensional Propositional","Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes #AUTHOR_TAG , Hearst ( 1992 ) , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .","['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes #AUTHOR_TAG , Hearst ( 1992 ) , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']",0,"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes #AUTHOR_TAG , Hearst ( 1992 ) , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .']"
CCT263,W06-1639,Get out the vote,learning probabilistic models of relational structure,"['L Getoor', 'N Friedman', 'D Koller', 'B Taskar']",related work,"Most real-world data is stored in relational form. In contrast, most statistical learning methods work with ""flat"" data representations, forcing us to convert our data into a form that loses much of the relational structure. The recently introduced framework of probabilistic relational models (PRMs) allows us to represent probabilistic models over multiple entities that utilize the relations between them. In this paper, we propose the use of probabilistic models not only for the attributes in a relational model, but for the relational structure itself. We propose two mechanisms for modeling structural uncertainty: reference uncertainty and existence uncertainty. We describe the appropriate conditions for using each model and present learning algorithms for each. We present experimental results showing that the learned models can be used to predict relational structure and, moreover, the observed relational structure can be used to provide better predictions for the attributes in the model.","Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .","['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']",0,"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']"
CCT264,W06-1639,Get out the vote,diffusion kernels on graphs and other discrete input spaces,"['R I Kondor', 'J D Lafferty']",related work,"The application of kernel-based learning algorithms has, so far, largely been confined to realvalued data and a few special data types, such as strings. In this paper we propose a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea. In particular, we focus on generating kernels on graphs, for which we propose a special class of exponential kernels called diffusion kernels, which are based on the heat equation and can be regarded as the discretization of the familiar Gaussian kernel of Euclidean space.","Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , Bansal et al. ( 2002 ) , #AUTHOR_TAG , and Joachims ( 2003 ) .","['Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , Bansal et al. ( 2002 ) , #AUTHOR_TAG , and Joachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area.']",0,"['Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , Bansal et al. ( 2002 ) , #AUTHOR_TAG , and Joachims ( 2003 ) .']"
CCT265,W06-1639,Get out the vote,summarizing scientific articles experiments with relevance and rhetorical status,"['S Teufel', 'M Moens']",related work,"In this article we propose a strategy for the summarization of scientific articles that concentrates on the rhetorical status of statements in an article: Material for summaries is selected in such a way that summaries can highlight the new contribution of the source article and situate it with respect to earlier work. We provide a gold standard for summaries of this kind consisting of a substantial corpus of conference articles in computational linguistics annotated with human judgments of the rhetorical status and relevance of each sentence in the articles. We present several experiments measuring our judges' agreement on these annotations. We also present an algorithm that, on the basis of the annotated training material, selects content from unseen articles and classifies it into a fixed set of seven rhetorical categories. The output of this extraction and classification system can be viewed as a single-document summary in its own right; alternatively, it provides starting material for the generation of task-oriented and user-tailored summaries designed to give users an overview of a scientific field.","Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( Marcu , 2000 ; #AUTHOR_TAG ) .","['We used a simple method to learn to identify cross-speaker references indicating agreement.', 'More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( Marcu , 2000 ; #AUTHOR_TAG ) .']",0,"['We used a simple method to learn to identify cross-speaker references indicating agreement.', 'Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( Marcu , 2000 ; #AUTHOR_TAG ) .']"
CCT266,W06-1639,Get out the vote,coupling niche browsers and affect analysis for an opinion mining application,"['G Grefenstette', 'Y Qu', 'J G Shanahan', 'D A Evans']",related work,"Newspapers generally attempt to present the news objectively. But textual affect analysis shows that many words carry positive or negative emotional charge. In this article, we show that coupling niche browsing technology and affect analysis technology allows us to create a new application that measures the slant in opinion given to public figures in the popular press.","An exception is #AUTHOR_TAG , who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site .","['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'An exception is #AUTHOR_TAG , who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site .']",0,"['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'An exception is #AUTHOR_TAG , who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site .']"
CCT267,W06-1639,Get out the vote,optimizing to arbitrary nlp metrics using ensemble selection,"['A Munson', 'C Cardie', 'R Caruana']",method,"While there have been many successful applications of machine learning methods to tasks in NLP, learning algorithms are not typically designed to optimize NLP performance metrics. This paper evaluates an ensemble selection framework designed to optimize arbitrary metrics and automate the process of algorithm selection and parameter tuning. We report the results of experiments that instantiate the framework for three NLP tasks, using six learning algorithms, a wide variety of parameterizations, and 15 performance metrics. Based on our results, we make recommendations for subsequent machine-learning-based research for natural language learning.","Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .","['SVMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .']",3,"['Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .']"
CCT268,W06-1639,Get out the vote,language processing technologies for electronic rulemaking a project highlight,"['S Shulman', 'J Callan', 'E Hovy', 'S Zavestoski']",related work,"In this project, we are developing new text processing tools that help people perform advanced analysis of large collections of text commentary. This problem is increasingly faced by the United States federal government's regulation writers who formulate the rules and regulations that define the details of laws enacted by Congress. Our research focuses on text clustering, text searching using information retrieval, near-duplicate detection, opinion identification, stakeholder characterization, and extractive summarization, as well as the impact of such tools on the process of rulemaking itself. Versions of a Rule-Writer's Workbench will be built by Computer Science researchers at ISI and CMU, deployed annually for experimental use by our government partners, and evaluated by social science researchers from the Library and Information Science and Sociology departments at the Universities of Pittsburgh and San Francisco respectively. This three-year project started in October 2004 and is funded under the National Science Foundation's Digital Government program.","Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ) .","['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']",0,"['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ) .']"
CCT269,W06-1639,Get out the vote,a preliminary investigation into sentiment analysis of informal political discourse,"['T Mullen', 'R Malouf']",introduction,"With the rise of weblogs and the increasing tendency of online publications to turn to message-board style reader feedback venues, informal political discourse is becoming an important feature of the intellectual landscape of the Internet, creating a challenging and worthwhile area for experimentation in techniques for sentiment analysis. We describe preliminary statistical tests on a new dataset of political discussion group postings which indicate that posts made in direct response to other posts in a thread have a strong tendency to represent an opposing political viewpoint to the original post. We conclude that traditional text classification methods will be inadequate to the task of sentiment analysis in this domain, and that progress is to be made by exploiting information about how posters interact with each","For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .","['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006).', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .', 'Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text.']",0,"['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .']"
CCT270,W06-1639,Get out the vote,seeing stars when there aren’t many stars graphbased semisupervised learning for sentiment categorization,"['A B Goldberg', 'J Zhu']",related work,"We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference. Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., ""4 stars""), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text. In particular, we are interested in the situation where labeled data is scarce. We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve rating-inference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training.","Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).","['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']",0,"['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']"
CCT271,W06-1639,Get out the vote,directionbased text interpretation as an information access refinement,['M Hearst'],introduction,,"Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .","['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']",0,"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .']"
CCT272,W10-1758,From “disciplined subjectivity” to “taming wild thoughts”: Bion's elaboration of the analysing instrument,online largemargin training of syntactic and structural translation features,"['David Chiang', 'Yuval Marton', 'Philis Resnik']",conclusion,"Minimum-error-rate training (MERT) is a bottleneck for current development in statistical machine translation because it is limited in the number of weights it can reliably optimize. Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT. We first show that by parallel processing and exploiting more of the parse forest, we can obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost. We then test the method on two classes of features that address deficiencies in the Hiero hierarchical phrase-based model: first, we simultaneously train a large number of Marton and Resnik's soft syntactic constraints, and, second, we introduce a novel structural distortion model. In both cases we obtain significant improvements in translation performance. Optimizing them in combination, for a total of 56 feature weights, we improve performance by 2.6 Bleu on a subset of the NIST 2006 Arabic-English evaluation data.",Our plan is to implement a windowed or moving-average version of BLEU as in ( #AUTHOR_TAG ) .,"['We have thus far implemented two objective functions which operate on individual sentences without regard for choices made on other sentences.', 'When the final evaluation metric incorporates global statistics, however, an objective function which takes them into account is desirable.', 'For example, when using BLEU, it makes a big difference whether individual sentences are both longer and shorter than the reference or systematically shorter than the reference, but these two cases can not be distinguished by single-sentence objective functions.', 'Our plan is to implement a windowed or moving-average version of BLEU as in ( #AUTHOR_TAG ) .']",3,"['We have thus far implemented two objective functions which operate on individual sentences without regard for choices made on other sentences.', 'When the final evaluation metric incorporates global statistics, however, an objective function which takes them into account is desirable.', 'For example, when using BLEU, it makes a big difference whether individual sentences are both longer and shorter than the reference or systematically shorter than the reference, but these two cases can not be distinguished by single-sentence objective functions.', 'Our plan is to implement a windowed or moving-average version of BLEU as in ( #AUTHOR_TAG ) .']"
CCT273,W10-2910,The effect of syntactic representation on semantic role labeling,effective use of wordnet semantics via kernelbased learning,"['Roberto Basili', 'Marco Cammisa', 'Alessandro Moschitti']",conclusion,"Research on document similarity has shown that complex representations are not more accurate than the simple bag-of-words. Term clustering, e.g. using latent semantic indexing, word co-occurrences or synonym relations using a word ontology have been shown not very effective. In particular, when to extend the similarity function external prior knowledge is used, e.g. WordNet, the retrieval system decreases its performance. The critical issues here are methods and conditions to integrate such knowledge.    In this paper we propose kernel functions to add prior knowledge to learning algorithms for document classification. Such kernels use a term similarity measure based on the WordNet hierarchy. The kernel trick is used to implement such space in a balanced and statistically coherent way. Cross-validation results show the benefit of the approach for the Support Vector Machines when few training data is available.","The flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness , e.g. ( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ) .","['The flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness , e.g. ( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ) .', '(Basili et al., 2005;Bloehdorn et al., 2006).', 'However, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy.', 'Therefore, the use of the syntactic/semantic kernels, i.e. (Bloehdorn and Moschitti, 2007a;Bloehdorn and Moschitti, 2007b), to syntactically contextualize word similarities may improve the reranker accuracy.', '(ii) The latter can be further boosted by studying complex structural kernels, e.g.', '(Moschitti, 2008;Nguyen et al., 2009;Dinarelli et al., 2009).', '(iii) More specific predicate argument structures such those proposed in FrameNet, e.g.', '(Baker et al., 1998;Giuglea and Moschitti, 2004;Giuglea and Moschitti, 2006;Johansson and Nugues, 2008b) may be useful to characterize the opinion holder and the sentence semantic context.', 'Finally, while the strategy based on reranking resulted in a significant performance boost, it remains to be seen whether a higher accuracy can be achieved by developing a more sophisticated inference algorithm based on dynamic programming.', 'However, while the development of such an algorithm is an interesting problem, it will not necessarily result in a more usable system -when using a reranker, it is easy to trade accuracy for efficiency.']",0,"['The flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness , e.g. ( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ) .', 'However, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy.', 'Therefore, the use of the syntactic/semantic kernels, i.e. (Bloehdorn and Moschitti, 2007a;Bloehdorn and Moschitti, 2007b), to syntactically contextualize word similarities may improve the reranker accuracy.', '(ii) The latter can be further boosted by studying complex structural kernels, e.g.']"
CCT274,W10-4005,Identifying word translations in non-parallel texts,utilizing citations of foreign words in corpusbased dictionary generation,"['Reinhard Rapp', 'Michael Zock']",experiments,"Previous work concerned with the identification of word translations from text collections has been either based on parallel or on comparable corpora of the respective languages. In the case of comparable corpora basic dictionaries have been necessary to form a bridge between the languages under consideration. We present here a novel approach to identify word translations from a single monolingual corpus without necessarily requiring dictionaries, although, as will be shown, a dictionary can still be useful for improving the results. Our approach is based on the observation that for various reasons monolingual corpora typically contain many foreign words (for example citations). Relying on standard newsticker texts, we will show that their cooccurrence-based associations can be successfully used to identify word translations.",This contrasts with the findings described in #AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages .,"['The right column in Table 1 shows the scores if (using the product-of-ranks algorithm) four source languages are taken into account in parallel.', 'As can be seen, with an average score of 51.8 the improvement over the English only variant (50.6) is minimal.', 'This contrasts with the findings described in #AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages .', 'So this casts some doubt on these.', 'However, as English was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement.', 'This is not the case here, where we try to improve on a score of around 50 for English.', 'Remember that this is a somewhat conservative score as we count correct but alternative translations, as errors.', 'As this is already a performance much closer to the optimum, making further performance gains is more difficult.', 'Therefore, perhaps we should take it as a success that the product-of-ranks algorithm could achieve a minimal performance gain despite the fact that the influence of the non-English languages was probably mostly detrimental.']",1,['This contrasts with the findings described in #AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages .']
CCT275,W10-4005,Identifying word translations in non-parallel texts,utilizing citations of foreign words in corpusbased dictionary generation,"['Reinhard Rapp', 'Michael Zock']",introduction,"Previous work concerned with the identification of word translations from text collections has been either based on parallel or on comparable corpora of the respective languages. In the case of comparable corpora basic dictionaries have been necessary to form a bridge between the languages under consideration. We present here a novel approach to identify word translations from a single monolingual corpus without necessarily requiring dictionaries, although, as will be shown, a dictionary can still be useful for improving the results. Our approach is based on the observation that for various reasons monolingual corpora typically contain many foreign words (for example citations). Relying on standard newsticker texts, we will show that their cooccurrence-based associations can be successfully used to identify word translations.","As suggested in #AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i.e. the words occurring in a particular word equation ) within the association vector of a translation candidate , and by multiplying these ranks .","['So far, we always computed translations to single source words.', 'However, if we assume, for example, that we already have word equations for four languages, and all we want is to compute the translations into a fifth language, then we can simply extend our approach to what we call the product-of-ranks algorithm.', 'As suggested in #AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i.e. the words occurring in a particular word equation ) within the association vector of a translation candidate , and by multiplying these ranks .', 'So for each candidate we obtain a product of ranks.', 'We then assume that the candidate with the smallest product will be the best translation. 3', 'et us illustrate this by an example: If the given words are the variants of the word nervous in English, French, German, and Spanish, i.e. nervous, nerveux, nervös, and nervioso, and if we want to find out their translation into Italian, we would look at the association vectors of each word in our Italian target vocabulary.', 'The association strengths in these vectors need to be inversely sorted, and in each of them we will look up the positions of our four given words.', 'Then for each vector we compute the product of the four ranks, and finally sort the Italian vocabulary according to these products.', 'We would then expect that the correct Italian translation, namely nervoso, ends up in the first position, i.e. has the smallest value for its product of ranks.']",4,"['So far, we always computed translations to single source words.', 'As suggested in #AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i.e. the words occurring in a particular word equation ) within the association vector of a translation candidate , and by multiplying these ranks .', 'We then assume that the candidate with the smallest product will be the best translation. 3']"
CCT276,W10-4005,Identifying word translations in non-parallel texts,utilizing citations of foreign words in corpusbased dictionary generation,"['Reinhard Rapp', 'Michael Zock']",conclusion,"Previous work concerned with the identification of word translations from text collections has been either based on parallel or on comparable corpora of the respective languages. In the case of comparable corpora basic dictionaries have been necessary to form a bridge between the languages under consideration. We present here a novel approach to identify word translations from a single monolingual corpus without necessarily requiring dictionaries, although, as will be shown, a dictionary can still be useful for improving the results. Our approach is based on the observation that for various reasons monolingual corpora typically contain many foreign words (for example citations). Relying on standard newsticker texts, we will show that their cooccurrence-based associations can be successfully used to identify word translations.","Whereas #AUTHOR_TAG dealt only with an English corpus , the current work shows that this methodology is applicable to a wide range of languages and corpora .","['Whereas #AUTHOR_TAG dealt only with an English corpus , the current work shows that this methodology is applicable to a wide range of languages and corpora .', 'We were able to shed some light on criteria influencing performance, such as the selection of text type and the direction of a language pair.', 'For example, it is more promising to look at occurrences of English words in a German corpus rather than the other way around.', 'Because of the special status of English it is also advisable to use it as a pivot wherever possible.']",1,"['Whereas #AUTHOR_TAG dealt only with an English corpus , the current work shows that this methodology is applicable to a wide range of languages and corpora .']"
CCT277,W10-4215,Concept Type Prediction and Responsive Adaptation in a Dialogue System,extracting paraphrases from a parallel corpus,"['R Barzilay', 'K McKeown']",conclusion,"While paraphrasing is critical both for interpretation and generation of natural language, current systems use manual or semi-automatic methods to collect paraphrases. We present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple English translations of the same source text. Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases.",For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in #AUTHOR_TAG .,"['The current work has focussed on high-level mapping rules which can be used both for generation from databases and knowledge representations and also for generation from text.', 'In future work, we will focus on mapping text (in monologue form) to dialogue.', 'For this we need to combine the highlevel rules with rules for paraphrasing the text in the monologue with text for the dialogue acts that express the same information in dialogue form.', 'For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in #AUTHOR_TAG .', 'An important component of our future effort will be to evaluate whether automatically generating dialogues from naturally-occurring monologues, following the approach described here, results in dialogues that are fluent and coherent and preserve the information from the input monologue.']",3,['For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in #AUTHOR_TAG .']
CCT278,W11-0218,Generalising semantic category disambiguation with large lexical resources for fun and profit,boosting precision and recall of dictionarybased protein name recognition,"['Y Tsuruoka', 'J Tsujii']",conclusion,"Dictionary-based protein name recognition is the first step for practical information extraction from biomedical documents because it provides ID information of recognized terms unlike machine learning based approaches. However, dictionary based approaches have two serious problems: (1) a large number of false recognitions mainly caused by short names. (2) low recall due to spelling variation. In this paper, we tackle the former problem by using a machine learning method to filter out false positives. We also present an approximate string searching method to alleviate the latter problem. Experimental results using the GE-NIA corpus show that the filtering using a naive Bayes classifier greatly improves precision with slight loss of recall, resulting in a much better F-score.",A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .,"['While we expected to see clear benefits from both using Gazetteers and SimString features, our exper- iments returned negative results for the majority of the corpora.', 'For NLPBA, GENIA and ID we are aware that most of the instances are either proteins or belong to event trigger classes for which we may not have had adequate lexical resources for disambiguation.', 'By contrast, for Super GREC there are several distinct classes for which we expected lexical resources to have fair coverage for SimString and Gazetteer features.', 'While an advantage over Internal was observed for Super GREC, SimString features showed no benefit over Gazetteer features.', 'The methods exhibited the expected result on only one of the six corpora, CALBC CII, where there is a clear advantage for Gazetteer over Internal and a further clear advantage for SimString over Gazetteer.', 'Disappointingly, we did not succeed in establishing a clear improvement for more than one of the six corpora.', 'Although we have not been successful in Figure 6: Learning curve for Super GREC Figure 7: Learning curve for EPI proving our initial hypothesis we argue that our results calls for further study due to several concerns raised by the results remaining unanswered.', 'It may be that our notion of distance to lexical resource entries is too naive.', 'A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .', 'This would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry.']",3,"['A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .', 'This would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry.']"
CCT279,W14-1704,The Illinois-Columbia System in the CoNLL-2014 Shared Task,the ui system in the hoo 2012 shared task on error correction,"['A Rozovskaya', 'M Sammons', 'D Roth']",experiments,"We describe the University of Illinois (UI) system that participated in the Helping Our Own (HOO) 2012 shared task, which focuses on correcting preposition and determiner errors made by non-native English speakers. The task consisted of three metrics: Detection, Recognition, and Correction, and measured performance before and after additional revisions to the test data were made. Out of 14 teams that participated, our system scored first in Detection and Recognition and second in Correction before the revisions; and first in Detection and second in the other metrics after revisions. We describe our underlying approach, which relates to our previous work in this area, and propose an improvement to the earlier method, error inflation, which results in significant gains in performance.",The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .,"['The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .', 'The model makes use of the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.', 'The article module uses the POS and chunker output to generate some of its features and candidates (likely contexts for missing articles).']",5,['The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .']
CCT280,W14-1704,The Illinois-Columbia System in the CoNLL-2014 Shared Task,algorithm selection and model adaptation for esl correction tasks,"['A Rozovskaya', 'D Roth']",experiments,"We consider the problem of correcting errors made by English as a Second Language (ESL) writers and address two issues that are essential to making progress in ESL error correction - algorithm selection and model adaptation to the first language of the ESL learner.    A variety of learning algorithms have been applied to correct ESL mistakes, but often comparisons were made between incomparable data sets. We conduct an extensive, fair comparison of four popular learning methods for the task, reversing conclusions from earlier evaluations. Our results hold for different training sets, genres, and feature sets.    A second key issue in ESL error correction is the adaptation of a model to the first language of the writer. Errors made by non-native speakers exhibit certain regularities and, as we show, models perform much better when they use knowledge about error patterns of the non-native writers. We propose a novel way to adapt a learned algorithm to the first language of the writer that is both cheaper to implement and performs better than other adaptation methods.",The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks ( #AUTHOR_TAG ) .,"['The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks ( #AUTHOR_TAG ) .', 'Thus, the classifiers trained on the learner data make use of a discriminative model.', 'Because the Google corpus does not contain complete sentences but only n-gram counts of length up to five, training a discriminative model is not desirable, and we thus use NB (details in Rozovskaya and Roth (2011)).']",4,['The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks ( #AUTHOR_TAG ) .']
CCT281,W14-2106,Analyzing Argumentative Discourse Units in Online Interactions,identifying agreement and disagreement in conversational speech use of bayesian networks to model pragmatic dependencies,"['Michel Galley', 'Kathleen McKeown', 'Julia Hirschberg', 'Elizabeth Shriberg']",related work,"We describe a statistical approach for modeling agreements and disagreements in conversational interaction. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse. We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance. Our approach achieves 86.9% accuracy, a 4.9% increase over previous work.","Another line of research that is correlated with ours is recognition of agreement/disagreement ( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .","['Another line of research that is correlated with ours is recognition of agreement/disagreement ( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .', 'For future work, we can utilize textual features (contextual, dependency, discourse markers), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the Agree/Disagree classification accuracy.']",1,"['Another line of research that is correlated with ours is recognition of agreement/disagreement ( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .', 'For future work, we can utilize textual features (contextual, dependency, discourse markers), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the Agree/Disagree classification accuracy.']"
CCT282,W14-2106,Analyzing Argumentative Discourse Units in Online Interactions,topic independent identification of agreement and disagreement in social media dialogue,"['Amita Misra', 'Marilyn A Walker']",,"Research on the structure of dialogue has been hampered for years because large dialogue corpora have not been available. This has impacted the dialogue research community's ability to develop better theories, as well as good off the shelf tools for dialogue processing. Happily, an increasing amount of information and opinion exchange occur in natural dialogue in online forums, where people share their opinions about a vast range of topics. In particular we are interested in rejection in dialogue, also called disagreement and denial, where the size of available dialogue corpora, for the first time, offers an opportunity to empirically test theoretical accounts of the expression and inference of rejection in dialogue. In this paper, we test whether topic-independent features motivated by theoretical predictions can be used to recognize rejection in online forums in a topic independent way. Our results show that our theoretically motivated features achieve 66% accuracy, an improvement over a unigram baseline of an absolute 6%.Comment: @inproceedings{Misra2013TopicII, title={Topic Independent   Identification of Agreement and Disagreement in Social Media Dialogue},   author={Amita Misra and Marilyn A. Walker}, booktitle={SIGDIAL Conference},   year={2013}","In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( Galley et al. , 2004 ; #AUTHOR_TAG ) .","['The first row of the Table 6 represents the baseline results.', 'Though the precision is high for agreement category, the recall is quite low and that results in a poor overall F1 measure.', ""This shows that even though markers like 'agree' or 'disagree'  For the next set of experiments we used a supervised machine learning approach for the two-way classification (Agree/Disagree)."", 'We use Support Vector Machines (SVM) as our machine-learning algorithm for classification as implemented in Weka (Hall et al., 2009) and ran 10-fold cross validation.', 'As a SVM baseline, we first use all unigrams in Callout and Target as features (Table 6, Row 2).', 'We notice that the recall improves significantly when compared with the rule-based method.', 'To further improve the classification accuracy, we use Mutual Information (MI) to select the words in the Callouts and Targets that are likely to be associated with the categories Agree and Disagree, respectively.', 'Specifically, we sort each word based on its MI value and then select the first 180 words in each of the two categories to represent our new vocabulary set of 360 words.', 'The feature vector includes only words present in the MI list.', 'Compared to the all unigrams baseline, the MI-based unigrams improve the F1 by 4% (Agree) and 2% (Disagree) (Table 6).', 'The MI approach discovers the words that are highly associated with Agree/Disagree categories and these words turn to be useful features for classification.', 'In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( Galley et al. , 2004 ; #AUTHOR_TAG ) .']",4,"['In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( Galley et al. , 2004 ; #AUTHOR_TAG ) .']"
CCT283,W14-2106,Analyzing Argumentative Discourse Units in Online Interactions,topic independent identification of agreement and disagreement in social media dialogue,"['Amita Misra', 'Marilyn A Walker']",related work,"Research on the structure of dialogue has been hampered for years because large dialogue corpora have not been available. This has impacted the dialogue research community's ability to develop better theories, as well as good off the shelf tools for dialogue processing. Happily, an increasing amount of information and opinion exchange occur in natural dialogue in online forums, where people share their opinions about a vast range of topics. In particular we are interested in rejection in dialogue, also called disagreement and denial, where the size of available dialogue corpora, for the first time, offers an opportunity to empirically test theoretical accounts of the expression and inference of rejection in dialogue. In this paper, we test whether topic-independent features motivated by theoretical predictions can be used to recognize rejection in online forums in a topic independent way. Our results show that our theoretically motivated features achieve 66% accuracy, an improvement over a unigram baseline of an absolute 6%.Comment: @inproceedings{Misra2013TopicII, title={Topic Independent   Identification of Agreement and Disagreement in Social Media Dialogue},   author={Amita Misra and Marilyn A. Walker}, booktitle={SIGDIAL Conference},   year={2013}","Another line of research that is correlated with ours is recognition of agreement/disagreement ( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .","['Another line of research that is correlated with ours is recognition of agreement/disagreement ( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .', 'For future work, we can utilize textual features (contextual, dependency, discourse markers), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the Agree/Disagree classification accuracy.']",1,"['Another line of research that is correlated with ours is recognition of agreement/disagreement ( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .', 'For future work, we can utilize textual features (contextual, dependency, discourse markers), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the Agree/Disagree classification accuracy.']"
CCT284,W14-2106,Analyzing Argumentative Discourse Units in Online Interactions,identifying agreement and disagreement in conversational speech use of bayesian networks to model pragmatic dependencies,"['Michel Galley', 'Kathleen McKeown', 'Julia Hirschberg', 'Elizabeth Shriberg']",,"We describe a statistical approach for modeling agreements and disagreements in conversational interaction. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse. We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance. Our approach achieves 86.9% accuracy, a 4.9% increase over previous work.","In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( #AUTHOR_TAG ; Misra and Walker , 2013 ) .","['The first row of the Table 6 represents the baseline results.', 'Though the precision is high for agreement category, the recall is quite low and that results in a poor overall F1 measure.', ""This shows that even though markers like 'agree' or 'disagree'  For the next set of experiments we used a supervised machine learning approach for the two-way classification (Agree/Disagree)."", 'We use Support Vector Machines (SVM) as our machine-learning algorithm for classification as implemented in Weka (Hall et al., 2009) and ran 10-fold cross validation.', 'As a SVM baseline, we first use all unigrams in Callout and Target as features (Table 6, Row 2).', 'We notice that the recall improves significantly when compared with the rule-based method.', 'To further improve the classification accuracy, we use Mutual Information (MI) to select the words in the Callouts and Targets that are likely to be associated with the categories Agree and Disagree, respectively.', 'Specifically, we sort each word based on its MI value and then select the first 180 words in each of the two categories to represent our new vocabulary set of 360 words.', 'The feature vector includes only words present in the MI list.', 'Compared to the all unigrams baseline, the MI-based unigrams improve the F1 by 4% (Agree) and 2% (Disagree) (Table 6).', 'The MI approach discovers the words that are highly associated with Agree/Disagree categories and these words turn to be useful features for classification.', 'In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( #AUTHOR_TAG ; Misra and Walker , 2013 ) .']",4,"['In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( #AUTHOR_TAG ; Misra and Walker , 2013 ) .']"
