token_context,word_context,seg_context,sent_cotext,label
"['),  #TAUTHOR_TAG have shown that a']","['chinese treebank ( ctb ),  #TAUTHOR_TAG have shown that a']","['),  #TAUTHOR_TAG have shown that']","['', 'for constituent - based parsing using the chinese treebank ( ctb ),  #TAUTHOR_TAG have shown that a shift - reduce parser can give competitive accuracy scores together with high speeds, by using an svm to make a single decision at each point in the parsing process.', ""in this paper we describe a global discriminative model for chinese shift - reduce parsing, and compare it with wang et al.'s approach."", 'we apply the same shift - reduce procedure as  #TAUTHOR_TAG, but instead of using a local classifier for each transition - based action, we train a generalized perceptron model over complete sequences of actions, so that the parameters are learned in the context of complete parses.', 'we apply beam search to decoding instead of greedy search.', 'the parser still operates in linear time, but the use of beam - search allows the correction of local decision errors by global comparison.', ""using ctb2, our model achieved parseval f - scores comparable to wang et al.'s approach."", 'we also present accuracy scores for the much larger ctb5, using both a constituent - based and dependency - based evaluation.', 'the scores for the dependency - based evaluation were higher than the state - of - the - art dependency parsers for the ctb5 data']",4
"['0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel']","[', whereas when the corresponding node is non - terminal, c represents its constituent label ; t represents the pos - tag for a word. the context s 0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel']","[', whereas when the corresponding node is non - terminal, c represents its constituent label ; t represents the pos - tag for a word. the context s 0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel']","[', whereas when the corresponding node is non - terminal, c represents its constituent label ; t represents the pos - tag for a word. the context s 0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel function with an svm and did not manually create feature combinations. since we used', 'the linear perceptron algorithm we manually combined unigram features into bigram and trigram features. the "" bracket "" row shows bracket - related features, which were inspired by  #TAUTHOR_TAG. here brackets refer to left brackets including "" [UNK] "", "" "" "" and', '"" [UNK] "" and right brackets including "" [UNK] "", "" "" "" and "" [UNK] "". in the table, b represents the matching status of the last left bracket ( if any ) on the stack. it takes three different values : 1 ( no matching right bracket has', 'been pushed onto stack ), 2 ( a matching right bracket has been pushed onto stack ) and 3 ( a matching right bracket has been pushed onto stack, but then popped off ). the "" separ', '##ator "" row shows features that include one of the separator punctuations ( i. e. "" [UNK] "", "" [UNK] "", "" [UNK] "" and "" [UNK]', '']",4
"['0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel']","[', whereas when the corresponding node is non - terminal, c represents its constituent label ; t represents the pos - tag for a word. the context s 0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel']","[', whereas when the corresponding node is non - terminal, c represents its constituent label ; t represents the pos - tag for a word. the context s 0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel']","[', whereas when the corresponding node is non - terminal, c represents its constituent label ; t represents the pos - tag for a word. the context s 0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel function with an svm and did not manually create feature combinations. since we used', 'the linear perceptron algorithm we manually combined unigram features into bigram and trigram features. the "" bracket "" row shows bracket - related features, which were inspired by  #TAUTHOR_TAG. here brackets refer to left brackets including "" [UNK] "", "" "" "" and', '"" [UNK] "" and right brackets including "" [UNK] "", "" "" "" and "" [UNK] "". in the table, b represents the matching status of the last left bracket ( if any ) on the stack. it takes three different values : 1 ( no matching right bracket has', 'been pushed onto stack ), 2 ( a matching right bracket has been pushed onto stack ) and 3 ( a matching right bracket has been pushed onto stack, but then popped off ). the "" separ', '##ator "" row shows features that include one of the separator punctuations ( i. e. "" [UNK] "", "" [UNK] "", "" [UNK] "" and "" [UNK]', '']",4
"[' #TAUTHOR_TAG, and our']","[' #TAUTHOR_TAG, and our parser, respectively.', 'the']","[' #TAUTHOR_TAG, and our']","['experiments in this section were performed using ctb2 to allow comparison with previous work, with the ctb2 data extracted from chinese treebank 5 ( ctb5 table 3 : accuracies on ctb2 with gold - standard pos - tags own implementation of the perceptron - based tagger from  #AUTHOR_TAG.', 'the results of various models measured using sentences with less than 40 words and using goldstandard pos - tags are shown in table 3.', 'the rows represent the model from  #AUTHOR_TAG,  #AUTHOR_TAG, the svm and ensemble models from  #TAUTHOR_TAG, and our parser, respectively.', 'the accuracy of our parser is competitive using this test set.', 'the results of various models using automatically assigned pos - tags are shown in table 4.', 'the rows in the table represent the models from  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, the svm model from  #TAUTHOR_TAG, and the parser of this paper, respectively.', 'our parser gave comparable accuracies to the svm and ensemble models from  #TAUTHOR_TAG.', 'however, comparison with table 3 shows that our parser is more sensitive to pos - tagging errors than some of the other models.', '']",4
"['process from  #AUTHOR_TAG and  #TAUTHOR_TAG, and therefore']","['process from  #AUTHOR_TAG and  #TAUTHOR_TAG, and therefore']","['parser is based on the shift - reduce parsing process from  #AUTHOR_TAG and  #TAUTHOR_TAG, and therefore']","['parser is based on the shift - reduce parsing process from  #AUTHOR_TAG and  #TAUTHOR_TAG, and therefore it can be classified as a transition - based parser  #AUTHOR_TAG.', 'an important difference between our parser and the  #TAUTHOR_TAG is based on a local classifier that optimizes each individual choice.', 'instead of greedy local decoding, we used beam search in the decoder.', 'an early work that applies beam search to constituent parsing is  #AUTHOR_TAG.', ""the main difference between our parser and ratnaparkhi's is that we use a global discriminative model, whereas ratnaparkhi's parser has separate probabilities of actions chained together in a conditional model."", 'both our parser and the parser from  #AUTHOR_TAG use a global discriminative model and an incremental parsing process.', '']",4
"['is novel in our parser.', ' #AUTHOR_TAG and  #TAUTHOR_TAG only used']","['is novel in our parser.', ' #AUTHOR_TAG and  #TAUTHOR_TAG only used the first three transition actions, setting the']","['is novel in our parser.', ' #AUTHOR_TAG and  #TAUTHOR_TAG only used']","['', "" #AUTHOR_TAG and  #TAUTHOR_TAG only used the first three transition actions, setting the final state as all incoming words having been processed, and the stack containing only one node. however, there are a small number of sentences ( 14 out of 3475 from the training data ) that have unary - branching roots. for these sentences, wang's"", '']",6
"['0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel']","[', whereas when the corresponding node is non - terminal, c represents its constituent label ; t represents the pos - tag for a word. the context s 0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel']","[', whereas when the corresponding node is non - terminal, c represents its constituent label ; t represents the pos - tag for a word. the context s 0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel']","[', whereas when the corresponding node is non - terminal, c represents its constituent label ; t represents the pos - tag for a word. the context s 0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel function with an svm and did not manually create feature combinations. since we used', 'the linear perceptron algorithm we manually combined unigram features into bigram and trigram features. the "" bracket "" row shows bracket - related features, which were inspired by  #TAUTHOR_TAG. here brackets refer to left brackets including "" [UNK] "", "" "" "" and', '"" [UNK] "" and right brackets including "" [UNK] "", "" "" "" and "" [UNK] "". in the table, b represents the matching status of the last left bracket ( if any ) on the stack. it takes three different values : 1 ( no matching right bracket has', 'been pushed onto stack ), 2 ( a matching right bracket has been pushed onto stack ) and 3 ( a matching right bracket has been pushed onto stack, but then popped off ). the "" separ', '##ator "" row shows features that include one of the separator punctuations ( i. e. "" [UNK] "", "" [UNK] "", "" [UNK] "" and "" [UNK]', '']",3
"[' #TAUTHOR_TAG, and our']","[' #TAUTHOR_TAG, and our parser, respectively.', 'the']","[' #TAUTHOR_TAG, and our']","['experiments in this section were performed using ctb2 to allow comparison with previous work, with the ctb2 data extracted from chinese treebank 5 ( ctb5 table 3 : accuracies on ctb2 with gold - standard pos - tags own implementation of the perceptron - based tagger from  #AUTHOR_TAG.', 'the results of various models measured using sentences with less than 40 words and using goldstandard pos - tags are shown in table 3.', 'the rows represent the model from  #AUTHOR_TAG,  #AUTHOR_TAG, the svm and ensemble models from  #TAUTHOR_TAG, and our parser, respectively.', 'the accuracy of our parser is competitive using this test set.', 'the results of various models using automatically assigned pos - tags are shown in table 4.', 'the rows in the table represent the models from  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, the svm model from  #TAUTHOR_TAG, and the parser of this paper, respectively.', 'our parser gave comparable accuracies to the svm and ensemble models from  #TAUTHOR_TAG.', 'however, comparison with table 3 shows that our parser is more sensitive to pos - tagging errors than some of the other models.', '']",3
"[' #TAUTHOR_TAG, and our']","[' #TAUTHOR_TAG, and our parser, respectively.', 'the']","[' #TAUTHOR_TAG, and our']","['experiments in this section were performed using ctb2 to allow comparison with previous work, with the ctb2 data extracted from chinese treebank 5 ( ctb5 table 3 : accuracies on ctb2 with gold - standard pos - tags own implementation of the perceptron - based tagger from  #AUTHOR_TAG.', 'the results of various models measured using sentences with less than 40 words and using goldstandard pos - tags are shown in table 3.', 'the rows represent the model from  #AUTHOR_TAG,  #AUTHOR_TAG, the svm and ensemble models from  #TAUTHOR_TAG, and our parser, respectively.', 'the accuracy of our parser is competitive using this test set.', 'the results of various models using automatically assigned pos - tags are shown in table 4.', 'the rows in the table represent the models from  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, the svm model from  #TAUTHOR_TAG, and the parser of this paper, respectively.', 'our parser gave comparable accuracies to the svm and ensemble models from  #TAUTHOR_TAG.', 'however, comparison with table 3 shows that our parser is more sensitive to pos - tagging errors than some of the other models.', '']",3
"['##r graph, then defining the dependency relations between those concepts  #TAUTHOR_TAG.', '']","['amr graph, then defining the dependency relations between those concepts  #TAUTHOR_TAG.', '']","['then defining the dependency relations between those concepts  #TAUTHOR_TAG.', '']","['', 'amr is a formalism of sentence semantic structure by directed, acyclic, and rooted graphs, in which semantic relations such as predicateargument relations and noun - noun relations are expressed.', 'in this paper, we extract substructures corresponding to nps ( shown in figure 1 ) from the amr bank 1, and create a data set of np semantic structures.', 'in general, amr substructures are graphs.', 'however, since we found out that nps mostly form trees rather than graphs in the amr bank, we can assume that amr substructures corresponding to nps are trees.', 'thus, we define our task as predicting the amr tree structure, given a sequence of words in an np.', 'the previous method for amr parsing takes a train dev test 3504 463 398 table 1 : statistics of the extracted np data two - step approach : first identifying distinct concepts ( nodes ) in the amr graph, then defining the dependency relations between those concepts  #TAUTHOR_TAG.', '']",0
['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],"['by  #TAUTHOR_TAG.', 'then, we use the stanford parser  #AUTHOR_TAG']","['extract substructures ( subtrees ) corresponding to nps from the amr bank ( ldc2014t12 ).', 'in the amr bank, there is no alignment between the words and the concepts ( nodes ) in the amr graphs.', 'we obtain this alignment by using the rule - based alignment tool by  #TAUTHOR_TAG.', 'then, we use the stanford parser  #AUTHOR_TAG to obtain constituency trees, and extract nps that contain more than one noun and are not included by another np.', 'we exclude nps that contain named entities, because they would require various kinds of manually crafted rules for each type of named entity.', 'we also exclude nps that contain possessive pronouns or conjunctions, which prove problematic for the alignment tool.', 'table 1 shows the statistics of the extracted np data']",5
['by  #TAUTHOR_TAG for a reti'],['by  #TAUTHOR_TAG for a retired'],"['by  #TAUTHOR_TAG for a retired plant worker.', '∅ denotes an empty concept.', 'relation identification step.', 'their method is designed for parsing sentences into amr, but here, we use this method']","['adopt the method proposed by  #TAUTHOR_TAG for a retired plant worker.', '∅ denotes an empty concept.', 'relation identification step.', 'their method is designed for parsing sentences into amr, but here, we use this method for parsing nps.', 'in their method, concept identification is formulated as a sequence labeling problem  #AUTHOR_TAG and solved by the viterbi algorithm.', 'spans of words in the input sentence are labeled with concept subgraphs.', 'figure 2 illustrates the concept identification step for an np a retired plant worker.', 'after the concepts have been identified, these concepts are fixed, and the dependency relations between them are identified by an algorithm that finds the maximum spanning connected subgraph  #AUTHOR_TAG, which is similar to the maximum spanning tree ( mst ) algorithm used for dependency parsing ( mc  #AUTHOR_TAG.', 'they report that using gold concepts yields much better performance, implying that joint identification of concepts and relations can be helpful']",5
"['conduct an experiment using our np data set ( table 1 ).', 'we use the implementation 2 of  #TAUTHOR_TAG as our baseline.', 'for the baseline,']","['conduct an experiment using our np data set ( table 1 ).', 'we use the implementation 2 of  #TAUTHOR_TAG as our baseline.', 'for the baseline,']","['conduct an experiment using our np data set ( table 1 ).', 'we use the implementation 2 of  #TAUTHOR_TAG as our baseline.', 'for the baseline, we use the features of the default settings.', 'the method by  #TAUTHOR_TAG']","['conduct an experiment using our np data set ( table 1 ).', 'we use the implementation 2 of  #TAUTHOR_TAG as our baseline.', 'for the baseline, we use the features of the default settings.', 'the method by  #TAUTHOR_TAG can only generate the concepts that appear in the training data.', 'on the other hand, our method can generate concepts that do not appear in the training data using the concept generation rules lemma, dict pred, and dict noun in table 3.', '']",5
"['conduct an experiment using our np data set ( table 1 ).', 'we use the implementation 2 of  #TAUTHOR_TAG as our baseline.', 'for the baseline,']","['conduct an experiment using our np data set ( table 1 ).', 'we use the implementation 2 of  #TAUTHOR_TAG as our baseline.', 'for the baseline,']","['conduct an experiment using our np data set ( table 1 ).', 'we use the implementation 2 of  #TAUTHOR_TAG as our baseline.', 'for the baseline, we use the features of the default settings.', 'the method by  #TAUTHOR_TAG']","['conduct an experiment using our np data set ( table 1 ).', 'we use the implementation 2 of  #TAUTHOR_TAG as our baseline.', 'for the baseline, we use the features of the default settings.', 'the method by  #TAUTHOR_TAG can only generate the concepts that appear in the training data.', 'on the other hand, our method can generate concepts that do not appear in the training data using the concept generation rules lemma, dict pred, and dict noun in table 3.', '']",4
"['( smt )  #TAUTHOR_TAG.', 'models that adjust orders']","['( smt )  #TAUTHOR_TAG.', 'models that adjust orders']","['the translation quality in statistical machine translation ( smt )  #TAUTHOR_TAG.', 'models that adjust orders']","['word order between source and target languages significantly influences the translation quality in statistical machine translation ( smt )  #TAUTHOR_TAG.', 'models that adjust orders of translated phrases in decoding have been proposed to solve this problem  #AUTHOR_TAG.', 'however, such reordering models do not perform well for long - distance reordering.', 'in addition, their computational costs are expensive.', 'to address these problems, preordering ( xia and mc  #AUTHOR_TAG b ;  #TAUTHOR_TAG and postordering  #AUTHOR_TAG ( goto et al.,, 2013  #AUTHOR_TAG models have been proposed.', 'preordering reorders source sentences before translation, while post - ordering reorders sentences translated without considering the word order after translation.', 'in particular, preordering effectively improves the translation quality because it solves long - distance reordering and computational complexity issues  #TAUTHOR_TAG.', 'rule - based preordering methods either manually create reordering rules  #AUTHOR_TAG b ;  #AUTHOR_TAG or extract reordering rules from a corpus ( xia and mc  #AUTHOR_TAG.', 'on the other hand, studies in  #TAUTHOR_TAG apply machine learning to the preordering problem.', '']",0
"['( smt )  #TAUTHOR_TAG.', 'models that adjust orders']","['( smt )  #TAUTHOR_TAG.', 'models that adjust orders']","['the translation quality in statistical machine translation ( smt )  #TAUTHOR_TAG.', 'models that adjust orders']","['word order between source and target languages significantly influences the translation quality in statistical machine translation ( smt )  #TAUTHOR_TAG.', 'models that adjust orders of translated phrases in decoding have been proposed to solve this problem  #AUTHOR_TAG.', 'however, such reordering models do not perform well for long - distance reordering.', 'in addition, their computational costs are expensive.', 'to address these problems, preordering ( xia and mc  #AUTHOR_TAG b ;  #TAUTHOR_TAG and postordering  #AUTHOR_TAG ( goto et al.,, 2013  #AUTHOR_TAG models have been proposed.', 'preordering reorders source sentences before translation, while post - ordering reorders sentences translated without considering the word order after translation.', 'in particular, preordering effectively improves the translation quality because it solves long - distance reordering and computational complexity issues  #TAUTHOR_TAG.', 'rule - based preordering methods either manually create reordering rules  #AUTHOR_TAG b ;  #AUTHOR_TAG or extract reordering rules from a corpus ( xia and mc  #AUTHOR_TAG.', 'on the other hand, studies in  #TAUTHOR_TAG apply machine learning to the preordering problem.', '']",0
"['( smt )  #TAUTHOR_TAG.', 'models that adjust orders']","['( smt )  #TAUTHOR_TAG.', 'models that adjust orders']","['the translation quality in statistical machine translation ( smt )  #TAUTHOR_TAG.', 'models that adjust orders']","['word order between source and target languages significantly influences the translation quality in statistical machine translation ( smt )  #TAUTHOR_TAG.', 'models that adjust orders of translated phrases in decoding have been proposed to solve this problem  #AUTHOR_TAG.', 'however, such reordering models do not perform well for long - distance reordering.', 'in addition, their computational costs are expensive.', 'to address these problems, preordering ( xia and mc  #AUTHOR_TAG b ;  #TAUTHOR_TAG and postordering  #AUTHOR_TAG ( goto et al.,, 2013  #AUTHOR_TAG models have been proposed.', 'preordering reorders source sentences before translation, while post - ordering reorders sentences translated without considering the word order after translation.', 'in particular, preordering effectively improves the translation quality because it solves long - distance reordering and computational complexity issues  #TAUTHOR_TAG.', 'rule - based preordering methods either manually create reordering rules  #AUTHOR_TAG b ;  #AUTHOR_TAG or extract reordering rules from a corpus ( xia and mc  #AUTHOR_TAG.', 'on the other hand, studies in  #TAUTHOR_TAG apply machine learning to the preordering problem.', '']",0
"['( smt )  #TAUTHOR_TAG.', 'models that adjust orders']","['( smt )  #TAUTHOR_TAG.', 'models that adjust orders']","['the translation quality in statistical machine translation ( smt )  #TAUTHOR_TAG.', 'models that adjust orders']","['word order between source and target languages significantly influences the translation quality in statistical machine translation ( smt )  #TAUTHOR_TAG.', 'models that adjust orders of translated phrases in decoding have been proposed to solve this problem  #AUTHOR_TAG.', 'however, such reordering models do not perform well for long - distance reordering.', 'in addition, their computational costs are expensive.', 'to address these problems, preordering ( xia and mc  #AUTHOR_TAG b ;  #TAUTHOR_TAG and postordering  #AUTHOR_TAG ( goto et al.,, 2013  #AUTHOR_TAG models have been proposed.', 'preordering reorders source sentences before translation, while post - ordering reorders sentences translated without considering the word order after translation.', 'in particular, preordering effectively improves the translation quality because it solves long - distance reordering and computational complexity issues  #TAUTHOR_TAG.', 'rule - based preordering methods either manually create reordering rules  #AUTHOR_TAG b ;  #AUTHOR_TAG or extract reordering rules from a corpus ( xia and mc  #AUTHOR_TAG.', 'on the other hand, studies in  #TAUTHOR_TAG apply machine learning to the preordering problem.', '']",0
"['( smt )  #TAUTHOR_TAG.', 'models that adjust orders']","['( smt )  #TAUTHOR_TAG.', 'models that adjust orders']","['the translation quality in statistical machine translation ( smt )  #TAUTHOR_TAG.', 'models that adjust orders']","['word order between source and target languages significantly influences the translation quality in statistical machine translation ( smt )  #TAUTHOR_TAG.', 'models that adjust orders of translated phrases in decoding have been proposed to solve this problem  #AUTHOR_TAG.', 'however, such reordering models do not perform well for long - distance reordering.', 'in addition, their computational costs are expensive.', 'to address these problems, preordering ( xia and mc  #AUTHOR_TAG b ;  #TAUTHOR_TAG and postordering  #AUTHOR_TAG ( goto et al.,, 2013  #AUTHOR_TAG models have been proposed.', 'preordering reorders source sentences before translation, while post - ordering reorders sentences translated without considering the word order after translation.', 'in particular, preordering effectively improves the translation quality because it solves long - distance reordering and computational complexity issues  #TAUTHOR_TAG.', 'rule - based preordering methods either manually create reordering rules  #AUTHOR_TAG b ;  #AUTHOR_TAG or extract reordering rules from a corpus ( xia and mc  #AUTHOR_TAG.', 'on the other hand, studies in  #TAUTHOR_TAG apply machine learning to the preordering problem.', '']",0
"['( smt )  #TAUTHOR_TAG.', 'models that adjust orders']","['( smt )  #TAUTHOR_TAG.', 'models that adjust orders']","['the translation quality in statistical machine translation ( smt )  #TAUTHOR_TAG.', 'models that adjust orders']","['word order between source and target languages significantly influences the translation quality in statistical machine translation ( smt )  #TAUTHOR_TAG.', 'models that adjust orders of translated phrases in decoding have been proposed to solve this problem  #AUTHOR_TAG.', 'however, such reordering models do not perform well for long - distance reordering.', 'in addition, their computational costs are expensive.', 'to address these problems, preordering ( xia and mc  #AUTHOR_TAG b ;  #TAUTHOR_TAG and postordering  #AUTHOR_TAG ( goto et al.,, 2013  #AUTHOR_TAG models have been proposed.', 'preordering reorders source sentences before translation, while post - ordering reorders sentences translated without considering the word order after translation.', 'in particular, preordering effectively improves the translation quality because it solves long - distance reordering and computational complexity issues  #TAUTHOR_TAG.', 'rule - based preordering methods either manually create reordering rules  #AUTHOR_TAG b ;  #AUTHOR_TAG or extract reordering rules from a corpus ( xia and mc  #AUTHOR_TAG.', 'on the other hand, studies in  #TAUTHOR_TAG apply machine learning to the preordering problem.', '']",3
"['s τ  #AUTHOR_TAG as in  #TAUTHOR_TAG, which is calculated by']","[""based on kendall's τ  #AUTHOR_TAG as in  #TAUTHOR_TAG, which is calculated by""]","['s τ  #AUTHOR_TAG as in  #TAUTHOR_TAG, which is calculated by']","['created training data for preordering by labeling whether each node of the source - side syntax tree has reordered child nodes against a targetside sentence.', ""the label is determined based on kendall's τ  #AUTHOR_TAG as in  #TAUTHOR_TAG, which is calculated by equation ( 1 )."", '']",3
['and  #TAUTHOR_TAG was'],['and  #TAUTHOR_TAG was'],"['##m sentence pairs. the distortion limit of smt systems trained using preordered sentences', 'by rvnn and  #TAUTHOR_TAG was set to 0, while that without preordering was set to 6. compared']","['setting λ larger than 200 did not contribute to the translation quality. based on these, we further evaluated the rvnn with pos tags and syntactic categories where λ = 200. table 2 shows bleu and ribes scores of the test set on', 'pbsmt and nmt trained on the entire training data of', '1. 8m sentence pairs. the distortion limit of smt systems trained using preordered sentences', 'by rvnn and  #TAUTHOR_TAG was set to 0, while that without preordering was set to 6. compared to the plain pbsmt without preordering, both bleu and ribes increased significantly with preordering by rvnn', 'and  #TAUTHOR_TAG.', 'these scores were comparable ( statistically insignificant at p', '< 0. 05 ) between rvnn and  #TAUTHOR_TAG. in contrast to the case of pb - smt, nm', '##t without preordering achieved a significantly higher bleu score than nmt models with preordering by rvnn and  #TAUTHOR_TAG. this is', 'the same phenomenon in the chinese - to - japanese translation experiment reported in  #AUTHOR_TAG', '']",3
['comparable to the  #TAUTHOR_TAG that requires a manual feature design'],"['comparable to the  #TAUTHOR_TAG that requires a manual feature design.', 'as a future work,']","['method achieved a translation quality comparable to the  #TAUTHOR_TAG that requires a manual feature design.', 'as a future work,']","['this paper, we proposed a preordering method without a manual feature design for mt.', 'the experiments confirmed that the proposed method achieved a translation quality comparable to the  #TAUTHOR_TAG that requires a manual feature design.', 'as a future work, we plan to develop a model that jointly parses and preorders a source sentence.', 'in addition, we plan to integrate preordering into the nmt model']",3
"['s τ  #AUTHOR_TAG as in  #TAUTHOR_TAG, which is calculated by']","[""based on kendall's τ  #AUTHOR_TAG as in  #TAUTHOR_TAG, which is calculated by""]","['s τ  #AUTHOR_TAG as in  #TAUTHOR_TAG, which is calculated by']","['created training data for preordering by labeling whether each node of the source - side syntax tree has reordered child nodes against a targetside sentence.', ""the label is determined based on kendall's τ  #AUTHOR_TAG as in  #TAUTHOR_TAG, which is calculated by equation ( 1 )."", '']",5
"['intersection heuristic following  #TAUTHOR_TAG.', '']","['intersection heuristic following  #TAUTHOR_TAG.', '']","['with the intersection heuristic following  #TAUTHOR_TAG.', '']","['conducted english - to - japanese translation experiments using the aspec corpus  #AUTHOR_TAG.', 'this corpus provides 3m sentence pairs as training data, 1, 790 sentence pairs as development data, and 1, 812 sentence pairs as test data.', 'we used stanford corenlp 2 for tokenization and pos tagging, enju 3 for parsing of english, and mecab 4 for tokenization of japanese.', 'for word alignment, we used mgiza.', '5 source - totarget and target - to - source word alignments were calculated using ibm model 1 and hidden markov model, and they were combined with the intersection heuristic following  #TAUTHOR_TAG.', 'we implemented our rvnn preordering model with chainer.', '6 the aspec corpus was created using the sentence alignment method proposed in  #AUTHOR_TAG and was sorted based on the alignment confidence scores.', 'in this paper, we used 100k sentences sampled from the top 500k sentences as training data for preordering.', 'the vocabulary size n was set to 50k.', 'we used adam  #AUTHOR_TAG with a weight decay and gradient clipping for optimization.', 'the mini batch size k was set to 500.', 'we compared our model with the state - of - theart preordering method proposed in  #TAUTHOR_TAG.', 'we used its publicly available implementation, 7 and trained it on the same 100k sentences as our model.', '']",5
['and  #TAUTHOR_TAG was'],['and  #TAUTHOR_TAG was'],"['##m sentence pairs. the distortion limit of smt systems trained using preordered sentences', 'by rvnn and  #TAUTHOR_TAG was set to 0, while that without preordering was set to 6. compared']","['setting λ larger than 200 did not contribute to the translation quality. based on these, we further evaluated the rvnn with pos tags and syntactic categories where λ = 200. table 2 shows bleu and ribes scores of the test set on', 'pbsmt and nmt trained on the entire training data of', '1. 8m sentence pairs. the distortion limit of smt systems trained using preordered sentences', 'by rvnn and  #TAUTHOR_TAG was set to 0, while that without preordering was set to 6. compared to the plain pbsmt without preordering, both bleu and ribes increased significantly with preordering by rvnn', 'and  #TAUTHOR_TAG.', 'these scores were comparable ( statistically insignificant at p', '< 0. 05 ) between rvnn and  #TAUTHOR_TAG. in contrast to the case of pb - smt, nm', '##t without preordering achieved a significantly higher bleu score than nmt models with preordering by rvnn and  #TAUTHOR_TAG. this is', 'the same phenomenon in the chinese - to - japanese translation experiment reported in  #AUTHOR_TAG', '']",5
"['intersection heuristic following  #TAUTHOR_TAG.', '']","['intersection heuristic following  #TAUTHOR_TAG.', '']","['with the intersection heuristic following  #TAUTHOR_TAG.', '']","['conducted english - to - japanese translation experiments using the aspec corpus  #AUTHOR_TAG.', 'this corpus provides 3m sentence pairs as training data, 1, 790 sentence pairs as development data, and 1, 812 sentence pairs as test data.', 'we used stanford corenlp 2 for tokenization and pos tagging, enju 3 for parsing of english, and mecab 4 for tokenization of japanese.', 'for word alignment, we used mgiza.', '5 source - totarget and target - to - source word alignments were calculated using ibm model 1 and hidden markov model, and they were combined with the intersection heuristic following  #TAUTHOR_TAG.', 'we implemented our rvnn preordering model with chainer.', '6 the aspec corpus was created using the sentence alignment method proposed in  #AUTHOR_TAG and was sorted based on the alignment confidence scores.', 'in this paper, we used 100k sentences sampled from the top 500k sentences as training data for preordering.', 'the vocabulary size n was set to 50k.', 'we used adam  #AUTHOR_TAG with a weight decay and gradient clipping for optimization.', 'the mini batch size k was set to 500.', 'we compared our model with the state - of - theart preordering method proposed in  #TAUTHOR_TAG.', 'we used its publicly available implementation, 7 and trained it on the same 100k sentences as our model.', '']",7
['and  #TAUTHOR_TAG was'],['and  #TAUTHOR_TAG was'],"['##m sentence pairs. the distortion limit of smt systems trained using preordered sentences', 'by rvnn and  #TAUTHOR_TAG was set to 0, while that without preordering was set to 6. compared']","['setting λ larger than 200 did not contribute to the translation quality. based on these, we further evaluated the rvnn with pos tags and syntactic categories where λ = 200. table 2 shows bleu and ribes scores of the test set on', 'pbsmt and nmt trained on the entire training data of', '1. 8m sentence pairs. the distortion limit of smt systems trained using preordered sentences', 'by rvnn and  #TAUTHOR_TAG was set to 0, while that without preordering was set to 6. compared to the plain pbsmt without preordering, both bleu and ribes increased significantly with preordering by rvnn', 'and  #TAUTHOR_TAG.', 'these scores were comparable ( statistically insignificant at p', '< 0. 05 ) between rvnn and  #TAUTHOR_TAG. in contrast to the case of pb - smt, nm', '##t without preordering achieved a significantly higher bleu score than nmt models with preordering by rvnn and  #TAUTHOR_TAG. this is', 'the same phenomenon in the chinese - to - japanese translation experiment reported in  #AUTHOR_TAG', '']",7
['and  #TAUTHOR_TAG was'],['and  #TAUTHOR_TAG was'],"['##m sentence pairs. the distortion limit of smt systems trained using preordered sentences', 'by rvnn and  #TAUTHOR_TAG was set to 0, while that without preordering was set to 6. compared']","['setting λ larger than 200 did not contribute to the translation quality. based on these, we further evaluated the rvnn with pos tags and syntactic categories where λ = 200. table 2 shows bleu and ribes scores of the test set on', 'pbsmt and nmt trained on the entire training data of', '1. 8m sentence pairs. the distortion limit of smt systems trained using preordered sentences', 'by rvnn and  #TAUTHOR_TAG was set to 0, while that without preordering was set to 6. compared to the plain pbsmt without preordering, both bleu and ribes increased significantly with preordering by rvnn', 'and  #TAUTHOR_TAG.', 'these scores were comparable ( statistically insignificant at p', '< 0. 05 ) between rvnn and  #TAUTHOR_TAG. in contrast to the case of pb - smt, nm', '##t without preordering achieved a significantly higher bleu score than nmt models with preordering by rvnn and  #TAUTHOR_TAG. this is', 'the same phenomenon in the chinese - to - japanese translation experiment reported in  #AUTHOR_TAG', '']",7
['and  #TAUTHOR_TAG was'],['and  #TAUTHOR_TAG was'],"['##m sentence pairs. the distortion limit of smt systems trained using preordered sentences', 'by rvnn and  #TAUTHOR_TAG was set to 0, while that without preordering was set to 6. compared']","['setting λ larger than 200 did not contribute to the translation quality. based on these, we further evaluated the rvnn with pos tags and syntactic categories where λ = 200. table 2 shows bleu and ribes scores of the test set on', 'pbsmt and nmt trained on the entire training data of', '1. 8m sentence pairs. the distortion limit of smt systems trained using preordered sentences', 'by rvnn and  #TAUTHOR_TAG was set to 0, while that without preordering was set to 6. compared to the plain pbsmt without preordering, both bleu and ribes increased significantly with preordering by rvnn', 'and  #TAUTHOR_TAG.', 'these scores were comparable ( statistically insignificant at p', '< 0. 05 ) between rvnn and  #TAUTHOR_TAG. in contrast to the case of pb - smt, nm', '##t without preordering achieved a significantly higher bleu score than nmt models with preordering by rvnn and  #TAUTHOR_TAG. this is', 'the same phenomenon in the chinese - to - japanese translation experiment reported in  #AUTHOR_TAG', '']",7
['and  #TAUTHOR_TAG was'],['and  #TAUTHOR_TAG was'],"['##m sentence pairs. the distortion limit of smt systems trained using preordered sentences', 'by rvnn and  #TAUTHOR_TAG was set to 0, while that without preordering was set to 6. compared']","['setting λ larger than 200 did not contribute to the translation quality. based on these, we further evaluated the rvnn with pos tags and syntactic categories where λ = 200. table 2 shows bleu and ribes scores of the test set on', 'pbsmt and nmt trained on the entire training data of', '1. 8m sentence pairs. the distortion limit of smt systems trained using preordered sentences', 'by rvnn and  #TAUTHOR_TAG was set to 0, while that without preordering was set to 6. compared to the plain pbsmt without preordering, both bleu and ribes increased significantly with preordering by rvnn', 'and  #TAUTHOR_TAG.', 'these scores were comparable ( statistically insignificant at p', '< 0. 05 ) between rvnn and  #TAUTHOR_TAG. in contrast to the case of pb - smt, nm', '##t without preordering achieved a significantly higher bleu score than nmt models with preordering by rvnn and  #TAUTHOR_TAG. this is', 'the same phenomenon in the chinese - to - japanese translation experiment reported in  #AUTHOR_TAG', '']",4
"['.', ' #TAUTHOR_TAG adapted the experimental setup of  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG to']","['bert is initially trained.', ' #TAUTHOR_TAG adapted the experimental setup of  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG to']","['.', ' #TAUTHOR_TAG adapted the experimental setup of  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG to']","['general - purpose sentence representations which accurately model sentential semantic content is a current goal of natural language processing research  #AUTHOR_TAG.', 'a prominent and successful approach is to pre - train neural networks to encode sentences into fixed length vectors  #AUTHOR_TAG, with common architecture choices based on recurrent neural networks  #AUTHOR_TAG, convolutional neural networks, or transformers  #AUTHOR_TAG.', 'many core linguistic phenomena that one would like to model in general - purpose sentence representations depend on syntactic structure  #AUTHOR_TAG.', 'despite the fact that none of the aforementioned architectures have explicit syntactic structural representations, there is some evidence that these models can approximate such structure - dependent phenomena under certain conditions  #AUTHOR_TAG mc  #AUTHOR_TAG, in addition to their widespread success in practical tasks.', 'the recently introduced bert model  #AUTHOR_TAG, which is based on transformers, achieves state - of - the - art results on eleven natural language processing tasks.', ""in this work, we assess bert's ability to learn structure - dependent linguistic phenomena of agreement relations."", 'to test whether bert is sensitive to agreement relations, we use the cloze test  #AUTHOR_TAG also called the "" masked language model "" objective ), in which we mask out one of two words in an agreement relation and ask bert to predict the masked word, one of the two tasks on which bert is initially trained.', "" #TAUTHOR_TAG adapted the experimental setup of  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG to use the cloze test to assess bert's sensitivity to number agreement in english subject - verb agreement relations."", 'the results showed that the single - language bert model performed surprisingly well at this task ( above 80 % accuracy in all experiments ), even when there were multiple "" distractors "" in the sentence ( other nouns that differed from the subject in number ).', 'this suggests that bert is actually learning to approximate structure - dependent computation, and not simply relying on flawed heuristics.', 'however, english subject - verb agreement is a rather restricted phenomenon, with the majority of verbs having only two inflected forms and only one morphosyntactic feature ( number ) involved.', 'to what extent does  #TAUTHOR_TAG result hold for subject - verb agreement in other languages, including more morphologically rich ones, as well as for other types of agreement relations?', 'building on  #TAUTHOR_TAG work, we expand the experiment to 26 languages and four types of agreement relations, which include more challenging examples.', 'in section 2, we define what is meant by agreement relations and outline the particular agreement relations under study.', 'section 3 introduces our newly curated cross - linguistic dataset of agreement relations, while section 4 discusses our experimental setup.', 'we report the results of our experiments in section 5.', 'all data and code are available at https :']",0
"['verb agreement in number  #TAUTHOR_TAG', '. in our']","['using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject - verb agreement in number  #TAUTHOR_TAG', '. in our work, we study']","['of syntactic structure in modern neural networks has focussed on subject - verb agreement in number  #TAUTHOR_TAG', '. in our']","['', '( 4 ) and the subject and predicated adjective in ( 5 ) agree for both number and gender. ( 2 ) les cles de la port', ""##e se trouvent sur la table.'the keys to the door are on the table.'' the keys to the door"", ""are broken.'previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject - verb agreement in number  #TAUTHOR_TAG"", '. in our work, we study all four types of agreement relations and all four features discussed above.', ""moreover, previous work using any method to assess bert's knowledge of syntactic structure has focussed exclusively on the single - language english model  #TAUTHOR_TAG. we expand this line of work to 26 languages. not all"", 'languages in our sample exhibit all four types of agreement nor use all four features examined, but', '']",0
"['verb agreement in number  #TAUTHOR_TAG', '. in our']","['using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject - verb agreement in number  #TAUTHOR_TAG', '. in our work, we study']","['of syntactic structure in modern neural networks has focussed on subject - verb agreement in number  #TAUTHOR_TAG', '. in our']","['', '( 4 ) and the subject and predicated adjective in ( 5 ) agree for both number and gender. ( 2 ) les cles de la port', ""##e se trouvent sur la table.'the keys to the door are on the table.'' the keys to the door"", ""are broken.'previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject - verb agreement in number  #TAUTHOR_TAG"", '. in our work, we study all four types of agreement relations and all four features discussed above.', ""moreover, previous work using any method to assess bert's knowledge of syntactic structure has focussed exclusively on the single - language english model  #TAUTHOR_TAG. we expand this line of work to 26 languages. not all"", 'languages in our sample exhibit all four types of agreement nor use all four features examined, but', '']",0
"[' #TAUTHOR_TAG.', 'as in']","[' #TAUTHOR_TAG.', 'as in']","[' #TAUTHOR_TAG.', 'as in']","[""experiment is designed to measure bert's ability to model syntactic structure."", 'our experimental set up is an adaptation of that of  #TAUTHOR_TAG.', 'as in previous work, we mask one word involved in an agreement relation and ask bert to predict it.', '']",0
['by  #TAUTHOR_TAG showed that bert captures english'],['by  #TAUTHOR_TAG showed that bert captures english subject - verb number agreement well despite this'],"['by  #TAUTHOR_TAG showed that bert captures english subject - verb number agreement well despite this lack of explicit structural representation.', 'we replicated this result using a different evaluation methodology']","['linguistic phenomena depend on syntactic structure.', 'yet current state - of - the - art models in language representations, such as bert, do not have explicit syntactic structural representations.', 'previous work by  #TAUTHOR_TAG showed that bert captures english subject - verb number agreement well despite this lack of explicit structural representation.', 'we replicated this result using a different evaluation methodology that addresses shortcomings in the original methodology and expanded the study to 26 languages.', 'our study further broadened existing work by considering the most cross - linguistically common agreement types as well as the most common morphosyntactic features.', 'the main result of this expansion into more languages, types and features is that bert, without explicit syntactic structure, is still able to capture syntax - sensitive agreement patterns well.', 'however, our analysis highlights an important qualification of this result.', ""we showed that bert's ability to model syntaxsensitive agreement relations decreases slightly as the dependency becomes longer range, and as the number of distractors increases."", 'we release our new curated cross - linguistic datasets and code in the hope that it is useful to future research that may probe why this pattern appears.', '']",0
"['.', ' #TAUTHOR_TAG adapted the experimental setup of  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG to']","['bert is initially trained.', ' #TAUTHOR_TAG adapted the experimental setup of  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG to']","['.', ' #TAUTHOR_TAG adapted the experimental setup of  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG to']","['general - purpose sentence representations which accurately model sentential semantic content is a current goal of natural language processing research  #AUTHOR_TAG.', 'a prominent and successful approach is to pre - train neural networks to encode sentences into fixed length vectors  #AUTHOR_TAG, with common architecture choices based on recurrent neural networks  #AUTHOR_TAG, convolutional neural networks, or transformers  #AUTHOR_TAG.', 'many core linguistic phenomena that one would like to model in general - purpose sentence representations depend on syntactic structure  #AUTHOR_TAG.', 'despite the fact that none of the aforementioned architectures have explicit syntactic structural representations, there is some evidence that these models can approximate such structure - dependent phenomena under certain conditions  #AUTHOR_TAG mc  #AUTHOR_TAG, in addition to their widespread success in practical tasks.', 'the recently introduced bert model  #AUTHOR_TAG, which is based on transformers, achieves state - of - the - art results on eleven natural language processing tasks.', ""in this work, we assess bert's ability to learn structure - dependent linguistic phenomena of agreement relations."", 'to test whether bert is sensitive to agreement relations, we use the cloze test  #AUTHOR_TAG also called the "" masked language model "" objective ), in which we mask out one of two words in an agreement relation and ask bert to predict the masked word, one of the two tasks on which bert is initially trained.', "" #TAUTHOR_TAG adapted the experimental setup of  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG to use the cloze test to assess bert's sensitivity to number agreement in english subject - verb agreement relations."", 'the results showed that the single - language bert model performed surprisingly well at this task ( above 80 % accuracy in all experiments ), even when there were multiple "" distractors "" in the sentence ( other nouns that differed from the subject in number ).', 'this suggests that bert is actually learning to approximate structure - dependent computation, and not simply relying on flawed heuristics.', 'however, english subject - verb agreement is a rather restricted phenomenon, with the majority of verbs having only two inflected forms and only one morphosyntactic feature ( number ) involved.', 'to what extent does  #TAUTHOR_TAG result hold for subject - verb agreement in other languages, including more morphologically rich ones, as well as for other types of agreement relations?', 'building on  #TAUTHOR_TAG work, we expand the experiment to 26 languages and four types of agreement relations, which include more challenging examples.', 'in section 2, we define what is meant by agreement relations and outline the particular agreement relations under study.', 'section 3 introduces our newly curated cross - linguistic dataset of agreement relations, while section 4 discusses our experimental setup.', 'we report the results of our experiments in section 5.', 'all data and code are available at https :']",1
"['.', ' #TAUTHOR_TAG adapted the experimental setup of  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG to']","['bert is initially trained.', ' #TAUTHOR_TAG adapted the experimental setup of  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG to']","['.', ' #TAUTHOR_TAG adapted the experimental setup of  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG to']","['general - purpose sentence representations which accurately model sentential semantic content is a current goal of natural language processing research  #AUTHOR_TAG.', 'a prominent and successful approach is to pre - train neural networks to encode sentences into fixed length vectors  #AUTHOR_TAG, with common architecture choices based on recurrent neural networks  #AUTHOR_TAG, convolutional neural networks, or transformers  #AUTHOR_TAG.', 'many core linguistic phenomena that one would like to model in general - purpose sentence representations depend on syntactic structure  #AUTHOR_TAG.', 'despite the fact that none of the aforementioned architectures have explicit syntactic structural representations, there is some evidence that these models can approximate such structure - dependent phenomena under certain conditions  #AUTHOR_TAG mc  #AUTHOR_TAG, in addition to their widespread success in practical tasks.', 'the recently introduced bert model  #AUTHOR_TAG, which is based on transformers, achieves state - of - the - art results on eleven natural language processing tasks.', ""in this work, we assess bert's ability to learn structure - dependent linguistic phenomena of agreement relations."", 'to test whether bert is sensitive to agreement relations, we use the cloze test  #AUTHOR_TAG also called the "" masked language model "" objective ), in which we mask out one of two words in an agreement relation and ask bert to predict the masked word, one of the two tasks on which bert is initially trained.', "" #TAUTHOR_TAG adapted the experimental setup of  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG to use the cloze test to assess bert's sensitivity to number agreement in english subject - verb agreement relations."", 'the results showed that the single - language bert model performed surprisingly well at this task ( above 80 % accuracy in all experiments ), even when there were multiple "" distractors "" in the sentence ( other nouns that differed from the subject in number ).', 'this suggests that bert is actually learning to approximate structure - dependent computation, and not simply relying on flawed heuristics.', 'however, english subject - verb agreement is a rather restricted phenomenon, with the majority of verbs having only two inflected forms and only one morphosyntactic feature ( number ) involved.', 'to what extent does  #TAUTHOR_TAG result hold for subject - verb agreement in other languages, including more morphologically rich ones, as well as for other types of agreement relations?', 'building on  #TAUTHOR_TAG work, we expand the experiment to 26 languages and four types of agreement relations, which include more challenging examples.', 'in section 2, we define what is meant by agreement relations and outline the particular agreement relations under study.', 'section 3 introduces our newly curated cross - linguistic dataset of agreement relations, while section 4 discusses our experimental setup.', 'we report the results of our experiments in section 5.', 'all data and code are available at https :']",6
"['verb agreement in number  #TAUTHOR_TAG', '. in our']","['using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject - verb agreement in number  #TAUTHOR_TAG', '. in our work, we study']","['of syntactic structure in modern neural networks has focussed on subject - verb agreement in number  #TAUTHOR_TAG', '. in our']","['', '( 4 ) and the subject and predicated adjective in ( 5 ) agree for both number and gender. ( 2 ) les cles de la port', ""##e se trouvent sur la table.'the keys to the door are on the table.'' the keys to the door"", ""are broken.'previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject - verb agreement in number  #TAUTHOR_TAG"", '. in our work, we study all four types of agreement relations and all four features discussed above.', ""moreover, previous work using any method to assess bert's knowledge of syntactic structure has focussed exclusively on the single - language english model  #TAUTHOR_TAG. we expand this line of work to 26 languages. not all"", 'languages in our sample exhibit all four types of agreement nor use all four features examined, but', '']",6
"[' #TAUTHOR_TAG.', 'as in']","[' #TAUTHOR_TAG.', 'as in']","[' #TAUTHOR_TAG.', 'as in']","[""experiment is designed to measure bert's ability to model syntactic structure."", 'our experimental set up is an adaptation of that of  #TAUTHOR_TAG.', 'as in previous work, we mask one word involved in an agreement relation and ask bert to predict it.', '']",6
['by  #TAUTHOR_TAG showed that bert captures english'],['by  #TAUTHOR_TAG showed that bert captures english subject - verb number agreement well despite this'],"['by  #TAUTHOR_TAG showed that bert captures english subject - verb number agreement well despite this lack of explicit structural representation.', 'we replicated this result using a different evaluation methodology']","['linguistic phenomena depend on syntactic structure.', 'yet current state - of - the - art models in language representations, such as bert, do not have explicit syntactic structural representations.', 'previous work by  #TAUTHOR_TAG showed that bert captures english subject - verb number agreement well despite this lack of explicit structural representation.', 'we replicated this result using a different evaluation methodology that addresses shortcomings in the original methodology and expanded the study to 26 languages.', 'our study further broadened existing work by considering the most cross - linguistically common agreement types as well as the most common morphosyntactic features.', 'the main result of this expansion into more languages, types and features is that bert, without explicit syntactic structure, is still able to capture syntax - sensitive agreement patterns well.', 'however, our analysis highlights an important qualification of this result.', ""we showed that bert's ability to model syntaxsensitive agreement relations decreases slightly as the dependency becomes longer range, and as the number of distractors increases."", 'we release our new curated cross - linguistic datasets and code in the hope that it is useful to future research that may probe why this pattern appears.', '']",6
"['verb agreement in number  #TAUTHOR_TAG', '. in our']","['using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject - verb agreement in number  #TAUTHOR_TAG', '. in our work, we study']","['of syntactic structure in modern neural networks has focussed on subject - verb agreement in number  #TAUTHOR_TAG', '. in our']","['', '( 4 ) and the subject and predicated adjective in ( 5 ) agree for both number and gender. ( 2 ) les cles de la port', ""##e se trouvent sur la table.'the keys to the door are on the table.'' the keys to the door"", ""are broken.'previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject - verb agreement in number  #TAUTHOR_TAG"", '. in our work, we study all four types of agreement relations and all four features discussed above.', ""moreover, previous work using any method to assess bert's knowledge of syntactic structure has focussed exclusively on the single - language english model  #TAUTHOR_TAG. we expand this line of work to 26 languages. not all"", 'languages in our sample exhibit all four types of agreement nor use all four features examined, but', '']",5
"[' #TAUTHOR_TAG.', 'as in']","[' #TAUTHOR_TAG.', 'as in']","[' #TAUTHOR_TAG.', 'as in']","[""experiment is designed to measure bert's ability to model syntactic structure."", 'our experimental set up is an adaptation of that of  #TAUTHOR_TAG.', 'as in previous work, we mask one word involved in an agreement relation and ask bert to predict it.', '']",5
"[' #TAUTHOR_TAG.', 'as in']","[' #TAUTHOR_TAG.', 'as in']","[' #TAUTHOR_TAG.', 'as in']","[""experiment is designed to measure bert's ability to model syntactic structure."", 'our experimental set up is an adaptation of that of  #TAUTHOR_TAG.', 'as in previous work, we mask one word involved in an agreement relation and ask bert to predict it.', '']",5
"[' #TAUTHOR_TAG.', 'as in']","[' #TAUTHOR_TAG.', 'as in']","[' #TAUTHOR_TAG.', 'as in']","[""experiment is designed to measure bert's ability to model syntactic structure."", 'our experimental set up is an adaptation of that of  #TAUTHOR_TAG.', 'as in previous work, we mask one word involved in an agreement relation and ask bert to predict it.', '']",4
"[' #TAUTHOR_TAG.', 'as in']","[' #TAUTHOR_TAG.', 'as in']","[' #TAUTHOR_TAG.', 'as in']","[""experiment is designed to measure bert's ability to model syntactic structure."", 'our experimental set up is an adaptation of that of  #TAUTHOR_TAG.', 'as in previous work, we mask one word involved in an agreement relation and ask bert to predict it.', '']",4
['by  #TAUTHOR_TAG showed that bert captures english'],['by  #TAUTHOR_TAG showed that bert captures english subject - verb number agreement well despite this'],"['by  #TAUTHOR_TAG showed that bert captures english subject - verb number agreement well despite this lack of explicit structural representation.', 'we replicated this result using a different evaluation methodology']","['linguistic phenomena depend on syntactic structure.', 'yet current state - of - the - art models in language representations, such as bert, do not have explicit syntactic structural representations.', 'previous work by  #TAUTHOR_TAG showed that bert captures english subject - verb number agreement well despite this lack of explicit structural representation.', 'we replicated this result using a different evaluation methodology that addresses shortcomings in the original methodology and expanded the study to 26 languages.', 'our study further broadened existing work by considering the most cross - linguistically common agreement types as well as the most common morphosyntactic features.', 'the main result of this expansion into more languages, types and features is that bert, without explicit syntactic structure, is still able to capture syntax - sensitive agreement patterns well.', 'however, our analysis highlights an important qualification of this result.', ""we showed that bert's ability to model syntaxsensitive agreement relations decreases slightly as the dependency becomes longer range, and as the number of distractors increases."", 'we release our new curated cross - linguistic datasets and code in the hope that it is useful to future research that may probe why this pattern appears.', '']",4
"['. 8 similar to  #TAUTHOR_TAG and use a batch size', 'of 50. other practices for lm training were the same as [ 22 ] and [ 21 ] for transformer']","['. 8 similar to  #TAUTHOR_TAG and use a batch size', 'of 50. other practices for lm training were the same as [ 22 ] and [ 21 ] for transformer']","['. 7 and β2 = 0. 8 similar to  #TAUTHOR_TAG and use a batch size', 'of 50. other practices for lm training were the same as [ 22 ] and [ 21 ] for transformer']","['text, making the model non - differentiable hence, we update parameters for the generator', 'model with policy gradients as described in yu [ 16 ]. we utilize awd - lstm [ 21 ] and transformerxl [ 22 ] based language models. for model hyperparameters please to refer to supplementary', 'section table 2. we use adam optimizer [ 23 ] with β1 = 0. 7 and β2 = 0. 8 similar to  #TAUTHOR_TAG and use a batch size', 'of 50. other practices for lm training were the same as [ 22 ] and [ 21 ] for transformer - xl and awd - lstm', 'respectively. we refer to our proposed gan as creative - gan and compare it to a baseline ( a language model equivalent to our pre - trained', 'generator ) and a gumbelgan model [ 15 ] across all proposed datasets. we use three creative', 'english datasets with distinct linguistic characteristics : ( 1 ) a corpus of 740 classical and contemporary english poems, ( 2 ) a corpus of 14950 metaphor sentences retrieved from a metaphor database website 1 and ( 3 ) a', 'corpus of 1500 song lyrics ranging across genres. the mix of linguistic styles within this corpus offers the potential for', 'interesting variation during the generation phase. we use the same pre - processing as in earlier work  #TAUTHOR_TAG 24 ]. we reserve 10 % of our data for test set and another 10 % for our validation set. we first pre -', ""train our generator on the gutenberg dataset [ 25 ] for 20 epochs and then fine - tune  #TAUTHOR_TAG them to our target datasets with a language modeling objective. the discriminator's encoder is initialized to the same weights as"", 'our fine - tuned language model. once we have our fine - tuned encoders for each target dataset, we train in an adversarial manner.', 'the discriminator objective here is to score the quality of the creative text. the discriminator is trained for 3 iterations for every iteration of the generator, a practice seen in previous work [ 26 ]', '. creative - gan relies on using the reward from the discriminator [ 13, 16 ] for backpropagation. we follow a similar training procedure for gumbelgan.', 'outputs are generated through sampling over a multinomial distribution for all methods, instead of arg', '##max on the log - likelihood probabilities, as sampling has shown to produce better output quality [ 5 ]. please refer to supplementary section table 3 for training parameters of each dataset and table 2 for hyperparameters', 'of each encoder. we pick these values after experimentation with our', 'validation set. training and output generation code can be found online 2']",5
"['. 8 similar to  #TAUTHOR_TAG and use a batch size', 'of 50. other practices for lm training were the same as [ 22 ] and [ 21 ] for transformer']","['. 8 similar to  #TAUTHOR_TAG and use a batch size', 'of 50. other practices for lm training were the same as [ 22 ] and [ 21 ] for transformer']","['. 7 and β2 = 0. 8 similar to  #TAUTHOR_TAG and use a batch size', 'of 50. other practices for lm training were the same as [ 22 ] and [ 21 ] for transformer']","['text, making the model non - differentiable hence, we update parameters for the generator', 'model with policy gradients as described in yu [ 16 ]. we utilize awd - lstm [ 21 ] and transformerxl [ 22 ] based language models. for model hyperparameters please to refer to supplementary', 'section table 2. we use adam optimizer [ 23 ] with β1 = 0. 7 and β2 = 0. 8 similar to  #TAUTHOR_TAG and use a batch size', 'of 50. other practices for lm training were the same as [ 22 ] and [ 21 ] for transformer - xl and awd - lstm', 'respectively. we refer to our proposed gan as creative - gan and compare it to a baseline ( a language model equivalent to our pre - trained', 'generator ) and a gumbelgan model [ 15 ] across all proposed datasets. we use three creative', 'english datasets with distinct linguistic characteristics : ( 1 ) a corpus of 740 classical and contemporary english poems, ( 2 ) a corpus of 14950 metaphor sentences retrieved from a metaphor database website 1 and ( 3 ) a', 'corpus of 1500 song lyrics ranging across genres. the mix of linguistic styles within this corpus offers the potential for', 'interesting variation during the generation phase. we use the same pre - processing as in earlier work  #TAUTHOR_TAG 24 ]. we reserve 10 % of our data for test set and another 10 % for our validation set. we first pre -', ""train our generator on the gutenberg dataset [ 25 ] for 20 epochs and then fine - tune  #TAUTHOR_TAG them to our target datasets with a language modeling objective. the discriminator's encoder is initialized to the same weights as"", 'our fine - tuned language model. once we have our fine - tuned encoders for each target dataset, we train in an adversarial manner.', 'the discriminator objective here is to score the quality of the creative text. the discriminator is trained for 3 iterations for every iteration of the generator, a practice seen in previous work [ 26 ]', '. creative - gan relies on using the reward from the discriminator [ 13, 16 ] for backpropagation. we follow a similar training procedure for gumbelgan.', 'outputs are generated through sampling over a multinomial distribution for all methods, instead of arg', '##max on the log - likelihood probabilities, as sampling has shown to produce better output quality [ 5 ]. please refer to supplementary section table 3 for training parameters of each dataset and table 2 for hyperparameters', 'of each encoder. we pick these values after experimentation with our', 'validation set. training and output generation code can be found online 2']",5
"['. 8 similar to  #TAUTHOR_TAG and use a batch size', 'of 50. other practices for lm training were the same as [ 22 ] and [ 21 ] for transformer']","['. 8 similar to  #TAUTHOR_TAG and use a batch size', 'of 50. other practices for lm training were the same as [ 22 ] and [ 21 ] for transformer']","['. 7 and β2 = 0. 8 similar to  #TAUTHOR_TAG and use a batch size', 'of 50. other practices for lm training were the same as [ 22 ] and [ 21 ] for transformer']","['text, making the model non - differentiable hence, we update parameters for the generator', 'model with policy gradients as described in yu [ 16 ]. we utilize awd - lstm [ 21 ] and transformerxl [ 22 ] based language models. for model hyperparameters please to refer to supplementary', 'section table 2. we use adam optimizer [ 23 ] with β1 = 0. 7 and β2 = 0. 8 similar to  #TAUTHOR_TAG and use a batch size', 'of 50. other practices for lm training were the same as [ 22 ] and [ 21 ] for transformer - xl and awd - lstm', 'respectively. we refer to our proposed gan as creative - gan and compare it to a baseline ( a language model equivalent to our pre - trained', 'generator ) and a gumbelgan model [ 15 ] across all proposed datasets. we use three creative', 'english datasets with distinct linguistic characteristics : ( 1 ) a corpus of 740 classical and contemporary english poems, ( 2 ) a corpus of 14950 metaphor sentences retrieved from a metaphor database website 1 and ( 3 ) a', 'corpus of 1500 song lyrics ranging across genres. the mix of linguistic styles within this corpus offers the potential for', 'interesting variation during the generation phase. we use the same pre - processing as in earlier work  #TAUTHOR_TAG 24 ]. we reserve 10 % of our data for test set and another 10 % for our validation set. we first pre -', ""train our generator on the gutenberg dataset [ 25 ] for 20 epochs and then fine - tune  #TAUTHOR_TAG them to our target datasets with a language modeling objective. the discriminator's encoder is initialized to the same weights as"", 'our fine - tuned language model. once we have our fine - tuned encoders for each target dataset, we train in an adversarial manner.', 'the discriminator objective here is to score the quality of the creative text. the discriminator is trained for 3 iterations for every iteration of the generator, a practice seen in previous work [ 26 ]', '. creative - gan relies on using the reward from the discriminator [ 13, 16 ] for backpropagation. we follow a similar training procedure for gumbelgan.', 'outputs are generated through sampling over a multinomial distribution for all methods, instead of arg', '##max on the log - likelihood probabilities, as sampling has shown to produce better output quality [ 5 ]. please refer to supplementary section table 3 for training parameters of each dataset and table 2 for hyperparameters', 'of each encoder. we pick these values after experimentation with our', 'validation set. training and output generation code can be found online 2']",5
"['. 8 similar to  #TAUTHOR_TAG and use a batch size', 'of 50. other practices for lm training were the same as [ 22 ] and [ 21 ] for transformer']","['. 8 similar to  #TAUTHOR_TAG and use a batch size', 'of 50. other practices for lm training were the same as [ 22 ] and [ 21 ] for transformer']","['. 7 and β2 = 0. 8 similar to  #TAUTHOR_TAG and use a batch size', 'of 50. other practices for lm training were the same as [ 22 ] and [ 21 ] for transformer']","['text, making the model non - differentiable hence, we update parameters for the generator', 'model with policy gradients as described in yu [ 16 ]. we utilize awd - lstm [ 21 ] and transformerxl [ 22 ] based language models. for model hyperparameters please to refer to supplementary', 'section table 2. we use adam optimizer [ 23 ] with β1 = 0. 7 and β2 = 0. 8 similar to  #TAUTHOR_TAG and use a batch size', 'of 50. other practices for lm training were the same as [ 22 ] and [ 21 ] for transformer - xl and awd - lstm', 'respectively. we refer to our proposed gan as creative - gan and compare it to a baseline ( a language model equivalent to our pre - trained', 'generator ) and a gumbelgan model [ 15 ] across all proposed datasets. we use three creative', 'english datasets with distinct linguistic characteristics : ( 1 ) a corpus of 740 classical and contemporary english poems, ( 2 ) a corpus of 14950 metaphor sentences retrieved from a metaphor database website 1 and ( 3 ) a', 'corpus of 1500 song lyrics ranging across genres. the mix of linguistic styles within this corpus offers the potential for', 'interesting variation during the generation phase. we use the same pre - processing as in earlier work  #TAUTHOR_TAG 24 ]. we reserve 10 % of our data for test set and another 10 % for our validation set. we first pre -', ""train our generator on the gutenberg dataset [ 25 ] for 20 epochs and then fine - tune  #TAUTHOR_TAG them to our target datasets with a language modeling objective. the discriminator's encoder is initialized to the same weights as"", 'our fine - tuned language model. once we have our fine - tuned encoders for each target dataset, we train in an adversarial manner.', 'the discriminator objective here is to score the quality of the creative text. the discriminator is trained for 3 iterations for every iteration of the generator, a practice seen in previous work [ 26 ]', '. creative - gan relies on using the reward from the discriminator [ 13, 16 ] for backpropagation. we follow a similar training procedure for gumbelgan.', 'outputs are generated through sampling over a multinomial distribution for all methods, instead of arg', '##max on the log - likelihood probabilities, as sampling has shown to produce better output quality [ 5 ]. please refer to supplementary section table 3 for training parameters of each dataset and table 2 for hyperparameters', 'of each encoder. we pick these values after experimentation with our', 'validation set. training and output generation code can be found online 2']",3
['is a powerful architecture that'],['is a powerful architecture that'],['is a powerful architecture that'],"['is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction.', 'at the core of the  #TAUTHOR_TAG is the attention mechanism, which concurrently processes all inputs in the streams.', 'in this paper, we present a new formulation of attention via the lens of the kernel.', 'to be more precise, we realize that the attention can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs.', 'this new formulation gives us a better way to understand individual components of the  #TAUTHOR_TAG attention, such as the better way to integrate the positional embedding.', ""another important advantage of our kernel - based formulation is that it paves the way to a larger space of composing  #TAUTHOR_TAG's attention."", 'as an example, we propose a new variant of  #TAUTHOR_TAG attention which models the input as a product of symmetric kernels.', 'this approach achieves competitive performance to the current state of the art model with less computation.', 'in our experiments, we empirically study different kernel construction strategies on two widely used tasks : neural machine translation and sequence prediction']",0
['is a powerful architecture that'],['is a powerful architecture that'],['is a powerful architecture that'],"['is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction.', 'at the core of the  #TAUTHOR_TAG is the attention mechanism, which concurrently processes all inputs in the streams.', 'in this paper, we present a new formulation of attention via the lens of the kernel.', 'to be more precise, we realize that the attention can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs.', 'this new formulation gives us a better way to understand individual components of the  #TAUTHOR_TAG attention, such as the better way to integrate the positional embedding.', ""another important advantage of our kernel - based formulation is that it paves the way to a larger space of composing  #TAUTHOR_TAG's attention."", 'as an example, we propose a new variant of  #TAUTHOR_TAG attention which models the input as a product of symmetric kernels.', 'this approach achieves competitive performance to the current state of the art model with less computation.', 'in our experiments, we empirically study different kernel construction strategies on two widely used tasks : neural machine translation and sequence prediction']",0
"[', language understanding']","[', language understanding']","[', language understanding']","[', language understanding  #AUTHOR_TAG, sequence prediction  #AUTHOR_TAG, image generation  #AUTHOR_TAG, video activity classification  #AUTHOR_TAG, music generation  #AUTHOR_TAG a ), and multimodal sentiment analysis  #AUTHOR_TAG a ).', 'instead of performing recurrence ( e. g., rnn ) or convolution ( e. g., tcn ) over the sequences,  #TAUTHOR_TAG is a feed - forward model that concurrently processes the entire sequence.', 'at the core of the  #TAUTHOR_TAG is its attention mechanism, which is proposed to integrate the dependencies between the inputs.', 'there are up to three types of attention within the full  #TAUTHOR_TAG : 1 ) encoder self - attention considers the source sentence as input, generating a sequence of encoded representations, where each encoded token has a global dependency with other tokens in the input sequence.', '']",0
"[', language understanding']","[', language understanding']","[', language understanding']","[', language understanding  #AUTHOR_TAG, sequence prediction  #AUTHOR_TAG, image generation  #AUTHOR_TAG, video activity classification  #AUTHOR_TAG, music generation  #AUTHOR_TAG a ), and multimodal sentiment analysis  #AUTHOR_TAG a ).', 'instead of performing recurrence ( e. g., rnn ) or convolution ( e. g., tcn ) over the sequences,  #TAUTHOR_TAG is a feed - forward model that concurrently processes the entire sequence.', 'at the core of the  #TAUTHOR_TAG is its attention mechanism, which is proposed to integrate the dependencies between the inputs.', 'there are up to three types of attention within the full  #TAUTHOR_TAG : 1 ) encoder self - attention considers the source sentence as input, generating a sequence of encoded representations, where each encoded token has a global dependency with other tokens in the input sequence.', '']",0
"[', language understanding']","[', language understanding']","[', language understanding']","[', language understanding  #AUTHOR_TAG, sequence prediction  #AUTHOR_TAG, image generation  #AUTHOR_TAG, video activity classification  #AUTHOR_TAG, music generation  #AUTHOR_TAG a ), and multimodal sentiment analysis  #AUTHOR_TAG a ).', 'instead of performing recurrence ( e. g., rnn ) or convolution ( e. g., tcn ) over the sequences,  #TAUTHOR_TAG is a feed - forward model that concurrently processes the entire sequence.', 'at the core of the  #TAUTHOR_TAG is its attention mechanism, which is proposed to integrate the dependencies between the inputs.', 'there are up to three types of attention within the full  #TAUTHOR_TAG : 1 ) encoder self - attention considers the source sentence as input, generating a sequence of encoded representations, where each encoded token has a global dependency with other tokens in the input sequence.', '']",0
"[', language understanding']","[', language understanding']","[', language understanding']","[', language understanding  #AUTHOR_TAG, sequence prediction  #AUTHOR_TAG, image generation  #AUTHOR_TAG, video activity classification  #AUTHOR_TAG, music generation  #AUTHOR_TAG a ), and multimodal sentiment analysis  #AUTHOR_TAG a ).', 'instead of performing recurrence ( e. g., rnn ) or convolution ( e. g., tcn ) over the sequences,  #TAUTHOR_TAG is a feed - forward model that concurrently processes the entire sequence.', 'at the core of the  #TAUTHOR_TAG is its attention mechanism, which is proposed to integrate the dependencies between the inputs.', 'there are up to three types of attention within the full  #TAUTHOR_TAG : 1 ) encoder self - attention considers the source sentence as input, generating a sequence of encoded representations, where each encoded token has a global dependency with other tokens in the input sequence.', '']",0
"[', language understanding']","[', language understanding']","[', language understanding']","[', language understanding  #AUTHOR_TAG, sequence prediction  #AUTHOR_TAG, image generation  #AUTHOR_TAG, video activity classification  #AUTHOR_TAG, music generation  #AUTHOR_TAG a ), and multimodal sentiment analysis  #AUTHOR_TAG a ).', 'instead of performing recurrence ( e. g., rnn ) or convolution ( e. g., tcn ) over the sequences,  #TAUTHOR_TAG is a feed - forward model that concurrently processes the entire sequence.', 'at the core of the  #TAUTHOR_TAG is its attention mechanism, which is proposed to integrate the dependencies between the inputs.', 'there are up to three types of attention within the full  #TAUTHOR_TAG : 1 ) encoder self - attention considers the source sentence as input, generating a sequence of encoded representations, where each encoded token has a global dependency with other tokens in the input sequence.', '']",0
"[', language understanding']","[', language understanding']","[', language understanding']","[', language understanding  #AUTHOR_TAG, sequence prediction  #AUTHOR_TAG, image generation  #AUTHOR_TAG, video activity classification  #AUTHOR_TAG, music generation  #AUTHOR_TAG a ), and multimodal sentiment analysis  #AUTHOR_TAG a ).', 'instead of performing recurrence ( e. g., rnn ) or convolution ( e. g., tcn ) over the sequences,  #TAUTHOR_TAG is a feed - forward model that concurrently processes the entire sequence.', 'at the core of the  #TAUTHOR_TAG is its attention mechanism, which is proposed to integrate the dependencies between the inputs.', 'there are up to three types of attention within the full  #TAUTHOR_TAG : 1 ) encoder self - attention considers the source sentence as input, generating a sequence of encoded representations, where each encoded token has a global dependency with other tokens in the input sequence.', '']",0
"[', language understanding']","[', language understanding']","[', language understanding']","[', language understanding  #AUTHOR_TAG, sequence prediction  #AUTHOR_TAG, image generation  #AUTHOR_TAG, video activity classification  #AUTHOR_TAG, music generation  #AUTHOR_TAG a ), and multimodal sentiment analysis  #AUTHOR_TAG a ).', 'instead of performing recurrence ( e. g., rnn ) or convolution ( e. g., tcn ) over the sequences,  #TAUTHOR_TAG is a feed - forward model that concurrently processes the entire sequence.', 'at the core of the  #TAUTHOR_TAG is its attention mechanism, which is proposed to integrate the dependencies between the inputs.', 'there are up to three types of attention within the full  #TAUTHOR_TAG : 1 ) encoder self - attention considers the source sentence as input, generating a sequence of encoded representations, where each encoded token has a global dependency with other tokens in the input sequence.', '']",0
"['of attention in  #TAUTHOR_TAG via the lens of kernel.', 'the inspiration for connecting the kernel  #AUTHOR_TAG and attention instantiates from']","['of attention in  #TAUTHOR_TAG via the lens of kernel.', 'the inspiration for connecting the kernel  #AUTHOR_TAG and attention instantiates from']","['of attention in  #TAUTHOR_TAG via the lens of kernel.', 'the inspiration for connecting the kernel  #AUTHOR_TAG and attention instantiates from the observation : both operations concurrently processes all inputs and calculate the similarity between the inputs.', 'we']","['section aims at providing an understanding of attention in  #TAUTHOR_TAG via the lens of kernel.', 'the inspiration for connecting the kernel  #AUTHOR_TAG and attention instantiates from the observation : both operations concurrently processes all inputs and calculate the similarity between the inputs.', '']",0
"['word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be']","['it positional embedding ). note that f i can be the word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be']","['word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be learned during back - propagation  #AUTHOR_TAG. the feature vector are defined over a joint space x [UNK] = ( f × t ). the resulting permutationinvariant set is : followed the definition by  #TAUTHOR_TAG, we use queries', '( q ) / keys ( k ) / values ( v ) to represent the inputs for']","['x 1, x 2, [UNK], x t ] defines each element as x i = ( f i, t i ) with f i ∈', 'f being the nontemporal feature at time i and t i ∈ t as an temporal feature ( or we', 'called it positional embedding ). note that f i can be the word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be learned during back - propagation  #AUTHOR_TAG. the feature vector are defined over a joint space x [UNK] = ( f × t ). the resulting permutationinvariant set is : followed the definition by  #TAUTHOR_TAG, we use queries', '( q ) / keys ( k ) / values ( v ) to represent the inputs for the attention. to be more precise, x { q k v } is used for denoting a query / key / value', '']",0
"['word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be']","['it positional embedding ). note that f i can be the word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be']","['word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be learned during back - propagation  #AUTHOR_TAG. the feature vector are defined over a joint space x [UNK] = ( f × t ). the resulting permutationinvariant set is : followed the definition by  #TAUTHOR_TAG, we use queries', '( q ) / keys ( k ) / values ( v ) to represent the inputs for']","['x 1, x 2, [UNK], x t ] defines each element as x i = ( f i, t i ) with f i ∈', 'f being the nontemporal feature at time i and t i ∈ t as an temporal feature ( or we', 'called it positional embedding ). note that f i can be the word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be learned during back - propagation  #AUTHOR_TAG. the feature vector are defined over a joint space x [UNK] = ( f × t ). the resulting permutationinvariant set is : followed the definition by  #TAUTHOR_TAG, we use queries', '( q ) / keys ( k ) / values ( v ) to represent the inputs for the attention. to be more precise, x { q k v } is used for denoting a query / key / value', '']",0
"['word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be']","['it positional embedding ). note that f i can be the word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be']","['word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be learned during back - propagation  #AUTHOR_TAG. the feature vector are defined over a joint space x [UNK] = ( f × t ). the resulting permutationinvariant set is : followed the definition by  #TAUTHOR_TAG, we use queries', '( q ) / keys ( k ) / values ( v ) to represent the inputs for']","['x 1, x 2, [UNK], x t ] defines each element as x i = ( f i, t i ) with f i ∈', 'f being the nontemporal feature at time i and t i ∈ t as an temporal feature ( or we', 'called it positional embedding ). note that f i can be the word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be learned during back - propagation  #AUTHOR_TAG. the feature vector are defined over a joint space x [UNK] = ( f × t ). the resulting permutationinvariant set is : followed the definition by  #TAUTHOR_TAG, we use queries', '( q ) / keys ( k ) / values ( v ) to represent the inputs for the attention. to be more precise, x { q k v } is used for denoting a query / key / value', '']",0
"['word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be']","['it positional embedding ). note that f i can be the word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be']","['word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be learned during back - propagation  #AUTHOR_TAG. the feature vector are defined over a joint space x [UNK] = ( f × t ). the resulting permutationinvariant set is : followed the definition by  #TAUTHOR_TAG, we use queries', '( q ) / keys ( k ) / values ( v ) to represent the inputs for']","['x 1, x 2, [UNK], x t ] defines each element as x i = ( f i, t i ) with f i ∈', 'f being the nontemporal feature at time i and t i ∈ t as an temporal feature ( or we', 'called it positional embedding ). note that f i can be the word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be learned during back - propagation  #AUTHOR_TAG. the feature vector are defined over a joint space x [UNK] = ( f × t ). the resulting permutationinvariant set is : followed the definition by  #TAUTHOR_TAG, we use queries', '( q ) / keys ( k ) / values ( v ) to represent the inputs for the attention. to be more precise, x { q k v } is used for denoting a query / key / value', '']",0
"['word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be']","['it positional embedding ). note that f i can be the word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be']","['word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be learned during back - propagation  #AUTHOR_TAG. the feature vector are defined over a joint space x [UNK] = ( f × t ). the resulting permutationinvariant set is : followed the definition by  #TAUTHOR_TAG, we use queries', '( q ) / keys ( k ) / values ( v ) to represent the inputs for']","['x 1, x 2, [UNK], x t ] defines each element as x i = ( f i, t i ) with f i ∈', 'f being the nontemporal feature at time i and t i ∈ t as an temporal feature ( or we', 'called it positional embedding ). note that f i can be the word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be learned during back - propagation  #AUTHOR_TAG. the feature vector are defined over a joint space x [UNK] = ( f × t ). the resulting permutationinvariant set is : followed the definition by  #TAUTHOR_TAG, we use queries', '( q ) / keys ( k ) / values ( v ) to represent the inputs for the attention. to be more precise, x { q k v } is used for denoting a query / key / value', '']",0
"['word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be']","['it positional embedding ). note that f i can be the word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be']","['word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be learned during back - propagation  #AUTHOR_TAG. the feature vector are defined over a joint space x [UNK] = ( f × t ). the resulting permutationinvariant set is : followed the definition by  #TAUTHOR_TAG, we use queries', '( q ) / keys ( k ) / values ( v ) to represent the inputs for']","['x 1, x 2, [UNK], x t ] defines each element as x i = ( f i, t i ) with f i ∈', 'f being the nontemporal feature at time i and t i ∈ t as an temporal feature ( or we', 'called it positional embedding ). note that f i can be the word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be learned during back - propagation  #AUTHOR_TAG. the feature vector are defined over a joint space x [UNK] = ( f × t ). the resulting permutationinvariant set is : followed the definition by  #TAUTHOR_TAG, we use queries', '( q ) / keys ( k ) / values ( v ) to represent the inputs for the attention. to be more precise, x { q k v } is used for denoting a query / key / value', '']",0
"['m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we']","['a set with its elements that operate with', '( or are connected / visible to ) x', 'q. the filtering function m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we']","['', 'q. the filtering function m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we re - represent eq. ( 1 ) into the following definition', '., and a value']","['returns a set with its elements that operate with', '( or are connected / visible to ) x', 'q. the filtering function m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we re - represent eq. ( 1 ) into the following definition', '., and a value function v ( ⋅ ) [UNK] x → y, the attention function taking the input of a query feature x q ∈ x is defined as the definition 1 is a class of linear smoothers', ' #AUTHOR_TAG with kernel smoothing : where v ( x k ) outputs the "" values "" and is a probability function depends on k and n when k ( ⋅, ⋅ ) is always positive. in the prior work  #AUTHOR_TAG note', 'that the kernel form k ( x q, x k ) in the original  #TAUTHOR_TAG is a asymmetric exponential kernel with additional', 'mapping w q and w k  #AUTHOR_TAG 2. the new formulation defines a larger space for compos', '##ing attention by manipulating its individual components, and at the same time it is able to categorize different variants of attention in prior work  #AUTHOR_TAG b ;  #AUTHOR_TAG a', '). in the following, we study these components by dissecting eq.', '']",0
"['m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we']","['a set with its elements that operate with', '( or are connected / visible to ) x', 'q. the filtering function m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we']","['', 'q. the filtering function m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we re - represent eq. ( 1 ) into the following definition', '., and a value']","['returns a set with its elements that operate with', '( or are connected / visible to ) x', 'q. the filtering function m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we re - represent eq. ( 1 ) into the following definition', '., and a value function v ( ⋅ ) [UNK] x → y, the attention function taking the input of a query feature x q ∈ x is defined as the definition 1 is a class of linear smoothers', ' #AUTHOR_TAG with kernel smoothing : where v ( x k ) outputs the "" values "" and is a probability function depends on k and n when k ( ⋅, ⋅ ) is always positive. in the prior work  #AUTHOR_TAG note', 'that the kernel form k ( x q, x k ) in the original  #TAUTHOR_TAG is a asymmetric exponential kernel with additional', 'mapping w q and w k  #AUTHOR_TAG 2. the new formulation defines a larger space for compos', '##ing attention by manipulating its individual components, and at the same time it is able to categorize different variants of attention in prior work  #AUTHOR_TAG b ;  #AUTHOR_TAG a', '). in the following, we study these components by dissecting eq.', '']",0
"['m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we']","['a set with its elements that operate with', '( or are connected / visible to ) x', 'q. the filtering function m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we']","['', 'q. the filtering function m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we re - represent eq. ( 1 ) into the following definition', '., and a value']","['returns a set with its elements that operate with', '( or are connected / visible to ) x', 'q. the filtering function m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we re - represent eq. ( 1 ) into the following definition', '., and a value function v ( ⋅ ) [UNK] x → y, the attention function taking the input of a query feature x q ∈ x is defined as the definition 1 is a class of linear smoothers', ' #AUTHOR_TAG with kernel smoothing : where v ( x k ) outputs the "" values "" and is a probability function depends on k and n when k ( ⋅, ⋅ ) is always positive. in the prior work  #AUTHOR_TAG note', 'that the kernel form k ( x q, x k ) in the original  #TAUTHOR_TAG is a asymmetric exponential kernel with additional', 'mapping w q and w k  #AUTHOR_TAG 2. the new formulation defines a larger space for compos', '##ing attention by manipulating its individual components, and at the same time it is able to categorize different variants of attention in prior work  #AUTHOR_TAG b ;  #AUTHOR_TAG a', '). in the following, we study these components by dissecting eq.', '']",0
"['m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we']","['a set with its elements that operate with', '( or are connected / visible to ) x', 'q. the filtering function m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we']","['', 'q. the filtering function m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we re - represent eq. ( 1 ) into the following definition', '., and a value']","['returns a set with its elements that operate with', '( or are connected / visible to ) x', 'q. the filtering function m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we re - represent eq. ( 1 ) into the following definition', '., and a value function v ( ⋅ ) [UNK] x → y, the attention function taking the input of a query feature x q ∈ x is defined as the definition 1 is a class of linear smoothers', ' #AUTHOR_TAG with kernel smoothing : where v ( x k ) outputs the "" values "" and is a probability function depends on k and n when k ( ⋅, ⋅ ) is always positive. in the prior work  #AUTHOR_TAG note', 'that the kernel form k ( x q, x k ) in the original  #TAUTHOR_TAG is a asymmetric exponential kernel with additional', 'mapping w q and w k  #AUTHOR_TAG 2. the new formulation defines a larger space for compos', '##ing attention by manipulating its individual components, and at the same time it is able to categorize different variants of attention in prior work  #AUTHOR_TAG b ;  #AUTHOR_TAG a', '). in the following, we study these components by dissecting eq.', '']",0
"['( f × t ) has distinct design in variants of  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'since now the kernel feature']","['( f × t ) has distinct design in variants of  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'since now the kernel feature']","['( ⋅, ⋅ ) the kernel construction on x = ( f × t ) has distinct design in variants of  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'since now the kernel feature space considers a joint space, we will first discuss the kernel construction on']","['embedding k ( ⋅, ⋅ ) the kernel construction on x = ( f × t ) has distinct design in variants of  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'since now the kernel feature space considers a joint space, we will first discuss the kernel construction on f ( the non - positional feature space ) and then discuss how different variants integrate the positional embedding ( with the positional feature space t ) into the kernel.', 'kernel construction on f. all the work considered the scaled asymmetric exponential kernel with the mapping w q and w k  #AUTHOR_TAG for non - positional features f q and f k :', 'note that the usage of asymmetric kernel is also commonly used in various machine learning tasks  #AUTHOR_TAG, where they observed the kernel form can be flexible and even non - valid ( i. e., a kernel that is not symmetric and positive semi - definite ).', 'in section 3, we show that symmetric design of the kernel has similar performance for various sequence learning tasks, and we also examine different kernel choices ( i. e., linear, polynomial, and rbf kernel ).', 'kernel construction on x = ( f × t ).', 'the designs for integrating the positional embedding t q and t k are listed in the following.', '( i ) absolute positional embedding  #TAUTHOR_TAG, each t i is represented by a vector with each dimension being sine or cosine functions.', 'for learned positional embedding  #AUTHOR_TAG, each t i is a learned parameter and is fixed for the same position for different sequences.', 'these works defines the feature space as the direct sum of its temporal and non - temporal space : x = f ⊕ t.', 'via the lens of kernel, the kernel similarity is defined as', '( ii ) relative positional embedding in  #TAUTHOR_TAG - xl  #AUTHOR_TAG : t represents the indicator of the position in the sequence, and the kernel is chosen to be asymmetric of mixing sine and cosine functions :', 'with k fq t q, t k being an asymmetric kernel with coefficients inferred by f q : log k fq t q, t k = ∑ ( iii ) relative positional embedding of  #AUTHOR_TAG and music  #TAUTHOR_TAG  #AUTHOR_TAG b ) : t ⋅ represents the indicator of the position in the sequence, and the kernel is modified to be indexed by a look - up table :', 'where l tq−t k, fq = exp ( f q w q a tq−t k ) with a ⋅ being a learnable matrix having matrix width to be the length of the sequence.', 'we refer readers to  #AUTHOR_TAG for more details.', ' #AUTHOR_TAG showed that the way to integrate positional embedding is better through eq. ( 5 ) than through eq. ( 6 ) and is better through']",0
"['( f × t ) has distinct design in variants of  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'since now the kernel feature']","['( f × t ) has distinct design in variants of  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'since now the kernel feature']","['( ⋅, ⋅ ) the kernel construction on x = ( f × t ) has distinct design in variants of  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'since now the kernel feature space considers a joint space, we will first discuss the kernel construction on']","['embedding k ( ⋅, ⋅ ) the kernel construction on x = ( f × t ) has distinct design in variants of  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'since now the kernel feature space considers a joint space, we will first discuss the kernel construction on f ( the non - positional feature space ) and then discuss how different variants integrate the positional embedding ( with the positional feature space t ) into the kernel.', 'kernel construction on f. all the work considered the scaled asymmetric exponential kernel with the mapping w q and w k  #AUTHOR_TAG for non - positional features f q and f k :', 'note that the usage of asymmetric kernel is also commonly used in various machine learning tasks  #AUTHOR_TAG, where they observed the kernel form can be flexible and even non - valid ( i. e., a kernel that is not symmetric and positive semi - definite ).', 'in section 3, we show that symmetric design of the kernel has similar performance for various sequence learning tasks, and we also examine different kernel choices ( i. e., linear, polynomial, and rbf kernel ).', 'kernel construction on x = ( f × t ).', 'the designs for integrating the positional embedding t q and t k are listed in the following.', '( i ) absolute positional embedding  #TAUTHOR_TAG, each t i is represented by a vector with each dimension being sine or cosine functions.', 'for learned positional embedding  #AUTHOR_TAG, each t i is a learned parameter and is fixed for the same position for different sequences.', 'these works defines the feature space as the direct sum of its temporal and non - temporal space : x = f ⊕ t.', 'via the lens of kernel, the kernel similarity is defined as', '( ii ) relative positional embedding in  #TAUTHOR_TAG - xl  #AUTHOR_TAG : t represents the indicator of the position in the sequence, and the kernel is chosen to be asymmetric of mixing sine and cosine functions :', 'with k fq t q, t k being an asymmetric kernel with coefficients inferred by f q : log k fq t q, t k = ∑ ( iii ) relative positional embedding of  #AUTHOR_TAG and music  #TAUTHOR_TAG  #AUTHOR_TAG b ) : t ⋅ represents the indicator of the position in the sequence, and the kernel is modified to be indexed by a look - up table :', 'where l tq−t k, fq = exp ( f q w q a tq−t k ) with a ⋅ being a learnable matrix having matrix width to be the length of the sequence.', 'we refer readers to  #AUTHOR_TAG for more details.', ' #AUTHOR_TAG showed that the way to integrate positional embedding is better through eq. ( 5 ) than through eq. ( 6 ) and is better through']",0
"['( f × t ) has distinct design in variants of  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'since now the kernel feature']","['( f × t ) has distinct design in variants of  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'since now the kernel feature']","['( ⋅, ⋅ ) the kernel construction on x = ( f × t ) has distinct design in variants of  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'since now the kernel feature space considers a joint space, we will first discuss the kernel construction on']","['embedding k ( ⋅, ⋅ ) the kernel construction on x = ( f × t ) has distinct design in variants of  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'since now the kernel feature space considers a joint space, we will first discuss the kernel construction on f ( the non - positional feature space ) and then discuss how different variants integrate the positional embedding ( with the positional feature space t ) into the kernel.', 'kernel construction on f. all the work considered the scaled asymmetric exponential kernel with the mapping w q and w k  #AUTHOR_TAG for non - positional features f q and f k :', 'note that the usage of asymmetric kernel is also commonly used in various machine learning tasks  #AUTHOR_TAG, where they observed the kernel form can be flexible and even non - valid ( i. e., a kernel that is not symmetric and positive semi - definite ).', 'in section 3, we show that symmetric design of the kernel has similar performance for various sequence learning tasks, and we also examine different kernel choices ( i. e., linear, polynomial, and rbf kernel ).', 'kernel construction on x = ( f × t ).', 'the designs for integrating the positional embedding t q and t k are listed in the following.', '( i ) absolute positional embedding  #TAUTHOR_TAG, each t i is represented by a vector with each dimension being sine or cosine functions.', 'for learned positional embedding  #AUTHOR_TAG, each t i is a learned parameter and is fixed for the same position for different sequences.', 'these works defines the feature space as the direct sum of its temporal and non - temporal space : x = f ⊕ t.', 'via the lens of kernel, the kernel similarity is defined as', '( ii ) relative positional embedding in  #TAUTHOR_TAG - xl  #AUTHOR_TAG : t represents the indicator of the position in the sequence, and the kernel is chosen to be asymmetric of mixing sine and cosine functions :', 'with k fq t q, t k being an asymmetric kernel with coefficients inferred by f q : log k fq t q, t k = ∑ ( iii ) relative positional embedding of  #AUTHOR_TAG and music  #TAUTHOR_TAG  #AUTHOR_TAG b ) : t ⋅ represents the indicator of the position in the sequence, and the kernel is modified to be indexed by a look - up table :', 'where l tq−t k, fq = exp ( f q w q a tq−t k ) with a ⋅ being a learnable matrix having matrix width to be the length of the sequence.', 'we refer readers to  #AUTHOR_TAG for more details.', ' #AUTHOR_TAG showed that the way to integrate positional embedding is better through eq. ( 5 ) than through eq. ( 6 ) and is better through']",0
"['( f × t ) has distinct design in variants of  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'since now the kernel feature']","['( f × t ) has distinct design in variants of  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'since now the kernel feature']","['( ⋅, ⋅ ) the kernel construction on x = ( f × t ) has distinct design in variants of  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'since now the kernel feature space considers a joint space, we will first discuss the kernel construction on']","['embedding k ( ⋅, ⋅ ) the kernel construction on x = ( f × t ) has distinct design in variants of  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'since now the kernel feature space considers a joint space, we will first discuss the kernel construction on f ( the non - positional feature space ) and then discuss how different variants integrate the positional embedding ( with the positional feature space t ) into the kernel.', 'kernel construction on f. all the work considered the scaled asymmetric exponential kernel with the mapping w q and w k  #AUTHOR_TAG for non - positional features f q and f k :', 'note that the usage of asymmetric kernel is also commonly used in various machine learning tasks  #AUTHOR_TAG, where they observed the kernel form can be flexible and even non - valid ( i. e., a kernel that is not symmetric and positive semi - definite ).', 'in section 3, we show that symmetric design of the kernel has similar performance for various sequence learning tasks, and we also examine different kernel choices ( i. e., linear, polynomial, and rbf kernel ).', 'kernel construction on x = ( f × t ).', 'the designs for integrating the positional embedding t q and t k are listed in the following.', '( i ) absolute positional embedding  #TAUTHOR_TAG, each t i is represented by a vector with each dimension being sine or cosine functions.', 'for learned positional embedding  #AUTHOR_TAG, each t i is a learned parameter and is fixed for the same position for different sequences.', 'these works defines the feature space as the direct sum of its temporal and non - temporal space : x = f ⊕ t.', 'via the lens of kernel, the kernel similarity is defined as', '( ii ) relative positional embedding in  #TAUTHOR_TAG - xl  #AUTHOR_TAG : t represents the indicator of the position in the sequence, and the kernel is chosen to be asymmetric of mixing sine and cosine functions :', 'with k fq t q, t k being an asymmetric kernel with coefficients inferred by f q : log k fq t q, t k = ∑ ( iii ) relative positional embedding of  #AUTHOR_TAG and music  #TAUTHOR_TAG  #AUTHOR_TAG b ) : t ⋅ represents the indicator of the position in the sequence, and the kernel is modified to be indexed by a look - up table :', 'where l tq−t k, fq = exp ( f q w q a tq−t k ) with a ⋅ being a learnable matrix having matrix width to be the length of the sequence.', 'we refer readers to  #AUTHOR_TAG for more details.', ' #AUTHOR_TAG showed that the way to integrate positional embedding is better through eq. ( 5 ) than through eq. ( 6 ) and is better through']",0
"['current  #TAUTHOR_TAG  #AUTHOR_TAG :', '( ii ) transformer - xl  #AUTHOR_TAG, music transformer  #AUTHOR_TAG b ), self - attention with relative positional embedding  #AUTHOR_TAG :', 'compared']","['current  #TAUTHOR_TAG  #AUTHOR_TAG :', '( ii ) transformer - xl  #AUTHOR_TAG, music transformer  #AUTHOR_TAG b ), self - attention with relative positional embedding  #AUTHOR_TAG :', 'compared eq.']","['current  #TAUTHOR_TAG  #AUTHOR_TAG :', '( ii ) transformer - xl  #AUTHOR_TAG, music transformer  #AUTHOR_TAG b ), self - attention with relative positional embedding  #AUTHOR_TAG :', 'compared']","['current  #TAUTHOR_TAG  #AUTHOR_TAG :', '( ii ) transformer - xl  #AUTHOR_TAG, music transformer  #AUTHOR_TAG b ), self - attention with relative positional embedding  #AUTHOR_TAG :', 'compared eq. ( 7 ) to eq. ( 8 ), eq. ( 7 ) takes the positional embedding into account for constructing the value function.', 'in section 3, we empirically observe that constructing value function with eq. ( 8 ) constantly outperforms the construction with eq. ( 7 ), which suggests that we do not need positional embedding for value function']",0
"['m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['eq. ( 2 ), the returned set by the set filtering function m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['eq. ( 2 ), the returned set by the set filtering function m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG : for each query x q in the encoded sequence, m ( x q, s x k ) = s x k contains the keys being all the tokens in the encoded sequence.', 'note that encoder self - attention considers x q = x k with x q being the encoded sequence.', '( ii ) encoder - decoder attention in original  #TAUTHOR_TAG : for each query x q in decoded sequence, m ( x q, s x k ) = s x k contains the keys being all the tokens in the encoded sequence.', 'note that encode - decoder attention considers x q = x k with x q being the decoded sequence and x k being the encoded sequence.', '( iii ) decoder self - attention in original  #TAUTHOR_TAG : for each query x q in the decoded sequence, m ( x q, s x k ) returns a subset of s x k ( m ( x q, s x k ) ⊂ s x k ).', '']",0
"['m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['eq. ( 2 ), the returned set by the set filtering function m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['eq. ( 2 ), the returned set by the set filtering function m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG : for each query x q in the encoded sequence, m ( x q, s x k ) = s x k contains the keys being all the tokens in the encoded sequence.', 'note that encoder self - attention considers x q = x k with x q being the encoded sequence.', '( ii ) encoder - decoder attention in original  #TAUTHOR_TAG : for each query x q in decoded sequence, m ( x q, s x k ) = s x k contains the keys being all the tokens in the encoded sequence.', 'note that encode - decoder attention considers x q = x k with x q being the decoded sequence and x k being the encoded sequence.', '( iii ) decoder self - attention in original  #TAUTHOR_TAG : for each query x q in the decoded sequence, m ( x q, s x k ) returns a subset of s x k ( m ( x q, s x k ) ⊂ s x k ).', '']",0
"['m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['eq. ( 2 ), the returned set by the set filtering function m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['eq. ( 2 ), the returned set by the set filtering function m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG : for each query x q in the encoded sequence, m ( x q, s x k ) = s x k contains the keys being all the tokens in the encoded sequence.', 'note that encoder self - attention considers x q = x k with x q being the encoded sequence.', '( ii ) encoder - decoder attention in original  #TAUTHOR_TAG : for each query x q in decoded sequence, m ( x q, s x k ) = s x k contains the keys being all the tokens in the encoded sequence.', 'note that encode - decoder attention considers x q = x k with x q being the decoded sequence and x k being the encoded sequence.', '( iii ) decoder self - attention in original  #TAUTHOR_TAG : for each query x q in the decoded sequence, m ( x q, s x k ) returns a subset of s x k ( m ( x q, s x k ) ⊂ s x k ).', '']",0
"['m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['eq. ( 2 ), the returned set by the set filtering function m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['eq. ( 2 ), the returned set by the set filtering function m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG : for each query x q in the encoded sequence, m ( x q, s x k ) = s x k contains the keys being all the tokens in the encoded sequence.', 'note that encoder self - attention considers x q = x k with x q being the encoded sequence.', '( ii ) encoder - decoder attention in original  #TAUTHOR_TAG : for each query x q in decoded sequence, m ( x q, s x k ) = s x k contains the keys being all the tokens in the encoded sequence.', 'note that encode - decoder attention considers x q = x k with x q being the decoded sequence and x k being the encoded sequence.', '( iii ) decoder self - attention in original  #TAUTHOR_TAG : for each query x q in the decoded sequence, m ( x q, s x k ) returns a subset of s x k ( m ( x q, s x k ) ⊂ s x k ).', '']",0
"['m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['eq. ( 2 ), the returned set by the set filtering function m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['eq. ( 2 ), the returned set by the set filtering function m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG : for each query x q in the encoded sequence, m ( x q, s x k ) = s x k contains the keys being all the tokens in the encoded sequence.', 'note that encoder self - attention considers x q = x k with x q being the encoded sequence.', '( ii ) encoder - decoder attention in original  #TAUTHOR_TAG : for each query x q in decoded sequence, m ( x q, s x k ) = s x k contains the keys being all the tokens in the encoded sequence.', 'note that encode - decoder attention considers x q = x k with x q being the decoded sequence and x k being the encoded sequence.', '( iii ) decoder self - attention in original  #TAUTHOR_TAG : for each query x q in the decoded sequence, m ( x q, s x k ) returns a subset of s x k ( m ( x q, s x k ) ⊂ s x k ).', '']",0
"['m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['eq. ( 2 ), the returned set by the set filtering function m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['eq. ( 2 ), the returned set by the set filtering function m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG : for each query x q in the encoded sequence, m ( x q, s x k ) = s x k contains the keys being all the tokens in the encoded sequence.', 'note that encoder self - attention considers x q = x k with x q being the encoded sequence.', '( ii ) encoder - decoder attention in original  #TAUTHOR_TAG : for each query x q in decoded sequence, m ( x q, s x k ) = s x k contains the keys being all the tokens in the encoded sequence.', 'note that encode - decoder attention considers x q = x k with x q being the decoded sequence and x k being the encoded sequence.', '( iii ) decoder self - attention in original  #TAUTHOR_TAG : for each query x q in the decoded sequence, m ( x q, s x k ) returns a subset of s x k ( m ( x q, s x k ) ⊂ s x k ).', '']",0
"['m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['eq. ( 2 ), the returned set by the set filtering function m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['eq. ( 2 ), the returned set by the set filtering function m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG : for each query x q in the encoded sequence, m ( x q, s x k ) = s x k contains the keys being all the tokens in the encoded sequence.', 'note that encoder self - attention considers x q = x k with x q being the encoded sequence.', '( ii ) encoder - decoder attention in original  #TAUTHOR_TAG : for each query x q in decoded sequence, m ( x q, s x k ) = s x k contains the keys being all the tokens in the encoded sequence.', 'note that encode - decoder attention considers x q = x k with x q being the decoded sequence and x k being the encoded sequence.', '( iii ) decoder self - attention in original  #TAUTHOR_TAG : for each query x q in the decoded sequence, m ( x q, s x k ) returns a subset of s x k ( m ( x q, s x k ) ⊂ s x k ).', '']",0
"['m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['eq. ( 2 ), the returned set by the set filtering function m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG :']","['eq. ( 2 ), the returned set by the set filtering function m ( x q, s x k ) defines how many keys and which keys are operating with x q.', 'in the following, we itemize the corresponding designs for the variants in  #TAUTHOR_TAG : for each query x q in the encoded sequence, m ( x q, s x k ) = s x k contains the keys being all the tokens in the encoded sequence.', 'note that encoder self - attention considers x q = x k with x q being the encoded sequence.', '( ii ) encoder - decoder attention in original  #TAUTHOR_TAG : for each query x q in decoded sequence, m ( x q, s x k ) = s x k contains the keys being all the tokens in the encoded sequence.', 'note that encode - decoder attention considers x q = x k with x q being the decoded sequence and x k being the encoded sequence.', '( iii ) decoder self - attention in original  #TAUTHOR_TAG : for each query x q in the decoded sequence, m ( x q, s x k ) returns a subset of s x k ( m ( x q, s x k ) ⊂ s x k ).', '']",0
"['the  #TAUTHOR_TAG designs :', '']","['the  #TAUTHOR_TAG designs :', 'q1.']","['the  #TAUTHOR_TAG designs :', '']","['viewing the attention mechanism with eq. ( 2 ), we aims at answering the following questions regarding the  #TAUTHOR_TAG designs :', 'q1. what is the suggested way for incorporating positional embedding in the kernel function?', '']",0
"[') operation  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'however, we show that, for decoder']","['operation  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'however, we show that, for decoder self - attention, the operation is not order - agnostic.', 'for clarification,']","['is an order - agnostic ( or, permutation equivariant ) operation  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'however, we show that, for decoder']","['need of the positional embedding ( pe ) in the attention mechanism is based on the argument that the attention mechanism is an order - agnostic ( or, permutation equivariant ) operation  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'however, we show that, for decoder self - attention, the operation is not order - agnostic.', 'for clarification, we are not attacking the claim made by the prior work  #TAUTHOR_TAG 2017 ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, but we aim at providing a new look at the order - invariance problem when considering the attention mechanism with masks ( masks refer to the set filtering function in our kernel formulation ).', 'in other words, previous work did not consider the mask between queries and keys when discussing the order - invariance problem ( perez et al., 2019 ).', 'to put it formally, we first present the definition by for a permutation equivariance function :', 'definition 2.', 'denote π as the set of all permutations over [ n ] = { 1, [UNK], n }. a function f unc [UNK] x n → y n is permutation equivariant iff for any permutation π ∈ π, f unc ( πx ) = πf unc ( x ). showed that the standard attention ( encoder self - attention  #TAUTHOR_TAG ) is permutation equivariant.', 'here, we present the non - permutation - equivariant problem on the decoder self - attention :  #TAUTHOR_TAG is not permutation equivariant.', 'to proceed the proof, we need the following definition and propositions']",0
"[') operation  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'however, we show that, for decoder']","['operation  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'however, we show that, for decoder self - attention, the operation is not order - agnostic.', 'for clarification,']","['is an order - agnostic ( or, permutation equivariant ) operation  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'however, we show that, for decoder']","['need of the positional embedding ( pe ) in the attention mechanism is based on the argument that the attention mechanism is an order - agnostic ( or, permutation equivariant ) operation  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'however, we show that, for decoder self - attention, the operation is not order - agnostic.', 'for clarification, we are not attacking the claim made by the prior work  #TAUTHOR_TAG 2017 ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, but we aim at providing a new look at the order - invariance problem when considering the attention mechanism with masks ( masks refer to the set filtering function in our kernel formulation ).', 'in other words, previous work did not consider the mask between queries and keys when discussing the order - invariance problem ( perez et al., 2019 ).', 'to put it formally, we first present the definition by for a permutation equivariance function :', 'definition 2.', 'denote π as the set of all permutations over [ n ] = { 1, [UNK], n }. a function f unc [UNK] x n → y n is permutation equivariant iff for any permutation π ∈ π, f unc ( πx ) = πf unc ( x ). showed that the standard attention ( encoder self - attention  #TAUTHOR_TAG ) is permutation equivariant.', 'here, we present the non - permutation - equivariant problem on the decoder self - attention :  #TAUTHOR_TAG is not permutation equivariant.', 'to proceed the proof, we need the following definition and propositions']",0
"['than relating  #TAUTHOR_TAG attention mechanism with kernel methods,']","['than relating  #TAUTHOR_TAG attention mechanism with kernel methods,']","['than relating  #TAUTHOR_TAG attention mechanism with kernel methods,']","['than relating  #TAUTHOR_TAG attention mechanism with kernel methods, the prior work  #AUTHOR_TAG b ) related the attention mechanism with graph - structured learning.', 'for example, non - local neural networks  #AUTHOR_TAG made a connection between the attention and the non - local operation in image processing  #AUTHOR_TAG.', 'others  #AUTHOR_TAG b ) linked the attention to the message passing in graphical models.', 'in addition to the fundamental difference between graph - structured learning and kernel learning, the prior work  #AUTHOR_TAG b ) focused on presenting  #TAUTHOR_TAG for its particular application ( e. g., video classification  #AUTHOR_TAG and neural machine translation  #AUTHOR_TAG ).', 'alternatively, our work focuses on presenting a new formulation of  #TAUTHOR_TAG attention mechanism that gains us the possibility for understanding the attention mechanism better']",0
"['than relating  #TAUTHOR_TAG attention mechanism with kernel methods,']","['than relating  #TAUTHOR_TAG attention mechanism with kernel methods,']","['than relating  #TAUTHOR_TAG attention mechanism with kernel methods,']","['than relating  #TAUTHOR_TAG attention mechanism with kernel methods, the prior work  #AUTHOR_TAG b ) related the attention mechanism with graph - structured learning.', 'for example, non - local neural networks  #AUTHOR_TAG made a connection between the attention and the non - local operation in image processing  #AUTHOR_TAG.', 'others  #AUTHOR_TAG b ) linked the attention to the message passing in graphical models.', 'in addition to the fundamental difference between graph - structured learning and kernel learning, the prior work  #AUTHOR_TAG b ) focused on presenting  #TAUTHOR_TAG for its particular application ( e. g., video classification  #AUTHOR_TAG and neural machine translation  #AUTHOR_TAG ).', 'alternatively, our work focuses on presenting a new formulation of  #TAUTHOR_TAG attention mechanism that gains us the possibility for understanding the attention mechanism better']",0
['is a powerful architecture that'],['is a powerful architecture that'],['is a powerful architecture that'],"['is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction.', 'at the core of the  #TAUTHOR_TAG is the attention mechanism, which concurrently processes all inputs in the streams.', 'in this paper, we present a new formulation of attention via the lens of the kernel.', 'to be more precise, we realize that the attention can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs.', 'this new formulation gives us a better way to understand individual components of the  #TAUTHOR_TAG attention, such as the better way to integrate the positional embedding.', ""another important advantage of our kernel - based formulation is that it paves the way to a larger space of composing  #TAUTHOR_TAG's attention."", 'as an example, we propose a new variant of  #TAUTHOR_TAG attention which models the input as a product of symmetric kernels.', 'this approach achieves competitive performance to the current state of the art model with less computation.', 'in our experiments, we empirically study different kernel construction strategies on two widely used tasks : neural machine translation and sequence prediction']",1
['is a powerful architecture that'],['is a powerful architecture that'],['is a powerful architecture that'],"['is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction.', 'at the core of the  #TAUTHOR_TAG is the attention mechanism, which concurrently processes all inputs in the streams.', 'in this paper, we present a new formulation of attention via the lens of the kernel.', 'to be more precise, we realize that the attention can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs.', 'this new formulation gives us a better way to understand individual components of the  #TAUTHOR_TAG attention, such as the better way to integrate the positional embedding.', ""another important advantage of our kernel - based formulation is that it paves the way to a larger space of composing  #TAUTHOR_TAG's attention."", 'as an example, we propose a new variant of  #TAUTHOR_TAG attention which models the input as a product of symmetric kernels.', 'this approach achieves competitive performance to the current state of the art model with less computation.', 'in our experiments, we empirically study different kernel construction strategies on two widely used tasks : neural machine translation and sequence prediction']",1
"[', language understanding']","[', language understanding']","[', language understanding']","[', language understanding  #AUTHOR_TAG, sequence prediction  #AUTHOR_TAG, image generation  #AUTHOR_TAG, video activity classification  #AUTHOR_TAG, music generation  #AUTHOR_TAG a ), and multimodal sentiment analysis  #AUTHOR_TAG a ).', 'instead of performing recurrence ( e. g., rnn ) or convolution ( e. g., tcn ) over the sequences,  #TAUTHOR_TAG is a feed - forward model that concurrently processes the entire sequence.', 'at the core of the  #TAUTHOR_TAG is its attention mechanism, which is proposed to integrate the dependencies between the inputs.', 'there are up to three types of attention within the full  #TAUTHOR_TAG : 1 ) encoder self - attention considers the source sentence as input, generating a sequence of encoded representations, where each encoded token has a global dependency with other tokens in the input sequence.', '']",1
"[', language understanding']","[', language understanding']","[', language understanding']","[', language understanding  #AUTHOR_TAG, sequence prediction  #AUTHOR_TAG, image generation  #AUTHOR_TAG, video activity classification  #AUTHOR_TAG, music generation  #AUTHOR_TAG a ), and multimodal sentiment analysis  #AUTHOR_TAG a ).', 'instead of performing recurrence ( e. g., rnn ) or convolution ( e. g., tcn ) over the sequences,  #TAUTHOR_TAG is a feed - forward model that concurrently processes the entire sequence.', 'at the core of the  #TAUTHOR_TAG is its attention mechanism, which is proposed to integrate the dependencies between the inputs.', 'there are up to three types of attention within the full  #TAUTHOR_TAG : 1 ) encoder self - attention considers the source sentence as input, generating a sequence of encoded representations, where each encoded token has a global dependency with other tokens in the input sequence.', '']",1
"[') operation  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'however, we show that, for decoder']","['operation  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'however, we show that, for decoder self - attention, the operation is not order - agnostic.', 'for clarification,']","['is an order - agnostic ( or, permutation equivariant ) operation  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'however, we show that, for decoder']","['need of the positional embedding ( pe ) in the attention mechanism is based on the argument that the attention mechanism is an order - agnostic ( or, permutation equivariant ) operation  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'however, we show that, for decoder self - attention, the operation is not order - agnostic.', 'for clarification, we are not attacking the claim made by the prior work  #TAUTHOR_TAG 2017 ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, but we aim at providing a new look at the order - invariance problem when considering the attention mechanism with masks ( masks refer to the set filtering function in our kernel formulation ).', 'in other words, previous work did not consider the mask between queries and keys when discussing the order - invariance problem ( perez et al., 2019 ).', 'to put it formally, we first present the definition by for a permutation equivariance function :', 'definition 2.', 'denote π as the set of all permutations over [ n ] = { 1, [UNK], n }. a function f unc [UNK] x n → y n is permutation equivariant iff for any permutation π ∈ π, f unc ( πx ) = πf unc ( x ). showed that the standard attention ( encoder self - attention  #TAUTHOR_TAG ) is permutation equivariant.', 'here, we present the non - permutation - equivariant problem on the decoder self - attention :  #TAUTHOR_TAG is not permutation equivariant.', 'to proceed the proof, we need the following definition and propositions']",1
"['than relating  #TAUTHOR_TAG attention mechanism with kernel methods,']","['than relating  #TAUTHOR_TAG attention mechanism with kernel methods,']","['than relating  #TAUTHOR_TAG attention mechanism with kernel methods,']","['than relating  #TAUTHOR_TAG attention mechanism with kernel methods, the prior work  #AUTHOR_TAG b ) related the attention mechanism with graph - structured learning.', 'for example, non - local neural networks  #AUTHOR_TAG made a connection between the attention and the non - local operation in image processing  #AUTHOR_TAG.', 'others  #AUTHOR_TAG b ) linked the attention to the message passing in graphical models.', 'in addition to the fundamental difference between graph - structured learning and kernel learning, the prior work  #AUTHOR_TAG b ) focused on presenting  #TAUTHOR_TAG for its particular application ( e. g., video classification  #AUTHOR_TAG and neural machine translation  #AUTHOR_TAG ).', 'alternatively, our work focuses on presenting a new formulation of  #TAUTHOR_TAG attention mechanism that gains us the possibility for understanding the attention mechanism better']",1
['is a powerful architecture that'],['is a powerful architecture that'],['is a powerful architecture that'],"['is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction.', 'at the core of the  #TAUTHOR_TAG is the attention mechanism, which concurrently processes all inputs in the streams.', 'in this paper, we present a new formulation of attention via the lens of the kernel.', 'to be more precise, we realize that the attention can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs.', 'this new formulation gives us a better way to understand individual components of the  #TAUTHOR_TAG attention, such as the better way to integrate the positional embedding.', ""another important advantage of our kernel - based formulation is that it paves the way to a larger space of composing  #TAUTHOR_TAG's attention."", 'as an example, we propose a new variant of  #TAUTHOR_TAG attention which models the input as a product of symmetric kernels.', 'this approach achieves competitive performance to the current state of the art model with less computation.', 'in our experiments, we empirically study different kernel construction strategies on two widely used tasks : neural machine translation and sequence prediction']",6
"['of attention in  #TAUTHOR_TAG via the lens of kernel.', 'the inspiration for connecting the kernel  #AUTHOR_TAG and attention instantiates from']","['of attention in  #TAUTHOR_TAG via the lens of kernel.', 'the inspiration for connecting the kernel  #AUTHOR_TAG and attention instantiates from']","['of attention in  #TAUTHOR_TAG via the lens of kernel.', 'the inspiration for connecting the kernel  #AUTHOR_TAG and attention instantiates from the observation : both operations concurrently processes all inputs and calculate the similarity between the inputs.', 'we']","['section aims at providing an understanding of attention in  #TAUTHOR_TAG via the lens of kernel.', 'the inspiration for connecting the kernel  #AUTHOR_TAG and attention instantiates from the observation : both operations concurrently processes all inputs and calculate the similarity between the inputs.', '']",7
"['of attention in  #TAUTHOR_TAG via the lens of kernel.', 'the inspiration for connecting the kernel  #AUTHOR_TAG and attention instantiates from']","['of attention in  #TAUTHOR_TAG via the lens of kernel.', 'the inspiration for connecting the kernel  #AUTHOR_TAG and attention instantiates from']","['of attention in  #TAUTHOR_TAG via the lens of kernel.', 'the inspiration for connecting the kernel  #AUTHOR_TAG and attention instantiates from the observation : both operations concurrently processes all inputs and calculate the similarity between the inputs.', 'we']","['section aims at providing an understanding of attention in  #TAUTHOR_TAG via the lens of kernel.', 'the inspiration for connecting the kernel  #AUTHOR_TAG and attention instantiates from the observation : both operations concurrently processes all inputs and calculate the similarity between the inputs.', '']",7
"['m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we']","['a set with its elements that operate with', '( or are connected / visible to ) x', 'q. the filtering function m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we']","['', 'q. the filtering function m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we re - represent eq. ( 1 ) into the following definition', '., and a value']","['returns a set with its elements that operate with', '( or are connected / visible to ) x', 'q. the filtering function m ( ⋅, ⋅ ) plays as the role of the mask in decoder self - attention  #TAUTHOR_TAG. putting these altogether, we re - represent eq. ( 1 ) into the following definition', '., and a value function v ( ⋅ ) [UNK] x → y, the attention function taking the input of a query feature x q ∈ x is defined as the definition 1 is a class of linear smoothers', ' #AUTHOR_TAG with kernel smoothing : where v ( x k ) outputs the "" values "" and is a probability function depends on k and n when k ( ⋅, ⋅ ) is always positive. in the prior work  #AUTHOR_TAG note', 'that the kernel form k ( x q, x k ) in the original  #TAUTHOR_TAG is a asymmetric exponential kernel with additional', 'mapping w q and w k  #AUTHOR_TAG 2. the new formulation defines a larger space for compos', '##ing attention by manipulating its individual components, and at the same time it is able to categorize different variants of attention in prior work  #AUTHOR_TAG b ;  #AUTHOR_TAG a', '). in the following, we study these components by dissecting eq.', '']",7
"['. ( 2 ) connects to the variants of  #TAUTHOR_TAG.', 'by changing the kernel construction in section 2. 2. 2, we can define a larger space']","['far, we see how eq. ( 2 ) connects to the variants of  #TAUTHOR_TAG.', 'by changing the kernel construction in section 2. 2. 2, we can define a larger space']","['. ( 2 ) connects to the variants of  #TAUTHOR_TAG.', 'by changing the kernel construction in section 2. 2. 2, we can define a larger space']","['far, we see how eq. ( 2 ) connects to the variants of  #TAUTHOR_TAG.', 'by changing the kernel construction in section 2. 2. 2, we can define a larger space for composing attention.', 'in this paper, we present a new form of attention with a kernel that is 1 ) valid ( i. e., a kernel that is symmetric and positive semi - definite ) and 2 ) delicate in the sense of constructing a kernel on a joint space ( i. e., x = ( f × t ) ) :', 'where w f and w t are weight matrices.', 'the new form considers product of kernels with the first kernel measuring similarity between non - temporal features and the second kernel measuring similarity between temporal features.', 'both kernels are symmetric exponential kernel.', 'note that t i here is chosen as the mixture of sine and cosine functions as in the prior work  #TAUTHOR_TAG.', 'in our experiment, we find it reaching competitive performance as comparing to the current state - of - the - art designs ( eq. ( 5 ) by  #AUTHOR_TAG ).', 'we fix the size of the weight matrices w ⋅ in eq. ( 9 ) and eq. ( 5 ) which means we save 33 % of the parameters in attention from eq. ( 9']",7
"['the  #TAUTHOR_TAG designs :', '']","['the  #TAUTHOR_TAG designs :', 'q1.']","['the  #TAUTHOR_TAG designs :', '']","['viewing the attention mechanism with eq. ( 2 ), we aims at answering the following questions regarding the  #TAUTHOR_TAG designs :', 'q1. what is the suggested way for incorporating positional embedding in the kernel function?', '']",7
"['the  #TAUTHOR_TAG designs :', '']","['the  #TAUTHOR_TAG designs :', 'q1.']","['the  #TAUTHOR_TAG designs :', '']","['viewing the attention mechanism with eq. ( 2 ), we aims at answering the following questions regarding the  #TAUTHOR_TAG designs :', 'q1. what is the suggested way for incorporating positional embedding in the kernel function?', '']",7
"['2.', 'note that, for fairness, other than manipulating the kernel choice of the non - positional features, we fix the configuration by  #TAUTHOR_TAG for nmt and the configuration by dai et']","['2.', 'note that, for fairness, other than manipulating the kernel choice of the non - positional features, we fix the configuration by  #TAUTHOR_TAG for nmt and the configuration by dai et al.  #AUTHOR_TAG']","['is a valid kernel  #AUTHOR_TAG.', 'the numbers are shown in table 2.', 'note that, for fairness, other than manipulating the kernel choice of the non - positional features, we fix the configuration by  #TAUTHOR_TAG for nmt and the configuration by dai et']","['find the best kernel form in the attention mechanism, in addition to the exponential kernel ( see eq. ( 3 ) ), we compare different kernel forms ( i. e., linear, polynomial, and rbf kernel ) for the non - positional features.', 'we also provide the results for changing asymmetric to the symmetric kernel, when forcing w q = w k, so that the resulting kernel is a valid kernel  #AUTHOR_TAG.', 'the numbers are shown in table 2.', 'note that, for fairness, other than manipulating the kernel choice of the non - positional features, we fix the configuration by  #TAUTHOR_TAG for nmt and the configuration by dai et al.  #AUTHOR_TAG for sp.', 'we first observe that the linear kernel does not converge for both nmt and sp.', 'we argue the reason is that the linear kernel may have negative value and thus it violates the assumption in kernel smoother that the kernel score must be positive  #AUTHOR_TAG.', '']",7
"[') operation  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'however, we show that, for decoder']","['operation  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'however, we show that, for decoder self - attention, the operation is not order - agnostic.', 'for clarification,']","['is an order - agnostic ( or, permutation equivariant ) operation  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'however, we show that, for decoder']","['need of the positional embedding ( pe ) in the attention mechanism is based on the argument that the attention mechanism is an order - agnostic ( or, permutation equivariant ) operation  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'however, we show that, for decoder self - attention, the operation is not order - agnostic.', 'for clarification, we are not attacking the claim made by the prior work  #TAUTHOR_TAG 2017 ;  #AUTHOR_TAG b ;  #AUTHOR_TAG, but we aim at providing a new look at the order - invariance problem when considering the attention mechanism with masks ( masks refer to the set filtering function in our kernel formulation ).', 'in other words, previous work did not consider the mask between queries and keys when discussing the order - invariance problem ( perez et al., 2019 ).', 'to put it formally, we first present the definition by for a permutation equivariance function :', 'definition 2.', 'denote π as the set of all permutations over [ n ] = { 1, [UNK], n }. a function f unc [UNK] x n → y n is permutation equivariant iff for any permutation π ∈ π, f unc ( πx ) = πf unc ( x ). showed that the standard attention ( encoder self - attention  #TAUTHOR_TAG ) is permutation equivariant.', 'here, we present the non - permutation - equivariant problem on the decoder self - attention :  #TAUTHOR_TAG is not permutation equivariant.', 'to proceed the proof, we need the following definition and propositions']",7
"['this paper, we presented a kernel formulation for the attention mechanism in  #TAUTHOR_TAG, which allows us to define a larger space']","['this paper, we presented a kernel formulation for the attention mechanism in  #TAUTHOR_TAG, which allows us to define a larger space']","['this paper, we presented a kernel formulation for the attention mechanism in  #TAUTHOR_TAG, which allows us to define a larger space']","['this paper, we presented a kernel formulation for the attention mechanism in  #TAUTHOR_TAG, which allows us to define a larger space for designing attention.', 'as an example, we proposed a new variant of attention which reaches competitive performance when compared to previous state - of - the - art models.', 'via the lens of the kernel, we were able to better understand the role of individual components in  #TAUTHOR_TAG attention and categorize previous attention variants in a unified formulation.', 'among these components, we found the construction of the kernel function acts the most important role, and we studied different kernel forms and the ways to integrate positional embedding on neural machine translation and sequence prediction.', 'we hope our empirical study may potentially allow others to design better attention mechanisms given their particular applications']",7
"['word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be']","['it positional embedding ). note that f i can be the word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be']","['word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be learned during back - propagation  #AUTHOR_TAG. the feature vector are defined over a joint space x [UNK] = ( f × t ). the resulting permutationinvariant set is : followed the definition by  #TAUTHOR_TAG, we use queries', '( q ) / keys ( k ) / values ( v ) to represent the inputs for']","['x 1, x 2, [UNK], x t ] defines each element as x i = ( f i, t i ) with f i ∈', 'f being the nontemporal feature at time i and t i ∈ t as an temporal feature ( or we', 'called it positional embedding ). note that f i can be the word representation ( in neural machine translation  #TAUTHOR_TAG, a pixel in a', 'frame ( in video activity recognition  #AUTHOR_TAG ), or a music unit ( in music generation', ' #AUTHOR_TAG b ) ). t i can be a mixture of sine and cosine functions  #TAUTHOR_TAG or parameters that can be learned during back - propagation  #AUTHOR_TAG. the feature vector are defined over a joint space x [UNK] = ( f × t ). the resulting permutationinvariant set is : followed the definition by  #TAUTHOR_TAG, we use queries', '( q ) / keys ( k ) / values ( v ) to represent the inputs for the attention. to be more precise, x { q k v } is used for denoting a query / key / value', '']",5
"['. ( 2 ) connects to the variants of  #TAUTHOR_TAG.', 'by changing the kernel construction in section 2. 2. 2, we can define a larger space']","['far, we see how eq. ( 2 ) connects to the variants of  #TAUTHOR_TAG.', 'by changing the kernel construction in section 2. 2. 2, we can define a larger space']","['. ( 2 ) connects to the variants of  #TAUTHOR_TAG.', 'by changing the kernel construction in section 2. 2. 2, we can define a larger space']","['far, we see how eq. ( 2 ) connects to the variants of  #TAUTHOR_TAG.', 'by changing the kernel construction in section 2. 2. 2, we can define a larger space for composing attention.', 'in this paper, we present a new form of attention with a kernel that is 1 ) valid ( i. e., a kernel that is symmetric and positive semi - definite ) and 2 ) delicate in the sense of constructing a kernel on a joint space ( i. e., x = ( f × t ) ) :', 'where w f and w t are weight matrices.', 'the new form considers product of kernels with the first kernel measuring similarity between non - temporal features and the second kernel measuring similarity between temporal features.', 'both kernels are symmetric exponential kernel.', 'note that t i here is chosen as the mixture of sine and cosine functions as in the prior work  #TAUTHOR_TAG.', 'in our experiment, we find it reaching competitive performance as comparing to the current state - of - the - art designs ( eq. ( 5 ) by  #AUTHOR_TAG ).', 'we fix the size of the weight matrices w ⋅ in eq. ( 9 ) and eq. ( 5 ) which means we save 33 % of the parameters in attention from eq. ( 9']",5
['configuration by  #TAUTHOR_TAG for nmt and the configuration by  #AUTHOR_TAG'],['configuration by  #TAUTHOR_TAG for nmt and the configuration by  #AUTHOR_TAG'],"['↓ means the lower the better.', 'table 2 : kernel types.', 'other than manipulating the kernel choice of the non - positional features, we fix the configuration by  #TAUTHOR_TAG for nmt and the configuration by  #AUTHOR_TAG']","['', 'we present the results in table 1.', 'first, we see that by having pe as a look - up  #AUTHOR_TAG and sp stands for sequence prediction on wikitext - 103 dataset  #AUTHOR_TAG.', '↑ means the upper the better and ↓ means the lower the better.', 'table 2 : kernel types.', 'other than manipulating the kernel choice of the non - positional features, we fix the configuration by  #TAUTHOR_TAG for nmt and the configuration by  #AUTHOR_TAG for sp.', '34. 14 24. 13 24. 21 table, it outperforms the case with having pe as direct - sum in feature space, especially for sp task.', 'note that the look - up table is indexed by the relative position ( i. e., t q − t k ) instead of absolute position.', 'second, we see that pe in the product kernel proposed by dai et al.  #AUTHOR_TAG may not constantly outperform the other integration types ( it has lower bleu score for nmt ).', 'our proposed product kernel reaches the best result in nmt and is competitive to the best result in sp']",5
"['2.', 'note that, for fairness, other than manipulating the kernel choice of the non - positional features, we fix the configuration by  #TAUTHOR_TAG for nmt and the configuration by dai et']","['2.', 'note that, for fairness, other than manipulating the kernel choice of the non - positional features, we fix the configuration by  #TAUTHOR_TAG for nmt and the configuration by dai et al.  #AUTHOR_TAG']","['is a valid kernel  #AUTHOR_TAG.', 'the numbers are shown in table 2.', 'note that, for fairness, other than manipulating the kernel choice of the non - positional features, we fix the configuration by  #TAUTHOR_TAG for nmt and the configuration by dai et']","['find the best kernel form in the attention mechanism, in addition to the exponential kernel ( see eq. ( 3 ) ), we compare different kernel forms ( i. e., linear, polynomial, and rbf kernel ) for the non - positional features.', 'we also provide the results for changing asymmetric to the symmetric kernel, when forcing w q = w k, so that the resulting kernel is a valid kernel  #AUTHOR_TAG.', 'the numbers are shown in table 2.', 'note that, for fairness, other than manipulating the kernel choice of the non - positional features, we fix the configuration by  #TAUTHOR_TAG for nmt and the configuration by dai et al.  #AUTHOR_TAG for sp.', 'we first observe that the linear kernel does not converge for both nmt and sp.', 'we argue the reason is that the linear kernel may have negative value and thus it violates the assumption in kernel smoother that the kernel score must be positive  #AUTHOR_TAG.', '']",5
"['the  #TAUTHOR_TAG designs :', '']","['the  #TAUTHOR_TAG designs :', 'q1.']","['the  #TAUTHOR_TAG designs :', '']","['viewing the attention mechanism with eq. ( 2 ), we aims at answering the following questions regarding the  #TAUTHOR_TAG designs :', 'q1. what is the suggested way for incorporating positional embedding in the kernel function?', '']",3
['configuration by  #TAUTHOR_TAG for nmt and the configuration by  #AUTHOR_TAG'],['configuration by  #TAUTHOR_TAG for nmt and the configuration by  #AUTHOR_TAG'],"['↓ means the lower the better.', 'table 2 : kernel types.', 'other than manipulating the kernel choice of the non - positional features, we fix the configuration by  #TAUTHOR_TAG for nmt and the configuration by  #AUTHOR_TAG']","['', 'we present the results in table 1.', 'first, we see that by having pe as a look - up  #AUTHOR_TAG and sp stands for sequence prediction on wikitext - 103 dataset  #AUTHOR_TAG.', '↑ means the upper the better and ↓ means the lower the better.', 'table 2 : kernel types.', 'other than manipulating the kernel choice of the non - positional features, we fix the configuration by  #TAUTHOR_TAG for nmt and the configuration by  #AUTHOR_TAG for sp.', '34. 14 24. 13 24. 21 table, it outperforms the case with having pe as direct - sum in feature space, especially for sp task.', 'note that the look - up table is indexed by the relative position ( i. e., t q − t k ) instead of absolute position.', 'second, we see that pe in the product kernel proposed by dai et al.  #AUTHOR_TAG may not constantly outperform the other integration types ( it has lower bleu score for nmt ).', 'our proposed product kernel reaches the best result in nmt and is competitive to the best result in sp']",4
"['2.', 'note that, for fairness, other than manipulating the kernel choice of the non - positional features, we fix the configuration by  #TAUTHOR_TAG for nmt and the configuration by dai et']","['2.', 'note that, for fairness, other than manipulating the kernel choice of the non - positional features, we fix the configuration by  #TAUTHOR_TAG for nmt and the configuration by dai et al.  #AUTHOR_TAG']","['is a valid kernel  #AUTHOR_TAG.', 'the numbers are shown in table 2.', 'note that, for fairness, other than manipulating the kernel choice of the non - positional features, we fix the configuration by  #TAUTHOR_TAG for nmt and the configuration by dai et']","['find the best kernel form in the attention mechanism, in addition to the exponential kernel ( see eq. ( 3 ) ), we compare different kernel forms ( i. e., linear, polynomial, and rbf kernel ) for the non - positional features.', 'we also provide the results for changing asymmetric to the symmetric kernel, when forcing w q = w k, so that the resulting kernel is a valid kernel  #AUTHOR_TAG.', 'the numbers are shown in table 2.', 'note that, for fairness, other than manipulating the kernel choice of the non - positional features, we fix the configuration by  #TAUTHOR_TAG for nmt and the configuration by dai et al.  #AUTHOR_TAG for sp.', 'we first observe that the linear kernel does not converge for both nmt and sp.', 'we argue the reason is that the linear kernel may have negative value and thus it violates the assumption in kernel smoother that the kernel score must be positive  #AUTHOR_TAG.', '']",4
['original  #TAUTHOR_TAG'],['original  #TAUTHOR_TAG'],"['the performance ( from 24. 28 to 30. 92 ).', 'nonetheless, the performance is slightly better than considering pe from the original  #TAUTHOR_TAG']","['', 'then, it raises a concern : do we require pe in decoder self - attention?', 'by removing pe in decoder selfattention, we present the results in table 3.', 'from the table, we can see that, for nmt, removing pe only in decoder self - attention results in slight performance drop ( from 34. 71 to 34. 49 ).', 'however, removing pe in the entire model greatly degrades the performance ( from 34. 71 to 14. 47 ).', 'on the other hand, for sp, removing pe from our proposed attention variant dramatically degrades the performance ( from 24. 28 to 30. 92 ).', 'nonetheless, the performance is slightly better than considering pe from the original  #TAUTHOR_TAG']",4
"['this paper, we presented a kernel formulation for the attention mechanism in  #TAUTHOR_TAG, which allows us to define a larger space']","['this paper, we presented a kernel formulation for the attention mechanism in  #TAUTHOR_TAG, which allows us to define a larger space']","['this paper, we presented a kernel formulation for the attention mechanism in  #TAUTHOR_TAG, which allows us to define a larger space']","['this paper, we presented a kernel formulation for the attention mechanism in  #TAUTHOR_TAG, which allows us to define a larger space for designing attention.', 'as an example, we proposed a new variant of attention which reaches competitive performance when compared to previous state - of - the - art models.', 'via the lens of the kernel, we were able to better understand the role of individual components in  #TAUTHOR_TAG attention and categorize previous attention variants in a unified formulation.', 'among these components, we found the construction of the kernel function acts the most important role, and we studied different kernel forms and the ways to integrate positional embedding on neural machine translation and sequence prediction.', 'we hope our empirical study may potentially allow others to design better attention mechanisms given their particular applications']",4
['proposed  #TAUTHOR_TAG makes tagging'],['proposed  #TAUTHOR_TAG makes tagging'],['proposed  #TAUTHOR_TAG makes tagging as fast'],"['introduced a method of inferring a tag dictionary from annotated data to speed up part - of - speech tagging by limiting the set of possible tags for each word.', ""while ratnaparkhi's tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed  #TAUTHOR_TAG makes tagging as fast as with ratnaparkhi's tag dictionary, but with no decrease in accuracy."", ""in this paper, we show that a very simple semi - supervised variant of ratnaparkhi's method results in a much tighter tag dictionary than either ratnaparkhi's or our previous method, with accuracy as high as with our previous tag dictionary but much faster tagging - more than 100, 000 tokens per second in perl."", 'in this paper, we present a new method of constructing tag dictionaries for part - of - speech ( pos ) tagging.', 'a tag dictionary is simply a list of words 1 along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed.', 'tag dictionaries are commonly used to speed up pos - tag inference by restricting the tags considered for a particular word to those specified by the dictionary.', 'early work on pos tagging generally relied heavily on manually constructed tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus  #AUTHOR_TAG.', ' #AUTHOR_TAG relied only on a tag dictionary extracted from annotated data, but he used the annotated 1 according to the conventions of the field, pos tags are assigned to all tokens in a tokenized text, including punctuation marks and other non - word tokens.', 'in this paper, all of these will be covered by the term word.', 'tags from his test data as well as his training data to construct his tag dictionary, so his evaluation was not really fair.', '2  #AUTHOR_TAG seems to have been the first to use a tag dictionary automatically extracted only from training data.', ""ratnaparkhi's method of constructing a tag dictionary substantially speeds up tagging compared to considering every possible tag for every word, but it noticeably degrades accuracy when used with a current state - of - the - art tagging model."", ""we recently presented  #TAUTHOR_TAG a new method of constructing a tag dictionary that produces a tagging speed - up comparable to ratnaparkhi's, but with no decrease in tagging accuracy."", ""in this paper, we show that a very simple semi - supervised variant of ratnaparkhi's method results in a much tighter tag dictionary than either ratnaparkhi's or our previous method, with accuracy as high as we""]",0
['proposed  #TAUTHOR_TAG makes tagging'],['proposed  #TAUTHOR_TAG makes tagging'],['proposed  #TAUTHOR_TAG makes tagging as fast'],"['introduced a method of inferring a tag dictionary from annotated data to speed up part - of - speech tagging by limiting the set of possible tags for each word.', ""while ratnaparkhi's tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed  #TAUTHOR_TAG makes tagging as fast as with ratnaparkhi's tag dictionary, but with no decrease in accuracy."", ""in this paper, we show that a very simple semi - supervised variant of ratnaparkhi's method results in a much tighter tag dictionary than either ratnaparkhi's or our previous method, with accuracy as high as with our previous tag dictionary but much faster tagging - more than 100, 000 tokens per second in perl."", 'in this paper, we present a new method of constructing tag dictionaries for part - of - speech ( pos ) tagging.', 'a tag dictionary is simply a list of words 1 along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed.', 'tag dictionaries are commonly used to speed up pos - tag inference by restricting the tags considered for a particular word to those specified by the dictionary.', 'early work on pos tagging generally relied heavily on manually constructed tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus  #AUTHOR_TAG.', ' #AUTHOR_TAG relied only on a tag dictionary extracted from annotated data, but he used the annotated 1 according to the conventions of the field, pos tags are assigned to all tokens in a tokenized text, including punctuation marks and other non - word tokens.', 'in this paper, all of these will be covered by the term word.', 'tags from his test data as well as his training data to construct his tag dictionary, so his evaluation was not really fair.', '2  #AUTHOR_TAG seems to have been the first to use a tag dictionary automatically extracted only from training data.', ""ratnaparkhi's method of constructing a tag dictionary substantially speeds up tagging compared to considering every possible tag for every word, but it noticeably degrades accuracy when used with a current state - of - the - art tagging model."", ""we recently presented  #TAUTHOR_TAG a new method of constructing a tag dictionary that produces a tagging speed - up comparable to ratnaparkhi's, but with no decrease in tagging accuracy."", ""in this paper, we show that a very simple semi - supervised variant of ratnaparkhi's method results in a much tighter tag dictionary than either ratnaparkhi's or our previous method, with accuracy as high as we""]",0
"[') development set, compared to considering all tags for all words.', ""with a more accurate model, however, we found  #TAUTHOR_TAG that while ratnaparkhi's tag dictionary decreased""]","['on his penn treebank  #AUTHOR_TAG wall street journal ( wsj ) development set, compared to considering all tags for all words.', ""with a more accurate model, however, we found  #TAUTHOR_TAG that while ratnaparkhi's tag dictionary decreased""]","[') development set, compared to considering all tags for all words.', ""with a more accurate model, however, we found  #TAUTHOR_TAG that while ratnaparkhi's tag dictionary decreased""]","[""each word observed in an annotated training set, ratnaparkhi's tag dictionary includes all tags observed with that word in the training set, with all possible tags allowed for all other words."", 'ratnaparkhi reported that using this tag dictionary improved per - tag accuracy from 96. 31 % to 96. 43 % on his penn treebank  #AUTHOR_TAG wall street journal ( wsj ) development set, compared to considering all tags for all words.', ""with a more accurate model, however, we found  #TAUTHOR_TAG that while ratnaparkhi's tag dictionary decreased the average number of tags per token from 45 to 3. 7 on the current standard wsj development set, it also decreased per - tag accuracy from 97. 31 % to 97. 19 %."", 'this loss of accuracy can be explained by the fact that 0. 5 % of the development set tokens are known words with a tag not seen in the training set, for which our model achieved 44. 5 % accuracy with all word / tag pairs permitted.', ""with ratnaparkhi's dictionary, accuracy for these tokens is necessarily 0 %""]",0
"['previously presented  #TAUTHOR_TAG a tag dictionary constructed by using the annotated training set to compute a smoothed probability estimate for any possible tag given any possible word, and for each word in the training set, including in the dictionary the tags having an estimated probability greater than a fixed threshold t.', 'in this approach, the probability']","['previously presented  #TAUTHOR_TAG a tag dictionary constructed by using the annotated training set to compute a smoothed probability estimate for any possible tag given any possible word, and for each word in the training set, including in the dictionary the tags having an estimated probability greater than a fixed threshold t.', 'in this approach, the probability p ( t | w ) of tag']","['previously presented  #TAUTHOR_TAG a tag dictionary constructed by using the annotated training set to compute a smoothed probability estimate for any possible tag given any possible word, and for each word in the training set, including in the dictionary the tags having an estimated probability greater than a fixed threshold t.', 'in this approach, the probability']","['previously presented  #TAUTHOR_TAG a tag dictionary constructed by using the annotated training set to compute a smoothed probability estimate for any possible tag given any possible word, and for each word in the training set, including in the dictionary the tags having an estimated probability greater than a fixed threshold t.', 'in this approach, the probability p ( t | w ) of tag t given word w is computed by interpolating a discounted relative frequency estimate of p ( t | w ) with an estimate of p ( t ) based on "" diversity counts "", taking the count of a tag t to be the number of distinct words ever observed with that tag.', 'the distribution p ( t ) is also used to estimate tag probabilities for unknown words, so the set of possible tags for any word not explicitly listed is { t | p ( t ) > t }.', 'if we think of w followed by t as a word bigram, this is exactly like a bigram language model estimated by the interpolated kneser - ney ( kn ) method described by  #AUTHOR_TAG.', 'the way tag diversity counts are used has the desirable property that closed - class tags receive a very low estimated probability of being assigned to a rare or unknown word, even though they occur very often with a small number of frequent words.', 'a single value for discounting the count of all observed word / tag pairs is set to maximize the estimated probability of the reference tagging of the development set.', ""when t was chosen to be the highest threshold that preserves our model's 97. 31 % per tag wsj development set accuracy, we obtained an average of 3. 5 tags per token""]",0
"['paper  #TAUTHOR_TAG.', '']","['paper  #TAUTHOR_TAG.', '']","['pos tagging, and additional word - class features obtained by unsupervised clustering of a very large unannotated corpus provide information about words not occurring in the training set.', 'for full details of the feature set, see our previous paper  #TAUTHOR_TAG.', 'the model is trained by optimizing the multiclass svm hinge loss objective  #AUTHOR_TAG, using stochastic subgradient descent as described by  #AUTHOR_TAG, with early stopping']","['model structure, feature set, and learning method we use for pos tagging are essentially the same as those in our earlier work, treating pos tagging as a single - token independent multiclass classification task.', 'word - class - sequence features obtained by supervised clustering of the annotated training set replace the hidden tag - sequence features frequently used for pos tagging, and additional word - class features obtained by unsupervised clustering of a very large unannotated corpus provide information about words not occurring in the training set.', 'for full details of the feature set, see our previous paper  #TAUTHOR_TAG.', 'the model is trained by optimizing the multiclass svm hinge loss objective  #AUTHOR_TAG, using stochastic subgradient descent as described by  #AUTHOR_TAG, with early stopping and averaging.', 'the only difference from our previous training procedure is that we now use a tag dictionary to speed up training, while we previously used tag dictionaries only at test time.', 'our training procedure makes multiple passes through the training data considering each training example in turn, comparing the current model score of the correct tag for the example to that of the highest scoring incorrect tag and updating the model if the score of the correct tag does not exceed the score of the highest scoring incorrect tag by a specified margin.', 'in our new version of this procedure, we use the kn - smoothed tag dictionary described in section 1. 3. to speed up finding the highest scoring incorrect tag.', 'recall that the kn - smoothed tag dictionary estimates a non - zero probability p ( t | w ) for every possible word / tag pair, and that the possible tags for a given word are determinted by setting a threshold t on this probability.', 'in each pass through the training set, we use the same probability distribution p ( t | w ) determined from the statistics of the annotated training data, but we employ an adaptive method to determine what threshold t to use in each pass.', 'for the first pass through the training set, we set an initial threshold t 0 to the highest value such that for every token in the development set, p ( t | w ) ≥ t 0, where t is the correct tag for the token and w is the word for the token.', 'at the end of each training pass i, while evaluating the current model on the development set for early stopping using threshold t i−1, we also find the highest probability threshold t i such that choosing a lower threshold would not enable any additional correct taggings on the development set using the current model.', 'this threshold will normally be higher than t 0, because we disregard tokens in the development set for which the correct']",0
"[' #TAUTHOR_TAG, section 3. 1 ).', 'tagging took']","[' #TAUTHOR_TAG, section 3. 1 ).', 'tagging took']","['different tag dictionaries 6, 616, 812 unique words.', 'we tagged this corpus using the model described in section 2. 1 and a kn - smoothed tag dictionary as described in section 1. 3, with a threshold t = 0. 0005.', 'the tagger we used is based on the fastest of the methods described in our previous work  #TAUTHOR_TAG, section 3. 1 ).', 'tagging took about 26 hours using a single - threaded implementation in perl on a linux workstation equipped with intel xeon x5550 2. 67 ghz processors']","['', 'we tagged this corpus using the model described in section 2. 1 and a kn - smoothed tag dictionary as described in section 1. 3, with a threshold t = 0. 0005.', 'the tagger we used is based on the fastest of the methods described in our previous work  #TAUTHOR_TAG, section 3. 1 ).', 'tagging took about 26 hours using a single - threaded implementation in perl on a linux workstation equipped with intel xeon x5550 2. 67 ghz processors']",0
"['described  #TAUTHOR_TAG, in a singlet']","['described  #TAUTHOR_TAG, in a singlet']","['described  #TAUTHOR_TAG, in a singlethreaded implementation in perl on a linux workstation equipped with intel xeon x5550 2. 67 ghz processors. speed is rounded to the nearest 1, 000 tokens', 'per second,']","['second for each of the three tag dictionaries, using the fast tagging method we previously described  #TAUTHOR_TAG, in a singlethreaded implementation in perl on a linux workstation equipped with intel xeon x5550 2. 67 ghz processors. speed is rounded to the nearest 1, 000 tokens', 'per second, because we measured times to a precision of only about one part in one hundred. for the pruned kn - smoothed dictionary, we previously reported a speed of 49', ', 000 tokens per second under similar conditions. our current faster speed of 69, 000 tokens per second is due to an improved low - level implementation for computing the model scores for', 'permitted tags, and a slightly faster version of perl ( v5. 18. 2 ). the most restrictive tag dictionary, the pr', '##uned semi - supervised dictionary, allows only 1. 51 tags per token, and our implementation runs at 103', ', 000 tokens per second on the wsj development set. for our final experiments', ', we tested our tagger with this dictionary on the standard penn treebank wsj test set and on the penn treebank - 3 parsed brown corpus subset, as an out - of -', 'domain evaluation. for comparison, we tested our previous tagger and the fast version ( english - left3words - distsim ) of the stanford tagger  #AUTHOR_TAG recommended for practical use on the stanford tagger website, which we found to be by far the fastest of the six publicly available tag', '##gers tested in our previous work  #TAUTHOR_TAG. the results of these tests are shown in table 2 table 2 : wsj test set and brown corpus tagging', 'speeds and token accuracies for our previous tagger, we give three speeds : the speed we reported earlier, a speed for a', 'duplicate of the earlier experiment using the faster version of perl that we use here, and a third measurement including both the faster version of perl and our improved low - level tagger implementation. with the pruned semi - supervised', 'dictionary, our new tagger has slightly higher all - token accuracy than our previous tagger on both the wsj test set and brown corpus set, and it is much more accurate than the fast stanford tag', '##ger. the accuracy on the standard wsj test set is 97. 36 %, one of the highest ever reported. the new tagger is also much faster than either of the other taggers, achieving a speed of more than', '100, 000 tokens per second on the wsj test set, and almost 100, 000 tokens per second on the out - of - domain brown corpus data']",0
"['described  #TAUTHOR_TAG, in a singlet']","['described  #TAUTHOR_TAG, in a singlet']","['described  #TAUTHOR_TAG, in a singlethreaded implementation in perl on a linux workstation equipped with intel xeon x5550 2. 67 ghz processors. speed is rounded to the nearest 1, 000 tokens', 'per second,']","['second for each of the three tag dictionaries, using the fast tagging method we previously described  #TAUTHOR_TAG, in a singlethreaded implementation in perl on a linux workstation equipped with intel xeon x5550 2. 67 ghz processors. speed is rounded to the nearest 1, 000 tokens', 'per second, because we measured times to a precision of only about one part in one hundred. for the pruned kn - smoothed dictionary, we previously reported a speed of 49', ', 000 tokens per second under similar conditions. our current faster speed of 69, 000 tokens per second is due to an improved low - level implementation for computing the model scores for', 'permitted tags, and a slightly faster version of perl ( v5. 18. 2 ). the most restrictive tag dictionary, the pr', '##uned semi - supervised dictionary, allows only 1. 51 tags per token, and our implementation runs at 103', ', 000 tokens per second on the wsj development set. for our final experiments', ', we tested our tagger with this dictionary on the standard penn treebank wsj test set and on the penn treebank - 3 parsed brown corpus subset, as an out - of -', 'domain evaluation. for comparison, we tested our previous tagger and the fast version ( english - left3words - distsim ) of the stanford tagger  #AUTHOR_TAG recommended for practical use on the stanford tagger website, which we found to be by far the fastest of the six publicly available tag', '##gers tested in our previous work  #TAUTHOR_TAG. the results of these tests are shown in table 2 table 2 : wsj test set and brown corpus tagging', 'speeds and token accuracies for our previous tagger, we give three speeds : the speed we reported earlier, a speed for a', 'duplicate of the earlier experiment using the faster version of perl that we use here, and a third measurement including both the faster version of perl and our improved low - level tagger implementation. with the pruned semi - supervised', 'dictionary, our new tagger has slightly higher all - token accuracy than our previous tagger on both the wsj test set and brown corpus set, and it is much more accurate than the fast stanford tag', '##ger. the accuracy on the standard wsj test set is 97. 36 %, one of the highest ever reported. the new tagger is also much faster than either of the other taggers, achieving a speed of more than', '100, 000 tokens per second on the wsj test set, and almost 100, 000 tokens per second on the out - of - domain brown corpus data']",0
['proposed  #TAUTHOR_TAG makes tagging'],['proposed  #TAUTHOR_TAG makes tagging'],['proposed  #TAUTHOR_TAG makes tagging as fast'],"['introduced a method of inferring a tag dictionary from annotated data to speed up part - of - speech tagging by limiting the set of possible tags for each word.', ""while ratnaparkhi's tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed  #TAUTHOR_TAG makes tagging as fast as with ratnaparkhi's tag dictionary, but with no decrease in accuracy."", ""in this paper, we show that a very simple semi - supervised variant of ratnaparkhi's method results in a much tighter tag dictionary than either ratnaparkhi's or our previous method, with accuracy as high as with our previous tag dictionary but much faster tagging - more than 100, 000 tokens per second in perl."", 'in this paper, we present a new method of constructing tag dictionaries for part - of - speech ( pos ) tagging.', 'a tag dictionary is simply a list of words 1 along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed.', 'tag dictionaries are commonly used to speed up pos - tag inference by restricting the tags considered for a particular word to those specified by the dictionary.', 'early work on pos tagging generally relied heavily on manually constructed tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus  #AUTHOR_TAG.', ' #AUTHOR_TAG relied only on a tag dictionary extracted from annotated data, but he used the annotated 1 according to the conventions of the field, pos tags are assigned to all tokens in a tokenized text, including punctuation marks and other non - word tokens.', 'in this paper, all of these will be covered by the term word.', 'tags from his test data as well as his training data to construct his tag dictionary, so his evaluation was not really fair.', '2  #AUTHOR_TAG seems to have been the first to use a tag dictionary automatically extracted only from training data.', ""ratnaparkhi's method of constructing a tag dictionary substantially speeds up tagging compared to considering every possible tag for every word, but it noticeably degrades accuracy when used with a current state - of - the - art tagging model."", ""we recently presented  #TAUTHOR_TAG a new method of constructing a tag dictionary that produces a tagging speed - up comparable to ratnaparkhi's, but with no decrease in tagging accuracy."", ""in this paper, we show that a very simple semi - supervised variant of ratnaparkhi's method results in a much tighter tag dictionary than either ratnaparkhi's or our previous method, with accuracy as high as we""]",1
['recently proposed  #TAUTHOR_TAG'],['recently proposed  #TAUTHOR_TAG'],['recently proposed  #TAUTHOR_TAG'],"['a method of inferring a tag dictionary from annotated data to speed up part - of - speech tagging by limiting the set of possible tags for each word.', ""while ratnaparkhi's tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed  #TAUTHOR_TAG makes tagging as fast as with ratnaparkhi's tag dictionary, but with no decrease in accuracy."", ""in this paper, we show that a very simple semi - supervised variant of ratnaparkhi's method results in a much tighter tag dictionary than either ratnaparkhi's or our previous method, with accuracy as high as with our previous tag dictionary but much faster tagging - more than 100, 000 tokens per second in perl""]",1
"['tagging model.', 'we recently presented  #TAUTHOR_TAG a new method of constructing a tag dictionary']","['current state - of - the - art tagging model.', 'we recently presented  #TAUTHOR_TAG a new method of constructing a tag dictionary']","['when used with a current state - of - the - art tagging model.', 'we recently presented  #TAUTHOR_TAG a new method of constructing a tag dictionary']","['this paper, we present a new method of constructing tag dictionaries for part - of - speech ( pos ) tagging.', 'a tag dictionary is simply a list of words 1 along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed.', 'tag dictionaries are commonly used to speed up pos - tag inference by restricting the tags considered for a particular word to those specified by the dictionary.', 'early work on pos tagging generally relied heavily on manually constructed tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus  #AUTHOR_TAG.', ' #AUTHOR_TAG relied only on a tag dictionary extracted from annotated data, but he used the annotated tags from his test data as well as his training data to construct his tag dictionary, so his evaluation was not really fair.', '2  #AUTHOR_TAG seems to have been the first to use a tag dictionary automatically extracted only from training data.', ""ratnaparkhi's method of constructing a tag dictionary substantially speeds up tagging compared to considering every possible tag for every word, but it noticeably degrades accuracy when used with a current state - of - the - art tagging model."", ""we recently presented  #TAUTHOR_TAG a new method of constructing a tag dictionary that produces a tagging speed - up comparable to ratnaparkhi's, but with no decrease in tagging accuracy."", ""in this paper, we show that a very simple semi - supervised variant of ratnaparkhi's method results in a much tighter tag dictionary than either ratnaparkhi's or our previous method, with accuracy as high as we previously obtained, while allowing much faster tagging - more than 100, 000 tokens per second even in a perl implementation""]",1
['recently proposed  #TAUTHOR_TAG'],['recently proposed  #TAUTHOR_TAG'],['recently proposed  #TAUTHOR_TAG'],"['a method of inferring a tag dictionary from annotated data to speed up part - of - speech tagging by limiting the set of possible tags for each word.', ""while ratnaparkhi's tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed  #TAUTHOR_TAG makes tagging as fast as with ratnaparkhi's tag dictionary, but with no decrease in accuracy."", ""in this paper, we show that a very simple semi - supervised variant of ratnaparkhi's method results in a much tighter tag dictionary than either ratnaparkhi's or our previous method, with accuracy as high as with our previous tag dictionary but much faster tagging - more than 100, 000 tokens per second in perl""]",4
"['tagging model.', 'we recently presented  #TAUTHOR_TAG a new method of constructing a tag dictionary']","['current state - of - the - art tagging model.', 'we recently presented  #TAUTHOR_TAG a new method of constructing a tag dictionary']","['when used with a current state - of - the - art tagging model.', 'we recently presented  #TAUTHOR_TAG a new method of constructing a tag dictionary']","['this paper, we present a new method of constructing tag dictionaries for part - of - speech ( pos ) tagging.', 'a tag dictionary is simply a list of words 1 along with a set of possible tags for each word listed, plus one additional set of possible tags for all words not listed.', 'tag dictionaries are commonly used to speed up pos - tag inference by restricting the tags considered for a particular word to those specified by the dictionary.', 'early work on pos tagging generally relied heavily on manually constructed tag dictionaries, sometimes agumented with tag statistics derived from an annotated corpus  #AUTHOR_TAG.', ' #AUTHOR_TAG relied only on a tag dictionary extracted from annotated data, but he used the annotated tags from his test data as well as his training data to construct his tag dictionary, so his evaluation was not really fair.', '2  #AUTHOR_TAG seems to have been the first to use a tag dictionary automatically extracted only from training data.', ""ratnaparkhi's method of constructing a tag dictionary substantially speeds up tagging compared to considering every possible tag for every word, but it noticeably degrades accuracy when used with a current state - of - the - art tagging model."", ""we recently presented  #TAUTHOR_TAG a new method of constructing a tag dictionary that produces a tagging speed - up comparable to ratnaparkhi's, but with no decrease in tagging accuracy."", ""in this paper, we show that a very simple semi - supervised variant of ratnaparkhi's method results in a much tighter tag dictionary than either ratnaparkhi's or our previous method, with accuracy as high as we previously obtained, while allowing much faster tagging - more than 100, 000 tokens per second even in a perl implementation""]",4
[' #TAUTHOR_TAG for a tag dictionary that does not degrade tagging'],"[' #TAUTHOR_TAG for a tag dictionary that does not degrade tagging accuracy.', 'when combined with our previous']",[' #TAUTHOR_TAG for a tag dictionary that does not degrade tagging'],"['method of constructing a tag dictionary is technically very simple, but remarkably effective.', 'it reduces the mean number of possible tags per token by 57 % and increases the number of unambiguous tokens by by 47 %, compared to the previous state of the art  #TAUTHOR_TAG for a tag dictionary that does not degrade tagging accuracy.', 'when combined with our previous work on fast high - accuracy pos tagging, this tag dictionary produces by far the fastest pos tagger reported with anything close to comparable accuracy']",4
"['space.', ' #TAUTHOR_TAG rely on wikipedia anchors,']","['space.', ' #TAUTHOR_TAG rely on wikipedia anchors,']","['study the problem of jointly embedding a knowledge base and a text corpus.', 'the key issue is the alignment model making sure the vectors of entities, relations and words are in the same space.', ' #TAUTHOR_TAG rely on wikipedia anchors, making the applicable scope quite limited.', 'in this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors.', 'we require']","['study the problem of jointly embedding a knowledge base and a text corpus.', 'the key issue is the alignment model making sure the vectors of entities, relations and words are in the same space.', ' #TAUTHOR_TAG rely on wikipedia anchors, making the applicable scope quite limited.', 'in this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors.', 'we require the embedding vector of an entity not only to fit the structured constraints in kbs but also to be equal to the embedding vector computed from the text description.', 'extensive experiments show that, the proposed approach consistently performs comparably or even better than the method of  #TAUTHOR_TAG, which is encouraging as we do not use any anchor information']",1
['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],"['. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision']","['can be mapped to the embeddings learned from the triplet classification experiment. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision - recall curves are plot in', 'fig. ( 1 ). on both base extractors', ', the jointly embedding methods outperform separate embedding. moreover, "" jointly ( desp ) "" is slightly better', 'than "" jointly ( anchor ) "", which is in accordance with the results', 'from the link prediction experiment and the triplet classification experiment. analogical reasoning this task evaluates the quality of word embeddings  #AUTHOR_TAG b ). we use the original dataset released by  #AUTHOR_TAG b ) and follow the same evaluation protocol', '']",1
"['space.', ' #TAUTHOR_TAG rely on wikipedia anchors,']","['space.', ' #TAUTHOR_TAG rely on wikipedia anchors,']","['study the problem of jointly embedding a knowledge base and a text corpus.', 'the key issue is the alignment model making sure the vectors of entities, relations and words are in the same space.', ' #TAUTHOR_TAG rely on wikipedia anchors, making the applicable scope quite limited.', 'in this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors.', 'we require']","['study the problem of jointly embedding a knowledge base and a text corpus.', 'the key issue is the alignment model making sure the vectors of entities, relations and words are in the same space.', ' #TAUTHOR_TAG rely on wikipedia anchors, making the applicable scope quite limited.', 'in this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors.', 'we require the embedding vector of an entity not only to fit the structured constraints in kbs but also to be equal to the embedding vector computed from the text description.', 'extensive experiments show that, the proposed approach consistently performs comparably or even better than the method of  #TAUTHOR_TAG, which is encouraging as we do not use any anchor information']",4
['of  #TAUTHOR_TAG solves issue ( 1 ) by jointly embedding'],['of  #TAUTHOR_TAG solves issue ( 1 ) by jointly embedding'],"['of  #TAUTHOR_TAG solves issue ( 1 ) by jointly embedding entities, relations,']",[' #TAUTHOR_TAG'],4
"['of  #TAUTHOR_TAG,']","['of  #TAUTHOR_TAG, i. e.,']","['word ( s ) / phrase ( s ) "".', 'we follow the jointly embedding framework of  #TAUTHOR_TAG,']","['', 'the vocabulary of words is v. the union vocabulary of entities and words together is i = e ∪ v. in this paper "" word ( s ) "" refers to "" word ( s ) / phrase ( s ) "".', 'we follow the jointly embedding framework of  #TAUTHOR_TAG, i. e., learning optimal embeddings by minimizing the following loss', 'where l k, l t and l a are the component loss functions of the knowledge model, text model and alignment model respectively.', 'our focus is on a new alignment model l a while the knowledge model l k and text model l t are the same as the counterparts in  #TAUTHOR_TAG.', 'however, to make the content self - contained, we still need to briefly explain l k and l t.', 'knowledge model describes the plausibility of a triplet ( h, r, t ) by defining', 'where z ( h, r, t ) = b − 0. 5 · h + r − t 2 2, b = 7 as suggested by  #TAUTHOR_TAG.', 'pr ( r | h, t ) and pr ( t | h, r ) are defined in the same way.', 'the loss function of knowledge model is then defined as', 'log pr ( h | r, t ) + log pr ( t | h, r ) + log pr ( r | h, t ) ( 4 )', 'text model defines the probability of a pair of words w and v co - occurring in a text window :', 'where', 'then the loss function of text model is', 'alignment model this part is different from  #TAUTHOR_TAG.', 'for each word w in the description of entity e, we define pr ( w | e ), the conditional probability of predicting w given e :', 'pr ( w | e ) = exp { z ( e, w ) } w∈v exp { z ( e, w ) },', 'where z ( e, w ) = b − 0. 5 · e − w 2 2.', 'notice']",4
['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],"['. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision']","['can be mapped to the embeddings learned from the triplet classification experiment. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision - recall curves are plot in', 'fig. ( 1 ). on both base extractors', ', the jointly embedding methods outperform separate embedding. moreover, "" jointly ( desp ) "" is slightly better', 'than "" jointly ( anchor ) "", which is in accordance with the results', 'from the link prediction experiment and the triplet classification experiment. analogical reasoning this task evaluates the quality of word embeddings  #AUTHOR_TAG b ). we use the original dataset released by  #AUTHOR_TAG b ) and follow the same evaluation protocol', '']",4
['of  #TAUTHOR_TAG solves issue ( 1 ) by jointly embedding'],['of  #TAUTHOR_TAG solves issue ( 1 ) by jointly embedding'],"['of  #TAUTHOR_TAG solves issue ( 1 ) by jointly embedding entities, relations,']",[' #TAUTHOR_TAG'],0
"['embedding models thereafter including this paper are variants of this model  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'skip - gram this is an efficient']","['embedding models thereafter including this paper are variants of this model  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'skip - gram this is an efficient']","['. the model is simple, effective and efficient.', 'most knowledge embedding models thereafter including this paper are variants of this model  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'skip - gram this is an efficient word embedding method proposed by  #AUTHOR_TAG a ), which learns word embeddings from word concurrencies in text windows.', 'without any supervision,']","['##e this is a representative knowledge embedding model proposed by.', 'for a fact ( h, r, t ) in kbs, where h is the head entity, r is the relation, and t is the tail entity, transe models the relation r as a translation vector r connecting the embeddings h and t of the two entities, i. e., h + r is close to t. the model is simple, effective and efficient.', 'most knowledge embedding models thereafter including this paper are variants of this model  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'skip - gram this is an efficient word embedding method proposed by  #AUTHOR_TAG a ), which learns word embeddings from word concurrencies in text windows.', ""without any supervision, it amazingly recovers the semantic relations between words in a vector space such as'king'−'queen'≈'man'−'women '."", 'however, as it is unsupervised, it cannot tell the exact relation between two words.', ' #TAUTHOR_TAG combines knowledge embedding and word embedding in a joint framework so that the entities / relations and words are in the same vector space and hence operators like inner product ( similarity ) between them are meaningful.', 'this brings convenience to tasks requiring computation between knowledge bases and text.', 'meanwhile, jointly embedding utilizes information from both structured kbs and unstructured text and hence the knowledge embedding and word embedding can be enhanced by each other.', '']",0
"['embedding models thereafter including this paper are variants of this model  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'skip - gram this is an efficient']","['embedding models thereafter including this paper are variants of this model  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'skip - gram this is an efficient']","['. the model is simple, effective and efficient.', 'most knowledge embedding models thereafter including this paper are variants of this model  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'skip - gram this is an efficient word embedding method proposed by  #AUTHOR_TAG a ), which learns word embeddings from word concurrencies in text windows.', 'without any supervision,']","['##e this is a representative knowledge embedding model proposed by.', 'for a fact ( h, r, t ) in kbs, where h is the head entity, r is the relation, and t is the tail entity, transe models the relation r as a translation vector r connecting the embeddings h and t of the two entities, i. e., h + r is close to t. the model is simple, effective and efficient.', 'most knowledge embedding models thereafter including this paper are variants of this model  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'skip - gram this is an efficient word embedding method proposed by  #AUTHOR_TAG a ), which learns word embeddings from word concurrencies in text windows.', ""without any supervision, it amazingly recovers the semantic relations between words in a vector space such as'king'−'queen'≈'man'−'women '."", 'however, as it is unsupervised, it cannot tell the exact relation between two words.', ' #TAUTHOR_TAG combines knowledge embedding and word embedding in a joint framework so that the entities / relations and words are in the same vector space and hence operators like inner product ( similarity ) between them are meaningful.', 'this brings convenience to tasks requiring computation between knowledge bases and text.', 'meanwhile, jointly embedding utilizes information from both structured kbs and unstructured text and hence the knowledge embedding and word embedding can be enhanced by each other.', '']",0
"['embedding models thereafter including this paper are variants of this model  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'skip - gram this is an efficient']","['embedding models thereafter including this paper are variants of this model  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'skip - gram this is an efficient']","['. the model is simple, effective and efficient.', 'most knowledge embedding models thereafter including this paper are variants of this model  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'skip - gram this is an efficient word embedding method proposed by  #AUTHOR_TAG a ), which learns word embeddings from word concurrencies in text windows.', 'without any supervision,']","['##e this is a representative knowledge embedding model proposed by.', 'for a fact ( h, r, t ) in kbs, where h is the head entity, r is the relation, and t is the tail entity, transe models the relation r as a translation vector r connecting the embeddings h and t of the two entities, i. e., h + r is close to t. the model is simple, effective and efficient.', 'most knowledge embedding models thereafter including this paper are variants of this model  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'skip - gram this is an efficient word embedding method proposed by  #AUTHOR_TAG a ), which learns word embeddings from word concurrencies in text windows.', ""without any supervision, it amazingly recovers the semantic relations between words in a vector space such as'king'−'queen'≈'man'−'women '."", 'however, as it is unsupervised, it cannot tell the exact relation between two words.', ' #TAUTHOR_TAG combines knowledge embedding and word embedding in a joint framework so that the entities / relations and words are in the same vector space and hence operators like inner product ( similarity ) between them are meaningful.', 'this brings convenience to tasks requiring computation between knowledge bases and text.', 'meanwhile, jointly embedding utilizes information from both structured kbs and unstructured text and hence the knowledge embedding and word embedding can be enhanced by each other.', '']",5
"['of  #TAUTHOR_TAG,']","['of  #TAUTHOR_TAG, i. e.,']","['word ( s ) / phrase ( s ) "".', 'we follow the jointly embedding framework of  #TAUTHOR_TAG,']","['', 'the vocabulary of words is v. the union vocabulary of entities and words together is i = e ∪ v. in this paper "" word ( s ) "" refers to "" word ( s ) / phrase ( s ) "".', 'we follow the jointly embedding framework of  #TAUTHOR_TAG, i. e., learning optimal embeddings by minimizing the following loss', 'where l k, l t and l a are the component loss functions of the knowledge model, text model and alignment model respectively.', 'our focus is on a new alignment model l a while the knowledge model l k and text model l t are the same as the counterparts in  #TAUTHOR_TAG.', 'however, to make the content self - contained, we still need to briefly explain l k and l t.', 'knowledge model describes the plausibility of a triplet ( h, r, t ) by defining', 'where z ( h, r, t ) = b − 0. 5 · h + r − t 2 2, b = 7 as suggested by  #TAUTHOR_TAG.', 'pr ( r | h, t ) and pr ( t | h, r ) are defined in the same way.', 'the loss function of knowledge model is then defined as', 'log pr ( h | r, t ) + log pr ( t | h, r ) + log pr ( r | h, t ) ( 4 )', 'text model defines the probability of a pair of words w and v co - occurring in a text window :', 'where', 'then the loss function of text model is', 'alignment model this part is different from  #TAUTHOR_TAG.', 'for each word w in the description of entity e, we define pr ( w | e ), the conditional probability of predicting w given e :', 'pr ( w | e ) = exp { z ( e, w ) } w∈v exp { z ( e, w ) },', 'where z ( e, w ) = b − 0. 5 · e − w 2 2.', 'notice']",5
"['of  #TAUTHOR_TAG,']","['of  #TAUTHOR_TAG, i. e.,']","['word ( s ) / phrase ( s ) "".', 'we follow the jointly embedding framework of  #TAUTHOR_TAG,']","['', 'the vocabulary of words is v. the union vocabulary of entities and words together is i = e ∪ v. in this paper "" word ( s ) "" refers to "" word ( s ) / phrase ( s ) "".', 'we follow the jointly embedding framework of  #TAUTHOR_TAG, i. e., learning optimal embeddings by minimizing the following loss', 'where l k, l t and l a are the component loss functions of the knowledge model, text model and alignment model respectively.', 'our focus is on a new alignment model l a while the knowledge model l k and text model l t are the same as the counterparts in  #TAUTHOR_TAG.', 'however, to make the content self - contained, we still need to briefly explain l k and l t.', 'knowledge model describes the plausibility of a triplet ( h, r, t ) by defining', 'where z ( h, r, t ) = b − 0. 5 · h + r − t 2 2, b = 7 as suggested by  #TAUTHOR_TAG.', 'pr ( r | h, t ) and pr ( t | h, r ) are defined in the same way.', 'the loss function of knowledge model is then defined as', 'log pr ( h | r, t ) + log pr ( t | h, r ) + log pr ( r | h, t ) ( 4 )', 'text model defines the probability of a pair of words w and v co - occurring in a text window :', 'where', 'then the loss function of text model is', 'alignment model this part is different from  #TAUTHOR_TAG.', 'for each word w in the description of entity e, we define pr ( w | e ), the conditional probability of predicting w given e :', 'pr ( w | e ) = exp { z ( e, w ) } w∈v exp { z ( e, w ) },', 'where z ( e, w ) = b − 0. 5 · e − w 2 2.', 'notice']",5
"['of  #TAUTHOR_TAG,']","['of  #TAUTHOR_TAG, i. e.,']","['word ( s ) / phrase ( s ) "".', 'we follow the jointly embedding framework of  #TAUTHOR_TAG,']","['', 'the vocabulary of words is v. the union vocabulary of entities and words together is i = e ∪ v. in this paper "" word ( s ) "" refers to "" word ( s ) / phrase ( s ) "".', 'we follow the jointly embedding framework of  #TAUTHOR_TAG, i. e., learning optimal embeddings by minimizing the following loss', 'where l k, l t and l a are the component loss functions of the knowledge model, text model and alignment model respectively.', 'our focus is on a new alignment model l a while the knowledge model l k and text model l t are the same as the counterparts in  #TAUTHOR_TAG.', 'however, to make the content self - contained, we still need to briefly explain l k and l t.', 'knowledge model describes the plausibility of a triplet ( h, r, t ) by defining', 'where z ( h, r, t ) = b − 0. 5 · h + r − t 2 2, b = 7 as suggested by  #TAUTHOR_TAG.', 'pr ( r | h, t ) and pr ( t | h, r ) are defined in the same way.', 'the loss function of knowledge model is then defined as', 'log pr ( h | r, t ) + log pr ( t | h, r ) + log pr ( r | h, t ) ( 4 )', 'text model defines the probability of a pair of words w and v co - occurring in a text window :', 'where', 'then the loss function of text model is', 'alignment model this part is different from  #TAUTHOR_TAG.', 'for each word w in the description of entity e, we define pr ( w | e ), the conditional probability of predicting w given e :', 'pr ( w | e ) = exp { z ( e, w ) } w∈v exp { z ( e, w ) },', 'where z ( e, w ) = b − 0. 5 · e − w 2 2.', 'notice']",5
['protocol in  #TAUTHOR_TAG. we'],"['protocol in  #TAUTHOR_TAG. we train their models via our own implementation on our dataset. the', 'results are in']","['correct fact or not. it is used in  #AUTHOR_TAG b', ';  #AUTHOR_TAG a ). we follow the same protocol in  #TAUTHOR_TAG. we']","['01, 0. 025 }, the number of negative examples per positive example c in { 5, 10 }, the max', 'skiprange s in { 5, 10 }', 'and traverse the text corpus with only 1 epoch. the best configurations of "" jointly ( anchor ) "" and "" jointly ( desp ) "" are exactly the', 'same : k = 100, α = 0. 025, c = 10, s = 5. from the results, we observe that : ( 1 ) both jointly embedding methods are much better than the baseline transe, which demonstrates that external textual resources make entity', 'embeddings become more discriminative. intuitively, "" jointly ( anchor ) "" indicates "" how to use', 'an entity in text "", while "" jointly ( desp ) "" shows "" what is', 'the definition / meaning of an entity "". both are helpful to distinguish an entity from', 'others. ( 2 ) under the setting', 'of "" raw "", "" jointly ( desp ) "" and ""', 'jointly ( anchor ) "" are comparable. in other settings "" jointly ( desp ) "" wins. triplet classification this is a binary classification task,', 'predicting whether a candidate triplet ( h, r, t ) is a correct fact or not. it is used in  #AUTHOR_TAG b', ';  #AUTHOR_TAG a ). we follow the same protocol in  #TAUTHOR_TAG. we train their models via our own implementation on our dataset. the', 'results are in table 2. "" e - e "" means both sides of a triplet ( h, r, t ) are entities in kb, "" e -', '']",5
['protocol in  #TAUTHOR_TAG. we'],"['protocol in  #TAUTHOR_TAG. we train their models via our own implementation on our dataset. the', 'results are in']","['correct fact or not. it is used in  #AUTHOR_TAG b', ';  #AUTHOR_TAG a ). we follow the same protocol in  #TAUTHOR_TAG. we']","['01, 0. 025 }, the number of negative examples per positive example c in { 5, 10 }, the max', 'skiprange s in { 5, 10 }', 'and traverse the text corpus with only 1 epoch. the best configurations of "" jointly ( anchor ) "" and "" jointly ( desp ) "" are exactly the', 'same : k = 100, α = 0. 025, c = 10, s = 5. from the results, we observe that : ( 1 ) both jointly embedding methods are much better than the baseline transe, which demonstrates that external textual resources make entity', 'embeddings become more discriminative. intuitively, "" jointly ( anchor ) "" indicates "" how to use', 'an entity in text "", while "" jointly ( desp ) "" shows "" what is', 'the definition / meaning of an entity "". both are helpful to distinguish an entity from', 'others. ( 2 ) under the setting', 'of "" raw "", "" jointly ( desp ) "" and ""', 'jointly ( anchor ) "" are comparable. in other settings "" jointly ( desp ) "" wins. triplet classification this is a binary classification task,', 'predicting whether a candidate triplet ( h, r, t ) is a correct fact or not. it is used in  #AUTHOR_TAG b', ';  #AUTHOR_TAG a ). we follow the same protocol in  #TAUTHOR_TAG. we train their models via our own implementation on our dataset. the', 'results are in table 2. "" e - e "" means both sides of a triplet ( h, r, t ) are entities in kb, "" e -', '']",5
['protocol in  #TAUTHOR_TAG. we'],"['protocol in  #TAUTHOR_TAG. we train their models via our own implementation on our dataset. the', 'results are in']","['correct fact or not. it is used in  #AUTHOR_TAG b', ';  #AUTHOR_TAG a ). we follow the same protocol in  #TAUTHOR_TAG. we']","['01, 0. 025 }, the number of negative examples per positive example c in { 5, 10 }, the max', 'skiprange s in { 5, 10 }', 'and traverse the text corpus with only 1 epoch. the best configurations of "" jointly ( anchor ) "" and "" jointly ( desp ) "" are exactly the', 'same : k = 100, α = 0. 025, c = 10, s = 5. from the results, we observe that : ( 1 ) both jointly embedding methods are much better than the baseline transe, which demonstrates that external textual resources make entity', 'embeddings become more discriminative. intuitively, "" jointly ( anchor ) "" indicates "" how to use', 'an entity in text "", while "" jointly ( desp ) "" shows "" what is', 'the definition / meaning of an entity "". both are helpful to distinguish an entity from', 'others. ( 2 ) under the setting', 'of "" raw "", "" jointly ( desp ) "" and ""', 'jointly ( anchor ) "" are comparable. in other settings "" jointly ( desp ) "" wins. triplet classification this is a binary classification task,', 'predicting whether a candidate triplet ( h, r, t ) is a correct fact or not. it is used in  #AUTHOR_TAG b', ';  #AUTHOR_TAG a ). we follow the same protocol in  #TAUTHOR_TAG. we train their models via our own implementation on our dataset. the', 'results are in table 2. "" e - e "" means both sides of a triplet ( h, r, t ) are entities in kb, "" e -', '']",5
['protocol in  #TAUTHOR_TAG. we'],"['protocol in  #TAUTHOR_TAG. we train their models via our own implementation on our dataset. the', 'results are in']","['correct fact or not. it is used in  #AUTHOR_TAG b', ';  #AUTHOR_TAG a ). we follow the same protocol in  #TAUTHOR_TAG. we']","['01, 0. 025 }, the number of negative examples per positive example c in { 5, 10 }, the max', 'skiprange s in { 5, 10 }', 'and traverse the text corpus with only 1 epoch. the best configurations of "" jointly ( anchor ) "" and "" jointly ( desp ) "" are exactly the', 'same : k = 100, α = 0. 025, c = 10, s = 5. from the results, we observe that : ( 1 ) both jointly embedding methods are much better than the baseline transe, which demonstrates that external textual resources make entity', 'embeddings become more discriminative. intuitively, "" jointly ( anchor ) "" indicates "" how to use', 'an entity in text "", while "" jointly ( desp ) "" shows "" what is', 'the definition / meaning of an entity "". both are helpful to distinguish an entity from', 'others. ( 2 ) under the setting', 'of "" raw "", "" jointly ( desp ) "" and ""', 'jointly ( anchor ) "" are comparable. in other settings "" jointly ( desp ) "" wins. triplet classification this is a binary classification task,', 'predicting whether a candidate triplet ( h, r, t ) is a correct fact or not. it is used in  #AUTHOR_TAG b', ';  #AUTHOR_TAG a ). we follow the same protocol in  #TAUTHOR_TAG. we train their models via our own implementation on our dataset. the', 'results are in table 2. "" e - e "" means both sides of a triplet ( h, r, t ) are entities in kb, "" e -', '']",5
['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],"['. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision']","['can be mapped to the embeddings learned from the triplet classification experiment. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision - recall curves are plot in', 'fig. ( 1 ). on both base extractors', ', the jointly embedding methods outperform separate embedding. moreover, "" jointly ( desp ) "" is slightly better', 'than "" jointly ( anchor ) "", which is in accordance with the results', 'from the link prediction experiment and the triplet classification experiment. analogical reasoning this task evaluates the quality of word embeddings  #AUTHOR_TAG b ). we use the original dataset released by  #AUTHOR_TAG b ) and follow the same evaluation protocol', '']",5
['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],"['. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision']","['can be mapped to the embeddings learned from the triplet classification experiment. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision - recall curves are plot in', 'fig. ( 1 ). on both base extractors', ', the jointly embedding methods outperform separate embedding. moreover, "" jointly ( desp ) "" is slightly better', 'than "" jointly ( anchor ) "", which is in accordance with the results', 'from the link prediction experiment and the triplet classification experiment. analogical reasoning this task evaluates the quality of word embeddings  #AUTHOR_TAG b ). we use the original dataset released by  #AUTHOR_TAG b ) and follow the same evaluation protocol', '']",5
['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],"['. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision']","['can be mapped to the embeddings learned from the triplet classification experiment. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision - recall curves are plot in', 'fig. ( 1 ). on both base extractors', ', the jointly embedding methods outperform separate embedding. moreover, "" jointly ( desp ) "" is slightly better', 'than "" jointly ( anchor ) "", which is in accordance with the results', 'from the link prediction experiment and the triplet classification experiment. analogical reasoning this task evaluates the quality of word embeddings  #AUTHOR_TAG b ). we use the original dataset released by  #AUTHOR_TAG b ) and follow the same evaluation protocol', '']",5
['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],"['. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision']","['can be mapped to the embeddings learned from the triplet classification experiment. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision - recall curves are plot in', 'fig. ( 1 ). on both base extractors', ', the jointly embedding methods outperform separate embedding. moreover, "" jointly ( desp ) "" is slightly better', 'than "" jointly ( anchor ) "", which is in accordance with the results', 'from the link prediction experiment and the triplet classification experiment. analogical reasoning this task evaluates the quality of word embeddings  #AUTHOR_TAG b ). we use the original dataset released by  #AUTHOR_TAG b ) and follow the same evaluation protocol', '']",5
"['of  #TAUTHOR_TAG,']","['of  #TAUTHOR_TAG, i. e.,']","['word ( s ) / phrase ( s ) "".', 'we follow the jointly embedding framework of  #TAUTHOR_TAG,']","['', 'the vocabulary of words is v. the union vocabulary of entities and words together is i = e ∪ v. in this paper "" word ( s ) "" refers to "" word ( s ) / phrase ( s ) "".', 'we follow the jointly embedding framework of  #TAUTHOR_TAG, i. e., learning optimal embeddings by minimizing the following loss', 'where l k, l t and l a are the component loss functions of the knowledge model, text model and alignment model respectively.', 'our focus is on a new alignment model l a while the knowledge model l k and text model l t are the same as the counterparts in  #TAUTHOR_TAG.', 'however, to make the content self - contained, we still need to briefly explain l k and l t.', 'knowledge model describes the plausibility of a triplet ( h, r, t ) by defining', 'where z ( h, r, t ) = b − 0. 5 · h + r − t 2 2, b = 7 as suggested by  #TAUTHOR_TAG.', 'pr ( r | h, t ) and pr ( t | h, r ) are defined in the same way.', 'the loss function of knowledge model is then defined as', 'log pr ( h | r, t ) + log pr ( t | h, r ) + log pr ( r | h, t ) ( 4 )', 'text model defines the probability of a pair of words w and v co - occurring in a text window :', 'where', 'then the loss function of text model is', 'alignment model this part is different from  #TAUTHOR_TAG.', 'for each word w in the description of entity e, we define pr ( w | e ), the conditional probability of predicting w given e :', 'pr ( w | e ) = exp { z ( e, w ) } w∈v exp { z ( e, w ) },', 'where z ( e, w ) = b − 0. 5 · e − w 2 2.', 'notice']",3
['protocol in  #TAUTHOR_TAG. we'],"['protocol in  #TAUTHOR_TAG. we train their models via our own implementation on our dataset. the', 'results are in']","['correct fact or not. it is used in  #AUTHOR_TAG b', ';  #AUTHOR_TAG a ). we follow the same protocol in  #TAUTHOR_TAG. we']","['01, 0. 025 }, the number of negative examples per positive example c in { 5, 10 }, the max', 'skiprange s in { 5, 10 }', 'and traverse the text corpus with only 1 epoch. the best configurations of "" jointly ( anchor ) "" and "" jointly ( desp ) "" are exactly the', 'same : k = 100, α = 0. 025, c = 10, s = 5. from the results, we observe that : ( 1 ) both jointly embedding methods are much better than the baseline transe, which demonstrates that external textual resources make entity', 'embeddings become more discriminative. intuitively, "" jointly ( anchor ) "" indicates "" how to use', 'an entity in text "", while "" jointly ( desp ) "" shows "" what is', 'the definition / meaning of an entity "". both are helpful to distinguish an entity from', 'others. ( 2 ) under the setting', 'of "" raw "", "" jointly ( desp ) "" and ""', 'jointly ( anchor ) "" are comparable. in other settings "" jointly ( desp ) "" wins. triplet classification this is a binary classification task,', 'predicting whether a candidate triplet ( h, r, t ) is a correct fact or not. it is used in  #AUTHOR_TAG b', ';  #AUTHOR_TAG a ). we follow the same protocol in  #TAUTHOR_TAG. we train their models via our own implementation on our dataset. the', 'results are in table 2. "" e - e "" means both sides of a triplet ( h, r, t ) are entities in kb, "" e -', '']",3
['protocol in  #TAUTHOR_TAG. we'],"['protocol in  #TAUTHOR_TAG. we train their models via our own implementation on our dataset. the', 'results are in']","['correct fact or not. it is used in  #AUTHOR_TAG b', ';  #AUTHOR_TAG a ). we follow the same protocol in  #TAUTHOR_TAG. we']","['01, 0. 025 }, the number of negative examples per positive example c in { 5, 10 }, the max', 'skiprange s in { 5, 10 }', 'and traverse the text corpus with only 1 epoch. the best configurations of "" jointly ( anchor ) "" and "" jointly ( desp ) "" are exactly the', 'same : k = 100, α = 0. 025, c = 10, s = 5. from the results, we observe that : ( 1 ) both jointly embedding methods are much better than the baseline transe, which demonstrates that external textual resources make entity', 'embeddings become more discriminative. intuitively, "" jointly ( anchor ) "" indicates "" how to use', 'an entity in text "", while "" jointly ( desp ) "" shows "" what is', 'the definition / meaning of an entity "". both are helpful to distinguish an entity from', 'others. ( 2 ) under the setting', 'of "" raw "", "" jointly ( desp ) "" and ""', 'jointly ( anchor ) "" are comparable. in other settings "" jointly ( desp ) "" wins. triplet classification this is a binary classification task,', 'predicting whether a candidate triplet ( h, r, t ) is a correct fact or not. it is used in  #AUTHOR_TAG b', ';  #AUTHOR_TAG a ). we follow the same protocol in  #TAUTHOR_TAG. we train their models via our own implementation on our dataset. the', 'results are in table 2. "" e - e "" means both sides of a triplet ( h, r, t ) are entities in kb, "" e -', '']",3
['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],"['. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision']","['can be mapped to the embeddings learned from the triplet classification experiment. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision - recall curves are plot in', 'fig. ( 1 ). on both base extractors', ', the jointly embedding methods outperform separate embedding. moreover, "" jointly ( desp ) "" is slightly better', 'than "" jointly ( anchor ) "", which is in accordance with the results', 'from the link prediction experiment and the triplet classification experiment. analogical reasoning this task evaluates the quality of word embeddings  #AUTHOR_TAG b ). we use the original dataset released by  #AUTHOR_TAG b ) and follow the same evaluation protocol', '']",3
['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],"['. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision']","['can be mapped to the embeddings learned from the triplet classification experiment. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision - recall curves are plot in', 'fig. ( 1 ). on both base extractors', ', the jointly embedding methods outperform separate embedding. moreover, "" jointly ( desp ) "" is slightly better', 'than "" jointly ( anchor ) "", which is in accordance with the results', 'from the link prediction experiment and the triplet classification experiment. analogical reasoning this task evaluates the quality of word embeddings  #AUTHOR_TAG b ). we use the original dataset released by  #AUTHOR_TAG b ) and follow the same evaluation protocol', '']",3
['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],"['. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision']","['can be mapped to the embeddings learned from the triplet classification experiment. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision - recall curves are plot in', 'fig. ( 1 ). on both base extractors', ', the jointly embedding methods outperform separate embedding. moreover, "" jointly ( desp ) "" is slightly better', 'than "" jointly ( anchor ) "", which is in accordance with the results', 'from the link prediction experiment and the triplet classification experiment. analogical reasoning this task evaluates the quality of word embeddings  #AUTHOR_TAG b ). we use the original dataset released by  #AUTHOR_TAG b ) and follow the same evaluation protocol', '']",3
['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],['method in  #TAUTHOR_TAG to linearly combine the scores. the precision'],"['. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision']","['can be mapped to the embeddings learned from the triplet classification experiment. since both mintz and miml are probabilistic models, we use the same method in  #TAUTHOR_TAG to linearly combine the scores. the precision - recall curves are plot in', 'fig. ( 1 ). on both base extractors', ', the jointly embedding methods outperform separate embedding. moreover, "" jointly ( desp ) "" is slightly better', 'than "" jointly ( anchor ) "", which is in accordance with the results', 'from the link prediction experiment and the triplet classification experiment. analogical reasoning this task evaluates the quality of word embeddings  #AUTHOR_TAG b ). we use the original dataset released by  #AUTHOR_TAG b ) and follow the same evaluation protocol', '']",3
['task 2  #TAUTHOR_TAG required sentiment'],['task 2  #TAUTHOR_TAG required sentiment'],"['task 2  #TAUTHOR_TAG required sentiment analysis of twitter and sms text messages.', 'being the pre - decessor task of the challenge for']","['popularity of social networks and microblogging facilitated the sharing of opinions.', 'to know whether people are satisfied or not with a particular brand or product is of great interest to marketing companies.', 'much work has appeared in sa, trying to capture valuable information in expressions of contentment or discontentment.', 'important international scientific events, nlp related, include sa challenges and workshops.', 'this was the case in semeval - 2013, whose task 2  #TAUTHOR_TAG required sentiment analysis of twitter and sms text messages.', ""being the pre - decessor task of the challenge for which this work was developed, it is similar to this year's task 9."", 'the participating systems achieved better results in contextual polarity subtask ( a ) than those obtained for the overall message polarity subtask ( b ).', 'in that edition, the best results were obtained by systems in constrained mode.', 'the most common method was supervised ml with features that can be related to text words, syntactic function, discourse elements relation, internet slang and symbols, or clues from sentiment lexicons.', 'in that task, the nrc - canada system  #AUTHOR_TAG obtained the best performance, achieving an f1 of 88. 9 % in subtask a and 69 % in subtask b. that system used one svm classifier for each subtask, together with text surface based features, features associated with manually created and automatically generated sentiment lexicons, and n - gram features.', 'other systems with good results in that task were gu - mlt - lt ( gunther and  #AUTHOR_TAG and avaya  #AUTHOR_TAG.', 'the first was implemented in the python language.', 'it includes features for : text tokens after normalization, stems, word clusters, and two values for the accumulated positive and accumulated negative sentiwordnet  #AUTHOR_TAG scores, considering negation.', 'its machine learning classifier is based on linear models with stochastic gradient descent.', 'the approach taken in the avaya system centers on training highdimensional, linear classifiers with a combination of lexical and syntactic features.', 'this system uses bag - of - words features, with negation represented in word suffix, and including not only the raw word forms but also combinations with lemmas and pos tags.', 'then, word polarity features are added, using the mpqa lexicon  #AUTHOR_TAG, as well as syntactic dependency and pos tag features.', 'other features consider emoticons, capitalization, character repetition, and emphasis characters, such as asterisks and dashes.', 'the resulting model was trained with the liblinear  #AUTHOR_TAG classification library.', 'another nlp task very close to sa is polarity classification on the reputation of an entity.', 'here, instead the']",0
"['past edition  #TAUTHOR_TAG.', 'this time, we implemented']","['past edition  #TAUTHOR_TAG.', 'this time, we implemented']","['lower score in the unconstrained mode, something that happened also with many systems in the past edition  #TAUTHOR_TAG.', 'this time, we implemented']","['last year experience, we participated in semeval - 2014 task 9 to test our approach for a real - time sa system for the english used nowadays in social media content.', 'we changed the method for subtask a, now considering also the text around the area to classify, by dedicating new features to it, which led to good results.', 'our method for overall sentiment is ml based, using a restricted set of features that are dedicated to superficial text properties, negation presence, and sentiment lexicons.', 'without a deep linguistic analysis, our system achieved a reasonable result in subtask b. the evaluation of our solution, in both subtasks, shows an appreciable improvement, by 10 % or more, when compared to our results in 2013.', 'we believe that the additional training instances used in unconstrained mode and subtask b, about laptops and restaurants, have a writing style different from most of the test set documents.', 'and perhaps this is the cause for lower score in the unconstrained mode, something that happened also with many systems in the past edition  #TAUTHOR_TAG.', 'this time, we implemented the contextual polarity solution based on the subtask b classifier.', 'given the results, we intend to do, in the near future, a new iteration of our system where the overall classifier will depend on ( or receive features from ) the current subtask a classifier.', 'it seems to us that senti. ue feature engineering can be improved, maintaining this line of development.', 'once stabilized, the introduction of named entity recognition and a richer linguistic analysis will help to identify the sentiment target entities, as the ultimate goal for this system']",3
"['', '3,  #TAUTHOR_TAG the shallow word']","['', '3,  #TAUTHOR_TAG the shallow word']","['linear methods when trained with 120k documents. in [', '3,  #TAUTHOR_TAG the shallow word - cnn was shown to perform well, using training sets ( most intensively, 25k documents )']","['word - cnn. •  #AUTHOR_TAG [ 1 ] : very deep character - level cnns ( taking sequences of', 'characters as input ), which we abbreviate as char - cnn. although both studies report higher accuracy than previous work on their respective datasets, it is not clear how they compare with each other due to lack of direct comparison. in [ 1 ], the very deep char - cnn was shown to perform well with larger training data', '( up to 2. 6m documents ) but perform relatively poorly with smaller training data ; e. g., it underperformed linear methods when trained with 120k documents. in [', '3,  #TAUTHOR_TAG the shallow word - cnn was shown to perform well, using training sets ( most intensively, 25k documents ) that are mostly smaller than those used in [ 1 ]. while these results imply that the shallow word - cnn is likely to outperform the deep char - cnn when', 'trained with relatively small training sets such as those used in [ 3,  #TAUTHOR_TAG, the shallow word - cnn is untested on the', 'training sets as large as those used in [ 1 ]. hence, the purpose of this report is to fill the gap by testing the shallow word - cnns as in [ 3', '']",1
"['', '3,  #TAUTHOR_TAG the shallow word']","['', '3,  #TAUTHOR_TAG the shallow word']","['linear methods when trained with 120k documents. in [', '3,  #TAUTHOR_TAG the shallow word - cnn was shown to perform well, using training sets ( most intensively, 25k documents )']","['word - cnn. •  #AUTHOR_TAG [ 1 ] : very deep character - level cnns ( taking sequences of', 'characters as input ), which we abbreviate as char - cnn. although both studies report higher accuracy than previous work on their respective datasets, it is not clear how they compare with each other due to lack of direct comparison. in [ 1 ], the very deep char - cnn was shown to perform well with larger training data', '( up to 2. 6m documents ) but perform relatively poorly with smaller training data ; e. g., it underperformed linear methods when trained with 120k documents. in [', '3,  #TAUTHOR_TAG the shallow word - cnn was shown to perform well, using training sets ( most intensively, 25k documents ) that are mostly smaller than those used in [ 1 ]. while these results imply that the shallow word - cnn is likely to outperform the deep char - cnn when', 'trained with relatively small training sets such as those used in [ 3,  #TAUTHOR_TAG, the shallow word - cnn is untested on the', 'training sets as large as those used in [ 1 ]. hence, the purpose of this report is to fill the gap by testing the shallow word - cnns as in [ 3', '']",1
"['', '3,  #TAUTHOR_TAG the shallow word']","['', '3,  #TAUTHOR_TAG the shallow word']","['linear methods when trained with 120k documents. in [', '3,  #TAUTHOR_TAG the shallow word - cnn was shown to perform well, using training sets ( most intensively, 25k documents )']","['word - cnn. •  #AUTHOR_TAG [ 1 ] : very deep character - level cnns ( taking sequences of', 'characters as input ), which we abbreviate as char - cnn. although both studies report higher accuracy than previous work on their respective datasets, it is not clear how they compare with each other due to lack of direct comparison. in [ 1 ], the very deep char - cnn was shown to perform well with larger training data', '( up to 2. 6m documents ) but perform relatively poorly with smaller training data ; e. g., it underperformed linear methods when trained with 120k documents. in [', '3,  #TAUTHOR_TAG the shallow word - cnn was shown to perform well, using training sets ( most intensively, 25k documents ) that are mostly smaller than those used in [ 1 ]. while these results imply that the shallow word - cnn is likely to outperform the deep char - cnn when', 'trained with relatively small training sets such as those used in [ 3,  #TAUTHOR_TAG, the shallow word - cnn is untested on the', 'training sets as large as those used in [ 1 ]. hence, the purpose of this report is to fill the gap by testing the shallow word - cnns as in [ 3', '']",3
"[' #TAUTHOR_TAG, additional input']","[' #TAUTHOR_TAG, additional input']","[' #TAUTHOR_TAG, additional input']","['rates in table 1 ( b ), we show the error rate results of the shallow word - cnn in comparison with the best results of the deep char - cnn reported in [ 1 ] and the best results of linear models reported in [ 9 ].', 'on each dataset, the best results are shown in bold and the second best results are shown in the italic font.', 'on all datasets, the shallow word - cnn with tv - embeddings performs the best.', 'the second best performer is the shallow word - cnn without tv - embedding on all but ama. f ( amazon full ).', 'whereas the deep char - cnn underperforms traditional linear models when training data is relatively small, the shallow word - cnns with and without tv - embedding clearly outperform them on all the datasets.', 'we observe that, as in our previous work  #TAUTHOR_TAG, additional input produced by tv - embeddings led to substantial improvements.', 'the performances of word - cnn without tv - embedding might be further improved by having multiple region sizes [ 3, 6 ], but for simplicity, we did not attempt it in this work.', 'model size and computation time in table 2, we observe that, compared with the deep char - cnn, the shallow word - cnn has more parameters but computes much faster.', '']",3
"['the shallow word - cnns as in [ 3,  #TAUTHOR_TAG generally achieved better error rates than those of the very deep char - cnns reported in [ 1 ].', '• the shallow word - cnn computes much faster than the very deep char - cnn.', 'this is']","['the shallow word - cnns as in [ 3,  #TAUTHOR_TAG generally achieved better error rates than those of the very deep char - cnns reported in [ 1 ].', '• the shallow word - cnn computes much faster than the very deep char - cnn.', 'this is']","['the shallow word - cnns as in [ 3,  #TAUTHOR_TAG generally achieved better error rates than those of the very deep char - cnns reported in [ 1 ].', '• the shallow word - cnn computes much faster than the very deep char - cnn.', 'this is']","['the shallow word - cnns as in [ 3,  #TAUTHOR_TAG generally achieved better error rates than those of the very deep char - cnns reported in [ 1 ].', '• the shallow word - cnn computes much faster than the very deep char - cnn.', 'this is because the deep char - cnn needs to process more text units as there are many more characters than words per document, and because many layers need to be processed sequentially.', 'this is a practical advantage of the shallow word - cnn.', '• the shallow word - cnns use more parameters and therefore require more storage, which is a drawback in storage - tight situations.', 'reducing the number and / or dimensionality of tv - embeddings reduces the number of parameters though it comes with the expense of a small degradation of accuracy']",3
"['in  #TAUTHOR_TAG, tv - embedding', 'training was']","['in  #TAUTHOR_TAG, tv - embedding', 'training was']","['in  #TAUTHOR_TAG, tv - embedding', 'training was done using unlabeled data as an additional resource ; therefore,', '']","['predict adjacent', 'text regions ( one view )', ""based on a text region ( the other view ) '"", '. this training can be done with unlabeled data. [ 4 ] provides the definition and theoretical analysis of tv - embeddings. next, we use the tv - embedding to produce', 'additional input to the base model and train it with labeled data. this model can be easily extended to use multiple tv - embeddings, each of which, for example', ', uses a distinct vector representation of region, and so the region embedding function in the final model ( hollow ovals in figure 1 ( b ) )', 'can be written as : is the output of the tv - embedding indexed by i applied to the corresponding text region. in  #TAUTHOR_TAG, tv - embedding', 'training was done using unlabeled data as an additional resource ; therefore,', 'the proposed models were semi - supervised models. in the experiments reported below, due to the lack of standard unlabeled data for the tested datasets, we trained tv - embeddings on the labeled training data ignoring the labels ; thus, the resulting models are supervised ones. we trained four tv', '']",5
['as in  #TAUTHOR_TAG ; weighted square loss was minimized without regularization while'],['as in  #TAUTHOR_TAG ; weighted square loss was minimized without regularization while'],"['for use as validation data.', 'models were trained using the training set minus validation data, and model selection ( or hyper parameter tuning ) was done based on the performance on the validation data.', 'tv - embedding training was done as in  #TAUTHOR_TAG ; weighted square loss was minimized without regularization while the target regions ( adjacent regions ) were represented by bow vectors, and the data weights were set so that the negative sampling effect was achieved.', 'tv - embeddings were fixed ( i. e., no weight updating )']","['all datasets, we held out 10k data points from the training set for use as validation data.', 'models were trained using the training set minus validation data, and model selection ( or hyper parameter tuning ) was done based on the performance on the validation data.', 'tv - embedding training was done as in  #TAUTHOR_TAG ; weighted square loss was minimized without regularization while the target regions ( adjacent regions ) were represented by bow vectors, and the data weights were set so that the negative sampling effect was achieved.', 'tv - embeddings were fixed ( i. e., no weight updating ) during the final training with labeled data.', 'training with labels ( either with or without tv - embedding ) was done as follows.', 'a log loss ( or cross entropy ) with softmax was minimized.', 'optimization was done by mini - batch sgd with momentum 0. 9 and the mini - batch size was set to 100.', 'the number of epochs was fixed to 30 ( except for ag, the smallest, for which it was fixed to 100 ), and the learning rate was reduced once by multiplying 0. 1 after 24 epochs ( or 80 epochs on ag ).', 'in all layers, weights were initialized by the gaussian distribution of zero mean and standard deviation 0. 01.', 'the initial learning rate was treated as a hyper parameter.', 'regularization was done by applying dropout with 0. 5 to the input to the top layer and having a l2 regularization term with parameter 0. 0001 on the top layer weights.', ""' depth'counts the hidden layers with weights in the longest path."", '[ 9 ] reported the results of several linear methods, and we copied only the best results.', '[ 1 ] reported the results of deep char - cnn with three downsampling methods, and we copied only the best results.', 'the word - cnn results are our new results.', 'the best ( or second best ) results are shown in bold ( or italic ) font, respectively']",5
"['in  #TAUTHOR_TAG, tv - embedding', 'training was']","['in  #TAUTHOR_TAG, tv - embedding', 'training was']","['in  #TAUTHOR_TAG, tv - embedding', 'training was done using unlabeled data as an additional resource ; therefore,', '']","['predict adjacent', 'text regions ( one view )', ""based on a text region ( the other view ) '"", '. this training can be done with unlabeled data. [ 4 ] provides the definition and theoretical analysis of tv - embeddings. next, we use the tv - embedding to produce', 'additional input to the base model and train it with labeled data. this model can be easily extended to use multiple tv - embeddings, each of which, for example', ', uses a distinct vector representation of region, and so the region embedding function in the final model ( hollow ovals in figure 1 ( b ) )', 'can be written as : is the output of the tv - embedding indexed by i applied to the corresponding text region. in  #TAUTHOR_TAG, tv - embedding', 'training was done using unlabeled data as an additional resource ; therefore,', 'the proposed models were semi - supervised models. in the experiments reported below, due to the lack of standard unlabeled data for the tested datasets, we trained tv - embeddings on the labeled training data ignoring the labels ; thus, the resulting models are supervised ones. we trained four tv', '']",4
"['the other hand,  #TAUTHOR_TAG attempted to']","['the other hand,  #TAUTHOR_TAG attempted to']","['the other hand,  #TAUTHOR_TAG attempted to']","['', ' #AUTHOR_TAG have proposed a task of generating a definition for a phrase given its local context.', 'however, they follow the strict assumption that the target phrase is newly emerged and there is only a single local context available for the phrase, which makes the task of generating an accurate and coherent definition difficult ( perhaps as difficult as a human comprehending the phrase itself ).', 'on the other hand,  #TAUTHOR_TAG attempted to generate a definition of a word from an embedding induced from massive text ( which can be seen as global context ).', 'this is followed by  #AUTHOR_TAG that refers to a local context to disambiguate polysemous words by choosing relevant dimensions of their word embeddings.', 'al - though these research efforts revealed that both local and global contexts are useful in generating definitions, none of these studies exploited both contexts directly to describe unknown phrases.', 'in this study, we tackle the task of describing ( defining ) a phrase when given its local and global contexts.', 'we present log - cad, a neural description generator ( figure 1 ) to directly solve this task.', 'given an unknown phrase without sense definitions, our model obtains a phrase embedding as its global context by composing word embeddings while also encoding the local context.', 'the model therefore combines both pieces of information to generate a natural language description.', 'considering various applications where we need definitions of expressions, we evaluated our method with four datasets including wordnet  #TAUTHOR_TAG for general words, the oxford dictionary  #AUTHOR_TAG for polysemous words, urban dictionary  #AUTHOR_TAG for rare idioms or slang, and a newlycreated wikipedia dataset for entities.', 'our contributions are as follows :', '• we propose a general task of defining unknown phrases given their contexts.', 'this task is a generalization']",0
"['.', 'recently,  #TAUTHOR_TAG introduced a task of']","['word.', 'recently,  #TAUTHOR_TAG introduced a task of']","['', 'recently,  #TAUTHOR_TAG introduced a task of']",[' #TAUTHOR_TAG'],0
"['the other hand,  #TAUTHOR_TAG attempted to']","['the other hand,  #TAUTHOR_TAG attempted to']","['the other hand,  #TAUTHOR_TAG attempted to']","['', ' #AUTHOR_TAG have proposed a task of generating a definition for a phrase given its local context.', 'however, they follow the strict assumption that the target phrase is newly emerged and there is only a single local context available for the phrase, which makes the task of generating an accurate and coherent definition difficult ( perhaps as difficult as a human comprehending the phrase itself ).', 'on the other hand,  #TAUTHOR_TAG attempted to generate a definition of a word from an embedding induced from massive text ( which can be seen as global context ).', 'this is followed by  #AUTHOR_TAG that refers to a local context to disambiguate polysemous words by choosing relevant dimensions of their word embeddings.', 'al - though these research efforts revealed that both local and global contexts are useful in generating definitions, none of these studies exploited both contexts directly to describe unknown phrases.', 'in this study, we tackle the task of describing ( defining ) a phrase when given its local and global contexts.', 'we present log - cad, a neural description generator ( figure 1 ) to directly solve this task.', 'given an unknown phrase without sense definitions, our model obtains a phrase embedding as its global context by composing word embeddings while also encoding the local context.', 'the model therefore combines both pieces of information to generate a natural language description.', 'considering various applications where we need definitions of expressions, we evaluated our method with four datasets including wordnet  #TAUTHOR_TAG for general words, the oxford dictionary  #AUTHOR_TAG for polysemous words, urban dictionary  #AUTHOR_TAG for rare idioms or slang, and a newlycreated wikipedia dataset for entities.', 'our contributions are as follows :', '• we propose a general task of defining unknown phrases given their contexts.', 'this task is a generalization']",5
"['order to capture the surface information of x trg, we construct character - level cnns ( eq. ( 6 ) ) following  #TAUTHOR_TAG.', 'note that the input to the cnns is a sequence of words in x tr']","['order to capture the surface information of x trg, we construct character - level cnns ( eq. ( 6 ) ) following  #TAUTHOR_TAG.', 'note that the input to the cnns is a sequence of words in x trg, which are concatenated with special character']","['order to capture the surface information of x trg, we construct character - level cnns ( eq. ( 6 ) ) following  #TAUTHOR_TAG.', 'note that the input to the cnns is a sequence of words in x trg, which are concatenated with special character "", "" such as']","['order to capture the surface information of x trg, we construct character - level cnns ( eq. ( 6 ) ) following  #TAUTHOR_TAG.', 'note that the input to the cnns is a sequence of words in x trg, which are concatenated with special character "", "" such as "" sonic boom. "" following  #TAUTHOR_TAG, we set the cnn kernels of length 2 - 6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160 - dimensional vector c trg']",5
"['order to capture the surface information of x trg, we construct character - level cnns ( eq. ( 6 ) ) following  #TAUTHOR_TAG.', 'note that the input to the cnns is a sequence of words in x tr']","['order to capture the surface information of x trg, we construct character - level cnns ( eq. ( 6 ) ) following  #TAUTHOR_TAG.', 'note that the input to the cnns is a sequence of words in x trg, which are concatenated with special character']","['order to capture the surface information of x trg, we construct character - level cnns ( eq. ( 6 ) ) following  #TAUTHOR_TAG.', 'note that the input to the cnns is a sequence of words in x trg, which are concatenated with special character "", "" such as']","['order to capture the surface information of x trg, we construct character - level cnns ( eq. ( 6 ) ) following  #TAUTHOR_TAG.', 'note that the input to the cnns is a sequence of words in x trg, which are concatenated with special character "", "" such as "" sonic boom. "" following  #TAUTHOR_TAG, we set the cnn kernels of length 2 - 6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160 - dimensional vector c trg']",5
"['##net, we followed  #TAUTHOR_TAG and extracted data from']","['our model on the word description task on wordnet, we followed  #TAUTHOR_TAG and extracted data from wordnet using the dict - definition 9 toolkit.', '']","['##net, we followed  #TAUTHOR_TAG and extracted data from']","['evaluate our method by applying it to describe words in wordnet 5  #AUTHOR_TAG and oxford dictionary, 6 phrases in urban dictionary 7 and wikipedia / wikidata.', '8 for all of these datasets, a given word or phrase has an inventory of senses with corresponding definitions and usage examples.', 'these definitions are regarded as groundtruth descriptions.', 'datasets to evaluate our model on the word description task on wordnet, we followed  #TAUTHOR_TAG and extracted data from wordnet using the dict - definition 9 toolkit.', 'each entry in the data consists of three elements : ( 1 ) a word, ( 2 ) its definition, and ( 3 ) a usage example of the table 2 : domains, expressions to be described, and the coverage of pre - trained embeddings of the expressions to be described.', 'word.', 'we split this dataset to obtain train, validation, and test sets.', 'if a word has multiple definitions / examples, we treat them as different entries.', 'note that the words are mutually exclusive across the three sets.', 'the only difference between our dataset and theirs is that we extract the tuples only if the words have their usage examples in wordnet.', 'since not all entries in wordnet have usage examples, our dataset is a small subset of  #TAUTHOR_TAG.', '']",5
"['##net, we followed  #TAUTHOR_TAG and extracted data from']","['our model on the word description task on wordnet, we followed  #TAUTHOR_TAG and extracted data from wordnet using the dict - definition 9 toolkit.', '']","['##net, we followed  #TAUTHOR_TAG and extracted data from']","['evaluate our method by applying it to describe words in wordnet 5  #AUTHOR_TAG and oxford dictionary, 6 phrases in urban dictionary 7 and wikipedia / wikidata.', '8 for all of these datasets, a given word or phrase has an inventory of senses with corresponding definitions and usage examples.', 'these definitions are regarded as groundtruth descriptions.', 'datasets to evaluate our model on the word description task on wordnet, we followed  #TAUTHOR_TAG and extracted data from wordnet using the dict - definition 9 toolkit.', 'each entry in the data consists of three elements : ( 1 ) a word, ( 2 ) its definition, and ( 3 ) a usage example of the table 2 : domains, expressions to be described, and the coverage of pre - trained embeddings of the expressions to be described.', 'word.', 'we split this dataset to obtain train, validation, and test sets.', 'if a word has multiple definitions / examples, we treat them as different entries.', 'note that the words are mutually exclusive across the three sets.', 'the only difference between our dataset and theirs is that we extract the tuples only if the words have their usage examples in wordnet.', 'since not all entries in wordnet have usage examples, our dataset is a small subset of  #TAUTHOR_TAG.', '']",5
"[' #TAUTHOR_TAG.', 'it can access the global context of a phrase to be described, but has no ability to read the local context.', 'the local model is']","[' #TAUTHOR_TAG.', 'it can access the global context of a phrase to be described, but has no ability to read the local context.', 'the local model is']","[' #TAUTHOR_TAG.', 'it can access the global context of a phrase to be described, but has no ability to read the local context.', 'the local model is the reimplementation']","[' #TAUTHOR_TAG.', 'it can access the global context of a phrase to be described, but has no ability to read the local context.', 'the local model is the reimplementation of the best model ( dual encoder ) in  #AUTHOR_TAG.', 'in order to make a fair comparison of the effectiveness of local and global contexts, we slightly modify the original implementation by  #AUTHOR_TAG ; as the character - level encoder in the local model, we adopt cnns that are exactly the same as the other two models instead of the original lstms.', 'the i - attention is our reimplementation of the best model ( s + i - attention ) in  #AUTHOR_TAG.', 'similar to our model, it uses both local and global contexts.', 'unlike our model, however, it does not use character information to predict descriptions.', 'also, it cannot directly use the local context to predict the words in descriptions.', 'this is because the i - attention model indirectly uses the local context only to disambiguate the phrase embedding x trg as', 'here, the ffnn ( · ) function is a feed - forward neural network that maps the encoded local contexts h i to another space.', 'the mapped local contexts are then averaged over the length of the sentence x to obtain a representation of the local context.', '']",5
"['also on its global context.', 'to incorporate the different types of contexts, we propose to use a gate function similar to  #TAUTHOR_TAG to dynamically control']","['also on its global context.', 'to incorporate the different types of contexts, we propose to use a gate function similar to  #TAUTHOR_TAG to dynamically control']","['also on its global context.', 'to incorporate the different types of contexts, we propose to use a gate function similar to  #TAUTHOR_TAG to dynamically control']","['', 'to verify this idea, we propose to incorporate both local and global contexts to describe an unknown phrase.', 'figure 1 shows an illustration of our log - cad model.', 'similarly to the standard encoder - decoder model with attention  #AUTHOR_TAG, it has a context encoder and a description decoder.', 'the challenge here is that the decoder needs to be conditioned not only on the local context, but also on its global context.', 'to incorporate the different types of contexts, we propose to use a gate function similar to  #TAUTHOR_TAG to dynamically control how the global and local contexts influence the description']",3
"['( eq. ( 7 ) ) which is similar to  #TAUTHOR_TAG.', 'the gate ( · )']","['( eq. ( 7 ) ) which is similar to  #TAUTHOR_TAG.', 'the gate ( · )']","['order to capture the interaction between the local and global contexts, we adopt a gate ( · ) function ( eq. ( 7 ) ) which is similar to  #TAUTHOR_TAG.', 'the gate ( · ) function updates the lstm output']","['order to capture the interaction between the local and global contexts, we adopt a gate ( · ) function ( eq. ( 7 ) ) which is similar to  #TAUTHOR_TAG.', 'the gate ( · ) function updates the lstm output s t to s t depending on the global context x trg, local context d t, and character - level information c trg as', 'where σ ( · ), and ; denote the sigmoid function, element - wise multiplication, and vector concatenation, respectively.', 'w * and b * are weight matrices and bias terms, respectively.', 'here, the update gate z t controls how much the original hidden state s t is to be changed, and the reset gate r t controls how much the information from f t contributes to word generation at each time step']",3
"['models  #TAUTHOR_TAG.', 'though sm']","['models  #TAUTHOR_TAG.', 'though smt provides a strong']","['has been shown to improve further by adding neural network models  #TAUTHOR_TAG.', 'though sm']","['', 'the generalization of smt - based gec systems has been shown to improve further by adding neural network models  #TAUTHOR_TAG.', 'though smt provides a strong framework for gec, the traditional word - level smt is weak in generalizing beyond patterns seen in the training data  #AUTHOR_TAG.', 'this effect is particularly evident for spelling errors, since a large number of misspelled words produced by learners are not observed in the training data.', '']",0
"['models  #TAUTHOR_TAG.', 'though sm']","['models  #TAUTHOR_TAG.', 'though smt provides a strong']","['has been shown to improve further by adding neural network models  #TAUTHOR_TAG.', 'though sm']","['', 'the generalization of smt - based gec systems has been shown to improve further by adding neural network models  #TAUTHOR_TAG.', 'though smt provides a strong framework for gec, the traditional word - level smt is weak in generalizing beyond patterns seen in the training data  #AUTHOR_TAG.', 'this effect is particularly evident for spelling errors, since a large number of misspelled words produced by learners are not observed in the training data.', '']",0
"['systems  #TAUTHOR_TAG.', 'neural machine']","['gec systems  #TAUTHOR_TAG.', 'neural machine']","['art gec systems  #TAUTHOR_TAG.', 'neural machine translation approaches have also showed some promise  #AUTHOR_TAG.', 'a number of papers on']","['related work gec has gained popularity since the conll - 2014 shared task was organized.', 'unlike previous shared tasks  #AUTHOR_TAG that focused only on a few error types, the conll - 2014 shared task dealt with correction of all kinds of textual errors.', 'the smt approach, which was first used for correcting countability errors of mass nouns  #AUTHOR_TAG, became popular during the conll - 2014 shared task.', 'two of the top three teams used this approach in their systems.', 'it later became the most widely used approach and was used in state - of - the - art gec systems  #TAUTHOR_TAG.', 'neural machine translation approaches have also showed some promise  #AUTHOR_TAG.', 'a number of papers on gec were published in 2016.', ' #AUTHOR_TAG b ) showed that using neural network translation models in phrase - based smt decoding improves performance.', '']",0
"['systems  #TAUTHOR_TAG.', 'neural machine']","['gec systems  #TAUTHOR_TAG.', 'neural machine']","['art gec systems  #TAUTHOR_TAG.', 'neural machine translation approaches have also showed some promise  #AUTHOR_TAG.', 'a number of papers on']","['related work gec has gained popularity since the conll - 2014 shared task was organized.', 'unlike previous shared tasks  #AUTHOR_TAG that focused only on a few error types, the conll - 2014 shared task dealt with correction of all kinds of textual errors.', 'the smt approach, which was first used for correcting countability errors of mass nouns  #AUTHOR_TAG, became popular during the conll - 2014 shared task.', 'two of the top three teams used this approach in their systems.', 'it later became the most widely used approach and was used in state - of - the - art gec systems  #TAUTHOR_TAG.', 'neural machine translation approaches have also showed some promise  #AUTHOR_TAG.', 'a number of papers on gec were published in 2016.', ' #AUTHOR_TAG b ) showed that using neural network translation models in phrase - based smt decoding improves performance.', '']",0
"['models  #TAUTHOR_TAG.', 'though sm']","['models  #TAUTHOR_TAG.', 'though smt provides a strong']","['has been shown to improve further by adding neural network models  #TAUTHOR_TAG.', 'though sm']","['', 'the generalization of smt - based gec systems has been shown to improve further by adding neural network models  #TAUTHOR_TAG.', 'though smt provides a strong framework for gec, the traditional word - level smt is weak in generalizing beyond patterns seen in the training data  #AUTHOR_TAG.', 'this effect is particularly evident for spelling errors, since a large number of misspelled words produced by learners are not observed in the training data.', '']",3
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG, we add a neural network joint model ( nnjm ) feature to further improve the smt component.', 'we train the neural networks on gpus using log - likelihood objective function with']","[' #TAUTHOR_TAG, we add a neural network joint model ( nnjm ) feature to further improve the smt component.', 'we train the neural networks on gpus using log - likelihood objective function with self - normalization, following  #AUTHOR_TAG.', 'training of the neural network joint model is done using a theanobased  #AUTHOR_TAG implementation, corelm 1.', ' #AUTHOR_TAG a ) proposed adapting smt - based gec based on the native language of writers, by adaptive training of a pre - trained nnjm on in - domain data ( written by authors sharing the same native language ) using a regularized loss function.', 'we follow this adaptation method and perform subsequent adaptive training of the nnjm, but on a subset of training data with better annotation quality and a higher error - per - sentence ratio, favoring more corrections and thus increasing recall']",5
"['chains  #AUTHOR_TAG or more gen - eral directed acyclic graphs  #TAUTHOR_TAG. these graphs are scenario - specific, nodes in']","['chains  #AUTHOR_TAG or more gen - eral directed acyclic graphs  #TAUTHOR_TAG. these graphs are scenario - specific, nodes in']","['chains  #AUTHOR_TAG or more gen - eral directed acyclic graphs  #TAUTHOR_TAG. these graphs are scenario - specific, nodes in']","[', for example chains  #AUTHOR_TAG or more gen - eral directed acyclic graphs  #TAUTHOR_TAG. these graphs are scenario - specific, nodes in them correspond', 'to events ( and associated with sets of potential event mentions ) and arcs encode the temporal precedence relation. these', '']",0
"['chains  #AUTHOR_TAG or more gen - eral directed acyclic graphs  #TAUTHOR_TAG. these graphs are scenario - specific, nodes in']","['chains  #AUTHOR_TAG or more gen - eral directed acyclic graphs  #TAUTHOR_TAG. these graphs are scenario - specific, nodes in']","['chains  #AUTHOR_TAG or more gen - eral directed acyclic graphs  #TAUTHOR_TAG. these graphs are scenario - specific, nodes in']","[', for example chains  #AUTHOR_TAG or more gen - eral directed acyclic graphs  #TAUTHOR_TAG. these graphs are scenario - specific, nodes in them correspond', 'to events ( and associated with sets of potential event mentions ) and arcs encode the temporal precedence relation. these', '']",0
"['chains  #AUTHOR_TAG or more gen - eral directed acyclic graphs  #TAUTHOR_TAG. these graphs are scenario - specific, nodes in']","['chains  #AUTHOR_TAG or more gen - eral directed acyclic graphs  #TAUTHOR_TAG. these graphs are scenario - specific, nodes in']","['chains  #AUTHOR_TAG or more gen - eral directed acyclic graphs  #TAUTHOR_TAG. these graphs are scenario - specific, nodes in']","[', for example chains  #AUTHOR_TAG or more gen - eral directed acyclic graphs  #TAUTHOR_TAG. these graphs are scenario - specific, nodes in them correspond', 'to events ( and associated with sets of potential event mentions ) and arcs encode the temporal precedence relation. these', '']",0
['collected short textual descriptions ( called'],['collected short textual descriptions ( called'],"['collected short textual descriptions ( called event sequence descriptions, esds ) of various types of human activities ( e.']","['collected short textual descriptions ( called event sequence descriptions, esds ) of various types of human activities ( e. g., going to a restaurant, ironing clothes ) using crowdsourcing ( amazon mechanical turk ), this dataset was also complemented by descriptions provided in the omics corpus  #AUTHOR_TAG.', 'the datasets are fairly small, containing 30 esds per activity type in average ( we will refer to different activities as scenarios ), but the collection can easily be extended given the low cost of crowdsourcing.', 'the esds are written in a bullet - point style and the annotators were asked to follow the temporal order in writing.', 'consider an example esd for the scenario prepare coffee :', '']",0
['collected short textual descriptions ( called'],['collected short textual descriptions ( called'],"['collected short textual descriptions ( called event sequence descriptions, esds ) of various types of human activities ( e.']","['collected short textual descriptions ( called event sequence descriptions, esds ) of various types of human activities ( e. g., going to a restaurant, ironing clothes ) using crowdsourcing ( amazon mechanical turk ), this dataset was also complemented by descriptions provided in the omics corpus  #AUTHOR_TAG.', 'the datasets are fairly small, containing 30 esds per activity type in average ( we will refer to different activities as scenarios ), but the collection can easily be extended given the low cost of crowdsourcing.', 'the esds are written in a bullet - point style and the annotators were asked to follow the temporal order in writing.', 'consider an example esd for the scenario prepare coffee :', '']",0
"['chains  #AUTHOR_TAG or more gen - eral directed acyclic graphs  #TAUTHOR_TAG. these graphs are scenario - specific, nodes in']","['chains  #AUTHOR_TAG or more gen - eral directed acyclic graphs  #TAUTHOR_TAG. these graphs are scenario - specific, nodes in']","['chains  #AUTHOR_TAG or more gen - eral directed acyclic graphs  #TAUTHOR_TAG. these graphs are scenario - specific, nodes in']","[', for example chains  #AUTHOR_TAG or more gen - eral directed acyclic graphs  #TAUTHOR_TAG. these graphs are scenario - specific, nodes in them correspond', 'to events ( and associated with sets of potential event mentions ) and arcs encode the temporal precedence relation. these', '']",5
"['script induction by  #TAUTHOR_TAG, though, in principle, the method is applicable in arguably more general setting of  #AUTHOR_TAG']","['script induction by  #TAUTHOR_TAG, though, in principle, the method is applicable in arguably more general setting of  #AUTHOR_TAG']","['script induction by  #TAUTHOR_TAG, though, in principle, the method is applicable in arguably more general setting of  #AUTHOR_TAG']","['evaluate our approach on crowdsourced data collected for script induction by  #TAUTHOR_TAG, though, in principle, the method is applicable in arguably more general setting of  #AUTHOR_TAG']",5
['collected short textual descriptions ( called'],['collected short textual descriptions ( called'],"['collected short textual descriptions ( called event sequence descriptions, esds ) of various types of human activities ( e.']","['collected short textual descriptions ( called event sequence descriptions, esds ) of various types of human activities ( e. g., going to a restaurant, ironing clothes ) using crowdsourcing ( amazon mechanical turk ), this dataset was also complemented by descriptions provided in the omics corpus  #AUTHOR_TAG.', 'the datasets are fairly small, containing 30 esds per activity type in average ( we will refer to different activities as scenarios ), but the collection can easily be extended given the low cost of crowdsourcing.', 'the esds are written in a bullet - point style and the annotators were asked to follow the temporal order in writing.', 'consider an example esd for the scenario prepare coffee :', '']",4
"['system of  #TAUTHOR_TAG.', 'bs is a a hierarchical bayesian system of  #AUTHOR_TAG.', 'bl chooses the order of events based on']","['system of  #TAUTHOR_TAG.', 'bs is a a hierarchical bayesian system of  #AUTHOR_TAG.', 'bl chooses the order of events based on']","['( bl, msa ) and bsmsa is the system of  #TAUTHOR_TAG.', 'bs is a a hierarchical bayesian system of  #AUTHOR_TAG.', 'bl chooses the order of events based on the preferred order of the corresponding verbs in the training set : (']","['our experiments, we compared our event embedding model ( ee ) against three baseline systems ( bl, msa ) and bsmsa is the system of  #TAUTHOR_TAG.', 'bs is a a hierarchical bayesian system of  #AUTHOR_TAG.', 'bl chooses the order of events based on the preferred order of the corresponding verbs in the training set : ( e 1, e 2 ) is predicted to be in the stereotypical order if the number of times the corresponding verbs v 1 and v 2 appear in this order in the training esds exceeds the number of times they appear in the opposite order ( not necessary at adjacent positions ) ; a coin is tossed to break ties ( or if v 1 and v 2 are the same verb ).', 'we also compare to the version of our model which uses only verbs ( ee verbs ).', 'note that ee verbs is conceptually very similar to bl, as it essentially induces an ordering over verbs.', 'however, this ordering can benefit from the implicit transitivity assumption used in ee verbs ( and ee ), as we discussed in the introduction.', 'the results are presented in table 1.', '']",7
"['al. 1992 ).', ' #TAUTHOR_TAG introduces a thesaurus - based approach to statistical sense']","['al. 1992 ).', ' #TAUTHOR_TAG introduces a thesaurus - based approach to statistical sense']","['al. 1992 ).', ' #TAUTHOR_TAG introduces a thesaurus - based approach to statistical sense disambiguation which works on monolingual corpora without the need for sense - tagged training data.', 'by collecting statistical data of word occurrences in the context of different thesaurus categories from']","['corpus - based sense disambiguation methods require substantial amounts of sense - tagged training data  #AUTHOR_TAG or aligned bilingual corpora  #AUTHOR_TAG gale et al. 1992 ).', ' #TAUTHOR_TAG introduces a thesaurus - based approach to statistical sense disambiguation which works on monolingual corpora without the need for sense - tagged training data.', '']",0
"['to spurious senses of polysemous words.', 'like the thesaurusbased approach of  #TAUTHOR_TAG, our approach relies on the dilution of this noise by their distribution through all the 1792 defining concepts.', 'different words in the corpus have different numbers of senses and different senses have definitions of varying lengths.', 'the principle adopted in collecting']","['to spurious senses of polysemous words.', 'like the thesaurusbased approach of  #TAUTHOR_TAG, our approach relies on the dilution of this noise by their distribution through all the 1792 defining concepts.', 'different words in the corpus have different numbers of senses and different senses have definitions of varying lengths.', 'the principle adopted in collecting co - occurrence data is that']","['to spurious senses of polysemous words.', 'like the thesaurusbased approach of  #TAUTHOR_TAG, our approach relies on the dilution of this noise by their distribution through all the 1792 defining concepts.', 'different words in the corpus have different numbers of senses and different senses have definitions of varying lengths.', 'the principle adopted in collecting co - occurrence data is that every pair of content words which']","['', 'the corpus is pre - segrnented into sentences but not pre - processed in any other way ( sense - tagged or part - of - speech - tagged ).', 'the context of a word is defined to be the current sentence ) the system processes the corpus sentence by sentence and collects conceptual co - occurrence data for each defining concept which occurs in the sentence.', 'this allows the whole table to be constructed in a single run through the corpus.', 'since the training data is not sense tagged, the data collected will contain noise due to spurious senses of polysemous words.', 'like the thesaurusbased approach of  #TAUTHOR_TAG, our approach relies on the dilution of this noise by their distribution through all the 1792 defining concepts.', 'different words in the corpus have different numbers of senses and different senses have definitions of varying lengths.', 'the principle adopted in collecting co - occurrence data is that every pair of content words which co - occur in a sentence should have equal contribution to the conceptual cooccurrence data regardless of the number of definitions ( senses ) of the words and the lengths of the definitions.', 'in addition']",3
"[""of  #TAUTHOR_TAG tested on the grolier's encyclopedia. 2""]","[""of  #TAUTHOR_TAG tested on the grolier's encyclopedia. 2""]","['of  #TAUTHOR_TAG tested on the grolier\'s encyclopedia. 2. the "" correct "" sense of each test sample is chosen by hand']","['', 'test samples which our system fails to correctly disambiguate also shows that increasing the window size', 'will benefit the disambiguation process only in a very small proportion of these samples. the main cause of errors is the polysemous words in dictionary definitions which we will discuss in', 'section 6. 1o based on 1004998 words and 51763 sentences. 1. n marks the column with the number of tcst samples for each sense. dbcc ( defmition - bascd conceptual', 'cooccurrence ) and human mark the columns with the results of our system and the human subject in disambiguating the occurrences of the 12 words in the brown corpus, respectively', '. thes. ( thesaurus ) marks the column with the results of  #TAUTHOR_TAG tested on the grolier\'s encyclopedia. 2. the "" correct "" sense of each test sample is chosen by hand disambiguation carried out by the author using', 'the sentence as the context. a small proportion of test samples cannot be disambiguated within the given context and are excluded from the experiment. 3. the senses marked with * are used in', ' #TAUTHOR_TAG but no corresponding sense is found in ldoce. 4. the sense marked with * * is defined in', 'ldoce but not used in  #TAUTHOR_TAG. 6. in our experiment,', 'the words are disambiguated between all the senses listed except the ones marked with 7.', 'the rare senses listed in ldoce are not listed here. for some of the words, more than one sense listed in ldoce corresponds to a sense as used in  #TAUTHOR_TAG. in these cases, the senses used by ya', '']",3
['system is tested on the twelve words discussed in  #TAUTHOR_TAG and previous publications on sense'],['system is tested on the twelve words discussed in  #TAUTHOR_TAG and previous publications on sense'],"['system is tested on the twelve words discussed in  #TAUTHOR_TAG and previous publications on sense disambiguation.', 'results are shown in table 1.', 'our system achieves an average accuracy of 77']","['system is tested on the twelve words discussed in  #TAUTHOR_TAG and previous publications on sense disambiguation.', 'results are shown in table 1.', 'our system achieves an average accuracy of 77 % on a mean 3 - way sense distinction over the twelve words.', 'numerically, the result is not as good as the 92 % as reported in  #TAUTHOR_TAG.', 'however, direct comparison between the numerical results can be misleading since the experiments are carried out on two very different corpora both in size and genre.', ""firstly, yarowsky's system is trained with the 10 million word grolier's encyclopedia, which is a magnitude larger than the brown corpus used by our system."", 'secondly, and more importantly, the two corpora, which are also the test corpora, are very different in genre.', 'semantic coherence of text, on which both systems rely, is generally stronger in technical writing than in most other kinds of text.', 'statistical disambiguation systems which rely on semantic coherence will generally perform better on technical writing, which encyclopedia entry can be regarded as one kind of, than on most other kinds of text.', 'on the other hand, the brown corpus is a collection of text with all kinds of genre.', 'people make use of syntactic, semantic and pragmatic knowledge in sense disambiguation.', ""it is not very realistic to expect any system which only possesses semantic coherence knowledge ( including ours as well as yarowsky's ) to achieve a very high level of accuracy for all words in general text."", 'to provide a better evaluation of our approach, we have conducted an informal experiment aiming at establishing a more reasonable upper bound of the performance of such systems.', 'in the experiment, a human subject is asked to perform the same disambiguation task as our system, given the same contextual information, 7 since our system only uses semantic coherence information and has no deeper understanding of the meaning of the text, the human subject is asked to disambiguate the target word, given a list of all the content words in the context ( sentence ) of the target word in random order.', 'the words are put in random order because the system does not make use of syntactic information of the sentence either.', 'the human subject is also allowed access to a copy of ldoce which the system also uses.', 'the results are listed in table 1.', 'the actual upper bound of the performance of statistical methods using semantic coherence information only should be slightly better than the performance of human since the human is disadvantaged by a number of factors, including but not limited to : 1. it is unnatural for human to disambiguate in the']",5
"[""of  #TAUTHOR_TAG tested on the grolier's encyclopedia. 2""]","[""of  #TAUTHOR_TAG tested on the grolier's encyclopedia. 2""]","['of  #TAUTHOR_TAG tested on the grolier\'s encyclopedia. 2. the "" correct "" sense of each test sample is chosen by hand']","['', 'test samples which our system fails to correctly disambiguate also shows that increasing the window size', 'will benefit the disambiguation process only in a very small proportion of these samples. the main cause of errors is the polysemous words in dictionary definitions which we will discuss in', 'section 6. 1o based on 1004998 words and 51763 sentences. 1. n marks the column with the number of tcst samples for each sense. dbcc ( defmition - bascd conceptual', 'cooccurrence ) and human mark the columns with the results of our system and the human subject in disambiguating the occurrences of the 12 words in the brown corpus, respectively', '. thes. ( thesaurus ) marks the column with the results of  #TAUTHOR_TAG tested on the grolier\'s encyclopedia. 2. the "" correct "" sense of each test sample is chosen by hand disambiguation carried out by the author using', 'the sentence as the context. a small proportion of test samples cannot be disambiguated within the given context and are excluded from the experiment. 3. the senses marked with * are used in', ' #TAUTHOR_TAG but no corresponding sense is found in ldoce. 4. the sense marked with * * is defined in', 'ldoce but not used in  #TAUTHOR_TAG. 6. in our experiment,', 'the words are disambiguated between all the senses listed except the ones marked with 7.', 'the rare senses listed in ldoce are not listed here. for some of the words, more than one sense listed in ldoce corresponds to a sense as used in  #TAUTHOR_TAG. in these cases, the senses used by ya', '']",5
"[""of  #TAUTHOR_TAG tested on the grolier's encyclopedia. 2""]","[""of  #TAUTHOR_TAG tested on the grolier's encyclopedia. 2""]","['of  #TAUTHOR_TAG tested on the grolier\'s encyclopedia. 2. the "" correct "" sense of each test sample is chosen by hand']","['', 'test samples which our system fails to correctly disambiguate also shows that increasing the window size', 'will benefit the disambiguation process only in a very small proportion of these samples. the main cause of errors is the polysemous words in dictionary definitions which we will discuss in', 'section 6. 1o based on 1004998 words and 51763 sentences. 1. n marks the column with the number of tcst samples for each sense. dbcc ( defmition - bascd conceptual', 'cooccurrence ) and human mark the columns with the results of our system and the human subject in disambiguating the occurrences of the 12 words in the brown corpus, respectively', '. thes. ( thesaurus ) marks the column with the results of  #TAUTHOR_TAG tested on the grolier\'s encyclopedia. 2. the "" correct "" sense of each test sample is chosen by hand disambiguation carried out by the author using', 'the sentence as the context. a small proportion of test samples cannot be disambiguated within the given context and are excluded from the experiment. 3. the senses marked with * are used in', ' #TAUTHOR_TAG but no corresponding sense is found in ldoce. 4. the sense marked with * * is defined in', 'ldoce but not used in  #TAUTHOR_TAG. 6. in our experiment,', 'the words are disambiguated between all the senses listed except the ones marked with 7.', 'the rare senses listed in ldoce are not listed here. for some of the words, more than one sense listed in ldoce corresponds to a sense as used in  #TAUTHOR_TAG. in these cases, the senses used by ya', '']",5
['system is tested on the twelve words discussed in  #TAUTHOR_TAG and previous publications on sense'],['system is tested on the twelve words discussed in  #TAUTHOR_TAG and previous publications on sense'],"['system is tested on the twelve words discussed in  #TAUTHOR_TAG and previous publications on sense disambiguation.', 'results are shown in table 1.', 'our system achieves an average accuracy of 77']","['system is tested on the twelve words discussed in  #TAUTHOR_TAG and previous publications on sense disambiguation.', 'results are shown in table 1.', 'our system achieves an average accuracy of 77 % on a mean 3 - way sense distinction over the twelve words.', 'numerically, the result is not as good as the 92 % as reported in  #TAUTHOR_TAG.', 'however, direct comparison between the numerical results can be misleading since the experiments are carried out on two very different corpora both in size and genre.', ""firstly, yarowsky's system is trained with the 10 million word grolier's encyclopedia, which is a magnitude larger than the brown corpus used by our system."", 'secondly, and more importantly, the two corpora, which are also the test corpora, are very different in genre.', 'semantic coherence of text, on which both systems rely, is generally stronger in technical writing than in most other kinds of text.', 'statistical disambiguation systems which rely on semantic coherence will generally perform better on technical writing, which encyclopedia entry can be regarded as one kind of, than on most other kinds of text.', 'on the other hand, the brown corpus is a collection of text with all kinds of genre.', 'people make use of syntactic, semantic and pragmatic knowledge in sense disambiguation.', ""it is not very realistic to expect any system which only possesses semantic coherence knowledge ( including ours as well as yarowsky's ) to achieve a very high level of accuracy for all words in general text."", 'to provide a better evaluation of our approach, we have conducted an informal experiment aiming at establishing a more reasonable upper bound of the performance of such systems.', 'in the experiment, a human subject is asked to perform the same disambiguation task as our system, given the same contextual information, 7 since our system only uses semantic coherence information and has no deeper understanding of the meaning of the text, the human subject is asked to disambiguate the target word, given a list of all the content words in the context ( sentence ) of the target word in random order.', 'the words are put in random order because the system does not make use of syntactic information of the sentence either.', 'the human subject is also allowed access to a copy of ldoce which the system also uses.', 'the results are listed in table 1.', 'the actual upper bound of the performance of statistical methods using semantic coherence information only should be slightly better than the performance of human since the human is disadvantaged by a number of factors, including but not limited to : 1. it is unnatural for human to disambiguate in the']",4
"[""of  #TAUTHOR_TAG tested on the grolier's encyclopedia. 2""]","[""of  #TAUTHOR_TAG tested on the grolier's encyclopedia. 2""]","['of  #TAUTHOR_TAG tested on the grolier\'s encyclopedia. 2. the "" correct "" sense of each test sample is chosen by hand']","['', 'test samples which our system fails to correctly disambiguate also shows that increasing the window size', 'will benefit the disambiguation process only in a very small proportion of these samples. the main cause of errors is the polysemous words in dictionary definitions which we will discuss in', 'section 6. 1o based on 1004998 words and 51763 sentences. 1. n marks the column with the number of tcst samples for each sense. dbcc ( defmition - bascd conceptual', 'cooccurrence ) and human mark the columns with the results of our system and the human subject in disambiguating the occurrences of the 12 words in the brown corpus, respectively', '. thes. ( thesaurus ) marks the column with the results of  #TAUTHOR_TAG tested on the grolier\'s encyclopedia. 2. the "" correct "" sense of each test sample is chosen by hand disambiguation carried out by the author using', 'the sentence as the context. a small proportion of test samples cannot be disambiguated within the given context and are excluded from the experiment. 3. the senses marked with * are used in', ' #TAUTHOR_TAG but no corresponding sense is found in ldoce. 4. the sense marked with * * is defined in', 'ldoce but not used in  #TAUTHOR_TAG. 6. in our experiment,', 'the words are disambiguated between all the senses listed except the ones marked with 7.', 'the rare senses listed in ldoce are not listed here. for some of the words, more than one sense listed in ldoce corresponds to a sense as used in  #TAUTHOR_TAG. in these cases, the senses used by ya', '']",4
"[""of  #TAUTHOR_TAG tested on the grolier's encyclopedia. 2""]","[""of  #TAUTHOR_TAG tested on the grolier's encyclopedia. 2""]","['of  #TAUTHOR_TAG tested on the grolier\'s encyclopedia. 2. the "" correct "" sense of each test sample is chosen by hand']","['', 'test samples which our system fails to correctly disambiguate also shows that increasing the window size', 'will benefit the disambiguation process only in a very small proportion of these samples. the main cause of errors is the polysemous words in dictionary definitions which we will discuss in', 'section 6. 1o based on 1004998 words and 51763 sentences. 1. n marks the column with the number of tcst samples for each sense. dbcc ( defmition - bascd conceptual', 'cooccurrence ) and human mark the columns with the results of our system and the human subject in disambiguating the occurrences of the 12 words in the brown corpus, respectively', '. thes. ( thesaurus ) marks the column with the results of  #TAUTHOR_TAG tested on the grolier\'s encyclopedia. 2. the "" correct "" sense of each test sample is chosen by hand disambiguation carried out by the author using', 'the sentence as the context. a small proportion of test samples cannot be disambiguated within the given context and are excluded from the experiment. 3. the senses marked with * are used in', ' #TAUTHOR_TAG but no corresponding sense is found in ldoce. 4. the sense marked with * * is defined in', 'ldoce but not used in  #TAUTHOR_TAG. 6. in our experiment,', 'the words are disambiguated between all the senses listed except the ones marked with 7.', 'the rare senses listed in ldoce are not listed here. for some of the words, more than one sense listed in ldoce corresponds to a sense as used in  #TAUTHOR_TAG. in these cases, the senses used by ya', '']",4
"['combination of defining concepts unless they are almost identical in meaning.', 'on the other hand, the thesaurus - based method of  #TAUTHOR_TAG may suffer from loss of information']","['combination of defining concepts unless they are almost identical in meaning.', 'on the other hand, the thesaurus - based method of  #TAUTHOR_TAG may suffer from loss of information']","['own set of defining concepts.', 'although only 1792 defining concepts are used, the set of all possible combinations ( a power set of the defining concepts ) is so huge that it is very unlikely two word senses will have the same combination of defining concepts unless they are almost identical in meaning.', 'on the other hand, the thesaurus - based method of  #TAUTHOR_TAG may suffer from loss of information']","['attempts to tackle the data sparseness problem in general corpus - based work include the class - based approaches and similarity - based approaches.', 'in these approaches, relationships between a given pair of words are modelled by analogy with other words that resemble the given pair in some way.', 'the class - based approaches  #AUTHOR_TAG calculate co - occurrence data of words belonging to different classes, ~ rather than individual words, to enhance the co - occurrence data collected and to cover words which have low occurrence frequencies.', ' #AUTHOR_TAG argue that using a relatively small number of classes to model the similarity between words may lead to substantial loss of information.', 'in the similaritybased approaches  #AUTHOR_TAG ( dagan et al., & 1994  #AUTHOR_TAG, rather than a class, each word is modelled by its own set of similar words derived from statistical data collected from corpora.', 'however, deriving these sets of similar words requires a substantial amount of statistical data and thus these approaches require relatively large corpora to start with. ~ 2 our definition - based approach to statistical sense disambiguation is similar in spirit to the similaritybased approaches, with respect to the "" specificity "" of modelling individual words.', 'however, using definitions from existing dictionaries rather than derived sets of similar words allows our method to work on corpora of much smaller sizes.', 'in our approach, each word is modelled by its own set of defining concepts.', 'although only 1792 defining concepts are used, the set of all possible combinations ( a power set of the defining concepts ) is so huge that it is very unlikely two word senses will have the same combination of defining concepts unless they are almost identical in meaning.', 'on the other hand, the thesaurus - based method of  #TAUTHOR_TAG may suffer from loss of information ( since it is semi - class - based ) as well as data sparseness ( since h classes used in  #AUTHOR_TAG are based on the wordnet taxonomy while classes of  #AUTHOR_TAG and  #AUTHOR_TAG are derived from statistical data collected from corpora']",4
"['acquisition system in the context of word grouping.', 'such as that described in  #TAUTHOR_TAG are very unlikely to be capable of acquiring this finer']","['acquisition system in the context of word grouping.', 'such as that described in  #TAUTHOR_TAG are very unlikely to be capable of acquiring this finer']","['described in  #AUTHOR_TAG.', 'however, fully automatic lexically based approaches 3  #AUTHOR_TAG shows that the introduction of linguistic cues improves the performance of a statistical semantic knowledge acquisition system in the context of word grouping.', 'such as that described in  #TAUTHOR_TAG are very unlikely to be capable of acquiring this finer knowledge']","['', 'our success in using definitions of word senses to overcome the data sparseness problem may also lead to further improvement of sense disambiguation technologies.', 'in many cases, semantic coherence information is not adequate to select the correct sense, and knowledge about local constraints is needed.', '~ 3 for disambiguation of polysemous nouns, these constraints include the modifiers of these nouns and the verbs which take these nouns as objects, etc.', 'this knowledge has been successfully acquired from corpora in manual or semi - automatic approaches such as that described in  #AUTHOR_TAG.', 'however, fully automatic lexically based approaches 3  #AUTHOR_TAG shows that the introduction of linguistic cues improves the performance of a statistical semantic knowledge acquisition system in the context of word grouping.', 'such as that described in  #TAUTHOR_TAG are very unlikely to be capable of acquiring this finer knowledge because the problem of data sparseness becomes even more serious with the introduction of syntactic constraints.', 'our approach has overcome the data sparseness problem by using the defining concepts of words.', 'it is found to be effective in acquiring semantic coherence knowledge from a relatively small corpus.', 'it is possible that a similar approach based on dictionary definitions will be successful in acquiring knowledge of local constraints from a reasonably sized corpus']",4
"[' #AUTHOR_TAG ].', 'previous experiments with tasks like language modelling  #TAUTHOR_TAG have shown faster convergence and performance gains']","['to advancement in research towards animal training [  #AUTHOR_TAG ].', 'previous experiments with tasks like language modelling  #TAUTHOR_TAG have shown faster convergence and performance gains']","[' #AUTHOR_TAG ].', 'previous experiments with tasks like language modelling  #TAUTHOR_TAG have shown faster convergence and performance gains']","['', 'they also draw parallels with human learning curriculum and education system, where different concepts are introduced in an order at different times, and has led to advancement in research towards animal training [  #AUTHOR_TAG ].', 'previous experiments with tasks like language modelling  #TAUTHOR_TAG have shown faster convergence and performance gains by following a curriculum training regimen in the order of increasingly complicated syntactic and semantic tasks.', '[  #AUTHOR_TAG ] also find theoretical and experimental evidence for curriculum learning by pretraining on another task leading to faster convergence.', 'with this purview, we propose a syntactico - semantic curriculum training strategy for hi - en codemixed twitter sentiment analysis.', 'we explore various pretraining strategies encompassing language identification, part of speech tagging, and language modelling in different configurations.', 'we investigate the role of different transfer learning strategies by changing learning rates and gradient freezing to prevent catastrophic forgetting and interference between source and target tasks.', 'we also propose a new model for codemixed sentiment analysis based on character trigram sequences and pooling over time for representation learning.', 'we investigate the convergence rate and model performance across various learning strategies, and find faster model convergence and performance gains on the test set']",0
"['', ' #TAUTHOR_TAG propose a hierarchical multi']","['', ' #TAUTHOR_TAG propose a hierarchical multitask neural']","['language modelling task.', ' #TAUTHOR_TAG propose a hierarchical multitask neural architecture with']","['', 'al. [ 2009 ] introduced curriculum learning approaches towards both vision and language related task, and show significant convergence and performance gains for language modelling task.', ' #TAUTHOR_TAG']",0
"['', ' #TAUTHOR_TAG propose a hierarchical multi']","['', ' #TAUTHOR_TAG propose a hierarchical multitask neural']","['language modelling task.', ' #TAUTHOR_TAG propose a hierarchical multitask neural architecture with']","['', 'al. [ 2009 ] introduced curriculum learning approaches towards both vision and language related task, and show significant convergence and performance gains for language modelling task.', ' #TAUTHOR_TAG']",0
"['tasks, the highlight of the approach lies in the training regimen followed.', 'curriculum learning can be seen as a sequence of training criteria  #TAUTHOR_TAG, with increasing task or sample difficulty']","['tasks, the highlight of the approach lies in the training regimen followed.', 'curriculum learning can be seen as a sequence of training criteria  #TAUTHOR_TAG, with increasing task or sample difficulty']","['tasks, the highlight of the approach lies in the training regimen followed.', 'curriculum learning can be seen as a sequence of training criteria  #TAUTHOR_TAG, with increasing task or sample difficulty']","['our proposed model enables efficient transfer learning by progressive abstraction of representations for more complicated tasks, the highlight of the approach lies in the training regimen followed.', 'curriculum learning can be seen as a sequence of training criteria  #TAUTHOR_TAG, with increasing task or sample difficulty as the training progresses.', '']",0
"['tasks, the highlight of the approach lies in the training regimen followed.', 'curriculum learning can be seen as a sequence of training criteria  #TAUTHOR_TAG, with increasing task or sample difficulty']","['tasks, the highlight of the approach lies in the training regimen followed.', 'curriculum learning can be seen as a sequence of training criteria  #TAUTHOR_TAG, with increasing task or sample difficulty']","['tasks, the highlight of the approach lies in the training regimen followed.', 'curriculum learning can be seen as a sequence of training criteria  #TAUTHOR_TAG, with increasing task or sample difficulty']","['our proposed model enables efficient transfer learning by progressive abstraction of representations for more complicated tasks, the highlight of the approach lies in the training regimen followed.', 'curriculum learning can be seen as a sequence of training criteria  #TAUTHOR_TAG, with increasing task or sample difficulty as the training progresses.', '']",0
['noted in earlier efforts  #TAUTHOR_TAG ] towards fine'],['noted in earlier efforts  #TAUTHOR_TAG ] towards'],['noted in earlier efforts  #TAUTHOR_TAG ] towards finetuning pretrained models'],"['noted in earlier efforts  #TAUTHOR_TAG ] towards finetuning pretrained models for nlp tasks, aggressive finetuning can cause catastrophic forgetting, thus causing the model to simply fit over the target task and forget any capabilities gained during the pretraining stage.', 'on the other hand, too cautious finetuning can cause slow convergence and overfitting.', 'to this end, we experiment with different strategies which can be broadly categorized as :', '']",0
"['semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model']","['semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model']","['semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model accuracy by 2']","['input to the lstm stack is the sequence of character trigram dense representations, which we keep as 64 dimensional vectors.', 'we also explore other token representations such as sequence of unigrams, convolution over unigrams [  #AUTHOR_TAG ], and byte pair encoding ( bpe ) [  #AUTHOR_TAG ].', 'bpe is an unsupervised approach towards subword decomposition, and has shown improvements in mt systems and summarization.', 'we train our model from scratch for sentiment analysis using the above mentioned character encodings, and report the results in table 3.', 'our lstm stack consists of two layers of bidirectional lstms, with 64 hidden state dimensions.', 'we add a dropout layer with the dropout rate set to 0. 2 between the lstm layers to prevent overfitting.', 'we experiment with average pooling and max pooling concatenation over hidden states for semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model accuracy by 2. 2 % on sentiment analysis.', 'to evaluate our baseline for curriculum training experiments, we initially train the model from scratch on the single target task ( sentiment analysis ) for 25 epochs.', 'we approach the evaluation of our curriculum by training the model sequentially for four subtasks - language identification, pos tagging, language modelling and sentiment analysis.', 'we evaluate the strategy of pretraining with only pos tagging and language identification, and observe similar performance as no curriculum training.', 'we hypothesize the potential reasons for this drop and find a significant divergence in character trigram occurance between the source tasks ( pos + lang id ) and target task ( sentiment analysis ).', 'this experiement highlights the importance of inclusion of language model pretraining for better token level representation learning, and provides a better model prior for sequence representation ( lstm layer 2 output ).', 'we experiment with only language modelling as pretraining task, and observe significant gains over no curriculum strategy.', 'we note the convergence of our model with and without curriculum training, and observe that the curriculum training regimen causes faster convergence, as has been observed in previous works [  #TAUTHOR_TAG.', 'this is expected as the model is pretrained on prior tasks already have a general purpose representation learning, and only needs to adapt to the idiosyncrasies of the target task, i. e. sentiment analysis in this case.', 'as discussed in section 4. 3, for our transfer learning optimization experiments, we segment the optimization of different parameters of our model with different learning rates, in order to limit catastrophic forgetting and interference among the tasks, as proposed by  #TAUTHOR_TAG.', 'we segregate our model parameters in the following 4 groups :', '• emb layer', '• lstm layer 1', '• ls']",0
"['semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model']","['semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model']","['semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model accuracy by 2']","['input to the lstm stack is the sequence of character trigram dense representations, which we keep as 64 dimensional vectors.', 'we also explore other token representations such as sequence of unigrams, convolution over unigrams [  #AUTHOR_TAG ], and byte pair encoding ( bpe ) [  #AUTHOR_TAG ].', 'bpe is an unsupervised approach towards subword decomposition, and has shown improvements in mt systems and summarization.', 'we train our model from scratch for sentiment analysis using the above mentioned character encodings, and report the results in table 3.', 'our lstm stack consists of two layers of bidirectional lstms, with 64 hidden state dimensions.', 'we add a dropout layer with the dropout rate set to 0. 2 between the lstm layers to prevent overfitting.', 'we experiment with average pooling and max pooling concatenation over hidden states for semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model accuracy by 2. 2 % on sentiment analysis.', 'to evaluate our baseline for curriculum training experiments, we initially train the model from scratch on the single target task ( sentiment analysis ) for 25 epochs.', 'we approach the evaluation of our curriculum by training the model sequentially for four subtasks - language identification, pos tagging, language modelling and sentiment analysis.', 'we evaluate the strategy of pretraining with only pos tagging and language identification, and observe similar performance as no curriculum training.', 'we hypothesize the potential reasons for this drop and find a significant divergence in character trigram occurance between the source tasks ( pos + lang id ) and target task ( sentiment analysis ).', 'this experiement highlights the importance of inclusion of language model pretraining for better token level representation learning, and provides a better model prior for sequence representation ( lstm layer 2 output ).', 'we experiment with only language modelling as pretraining task, and observe significant gains over no curriculum strategy.', 'we note the convergence of our model with and without curriculum training, and observe that the curriculum training regimen causes faster convergence, as has been observed in previous works [  #TAUTHOR_TAG.', 'this is expected as the model is pretrained on prior tasks already have a general purpose representation learning, and only needs to adapt to the idiosyncrasies of the target task, i. e. sentiment analysis in this case.', 'as discussed in section 4. 3, for our transfer learning optimization experiments, we segment the optimization of different parameters of our model with different learning rates, in order to limit catastrophic forgetting and interference among the tasks, as proposed by  #TAUTHOR_TAG.', 'we segregate our model parameters in the following 4 groups :', '• emb layer', '• lstm layer 1', '• ls']",0
"['semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model']","['semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model']","['semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model accuracy by 2']","['input to the lstm stack is the sequence of character trigram dense representations, which we keep as 64 dimensional vectors.', 'we also explore other token representations such as sequence of unigrams, convolution over unigrams [  #AUTHOR_TAG ], and byte pair encoding ( bpe ) [  #AUTHOR_TAG ].', 'bpe is an unsupervised approach towards subword decomposition, and has shown improvements in mt systems and summarization.', 'we train our model from scratch for sentiment analysis using the above mentioned character encodings, and report the results in table 3.', 'our lstm stack consists of two layers of bidirectional lstms, with 64 hidden state dimensions.', 'we add a dropout layer with the dropout rate set to 0. 2 between the lstm layers to prevent overfitting.', 'we experiment with average pooling and max pooling concatenation over hidden states for semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model accuracy by 2. 2 % on sentiment analysis.', 'to evaluate our baseline for curriculum training experiments, we initially train the model from scratch on the single target task ( sentiment analysis ) for 25 epochs.', 'we approach the evaluation of our curriculum by training the model sequentially for four subtasks - language identification, pos tagging, language modelling and sentiment analysis.', 'we evaluate the strategy of pretraining with only pos tagging and language identification, and observe similar performance as no curriculum training.', 'we hypothesize the potential reasons for this drop and find a significant divergence in character trigram occurance between the source tasks ( pos + lang id ) and target task ( sentiment analysis ).', 'this experiement highlights the importance of inclusion of language model pretraining for better token level representation learning, and provides a better model prior for sequence representation ( lstm layer 2 output ).', 'we experiment with only language modelling as pretraining task, and observe significant gains over no curriculum strategy.', 'we note the convergence of our model with and without curriculum training, and observe that the curriculum training regimen causes faster convergence, as has been observed in previous works [  #TAUTHOR_TAG.', 'this is expected as the model is pretrained on prior tasks already have a general purpose representation learning, and only needs to adapt to the idiosyncrasies of the target task, i. e. sentiment analysis in this case.', 'as discussed in section 4. 3, for our transfer learning optimization experiments, we segment the optimization of different parameters of our model with different learning rates, in order to limit catastrophic forgetting and interference among the tasks, as proposed by  #TAUTHOR_TAG.', 'we segregate our model parameters in the following 4 groups :', '• emb layer', '• lstm layer 1', '• ls']",0
"['tasks, the highlight of the approach lies in the training regimen followed.', 'curriculum learning can be seen as a sequence of training criteria  #TAUTHOR_TAG, with increasing task or sample difficulty']","['tasks, the highlight of the approach lies in the training regimen followed.', 'curriculum learning can be seen as a sequence of training criteria  #TAUTHOR_TAG, with increasing task or sample difficulty']","['tasks, the highlight of the approach lies in the training regimen followed.', 'curriculum learning can be seen as a sequence of training criteria  #TAUTHOR_TAG, with increasing task or sample difficulty']","['our proposed model enables efficient transfer learning by progressive abstraction of representations for more complicated tasks, the highlight of the approach lies in the training regimen followed.', 'curriculum learning can be seen as a sequence of training criteria  #TAUTHOR_TAG, with increasing task or sample difficulty as the training progresses.', '']",1
['noted in earlier efforts  #TAUTHOR_TAG ] towards fine'],['noted in earlier efforts  #TAUTHOR_TAG ] towards'],['noted in earlier efforts  #TAUTHOR_TAG ] towards finetuning pretrained models'],"['noted in earlier efforts  #TAUTHOR_TAG ] towards finetuning pretrained models for nlp tasks, aggressive finetuning can cause catastrophic forgetting, thus causing the model to simply fit over the target task and forget any capabilities gained during the pretraining stage.', 'on the other hand, too cautious finetuning can cause slow convergence and overfitting.', 'to this end, we experiment with different strategies which can be broadly categorized as :', '']",1
['noted in earlier efforts  #TAUTHOR_TAG ] towards fine'],['noted in earlier efforts  #TAUTHOR_TAG ] towards'],['noted in earlier efforts  #TAUTHOR_TAG ] towards finetuning pretrained models'],"['noted in earlier efforts  #TAUTHOR_TAG ] towards finetuning pretrained models for nlp tasks, aggressive finetuning can cause catastrophic forgetting, thus causing the model to simply fit over the target task and forget any capabilities gained during the pretraining stage.', 'on the other hand, too cautious finetuning can cause slow convergence and overfitting.', 'to this end, we experiment with different strategies which can be broadly categorized as :', '']",3
['noted in earlier efforts  #TAUTHOR_TAG ] towards fine'],['noted in earlier efforts  #TAUTHOR_TAG ] towards'],['noted in earlier efforts  #TAUTHOR_TAG ] towards finetuning pretrained models'],"['noted in earlier efforts  #TAUTHOR_TAG ] towards finetuning pretrained models for nlp tasks, aggressive finetuning can cause catastrophic forgetting, thus causing the model to simply fit over the target task and forget any capabilities gained during the pretraining stage.', 'on the other hand, too cautious finetuning can cause slow convergence and overfitting.', 'to this end, we experiment with different strategies which can be broadly categorized as :', '']",3
"['semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model']","['semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model']","['semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model accuracy by 2']","['input to the lstm stack is the sequence of character trigram dense representations, which we keep as 64 dimensional vectors.', 'we also explore other token representations such as sequence of unigrams, convolution over unigrams [  #AUTHOR_TAG ], and byte pair encoding ( bpe ) [  #AUTHOR_TAG ].', 'bpe is an unsupervised approach towards subword decomposition, and has shown improvements in mt systems and summarization.', 'we train our model from scratch for sentiment analysis using the above mentioned character encodings, and report the results in table 3.', 'our lstm stack consists of two layers of bidirectional lstms, with 64 hidden state dimensions.', 'we add a dropout layer with the dropout rate set to 0. 2 between the lstm layers to prevent overfitting.', 'we experiment with average pooling and max pooling concatenation over hidden states for semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model accuracy by 2. 2 % on sentiment analysis.', 'to evaluate our baseline for curriculum training experiments, we initially train the model from scratch on the single target task ( sentiment analysis ) for 25 epochs.', 'we approach the evaluation of our curriculum by training the model sequentially for four subtasks - language identification, pos tagging, language modelling and sentiment analysis.', 'we evaluate the strategy of pretraining with only pos tagging and language identification, and observe similar performance as no curriculum training.', 'we hypothesize the potential reasons for this drop and find a significant divergence in character trigram occurance between the source tasks ( pos + lang id ) and target task ( sentiment analysis ).', 'this experiement highlights the importance of inclusion of language model pretraining for better token level representation learning, and provides a better model prior for sequence representation ( lstm layer 2 output ).', 'we experiment with only language modelling as pretraining task, and observe significant gains over no curriculum strategy.', 'we note the convergence of our model with and without curriculum training, and observe that the curriculum training regimen causes faster convergence, as has been observed in previous works [  #TAUTHOR_TAG.', 'this is expected as the model is pretrained on prior tasks already have a general purpose representation learning, and only needs to adapt to the idiosyncrasies of the target task, i. e. sentiment analysis in this case.', 'as discussed in section 4. 3, for our transfer learning optimization experiments, we segment the optimization of different parameters of our model with different learning rates, in order to limit catastrophic forgetting and interference among the tasks, as proposed by  #TAUTHOR_TAG.', 'we segregate our model parameters in the following 4 groups :', '• emb layer', '• lstm layer 1', '• ls']",3
"['semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model']","['semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model']","['semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model accuracy by 2']","['input to the lstm stack is the sequence of character trigram dense representations, which we keep as 64 dimensional vectors.', 'we also explore other token representations such as sequence of unigrams, convolution over unigrams [  #AUTHOR_TAG ], and byte pair encoding ( bpe ) [  #AUTHOR_TAG ].', 'bpe is an unsupervised approach towards subword decomposition, and has shown improvements in mt systems and summarization.', 'we train our model from scratch for sentiment analysis using the above mentioned character encodings, and report the results in table 3.', 'our lstm stack consists of two layers of bidirectional lstms, with 64 hidden state dimensions.', 'we add a dropout layer with the dropout rate set to 0. 2 between the lstm layers to prevent overfitting.', 'we experiment with average pooling and max pooling concatenation over hidden states for semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model accuracy by 2. 2 % on sentiment analysis.', 'to evaluate our baseline for curriculum training experiments, we initially train the model from scratch on the single target task ( sentiment analysis ) for 25 epochs.', 'we approach the evaluation of our curriculum by training the model sequentially for four subtasks - language identification, pos tagging, language modelling and sentiment analysis.', 'we evaluate the strategy of pretraining with only pos tagging and language identification, and observe similar performance as no curriculum training.', 'we hypothesize the potential reasons for this drop and find a significant divergence in character trigram occurance between the source tasks ( pos + lang id ) and target task ( sentiment analysis ).', 'this experiement highlights the importance of inclusion of language model pretraining for better token level representation learning, and provides a better model prior for sequence representation ( lstm layer 2 output ).', 'we experiment with only language modelling as pretraining task, and observe significant gains over no curriculum strategy.', 'we note the convergence of our model with and without curriculum training, and observe that the curriculum training regimen causes faster convergence, as has been observed in previous works [  #TAUTHOR_TAG.', 'this is expected as the model is pretrained on prior tasks already have a general purpose representation learning, and only needs to adapt to the idiosyncrasies of the target task, i. e. sentiment analysis in this case.', 'as discussed in section 4. 3, for our transfer learning optimization experiments, we segment the optimization of different parameters of our model with different learning rates, in order to limit catastrophic forgetting and interference among the tasks, as proposed by  #TAUTHOR_TAG.', 'we segregate our model parameters in the following 4 groups :', '• emb layer', '• lstm layer 1', '• ls']",3
"['semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model']","['semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model']","['semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model accuracy by 2']","['input to the lstm stack is the sequence of character trigram dense representations, which we keep as 64 dimensional vectors.', 'we also explore other token representations such as sequence of unigrams, convolution over unigrams [  #AUTHOR_TAG ], and byte pair encoding ( bpe ) [  #AUTHOR_TAG ].', 'bpe is an unsupervised approach towards subword decomposition, and has shown improvements in mt systems and summarization.', 'we train our model from scratch for sentiment analysis using the above mentioned character encodings, and report the results in table 3.', 'our lstm stack consists of two layers of bidirectional lstms, with 64 hidden state dimensions.', 'we add a dropout layer with the dropout rate set to 0. 2 between the lstm layers to prevent overfitting.', 'we experiment with average pooling and max pooling concatenation over hidden states for semantic prediction, similar to  #TAUTHOR_TAG, and observe increase in model accuracy by 2. 2 % on sentiment analysis.', 'to evaluate our baseline for curriculum training experiments, we initially train the model from scratch on the single target task ( sentiment analysis ) for 25 epochs.', 'we approach the evaluation of our curriculum by training the model sequentially for four subtasks - language identification, pos tagging, language modelling and sentiment analysis.', 'we evaluate the strategy of pretraining with only pos tagging and language identification, and observe similar performance as no curriculum training.', 'we hypothesize the potential reasons for this drop and find a significant divergence in character trigram occurance between the source tasks ( pos + lang id ) and target task ( sentiment analysis ).', 'this experiement highlights the importance of inclusion of language model pretraining for better token level representation learning, and provides a better model prior for sequence representation ( lstm layer 2 output ).', 'we experiment with only language modelling as pretraining task, and observe significant gains over no curriculum strategy.', 'we note the convergence of our model with and without curriculum training, and observe that the curriculum training regimen causes faster convergence, as has been observed in previous works [  #TAUTHOR_TAG.', 'this is expected as the model is pretrained on prior tasks already have a general purpose representation learning, and only needs to adapt to the idiosyncrasies of the target task, i. e. sentiment analysis in this case.', 'as discussed in section 4. 3, for our transfer learning optimization experiments, we segment the optimization of different parameters of our model with different learning rates, in order to limit catastrophic forgetting and interference among the tasks, as proposed by  #TAUTHOR_TAG.', 'we segregate our model parameters in the following 4 groups :', '• emb layer', '• lstm layer 1', '• ls']",3
['noted in earlier efforts  #TAUTHOR_TAG ] towards fine'],['noted in earlier efforts  #TAUTHOR_TAG ] towards'],['noted in earlier efforts  #TAUTHOR_TAG ] towards finetuning pretrained models'],"['noted in earlier efforts  #TAUTHOR_TAG ] towards finetuning pretrained models for nlp tasks, aggressive finetuning can cause catastrophic forgetting, thus causing the model to simply fit over the target task and forget any capabilities gained during the pretraining stage.', 'on the other hand, too cautious finetuning can cause slow convergence and overfitting.', 'to this end, we experiment with different strategies which can be broadly categorized as :', '']",5
['noted in earlier efforts  #TAUTHOR_TAG ] towards fine'],['noted in earlier efforts  #TAUTHOR_TAG ] towards'],['noted in earlier efforts  #TAUTHOR_TAG ] towards finetuning pretrained models'],"['noted in earlier efforts  #TAUTHOR_TAG ] towards finetuning pretrained models for nlp tasks, aggressive finetuning can cause catastrophic forgetting, thus causing the model to simply fit over the target task and forget any capabilities gained during the pretraining stage.', 'on the other hand, too cautious finetuning can cause slow convergence and overfitting.', 'to this end, we experiment with different strategies which can be broadly categorized as :', '']",5
"['cnn  #TAUTHOR_TAG features.', ' #AUTHOR_TAG a ) proposed a neural']","['cnn  #TAUTHOR_TAG features.', ' #AUTHOR_TAG a ) proposed a neural']","['cnn  #TAUTHOR_TAG features.', ' #AUTHOR_TAG a ) proposed a neural reranking model']","['proposed a seminal neural architecture for sequence labeling.', 'it captures word sequence information with a one - layer cnn based on pretrained word embeddings and handcrafted neural features, followed with a crf output layer.', 'dos  #AUTHOR_TAG extended this model by integrating character - level cnn features.', ' #AUTHOR_TAG built a deeper dilated cnn architecture to capture larger local features.', ' #AUTHOR_TAG was the first to exploit lstm for sequence labeling.', 'built a bilstm - crf structure, which has been extended by adding character - level lstm  #AUTHOR_TAG, gru  #AUTHOR_TAG, and cnn  #TAUTHOR_TAG features.', ' #AUTHOR_TAG a ) proposed a neural reranking model to improve ner models.', ' #TAUTHOR_TAG achieve state - of - the - art results in the literature.', ' #AUTHOR_TAG b ) compared several word - based lstm models for several sequence labeling tasks, reporting the score distributions over multiple runs rather than single value.', 'they investigated the influence of various hyperparameters and configurations.', 'our work is similar in comparing different neural architectures under unified settings, but differs in four main aspects : 1 ) their experiments are based on a bilstm with handcrafted word features, while our experiments are based on end - to - end neural models without human knowledge.', '2 ) their system gives relatively low performances on standard benchmarks 2, while ours can give comparable or better results with state - of - the - art models, rendering our observations more informative for practitioners.', '3 ) our findings are more consistent with  #TAUTHOR_TAG and tag scheme  #AUTHOR_TAG.', 'in contrast, many results of  #AUTHOR_TAG b ) contradict existing reports.', '4 ) we conduct a wider range of comparison for word sequence representations, including all combinations of character cnn / lstm and word cnn / lstm structures, while  #AUTHOR_TAG b ) studied the word lstm models only']",3
['literature  #TAUTHOR_TAG'],['literature  #TAUTHOR_TAG'],"['significantly ( p < 0. 01 ), with a slower convergence process during training.', 'our observation is consistent with most literature  #TAUTHOR_TAG']","['addition to model structures, external factors such as pretrained embeddings, tag scheme, and optimizer can significantly influence system performance.', 'we investigate a set of external factors on the ner dataset with the two best models : clstm + wlstm + crf and ccnn + wlstm + crf.', 'pretrained embedding.', 'figure 4 ( a ) shows the f1 - scores of the two best models on the ner test set with two different pretrained embeddings, as well as the random initialization.', 'compared with the random initialization, models using pretrained embeddings give significant improvements ( p < 0. 01 ).', 'the glove 100 - dimension embeddings give higher f1 - scores than senna  #AUTHOR_TAG on both models, which is consistent with the observation of  #AUTHOR_TAG.', 'tag scheme.', 'we examine two different tag schemes : bio and bioes  #AUTHOR_TAG.', 'the results are shown in figure 4 ( b ).', 'in our experiments, models using bioes are significantly ( p < 0. 05 ) better than bio.', 'our observation is consistent with most literature  #AUTHOR_TAG.', ' #AUTHOR_TAG b ) report that the difference between the schemes is insignificant.', 'running environment.', ' #AUTHOR_TAG observe that neural sequence labeling models can give better results on gpu rather than cpu.', 'we conduct repeated experiments on both gpu and cpu environments.', 'the results are shown in figure 4 ( b ).', 'models run on cpu give a lower mean f1 - score than models run on gpu, while the difference is insignificant ( p > 0. 2 ).', 'optimizer.', 'we compare different optimizers including sgd, adagrad  #AUTHOR_TAG, adadelta  #AUTHOR_TAG rmsprop  #AUTHOR_TAG and adam  #AUTHOR_TAG.', 'the results are shown in figure 5 5.', 'in contrast to  #AUTHOR_TAG b ), who reported that sgd is the worst optimizer, our results show that sgd outperforms all other optimizers significantly ( p < 0. 01 ), with a slower convergence process during training.', 'our observation is consistent with most literature  #TAUTHOR_TAG']",3
"['of several reports  #TAUTHOR_TAG, which take']","['of several reports  #TAUTHOR_TAG, which take']","['of several reports  #TAUTHOR_TAG, which take the ccnn + wlstm + crf architecture.', '']","['4, 5 and 6 show the results of the twelve models on ner, chunking and pos datasets, respectively.', 'existing work has also been listed in the tables for comparison.', 'to simplify the description, we use "" clstm "" and "" ccnn "" to represent character lstm and character cnn encoder, respectively.', 'similarly, "" wlstm "" and "" wcnn "" represent word lstm and word cnn structure, respectively.', 'as shown in table 4, most ner work focuses on wlstm + crf structures with different character sequence representations.', 'we re - implement the structure of several reports  #TAUTHOR_TAG, which take the ccnn + wlstm + crf architecture.', '']",5
['2016 twitter stance detection corpus  #TAUTHOR_TAG. in'],"['2016 twitter stance detection corpus  #TAUTHOR_TAG. in [ 3 ], a stance -']","['2016 twitter stance detection corpus  #TAUTHOR_TAG. in [ 3 ], a stance - community']","['', '1 ] on semeval 2016 twitter stance detection corpus  #TAUTHOR_TAG. in [ 3 ], a stance - community detection approach', 'called scifnet is proposed. scifnet creates networks of people who are stance targets, automatically from the related', 'document collections [ 3 ] using stance expansion and refinement techniques to arrive at stance - coherent networks. a tweet data set annotated with stance information regarding six predefined targets is proposed in [ 11 ] where this data set is annotated through crowdsourcing. the authors indicate that the data set is also annotated with sentiment information in addition', ""to stance, so it can help reveal sideways'17, july 2017, prague, czech republic"", ""d. kucuk associations between stance and sentiment [ 11 ]. lastly, in  #TAUTHOR_TAG, se - meval 2016's aforementioned shared task on"", 'twitter stance detection is described. also provided are the results of the evaluations of 19 systems participating in two', 'subtasks ( one with training data set provided and the other without an annotated data set ) of the shared task  #TAUTHOR_TAG', '. in this paper, we present a tweet data set in turkish annotated with stance information, where the corresponding annotations are made publicly available. the domain of the tweets comprises two popular football clubs which constitute the targets of the tweets', 'included. we also provide the evaluation results of svm classifiers ( for each target ) on this data set using unigram, bigram, and hashtag features. to', 'the best of our knowledge, the current study is the first one to target at stance detection in turkish tweets. together with the provided annotated data set', '']",0
['2016 twitter stance detection corpus  #TAUTHOR_TAG. in'],"['2016 twitter stance detection corpus  #TAUTHOR_TAG. in [ 3 ], a stance -']","['2016 twitter stance detection corpus  #TAUTHOR_TAG. in [ 3 ], a stance - community']","['', '1 ] on semeval 2016 twitter stance detection corpus  #TAUTHOR_TAG. in [ 3 ], a stance - community detection approach', 'called scifnet is proposed. scifnet creates networks of people who are stance targets, automatically from the related', 'document collections [ 3 ] using stance expansion and refinement techniques to arrive at stance - coherent networks. a tweet data set annotated with stance information regarding six predefined targets is proposed in [ 11 ] where this data set is annotated through crowdsourcing. the authors indicate that the data set is also annotated with sentiment information in addition', ""to stance, so it can help reveal sideways'17, july 2017, prague, czech republic"", ""d. kucuk associations between stance and sentiment [ 11 ]. lastly, in  #TAUTHOR_TAG, se - meval 2016's aforementioned shared task on"", 'twitter stance detection is described. also provided are the results of the evaluations of 19 systems participating in two', 'subtasks ( one with training data set provided and the other without an annotated data set ) of the shared task  #TAUTHOR_TAG', '. in this paper, we present a tweet data set in turkish annotated with stance information, where the corresponding annotations are made publicly available. the domain of the tweets comprises two popular football clubs which constitute the targets of the tweets', 'included. we also provide the evaluation results of svm classifiers ( for each target ) on this data set using unigram, bigram, and hashtag features. to', 'the best of our knowledge, the current study is the first one to target at stance detection in turkish tweets. together with the provided annotated data set', '']",0
['2016 twitter stance detection corpus  #TAUTHOR_TAG. in'],"['2016 twitter stance detection corpus  #TAUTHOR_TAG. in [ 3 ], a stance -']","['2016 twitter stance detection corpus  #TAUTHOR_TAG. in [ 3 ], a stance - community']","['', '1 ] on semeval 2016 twitter stance detection corpus  #TAUTHOR_TAG. in [ 3 ], a stance - community detection approach', 'called scifnet is proposed. scifnet creates networks of people who are stance targets, automatically from the related', 'document collections [ 3 ] using stance expansion and refinement techniques to arrive at stance - coherent networks. a tweet data set annotated with stance information regarding six predefined targets is proposed in [ 11 ] where this data set is annotated through crowdsourcing. the authors indicate that the data set is also annotated with sentiment information in addition', ""to stance, so it can help reveal sideways'17, july 2017, prague, czech republic"", ""d. kucuk associations between stance and sentiment [ 11 ]. lastly, in  #TAUTHOR_TAG, se - meval 2016's aforementioned shared task on"", 'twitter stance detection is described. also provided are the results of the evaluations of 19 systems participating in two', 'subtasks ( one with training data set provided and the other without an annotated data set ) of the shared task  #TAUTHOR_TAG', '. in this paper, we present a tweet data set in turkish annotated with stance information, where the corresponding annotations are made publicly available. the domain of the tweets comprises two popular football clubs which constitute the targets of the tweets', 'included. we also provide the evaluation results of svm classifiers ( for each target ) on this data set using unigram, bigram, and hashtag features. to', 'the best of our knowledge, the current study is the first one to target at stance detection in turkish tweets. together with the provided annotated data set', '']",0
['2016 twitter stance detection corpus  #TAUTHOR_TAG. in'],"['2016 twitter stance detection corpus  #TAUTHOR_TAG. in [ 3 ], a stance -']","['2016 twitter stance detection corpus  #TAUTHOR_TAG. in [ 3 ], a stance - community']","['', '1 ] on semeval 2016 twitter stance detection corpus  #TAUTHOR_TAG. in [ 3 ], a stance - community detection approach', 'called scifnet is proposed. scifnet creates networks of people who are stance targets, automatically from the related', 'document collections [ 3 ] using stance expansion and refinement techniques to arrive at stance - coherent networks. a tweet data set annotated with stance information regarding six predefined targets is proposed in [ 11 ] where this data set is annotated through crowdsourcing. the authors indicate that the data set is also annotated with sentiment information in addition', ""to stance, so it can help reveal sideways'17, july 2017, prague, czech republic"", ""d. kucuk associations between stance and sentiment [ 11 ]. lastly, in  #TAUTHOR_TAG, se - meval 2016's aforementioned shared task on"", 'twitter stance detection is described. also provided are the results of the evaluations of 19 systems participating in two', 'subtasks ( one with training data set provided and the other without an annotated data set ) of the shared task  #TAUTHOR_TAG', '. in this paper, we present a tweet data set in turkish annotated with stance information, where the corresponding annotations are made publicly available. the domain of the tweets comprises two popular football clubs which constitute the targets of the tweets', 'included. we also provide the evaluation results of svm classifiers ( for each target ) on this data set using unigram, bigram, and hashtag features. to', 'the best of our knowledge, the current study is the first one to target at stance detection in turkish tweets. together with the provided annotated data set', '']",0
['2016 twitter stance detection corpus  #TAUTHOR_TAG. in'],"['2016 twitter stance detection corpus  #TAUTHOR_TAG. in [ 3 ], a stance -']","['2016 twitter stance detection corpus  #TAUTHOR_TAG. in [ 3 ], a stance - community']","['', '1 ] on semeval 2016 twitter stance detection corpus  #TAUTHOR_TAG. in [ 3 ], a stance - community detection approach', 'called scifnet is proposed. scifnet creates networks of people who are stance targets, automatically from the related', 'document collections [ 3 ] using stance expansion and refinement techniques to arrive at stance - coherent networks. a tweet data set annotated with stance information regarding six predefined targets is proposed in [ 11 ] where this data set is annotated through crowdsourcing. the authors indicate that the data set is also annotated with sentiment information in addition', ""to stance, so it can help reveal sideways'17, july 2017, prague, czech republic"", ""d. kucuk associations between stance and sentiment [ 11 ]. lastly, in  #TAUTHOR_TAG, se - meval 2016's aforementioned shared task on"", 'twitter stance detection is described. also provided are the results of the evaluations of 19 systems participating in two', 'subtasks ( one with training data set provided and the other without an annotated data set ) of the shared task  #TAUTHOR_TAG', '. in this paper, we present a tweet data set in turkish annotated with stance information, where the corresponding annotations are made publicly available. the domain of the tweets comprises two popular football clubs which constitute the targets of the tweets', 'included. we also provide the evaluation results of svm classifiers ( for each target ) on this data set using unigram, bigram, and hashtag features. to', 'the best of our knowledge, the current study is the first one to target at stance detection in turkish tweets. together with the provided annotated data set', '']",0
"['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', '']","['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', '']","['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', '']","['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', 'we should also note that svm - based sentiment analysis systems ( such as those given in [ 15 ] ) have been reported to achieve better f - measure rates for the positive sentiment class when compared with', '']",0
"['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', '']","['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', '']","['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', '']","['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', 'we should also note that svm - based sentiment analysis systems ( such as those given in [ 15 ] ) have been reported to achieve better f - measure rates for the positive sentiment class when compared with', '']",0
"['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', '']","['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', '']","['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', '']","['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', 'we should also note that svm - based sentiment analysis systems ( such as those given in [ 15 ] ) have been reported to achieve better f - measure rates for the positive sentiment class when compared with', '']",4
"['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', '']","['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', '']","['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', '']","['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', 'we should also note that svm - based sentiment analysis systems ( such as those given in [ 15 ] ) have been reported to achieve better f - measure rates for the positive sentiment class when compared with', '']",4
"['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', '']","['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', '']","['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', '']","['difference is that the data sets in  #TAUTHOR_TAG have been divided into training and test sets, while in our study we provide 10 - fold cross - validation results on the whole data set. on the other hand,', 'we should also note that svm - based sentiment analysis systems ( such as those given in [ 15 ] ) have been reported to achieve better f - measure rates for the positive sentiment class when compared with', '']",4
['as  #TAUTHOR_TAG'],['as  #TAUTHOR_TAG'],"['as  #TAUTHOR_TAG can be tested on our data set.', '•']","['work based on the current study includes the following :', '• the presented stance - annotated data set for turkish has been created by one annotator only ( the author of this study ), yet, the data set should better be revised and extended through crowdsourcing facilities.', 'when employing such a procedure, other stance classes like neither can be considered as well.', 'the procedure will improve the quality the data set as well as the quality of prospective systems to be trained and tested on it.', '• other features like emoticons ( as commonly used for sentiment analysis ), features based on hashtags, and ngram features can also be used by the classifiers and these classifiers can be tested on larger data sets.', 'other classification approaches could also be implemented and tested against our baseline classifiers.', 'particularly, related methods presented in recent studies such as  #TAUTHOR_TAG can be tested on our data set.', '• lastly, the svm classifiers utilized in this study and their prospective versions utilizing other features can be tested on stance data sets in other languages ( such as english ) for comparison purposes']",2
"['research papers, they achieve moderate results on social media texts  #TAUTHOR_TAG.', 'recent works go']","['research papers, they achieve moderate results on social media texts  #TAUTHOR_TAG.', 'recent works go']","['research papers, they achieve moderate results on social media texts  #TAUTHOR_TAG.', 'recent works go']","['', 'the most popular knowledgebased system for mapping texts to umls identifiers is metamap  #AUTHOR_TAG.', 'this linguisticbased system uses lexical lookup and variants by associating a score with phrases in a sentence.', 'the state - of - the - art baseline for clinical and scientific texts is dnorm  #AUTHOR_TAG.', 'dnorm adopts a pairwise learning - torank technique using vectors of query mentions and candidate concept terms.', 'this model outperforms metamap significantly, increasing the macro - averaged f - measure by 25 % on an ncbi disease dataset.', 'however, while these tools have proven to be effective for patient records and research papers, they achieve moderate results on social media texts  #TAUTHOR_TAG.', 'recent works go beyond string matching : these works have tried to view the problem of matching a one - or multi - word expression against a knowledge base as a supervised sequence labeling problem.', ' #TAUTHOR_TAG utilized convolutional neural networks ( cnns ) for phrase normalization in user reviews, while  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG applied recurrent neural networks ( rnns ) to ugts, achieving similar results.', 'these works were among the first applications of deep learning techniques to medical concept normalization.', 'the goal of this work is to study the use of deep neural models,']",1
"['research papers, they achieve moderate results on social media texts  #TAUTHOR_TAG.', 'recent works go']","['research papers, they achieve moderate results on social media texts  #TAUTHOR_TAG.', 'recent works go']","['research papers, they achieve moderate results on social media texts  #TAUTHOR_TAG.', 'recent works go']","['', 'the most popular knowledgebased system for mapping texts to umls identifiers is metamap  #AUTHOR_TAG.', 'this linguisticbased system uses lexical lookup and variants by associating a score with phrases in a sentence.', 'the state - of - the - art baseline for clinical and scientific texts is dnorm  #AUTHOR_TAG.', 'dnorm adopts a pairwise learning - torank technique using vectors of query mentions and candidate concept terms.', 'this model outperforms metamap significantly, increasing the macro - averaged f - measure by 25 % on an ncbi disease dataset.', 'however, while these tools have proven to be effective for patient records and research papers, they achieve moderate results on social media texts  #TAUTHOR_TAG.', 'recent works go beyond string matching : these works have tried to view the problem of matching a one - or multi - word expression against a knowledge base as a supervised sequence labeling problem.', ' #TAUTHOR_TAG utilized convolutional neural networks ( cnns ) for phrase normalization in user reviews, while  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG applied recurrent neural networks ( rnns ) to ugts, achieving similar results.', 'these works were among the first applications of deep learning techniques to medical concept normalization.', 'the goal of this work is to study the use of deep neural models,']",0
['( 1 )  #TAUTHOR_TAG 79. 98 - - -'],['( 1 )  #TAUTHOR_TAG 79. 98 - - - - attentional char - cnn  #AUTHOR_TAG 84. 65 - - - - hierarchical char - cnn  #AUTHOR_TAG -'],['( 1 )  #TAUTHOR_TAG 79. 98 - - -'],"['2004, the research community started to address the needs to automatically detect biomedical entities in free texts through shared tasks.', ' #AUTHOR_TAG survey the work done in the organization of biomedical nlp ( bionlp ) challenge evaluations up to 2014.', 'these tasks are devoted to the normalization of ( 1 )  #TAUTHOR_TAG 79. 98 - - - - attentional char - cnn  #AUTHOR_TAG 84. 65 - - - - hierarchical char - cnn  #AUTHOR_TAG - table 2 : the performance of the proposed models and the state - of - the - art methods in terms of accuracy.', '( 4 ) diseases from clinical reports ( share / clef ehealth 2013 ; semeval 2014 task 7 ).', 'similarly, the clef health 2016 and 2017 labs addressed the problem of icd coding of freeform death certificates ( without specified entity mentions ).', 'traditionally, linguistic approaches based on dictionaries, association measures, and syntactic properties have been used to map texts to a concept from a controlled vocabulary  #AUTHOR_TAG.', ' #AUTHOR_TAG proposed the dnorm system based on a pairwise learningto - rank technique using vectors of query mentions and candidate concept terms.', 'these vectors are obtained from a tf - idf representation of all tokens from training mentions and concept terms.', ' #AUTHOR_TAG utilized a hybrid method combining simple dictionary projection and mono - label supervised classification from icd coding.', 'nevertheless, the majority of biomedical research on medical concept extraction primarily focused on scientific literature and clinical records  #AUTHOR_TAG.', ' #AUTHOR_TAG applied a popular dictionary look - up system ctakes on user reviews.', ""ctakes based on additional psytar's dictionaries achieves twice better results ( 0. 49 f1 score on the exact matching )."", 'thus, dictionaries gathered from layperson language can efficiently improve automatic performance.', 'the 2017 smm4h shared task  #AUTHOR_TAG was the first effort for the evaluation of nlp methods for the normalization of health - related text from social media on publicly released data.', 'recent advances in neural networks have been utilized for concept normalization : recent studies have employed convolutional neural networks  #AUTHOR_TAG and recurrent neural networks  #AUTHOR_TAG.', 'these works have trained neural networks from scratch using only entity mentions from training data and pre - trained word embeddings.', 'to sum up, most methods have dealt with encoding information an entity mention itself, ignoring the broader context where it occurred.', 'moreover, these studies did not examine an evaluation methodology tailored to the task']",0
"[')  #TAUTHOR_TAG.', '']","['( ea )  #TAUTHOR_TAG.', '']","[')  #TAUTHOR_TAG.', '']","['this work, we propose a semisupervised extension to a well - known supervised domain adaptation approach ( ea )  #TAUTHOR_TAG.', 'our proposed approach ( ea + + ) builds on the notion of augmented space ( introduced in ea ) and harnesses unlabeled data in target domain to ameliorate the transfer of information from source to target.', 'this semisupervised approach to domain adaptation is extremely simple to implement, and can be applied as a pre - processing step to any supervised learner.', 'experimental results on sequential labeling tasks demonstrate the efficacy of the proposed method']",4
"['with respect to the target domain.', 'in this paper, we consider linear hypotheses only.', 'however, the proposed techniques extend to non - linear hypotheses, as mentioned in  #TAUTHOR_TAG.', 'source and target empirical']","['with respect to the target domain.', 'in this paper, we consider linear hypotheses only.', 'however, the proposed techniques extend to non - linear hypotheses, as mentioned in  #TAUTHOR_TAG.', 'source and target empirical']","['with respect to the target domain.', 'in this paper, we consider linear hypotheses only.', 'however, the proposed techniques extend to non - linear hypotheses, as mentioned in  #TAUTHOR_TAG.', 'source and target empirical errors for hypothesis h are denoted']","['x ⊂ r d denote the instance space and y = { −1, + 1 } denote the label space.', 'we have a set of source labeled examples l s ( ∼ d s ( x, y ) ) and a set of target labeled examples', 'we also have target unlabeled data denoted by u t ( ∼ d t ( x ) ), where | u t | = u t.', 'our goal is to learn a hypothesis h : x → y having low expected error with respect to the target domain.', 'in this paper, we consider linear hypotheses only.', 'however, the proposed techniques extend to non - linear hypotheses, as mentioned in  #TAUTHOR_TAG.', 'source and target empirical errors for hypothesis h are denoted by o s ( h, f s ) ando t ( h, f t ) respectively, where f s and f t are source and target labeling functions.', 'similarly, the corresponding expected errors are denoted by o s ( h, f s ) and o t ( h, f t ).', 'shorthand notions ofo s, o t, o s and o t have also been used']",4
"['of easyadapt proposed in  #TAUTHOR_TAG.', 'let us']","['of easyadapt proposed in  #TAUTHOR_TAG.', 'let us']","['this section, we give a brief overview of easyadapt proposed in  #TAUTHOR_TAG.', 'let us denote r d as the original space.', 'ea operates in an augmented space denoted byx ⊂ r 3d ( for a single pair of source and target domain ).', 'for k domains, the augmented space blow']","['this section, we give a brief overview of easyadapt proposed in  #TAUTHOR_TAG.', 'let us denote r d as the original space.', 'ea operates in an augmented space denoted byx ⊂ r 3d ( for a single pair of source and target domain ).', 'for k domains, the augmented space blows up to r ( k + 1 ) d.', 'the augmented feature maps φ s, φ t : x →x for source and target domains are defined as, source and target domain features are transformed using these feature maps and the augmented feature space so constructed is passed onto the underlying supervised classifier.', 'one of the most appealing properties of easyadapt is that it is agnostic of the underlying supervised classifier being used to learn in the augmented space.', 'almost any standard supervised learning approach for linear classifiers ( for e. g., svms, perceptrons ) can be used to learn a linear hypothesish ∈ r 3d in the augmented space.', 'as mentioned earlier, this work considers linear hypotheses only and the the proposed techniques can be extended  #TAUTHOR_TAG to non - linear hypotheses.', 'let us denote h = h c, h s, h t, where each of h c, h s, h t is of dimension d and represent the common, sourcespecific and target - specific components ofh, respectively.', 'during prediction on target data, the incoming target feature x is transformed to obtain φ t ( x ) andh is applied on this transformed feature.', 'this is equivalent to applying ( h c + h t ) on x.', 'a good intuitive insight into why this simple algorithm works so well in practice and outperforms most state - of - the - art algorithms is given in  #TAUTHOR_TAG.', 'briefly, it can be thought to be simultaneously training two hypotheses : w s = ( h c + h s ) for source domain and w t = ( h c + g t ) for target domain.', 'the commonality between the domains is represented by h c whereas the source and target domain specific information is captured by h s and h t, respectively.', 'this technique can be easily extended to a multi - domain scenario by making more copies of the original feature space ( ( k + 1 ) copies in case of k domains ).', 'a kernelized version of the algorithm has also been presented in  #TAUTHOR_TAG']",4
"[')  #TAUTHOR_TAG.', '']","['( ea )  #TAUTHOR_TAG.', '']","[')  #TAUTHOR_TAG.', '']","['this work, we propose a semisupervised extension to a well - known supervised domain adaptation approach ( ea )  #TAUTHOR_TAG.', 'our proposed approach ( ea + + ) builds on the notion of augmented space ( introduced in ea ) and harnesses unlabeled data in target domain to ameliorate the transfer of information from source to target.', 'this semisupervised approach to domain adaptation is extremely simple to implement, and can be applied as a pre - processing step to any supervised learner.', 'experimental results on sequential labeling tasks demonstrate the efficacy of the proposed method']",6
"['with respect to the target domain.', 'in this paper, we consider linear hypotheses only.', 'however, the proposed techniques extend to non - linear hypotheses, as mentioned in  #TAUTHOR_TAG.', 'source and target empirical']","['with respect to the target domain.', 'in this paper, we consider linear hypotheses only.', 'however, the proposed techniques extend to non - linear hypotheses, as mentioned in  #TAUTHOR_TAG.', 'source and target empirical']","['with respect to the target domain.', 'in this paper, we consider linear hypotheses only.', 'however, the proposed techniques extend to non - linear hypotheses, as mentioned in  #TAUTHOR_TAG.', 'source and target empirical errors for hypothesis h are denoted']","['x ⊂ r d denote the instance space and y = { −1, + 1 } denote the label space.', 'we have a set of source labeled examples l s ( ∼ d s ( x, y ) ) and a set of target labeled examples', 'we also have target unlabeled data denoted by u t ( ∼ d t ( x ) ), where | u t | = u t.', 'our goal is to learn a hypothesis h : x → y having low expected error with respect to the target domain.', 'in this paper, we consider linear hypotheses only.', 'however, the proposed techniques extend to non - linear hypotheses, as mentioned in  #TAUTHOR_TAG.', 'source and target empirical errors for hypothesis h are denoted by o s ( h, f s ) ando t ( h, f t ) respectively, where f s and f t are source and target labeling functions.', 'similarly, the corresponding expected errors are denoted by o s ( h, f s ) and o t ( h, f t ).', 'shorthand notions ofo s, o t, o s and o t have also been used']",6
"['of easyadapt proposed in  #TAUTHOR_TAG.', 'let us']","['of easyadapt proposed in  #TAUTHOR_TAG.', 'let us']","['this section, we give a brief overview of easyadapt proposed in  #TAUTHOR_TAG.', 'let us denote r d as the original space.', 'ea operates in an augmented space denoted byx ⊂ r 3d ( for a single pair of source and target domain ).', 'for k domains, the augmented space blow']","['this section, we give a brief overview of easyadapt proposed in  #TAUTHOR_TAG.', 'let us denote r d as the original space.', 'ea operates in an augmented space denoted byx ⊂ r 3d ( for a single pair of source and target domain ).', 'for k domains, the augmented space blows up to r ( k + 1 ) d.', 'the augmented feature maps φ s, φ t : x →x for source and target domains are defined as, source and target domain features are transformed using these feature maps and the augmented feature space so constructed is passed onto the underlying supervised classifier.', 'one of the most appealing properties of easyadapt is that it is agnostic of the underlying supervised classifier being used to learn in the augmented space.', 'almost any standard supervised learning approach for linear classifiers ( for e. g., svms, perceptrons ) can be used to learn a linear hypothesish ∈ r 3d in the augmented space.', 'as mentioned earlier, this work considers linear hypotheses only and the the proposed techniques can be extended  #TAUTHOR_TAG to non - linear hypotheses.', 'let us denote h = h c, h s, h t, where each of h c, h s, h t is of dimension d and represent the common, sourcespecific and target - specific components ofh, respectively.', 'during prediction on target data, the incoming target feature x is transformed to obtain φ t ( x ) andh is applied on this transformed feature.', 'this is equivalent to applying ( h c + h t ) on x.', 'a good intuitive insight into why this simple algorithm works so well in practice and outperforms most state - of - the - art algorithms is given in  #TAUTHOR_TAG.', 'briefly, it can be thought to be simultaneously training two hypotheses : w s = ( h c + h s ) for source domain and w t = ( h c + g t ) for target domain.', 'the commonality between the domains is represented by h c whereas the source and target domain specific information is captured by h s and h t, respectively.', 'this technique can be easily extended to a multi - domain scenario by making more copies of the original feature space ( ( k + 1 ) copies in case of k domains ).', 'a kernelized version of the algorithm has also been presented in  #TAUTHOR_TAG']",6
"['sequential labeling tasks in nlp was proposed in  #TAUTHOR_TAG.', '']","['sequential labeling tasks in nlp was proposed in  #TAUTHOR_TAG.', '']","['sequential labeling tasks in nlp was proposed in  #TAUTHOR_TAG.', '']","['domain adaptation approach for sequential labeling tasks in nlp was proposed in  #TAUTHOR_TAG.', 'the proposed approach, termed easyadapt ( ea ), augments the source domain feature space using features from labeled data in target domain.', 'ea is simple, easy to extend and implement as a preprocessing step and most importantly is agnostic of the underlying classifier.', 'however, ea requires labeled data in the target and hence applies to fully supervised ( labeled data in source and target ) domain adaptation settings only.', 'in this paper, we propose a semi - supervised 1 ( labeled data in source, and both labeled and unlabeled data in target ) approach to leverage unlabeled data for easyadapt ( which we call ea + + ) and empirically demonstrate its superior performance over ea as well as few other existing approaches.', 'there exists prior work on supervised domain adaptation ( or multi - task learning ) that can be related to easyadapt.', 'an algorithm for multitask learning using shared parameters was proposed  #AUTHOR_TAG for multi - task regularization where each task parameter was represented as sum of a mean parameter ( that stays same for all tasks ) and its deviation from this mean.', 'svm was used as the base classifier and the algorithm was formulated in the standard svm dual optimization setting.', 'subsequently, this framework  #AUTHOR_TAG was extended  #AUTHOR_TAG to online multidomain setting.', '']",0
"['of easyadapt proposed in  #TAUTHOR_TAG.', 'let us']","['of easyadapt proposed in  #TAUTHOR_TAG.', 'let us']","['this section, we give a brief overview of easyadapt proposed in  #TAUTHOR_TAG.', 'let us denote r d as the original space.', 'ea operates in an augmented space denoted byx ⊂ r 3d ( for a single pair of source and target domain ).', 'for k domains, the augmented space blow']","['this section, we give a brief overview of easyadapt proposed in  #TAUTHOR_TAG.', 'let us denote r d as the original space.', 'ea operates in an augmented space denoted byx ⊂ r 3d ( for a single pair of source and target domain ).', 'for k domains, the augmented space blows up to r ( k + 1 ) d.', 'the augmented feature maps φ s, φ t : x →x for source and target domains are defined as, source and target domain features are transformed using these feature maps and the augmented feature space so constructed is passed onto the underlying supervised classifier.', 'one of the most appealing properties of easyadapt is that it is agnostic of the underlying supervised classifier being used to learn in the augmented space.', 'almost any standard supervised learning approach for linear classifiers ( for e. g., svms, perceptrons ) can be used to learn a linear hypothesish ∈ r 3d in the augmented space.', 'as mentioned earlier, this work considers linear hypotheses only and the the proposed techniques can be extended  #TAUTHOR_TAG to non - linear hypotheses.', 'let us denote h = h c, h s, h t, where each of h c, h s, h t is of dimension d and represent the common, sourcespecific and target - specific components ofh, respectively.', 'during prediction on target data, the incoming target feature x is transformed to obtain φ t ( x ) andh is applied on this transformed feature.', 'this is equivalent to applying ( h c + h t ) on x.', 'a good intuitive insight into why this simple algorithm works so well in practice and outperforms most state - of - the - art algorithms is given in  #TAUTHOR_TAG.', 'briefly, it can be thought to be simultaneously training two hypotheses : w s = ( h c + h s ) for source domain and w t = ( h c + g t ) for target domain.', 'the commonality between the domains is represented by h c whereas the source and target domain specific information is captured by h s and h t, respectively.', 'this technique can be easily extended to a multi - domain scenario by making more copies of the original feature space ( ( k + 1 ) copies in case of k domains ).', 'a kernelized version of the algorithm has also been presented in  #TAUTHOR_TAG']",0
"['of easyadapt proposed in  #TAUTHOR_TAG.', 'let us']","['of easyadapt proposed in  #TAUTHOR_TAG.', 'let us']","['this section, we give a brief overview of easyadapt proposed in  #TAUTHOR_TAG.', 'let us denote r d as the original space.', 'ea operates in an augmented space denoted byx ⊂ r 3d ( for a single pair of source and target domain ).', 'for k domains, the augmented space blow']","['this section, we give a brief overview of easyadapt proposed in  #TAUTHOR_TAG.', 'let us denote r d as the original space.', 'ea operates in an augmented space denoted byx ⊂ r 3d ( for a single pair of source and target domain ).', 'for k domains, the augmented space blows up to r ( k + 1 ) d.', 'the augmented feature maps φ s, φ t : x →x for source and target domains are defined as, source and target domain features are transformed using these feature maps and the augmented feature space so constructed is passed onto the underlying supervised classifier.', 'one of the most appealing properties of easyadapt is that it is agnostic of the underlying supervised classifier being used to learn in the augmented space.', 'almost any standard supervised learning approach for linear classifiers ( for e. g., svms, perceptrons ) can be used to learn a linear hypothesish ∈ r 3d in the augmented space.', 'as mentioned earlier, this work considers linear hypotheses only and the the proposed techniques can be extended  #TAUTHOR_TAG to non - linear hypotheses.', 'let us denote h = h c, h s, h t, where each of h c, h s, h t is of dimension d and represent the common, sourcespecific and target - specific components ofh, respectively.', 'during prediction on target data, the incoming target feature x is transformed to obtain φ t ( x ) andh is applied on this transformed feature.', 'this is equivalent to applying ( h c + h t ) on x.', 'a good intuitive insight into why this simple algorithm works so well in practice and outperforms most state - of - the - art algorithms is given in  #TAUTHOR_TAG.', 'briefly, it can be thought to be simultaneously training two hypotheses : w s = ( h c + h s ) for source domain and w t = ( h c + g t ) for target domain.', 'the commonality between the domains is represented by h c whereas the source and target domain specific information is captured by h s and h t, respectively.', 'this technique can be easily extended to a multi - domain scenario by making more copies of the original feature space ( ( k + 1 ) copies in case of k domains ).', 'a kernelized version of the algorithm has also been presented in  #TAUTHOR_TAG']",0
"['of easyadapt proposed in  #TAUTHOR_TAG.', 'let us']","['of easyadapt proposed in  #TAUTHOR_TAG.', 'let us']","['this section, we give a brief overview of easyadapt proposed in  #TAUTHOR_TAG.', 'let us denote r d as the original space.', 'ea operates in an augmented space denoted byx ⊂ r 3d ( for a single pair of source and target domain ).', 'for k domains, the augmented space blow']","['this section, we give a brief overview of easyadapt proposed in  #TAUTHOR_TAG.', 'let us denote r d as the original space.', 'ea operates in an augmented space denoted byx ⊂ r 3d ( for a single pair of source and target domain ).', 'for k domains, the augmented space blows up to r ( k + 1 ) d.', 'the augmented feature maps φ s, φ t : x →x for source and target domains are defined as, source and target domain features are transformed using these feature maps and the augmented feature space so constructed is passed onto the underlying supervised classifier.', 'one of the most appealing properties of easyadapt is that it is agnostic of the underlying supervised classifier being used to learn in the augmented space.', 'almost any standard supervised learning approach for linear classifiers ( for e. g., svms, perceptrons ) can be used to learn a linear hypothesish ∈ r 3d in the augmented space.', 'as mentioned earlier, this work considers linear hypotheses only and the the proposed techniques can be extended  #TAUTHOR_TAG to non - linear hypotheses.', 'let us denote h = h c, h s, h t, where each of h c, h s, h t is of dimension d and represent the common, sourcespecific and target - specific components ofh, respectively.', 'during prediction on target data, the incoming target feature x is transformed to obtain φ t ( x ) andh is applied on this transformed feature.', 'this is equivalent to applying ( h c + h t ) on x.', 'a good intuitive insight into why this simple algorithm works so well in practice and outperforms most state - of - the - art algorithms is given in  #TAUTHOR_TAG.', 'briefly, it can be thought to be simultaneously training two hypotheses : w s = ( h c + h s ) for source domain and w t = ( h c + g t ) for target domain.', 'the commonality between the domains is represented by h c whereas the source and target domain specific information is captured by h s and h t, respectively.', 'this technique can be easily extended to a multi - domain scenario by making more copies of the original feature space ( ( k + 1 ) copies in case of k domains ).', 'a kernelized version of the algorithm has also been presented in  #TAUTHOR_TAG']",0
"[' #TAUTHOR_TAG.', 'however, despite their simplicity and empirical success, it is not theoretically apparent why']","[' #TAUTHOR_TAG.', 'however, despite their simplicity and empirical success, it is not theoretically apparent why']","['domain adaptation  #TAUTHOR_TAG.', 'however, despite their simplicity and empirical success, it is not theoretically apparent why these algorithms perform so']","['both ea and ea + +, we use features from source and target space to construct an augmented feature space.', 'in other words, we are sharing features across source and target labeled data.', 'we term such algorithms as feature sharing algorithms.', 'feature sharing algorithms are effective for domain adaptation because they are simple, easy to implement as a preprocessing step and outperform many existing state - of - the - art techniques ( shown previously for domain adaptation  #TAUTHOR_TAG.', 'however, despite their simplicity and empirical success, it is not theoretically apparent why these algorithms perform so well.', 'prior work provides some intuitions but is mostly empirical and a formal theoretical analysis to justify fsas ( for domain adaptation ) is clearly missing.', 'prior work  #AUTHOR_TAG analyzes the multi - task regularization approach  #AUTHOR_TAG ( which is related to ea ) but they consider a cumulative loss in multi - task ( or multi - domain ) setting.', 'this does not apply to domain adaptation setting where we are mainly interested in loss in the target domain only.', 'theoretically analyzing the superior performance of ea and ea + + and providing generalization guarantees is an interesting line of future work.', 'one approach would be to model the feature sharing approach in terms of coregularization ; an idea that originated in the context of multiview learning and for which some theoretical analysis has already been done  #AUTHOR_TAG.', 'additionally, the aforementioned techniques, namely, sourceonly, targetonly, all have been empirically compared to ea and ea + +.', 'it would be interesting to formally frame these approaches and see whether their empirical performance can be justified within a theoretical framework']",0
['experimental setup used in  #TAUTHOR_TAG and'],['experimental setup used in  #TAUTHOR_TAG and'],['follow the same experimental setup used in  #TAUTHOR_TAG and'],"['follow the same experimental setup used in  #TAUTHOR_TAG and perform two sequence labelling tasks ( a ) named - entity - recognition ( ner ), and ( b ) part - of - speech - tagging ( pos ) on the following datasets :', 'pubmed - pos : introduced by  #AUTHOR_TAG, this dataset consists of two domains.', 'task is to perform part - of - speech tagging on unlabeled pubmed abstracts with a classifier trained on labeled wsj and pubmed data.', 'treebank - brown.', 'treebank - chunk data consists of the following domains : the standard wsj domain ( the same data as for conll 2000 ), the atis switchboard domain and the brown corpus.', 'the brown corpus consists of data combined from six subdomains.', 'treebankchunk is a shallow parsing task based on the data from the penn treebank.', 'treebankbrown is identical to the treebank - chunk task, however, in treebank - brown we consider all of the brown corpus to be a single domain.', 'table 1 presents a summary of the datasets used.', 'all datasets use roughly the same feature set which are lexical information ( words, stems, capitalization, prefixes and suffixes ), membership on gazetteers, etc.', 'we use an averaged perceptron classifier from the megam framework ( implementation due to ( daume iii, 2004 ) ) for all the aforementioned tasks.', 'the training sample size varies from 1k to 16k.', 'in all cases, the amount of unlabeled target data was equal to the total amount of labeled source and target data']",3
"['', 'we also note that ea performs poorly for some cases, as was shown  #TAUTHOR_TAG earlier']","['', 'we also note that ea performs poorly for some cases, as was shown  #TAUTHOR_TAG earlier']","['', 'we also note that ea performs poorly for some cases, as was shown  #TAUTHOR_TAG earlier']","['', 'for ea + +, the x - value plotted denotes the amount of unlabeled target data used ( in addition to an equal amount of source + target labeled data, as in all or ea ).', 'we plot this number for ea + +, just to compare its improvement over ea when using an additional ( and equal ) amount of unlabeled target data.', 'this accounts for the different x values plotted for the different curves.', 'in all cases, the y - axis denotes the error rate.', 'as can be seen in figure 3 ( a ), ea + + performs better than the normal ea ( which uses labeled data only ).', 'the labeled and unlabeled case start together but with increase in number of samples their gap increases with the unlabeled case resulting in much lower error as compared to the labeled case.', 'similar trends were observed in other data sets as can be seen in figure 3 ( b ).', 'we also note that ea performs poorly for some cases, as was shown  #TAUTHOR_TAG earlier']",3
['experimental setup used in  #TAUTHOR_TAG and'],['experimental setup used in  #TAUTHOR_TAG and'],['follow the same experimental setup used in  #TAUTHOR_TAG and'],"['follow the same experimental setup used in  #TAUTHOR_TAG and perform two sequence labelling tasks ( a ) named - entity - recognition ( ner ), and ( b ) part - of - speech - tagging ( pos ) on the following datasets :', 'pubmed - pos : introduced by  #AUTHOR_TAG, this dataset consists of two domains.', 'task is to perform part - of - speech tagging on unlabeled pubmed abstracts with a classifier trained on labeled wsj and pubmed data.', 'treebank - brown.', 'treebank - chunk data consists of the following domains : the standard wsj domain ( the same data as for conll 2000 ), the atis switchboard domain and the brown corpus.', 'the brown corpus consists of data combined from six subdomains.', 'treebankchunk is a shallow parsing task based on the data from the penn treebank.', 'treebankbrown is identical to the treebank - chunk task, however, in treebank - brown we consider all of the brown corpus to be a single domain.', 'table 1 presents a summary of the datasets used.', 'all datasets use roughly the same feature set which are lexical information ( words, stems, capitalization, prefixes and suffixes ), membership on gazetteers, etc.', 'we use an averaged perceptron classifier from the megam framework ( implementation due to ( daume iii, 2004 ) ) for all the aforementioned tasks.', 'the training sample size varies from 1k to 16k.', 'in all cases, the amount of unlabeled target data was equal to the total amount of labeled source and target data']",5
"['shared tasks  #AUTHOR_TAG a ;  #AUTHOR_TAG.', ' #TAUTHOR_TAG showed that the']","['shared tasks  #AUTHOR_TAG a ;  #AUTHOR_TAG.', ' #TAUTHOR_TAG showed that the']","['parsing shared tasks  #AUTHOR_TAG a ;  #AUTHOR_TAG.', ' #TAUTHOR_TAG showed that the performance']","['parsers can recover much of the predicate - argument structure of a sentence, while being relatively efficient to train and extremely fast at parsing.', 'dependency parsers have been gaining in popularity in recent times due to the availability of large dependency treebanks for several languages and parsing shared tasks  #AUTHOR_TAG a ;  #AUTHOR_TAG.', ' #TAUTHOR_TAG showed that the performance of malt  #AUTHOR_TAG b ) on the free word order language, hindi, is improved by using lexical categories from combinatory categorial grammar ( ccg )  #AUTHOR_TAG.', 'in this paper, we extend  #TAUTHOR_TAG and show that ccg categories are useful even in the case of english, a typologically different language, where parsing accuracy of dependency parsers is already extremely high.', 'in addition, we also demonstrate the utility of ccg categories to mst ( mc  #AUTHOR_TAG for both languages.', 'ccg lexical categories contain subcategorization information regarding the dependencies of predicates, including longdistance dependencies.', 'we show that providing this subcategorization information in the form of ccg categories can help both malt and mst on precisely those dependencies for which they are known to have weak rates of recovery.', 'the result is particularly interesting for malt, the fast greedy parser, as the improvement in malt comes without significantly compromising its speed, so that it can be practically applied in web scale parsing.', 'our results apply both to english, a fixed word order and morphologically simple language, and to hindi, a free word order and morphologically rich language, indicating that ccg categories from a supertagger are an easy and robust way of introducing lexicalized subcategorization information into dependency parsers']",0
"['##ank test set.', 'conversely,  #TAUTHOR_TAG showed that a hindi']","['in labelled f - score of the ccgbank test set.', 'conversely,  #TAUTHOR_TAG showed that a hindi']","['in labelled f - score of the ccgbank test set.', 'conversely,  #TAUTHOR_TAG showed that a hindi dependency parser ( malt ) could be improved by using ccg categories.', 'using an algorithm similar to  #AUTHOR_TAG and  #AUTHOR_TAG,  #TAUTHOR_TAG from a hindi dependency treebank and built']","['##s using different grammar formalisms have different strengths and weaknesses, and prior work has shown that information from one formalism can improve the performance of a parser in another formalism.', ' #AUTHOR_TAG achieved a 1. 4 % improvement in accuracy over a state - of - the - art hpsg parser by using dependencies from a dependency parser for constraining wide - coverage rules in the hpsg parser.', ' #AUTHOR_TAG incorporated higher - order dependency features into a cube decoding phrasestructure parser and obtained significant gains on dependency recovery for both in - domain and out - of - domain test sets.', ' #AUTHOR_TAG improved a ccg parser using dependency features.', 'they extracted n - best parses from a ccg parser and provided dependency pierre vinken will join the board as a nonexecutive director nov. 29', 'figure 1 : a ccg derivation and the stanford scheme dependencies for an example sentence.', 'features from a dependency parser to a re - ranker with an improvement of 0. 35 % in labelled f - score of the ccgbank test set.', 'conversely,  #TAUTHOR_TAG showed that a hindi dependency parser ( malt ) could be improved by using ccg categories.', 'using an algorithm similar to  #AUTHOR_TAG and  #AUTHOR_TAG,  #TAUTHOR_TAG from a hindi dependency treebank and built a supertagger.', ' #TAUTHOR_TAG ccg categories from a supertagger as features to malt and obtained overall improvements of 0. 3 % and 0. 4 % in unlabelled and labelled attachment scores respectively.', 'figure 1 shows a ccg derivation with ccg lexical categories for each word and stanford scheme dependencies  #AUTHOR_TAG for an example english sentence.', '( details of ccg and dependency parsing are given by  #AUTHOR_TAG and kubler et al. ( 2009']",0
"['##ank test set.', 'conversely,  #TAUTHOR_TAG showed that a hindi']","['in labelled f - score of the ccgbank test set.', 'conversely,  #TAUTHOR_TAG showed that a hindi']","['in labelled f - score of the ccgbank test set.', 'conversely,  #TAUTHOR_TAG showed that a hindi dependency parser ( malt ) could be improved by using ccg categories.', 'using an algorithm similar to  #AUTHOR_TAG and  #AUTHOR_TAG,  #TAUTHOR_TAG from a hindi dependency treebank and built']","['##s using different grammar formalisms have different strengths and weaknesses, and prior work has shown that information from one formalism can improve the performance of a parser in another formalism.', ' #AUTHOR_TAG achieved a 1. 4 % improvement in accuracy over a state - of - the - art hpsg parser by using dependencies from a dependency parser for constraining wide - coverage rules in the hpsg parser.', ' #AUTHOR_TAG incorporated higher - order dependency features into a cube decoding phrasestructure parser and obtained significant gains on dependency recovery for both in - domain and out - of - domain test sets.', ' #AUTHOR_TAG improved a ccg parser using dependency features.', 'they extracted n - best parses from a ccg parser and provided dependency pierre vinken will join the board as a nonexecutive director nov. 29', 'figure 1 : a ccg derivation and the stanford scheme dependencies for an example sentence.', 'features from a dependency parser to a re - ranker with an improvement of 0. 35 % in labelled f - score of the ccgbank test set.', 'conversely,  #TAUTHOR_TAG showed that a hindi dependency parser ( malt ) could be improved by using ccg categories.', 'using an algorithm similar to  #AUTHOR_TAG and  #AUTHOR_TAG,  #TAUTHOR_TAG from a hindi dependency treebank and built a supertagger.', ' #TAUTHOR_TAG ccg categories from a supertagger as features to malt and obtained overall improvements of 0. 3 % and 0. 4 % in unlabelled and labelled attachment scores respectively.', 'figure 1 shows a ccg derivation with ccg lexical categories for each word and stanford scheme dependencies  #AUTHOR_TAG for an example english sentence.', '( details of ccg and dependency parsing are given by  #AUTHOR_TAG and kubler et al. ( 2009']",0
"['##ank test set.', 'conversely,  #TAUTHOR_TAG showed that a hindi']","['in labelled f - score of the ccgbank test set.', 'conversely,  #TAUTHOR_TAG showed that a hindi']","['in labelled f - score of the ccgbank test set.', 'conversely,  #TAUTHOR_TAG showed that a hindi dependency parser ( malt ) could be improved by using ccg categories.', 'using an algorithm similar to  #AUTHOR_TAG and  #AUTHOR_TAG,  #TAUTHOR_TAG from a hindi dependency treebank and built']","['##s using different grammar formalisms have different strengths and weaknesses, and prior work has shown that information from one formalism can improve the performance of a parser in another formalism.', ' #AUTHOR_TAG achieved a 1. 4 % improvement in accuracy over a state - of - the - art hpsg parser by using dependencies from a dependency parser for constraining wide - coverage rules in the hpsg parser.', ' #AUTHOR_TAG incorporated higher - order dependency features into a cube decoding phrasestructure parser and obtained significant gains on dependency recovery for both in - domain and out - of - domain test sets.', ' #AUTHOR_TAG improved a ccg parser using dependency features.', 'they extracted n - best parses from a ccg parser and provided dependency pierre vinken will join the board as a nonexecutive director nov. 29', 'figure 1 : a ccg derivation and the stanford scheme dependencies for an example sentence.', 'features from a dependency parser to a re - ranker with an improvement of 0. 35 % in labelled f - score of the ccgbank test set.', 'conversely,  #TAUTHOR_TAG showed that a hindi dependency parser ( malt ) could be improved by using ccg categories.', 'using an algorithm similar to  #AUTHOR_TAG and  #AUTHOR_TAG,  #TAUTHOR_TAG from a hindi dependency treebank and built a supertagger.', ' #TAUTHOR_TAG ccg categories from a supertagger as features to malt and obtained overall improvements of 0. 3 % and 0. 4 % in unlabelled and labelled attachment scores respectively.', 'figure 1 shows a ccg derivation with ccg lexical categories for each word and stanford scheme dependencies  #AUTHOR_TAG for an example english sentence.', '( details of ccg and dependency parsing are given by  #AUTHOR_TAG and kubler et al. ( 2009']",0
"[""for english, and  #TAUTHOR_TAG's supertag""]","[""used  #AUTHOR_TAG's supertagger for english, and  #TAUTHOR_TAG's supertagger for hindi."", 'both are maximum entropy based ccg supertaggers.', ' #AUTHOR_TAG supertagger uses different features like word, partof - speech, and contextual and complex bi - gram features to']","[""for english, and  #TAUTHOR_TAG's supertag""]","[""used  #AUTHOR_TAG's supertagger for english, and  #TAUTHOR_TAG's supertagger for hindi."", 'both are maximum entropy based ccg supertaggers.', ' #AUTHOR_TAG supertagger uses different features like word, partof - speech, and contextual and complex bi - gram features to obtain a 1 - best accuracy of 91. 5 % on the development set.', 'in addition to the above mentioned features,  #TAUTHOR_TAG employed morphological features useful for hindi.', 'the 1 - best accuracy of hindi supertagger for finegrained and coarse - grained lexicon is 82. 92 % and 84. 40 % respectively']",0
"['.', 'in contrast, for malt,  #TAUTHOR_TAG had shown that coarse - grained supertags gave larger']","['respectively.', 'in contrast, for malt,  #TAUTHOR_TAG had shown that coarse - grained supertags gave larger']","['', 'in contrast, for malt,  #TAUTHOR_TAG had shown that coarse - grained supertags gave larger improvements of 0. 3']","['', 'in contrast, for malt,  #TAUTHOR_TAG had shown that coarse - grained supertags gave larger improvements of 0. 3 % and 0. 4 % in uas and las respectively.', 'due to better handling of error propagation in mst, the richer information in fine - grained categories may have surpassed the slightly lower supertagger performance, compared to coarse - grained categories']",0
"['shared tasks  #AUTHOR_TAG a ;  #AUTHOR_TAG.', ' #TAUTHOR_TAG showed that the']","['shared tasks  #AUTHOR_TAG a ;  #AUTHOR_TAG.', ' #TAUTHOR_TAG showed that the']","['parsing shared tasks  #AUTHOR_TAG a ;  #AUTHOR_TAG.', ' #TAUTHOR_TAG showed that the performance']","['parsers can recover much of the predicate - argument structure of a sentence, while being relatively efficient to train and extremely fast at parsing.', 'dependency parsers have been gaining in popularity in recent times due to the availability of large dependency treebanks for several languages and parsing shared tasks  #AUTHOR_TAG a ;  #AUTHOR_TAG.', ' #TAUTHOR_TAG showed that the performance of malt  #AUTHOR_TAG b ) on the free word order language, hindi, is improved by using lexical categories from combinatory categorial grammar ( ccg )  #AUTHOR_TAG.', 'in this paper, we extend  #TAUTHOR_TAG and show that ccg categories are useful even in the case of english, a typologically different language, where parsing accuracy of dependency parsers is already extremely high.', 'in addition, we also demonstrate the utility of ccg categories to mst ( mc  #AUTHOR_TAG for both languages.', 'ccg lexical categories contain subcategorization information regarding the dependencies of predicates, including longdistance dependencies.', 'we show that providing this subcategorization information in the form of ccg categories can help both malt and mst on precisely those dependencies for which they are known to have weak rates of recovery.', 'the result is particularly interesting for malt, the fast greedy parser, as the improvement in malt comes without significantly compromising its speed, so that it can be practically applied in web scale parsing.', 'our results apply both to english, a fixed word order and morphologically simple language, and to hindi, a free word order and morphologically rich language, indicating that ccg categories from a supertagger are an easy and robust way of introducing lexicalized subcategorization information into dependency parsers']",6
"['order and morphologically richer language ), extending the result of  #TAUTHOR_TAG.', 'the result is particularly interesting in']","['order and morphologically richer language ), extending the result of  #TAUTHOR_TAG.', 'the result is particularly interesting in']","['english ( a fixed word order language ) and hindi ( free word order and morphologically richer language ), extending the result of  #TAUTHOR_TAG.', 'the result is particularly interesting in']","['have shown that informative ccg categories, which contain both local subcategorization information and capture long distance dependencies elegantly, improve the performance of two dependency parsers, malt and mst, by helping in recovering long distance relations for malt and local verbal arguments for mst.', 'this is true both in the case of english ( a fixed word order language ) and hindi ( free word order and morphologically richer language ), extending the result of  #TAUTHOR_TAG.', 'the result is particularly interesting in the case of malt which cannot directly use valency information, which ccg categories provide indirectly.', 'it leads to an improvement in performance without significantly compromising speed and hence promises to be applicable to web scale processing']",6
"['testing sentences.', 'we used the english  #AUTHOR_TAG and  #TAUTHOR_TAG ( ambati']","['testing sentences.', 'we used the english  #AUTHOR_TAG and  #TAUTHOR_TAG ( ambati']","['##8 testing sentences.', 'we used the english  #AUTHOR_TAG and  #TAUTHOR_TAG ( ambati et al., 1 http : / / w3. msi. vxu. se / nivre / research / penn2malt. html 2013 ) for our experiments.', 'for hindi we used two lexicons : a fine - grained one ( with morphological information ) and a coarse - grained one ( without morphological information )']","['', 'we used the english  #AUTHOR_TAG and  #TAUTHOR_TAG ( ambati et al., 1 http : / / w3. msi. vxu. se / nivre / research / penn2malt. html 2013 ) for our experiments.', 'for hindi we used two lexicons : a fine - grained one ( with morphological information ) and a coarse - grained one ( without morphological information )']",5
"[""for english, and  #TAUTHOR_TAG's supertag""]","[""used  #AUTHOR_TAG's supertagger for english, and  #TAUTHOR_TAG's supertagger for hindi."", 'both are maximum entropy based ccg supertaggers.', ' #AUTHOR_TAG supertagger uses different features like word, partof - speech, and contextual and complex bi - gram features to']","[""for english, and  #TAUTHOR_TAG's supertag""]","[""used  #AUTHOR_TAG's supertagger for english, and  #TAUTHOR_TAG's supertagger for hindi."", 'both are maximum entropy based ccg supertaggers.', ' #AUTHOR_TAG supertagger uses different features like word, partof - speech, and contextual and complex bi - gram features to obtain a 1 - best accuracy of 91. 5 % on the development set.', 'in addition to the above mentioned features,  #TAUTHOR_TAG employed morphological features useful for hindi.', 'the 1 - best accuracy of hindi supertagger for finegrained and coarse - grained lexicon is 82. 92 % and 84. 40 % respectively']",5
"['our experiments using automatic features  #TAUTHOR_TAG.', '( pos, chunk and morphological information ) extracted using a hindi shallow parser 2']","['our experiments using automatic features  #TAUTHOR_TAG.', '( pos, chunk and morphological information ) extracted using a hindi shallow parser 2']","['our experiments using automatic features  #TAUTHOR_TAG.', '( pos, chunk and morphological information ) extracted using a hindi shallow parser 2']","['has been a significant amount of work on parsing english and hindi using the malt and mst parsers in the recent past  #AUTHOR_TAG a ;  #AUTHOR_TAG.', 'we first run these parsers with previous best settings ( mc  #AUTHOR_TAG and treat them as our baseline.', 'in the case of english, malt uses arc - standard and stack - projective parsing algorithms for conll and stanford schemes respectively and liblin - ear learner  #AUTHOR_TAG for both the schemes.', 'mst uses 1st - order features, and a projective parsing algorithm with 5 - best mira training for both the schemes.', 'for hindi, malt uses the arc - standard parsing algorithm with a liblin - ear learner.', 'mst uses 2nd - order features, nonprojective algorithm with 5 - best mira training.', 'for english, we assigned pos - tags using a perceptron tagger  #AUTHOR_TAG.', 'for hindi, we also did all our experiments using automatic features  #TAUTHOR_TAG.', '( pos, chunk and morphological information ) extracted using a hindi shallow parser 2']",5
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG, we used supertags which occurred']","[' #TAUTHOR_TAG, we used supertags which occurred at least k times in the training data, and backed off to coarse pos - tags otherwise.', 'for english k = 1, i. e., when we use ccg categories for all words, gave the best results.', 'k = 15 gave the best results for hindi due to sparsity issues, as the data for hindi is small.', ""we provided a supertag as an atomic symbol similar to a pos tag and didn't split it into a list of argument and result categories."", 'we explored both stanford and conll schemes for english and fine and coarsegrained ccg categories for hindi.', 'all feature and parser tuning was done on the development data.', 'we assigned automatic pos - tags and supertags to the training data']",5
"['- score over the baseline.', ' #TAUTHOR_TAG showed that for hindi, providing ccg categories as features improved malt in better']","['obj labels respectively in f - score over the baseline.', ' #TAUTHOR_TAG showed that for hindi, providing ccg categories as features improved malt in better']","['- score over the baseline.', ' #TAUTHOR_TAG showed that for hindi, providing ccg categories as features improved malt in better handling of long distance dependencies.', 'the percentage of dependencies in']","['the case of hindi, for mst, providing ccg categories gave an increment of 0. 5 %, 0. 4 % and 0. 3 % for root, subj and obj labels respectively in f - score over the baseline.', ' #TAUTHOR_TAG showed that for hindi, providing ccg categories as features improved malt in better handling of long distance dependencies.', 'the percentage of dependencies in the 1−5, 6−10 and > 10 distance ranges are 82. 2 %, 8. 6 % and 9. 2 % respectively out of the total of around 40, 000 dependencies.', 'similar to english, there was very slight improvement for short distance dependencies ( 1−5 ).', 'but for longer distances, 6−10, and > 10, there was significant improvement of 1. 3 % and 1. 3 % respectively for mst.', ' #TAUTHOR_TAG reported similar improvements for malt as well']",5
"['.', 'in contrast, for malt,  #TAUTHOR_TAG had shown that coarse - grained supertags gave larger']","['respectively.', 'in contrast, for malt,  #TAUTHOR_TAG had shown that coarse - grained supertags gave larger']","['', 'in contrast, for malt,  #TAUTHOR_TAG had shown that coarse - grained supertags gave larger improvements of 0. 3']","['', 'in contrast, for malt,  #TAUTHOR_TAG had shown that coarse - grained supertags gave larger improvements of 0. 3 % and 0. 4 % in uas and las respectively.', 'due to better handling of error propagation in mst, the richer information in fine - grained categories may have surpassed the slightly lower supertagger performance, compared to coarse - grained categories']",4
"['- score over the baseline.', ' #TAUTHOR_TAG showed that for hindi, providing ccg categories as features improved malt in better']","['obj labels respectively in f - score over the baseline.', ' #TAUTHOR_TAG showed that for hindi, providing ccg categories as features improved malt in better']","['- score over the baseline.', ' #TAUTHOR_TAG showed that for hindi, providing ccg categories as features improved malt in better handling of long distance dependencies.', 'the percentage of dependencies in']","['the case of hindi, for mst, providing ccg categories gave an increment of 0. 5 %, 0. 4 % and 0. 3 % for root, subj and obj labels respectively in f - score over the baseline.', ' #TAUTHOR_TAG showed that for hindi, providing ccg categories as features improved malt in better handling of long distance dependencies.', 'the percentage of dependencies in the 1−5, 6−10 and > 10 distance ranges are 82. 2 %, 8. 6 % and 9. 2 % respectively out of the total of around 40, 000 dependencies.', 'similar to english, there was very slight improvement for short distance dependencies ( 1−5 ).', 'but for longer distances, 6−10, and > 10, there was significant improvement of 1. 3 % and 1. 3 % respectively for mst.', ' #TAUTHOR_TAG reported similar improvements for malt as well']",3
['b ) ;  #AUTHOR_TAG c ) ;  #TAUTHOR_TAG'],['b ) ;  #AUTHOR_TAG c ) ;  #TAUTHOR_TAG'],['a ) ;  #AUTHOR_TAG b ) ;  #AUTHOR_TAG c ) ;  #TAUTHOR_TAG'],"['', 'previous work addressing the problem can be roughly classified into three categories : ( 1 ) learning word embeddings from large collections of text using variants of neural networks (  #AUTHOR_TAG a ) ;  #AUTHOR_TAG b ) ;  #AUTHOR_TAG c ) ;  #TAUTHOR_TAG or global matrix factorization (  #AUTHOR_TAG ;  #AUTHOR_TAG ) ; ( 2 ) extracting knowledge from existing semantic networks, such as wordnet (  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ) and conceptnet (  #AUTHOR_TAG ) ; ( 3 ) combining the above two models by various ways (  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ; summers -  #AUTHOR_TAG ).', 'the empirical evidence shows that the word representations learned from neural network models do an especially good job in capturing not only attributional similarities between words but also similarities between pairs of words (  #AUTHOR_TAG c ) ).', ' #AUTHOR_TAG generalize the skip - gram model with negative sampling to include arbitrary word contexts and present the dependency - based word embeddings, which are learned from syntactic contexts derived from dependency parse - trees.', '']",0
"['broad topical similarities, while the dependency - based word embeddings can capture more functional similarities  #TAUTHOR_TAG']","['broad topical similarities, while the dependency - based word embeddings can capture more functional similarities  #TAUTHOR_TAG']","['broad topical similarities, while the dependency - based word embeddings can capture more functional similarities  #TAUTHOR_TAG']","['', 'however, a context window with a smaller size k may miss some important contexts while including some accidental ones.', 'recently, levy et al. propose the dependency - based word embeddings ( dep ), which generalize the skip - gram model with negative sampling, and move from linear bag - of - words contexts to syntactic contexts that are derived from automatically produced dependency parse - trees.', 'embeddings produced from different kinds of contexts can induce different word similarities.', 'the original skip - gram embeddings can yield broad topical similarities, while the dependency - based word embeddings can capture more functional similarities  #TAUTHOR_TAG']",0
"['and 15 tokens in length.', 'since we will compare our results mainly to  #TAUTHOR_TAG, we will only employ the gold and silver data.', 'name x3 "" tom ""', '']","['and 15 tokens in length.', 'since we will compare our results mainly to  #TAUTHOR_TAG, we will only employ the gold and silver data.', 'name x3 "" tom ""', '']","['and 15 tokens in length.', 'since we will compare our results mainly to  #TAUTHOR_TAG, we will only employ the gold and silver data.', 'name x3 "" tom ""', 'figure 1 : drs in box format ( a ), gold clause representation ( b )']","['##s are formal meaning representations based on discourse representation theory  #AUTHOR_TAG.', 'we use the version of drt as provided in the parallel meaning bank ( pmb, ), a semantically annotated parallel corpus, with texts in english, italian, german and dutch.', 'drss are rich meaning representations containing quantification, negation, reference resolution, comparison operators, discourse relations, concepts based on wordnet, and semantic roles based on verbnet.', 'all experiments are performed using the data of the pmb.', 'in our experiments, we only use the english texts and corresponding drss.', 'we use pmb release 2. 2. 0, which contains gold standard ( fully manually annotated ) data of which we use 4, 597 as train, 682 as dev and 650 as test instances.', 'it also contains 67, 965 silver ( partially manually annotated ) and 120, 662 bronze ( no manual annotations ) instances.', 'most sentences are between 5 and 15 tokens in length.', 'since we will compare our results mainly to  #TAUTHOR_TAG, we will only employ the gold and silver data.', 'name x3 "" tom ""', 'figure 1 : drs in box format ( a ), gold clause representation ( b ) and example system output ( c ) for i am not working for tom, with precision of 5 / 8 and recall of 5 / 9, resulting in an f - score of 58. 8']",5
"['as  #TAUTHOR_TAG, who represent the source']","['as  #TAUTHOR_TAG, who represent the source']","['as  #TAUTHOR_TAG, who represent the source sentence as a sequence of characters, with a special character indicating uppercase characters.', 'the target drs is also represented as a sequence of characters, with the exception of drs operators, thematic roles']","['represent the source and target data in the same way as  #TAUTHOR_TAG, who represent the source sentence as a sequence of characters, with a special character indicating uppercase characters.', 'the target drs is also represented as a sequence of characters, with the exception of drs operators, thematic roles and drs variables, which are represented as super characters  #AUTHOR_TAG b ), i. e. individual tokens.', 'since the variable names itself are meaningless, the drs variables are rewritten to a more general representation, using the de bruijn index ( de  #AUTHOR_TAG.', 'in a post - processing step, the original clause structured is restored.', '1 to include morphological and syntactic information, we apply a lemmatizer, pos - tagger and dependency parser using stanford corenlp  #AUTHOR_TAG, similar to  #AUTHOR_TAG for machine translation.', 'the lemmas and pos - tags are added as a token after each word.', 'for the dependency parse, we add the incoming arc for each word.', 'we also apply the easyccg parser of  #AUTHOR_TAG, using the supertags.', '2 finally, we exploit semantic information by using semantic tags  #AUTHOR_TAG.', 'semantic tags are language - neutral semantic categories, which get assigned to a word in a similar fashion as part - of - speech tags.', 'semantic tags are able to express important semantic distinctions, such as negation, modals and types of quantification.', 'we train a semantic tagger with the tnt tagger  #AUTHOR_TAG on the gold and silver standard data in the pmb release.', 'examples of the input to the model for each source of information are shown in table 1.', 'there are two ways to add the linguistic information ; ( 1 ) merging all the information ( i. e., input text and linguistic information ) in a single encoder, or ( 2 ) using multiple encoders ( i. e., encoding separately the input text and the linguistic information ).', 'multi - source encoders were initially introduced for multilingual translation  #AUTHOR_TAG libovicky and  #AUTHOR_TAG, but recently were used to introduce syntactic information to the model  #AUTHOR_TAG.', 'table 2 shows examples of how the input is structured for using one or more encoders.', 'experiments showed that using more than two encoders drastically decreased performance.', 'therefore, we merge all the linguistic information in a single encoder ( see last row of table 2 )']",5
"['by  #TAUTHOR_TAG.', 'however, their model was trained with opennmt  #AUTHOR_TAG, which']","['by  #TAUTHOR_TAG.', 'however, their model was trained with opennmt  #AUTHOR_TAG, which']","['by  #TAUTHOR_TAG.', 'however, their model was trained with opennmt  #AUTHOR_TAG, which does not support multiple encoders.', 'therefore, we switch to']",[' #TAUTHOR_TAG'],5
"['1.', 'the produced drss go through a strict syntactic and semantic validation process, as described in  #TAUTHOR_TAG.', 'if']","['1.', 'the produced drss go through a strict syntactic and semantic validation process, as described in  #TAUTHOR_TAG.', 'if']","['.', 'an example of the matching procedure is shown in figure 1.', 'the produced drss go through a strict syntactic and semantic validation process, as described in  #TAUTHOR_TAG.', 'if a produced drs is invalid,']","['drss are compared with the gold standard representations by using counter  #AUTHOR_TAG.', 'this is a tool that calculates micro precision, recall and f - score over matching clauses, similar to the smatch  #AUTHOR_TAG evaluation tool for amr parsing.', 'all clauses have the same weight in matching, except for ref clauses, which are ignored.', 'an example of the matching procedure is shown in figure 1.', 'the produced drss go through a strict syntactic and semantic validation process, as described in  #TAUTHOR_TAG.', 'if a produced drs is invalid, it is replaced by a dummy drs, which gets an f - score of 0. 0.', 'we check whether two systems differ significantly by performing approximate randomization  #AUTHOR_TAG, with α = 0. 05, r = 1000 and f ( model 1 ) > f ( model 2 ) as test statistic for each drs pair']",5
['##s 4  #TAUTHOR_TAG and'],['parsers 4  #TAUTHOR_TAG and'],"['previous parsers 4  #TAUTHOR_TAG and two baseline systems, spar']","['', 'if we also employ silver data, we again observe that the multi - encoder setup is preferable over a single encoder, for both isolating and stacking the linguistic features.', 'on isolation, the results are similar to only using gold data, with the exception of the semantic tags, which even hurt the performance now.', 'interestingly, when stacking the linguistic features, there is no improvement over only using the lemma of the source words.', 'we now compare our best models to previous parsers 4  #TAUTHOR_TAG and two baseline systems, spar and sim - spar.', 'as previously indicated,  #TAUTHOR_TAG used a similar sequence - to - sequence model as our current approach, but implemented in opennmt and without the linguistic features.', 'boxer  #AUTHOR_TAG ( bos,, 2015 ) is a drs parser that uses a statistical ccg parser for syntactic analysis and a compositional semantics based on λ - calculus, followed by pronoun and presupposition resolution.', '']",5
['##s 4  #TAUTHOR_TAG and'],['parsers 4  #TAUTHOR_TAG and'],"['previous parsers 4  #TAUTHOR_TAG and two baseline systems, spar']","['', 'if we also employ silver data, we again observe that the multi - encoder setup is preferable over a single encoder, for both isolating and stacking the linguistic features.', 'on isolation, the results are similar to only using gold data, with the exception of the semantic tags, which even hurt the performance now.', 'interestingly, when stacking the linguistic features, there is no improvement over only using the lemma of the source words.', 'we now compare our best models to previous parsers 4  #TAUTHOR_TAG and two baseline systems, spar and sim - spar.', 'as previously indicated,  #TAUTHOR_TAG used a similar sequence - to - sequence model as our current approach, but implemented in opennmt and without the linguistic features.', 'boxer  #AUTHOR_TAG ( bos,, 2015 ) is a drs parser that uses a statistical ccg parser for syntactic analysis and a compositional semantics based on λ - calculus, followed by pronoun and presupposition resolution.', '']",5
['##s 4  #TAUTHOR_TAG and'],['parsers 4  #TAUTHOR_TAG and'],"['previous parsers 4  #TAUTHOR_TAG and two baseline systems, spar']","['', 'if we also employ silver data, we again observe that the multi - encoder setup is preferable over a single encoder, for both isolating and stacking the linguistic features.', 'on isolation, the results are similar to only using gold data, with the exception of the semantic tags, which even hurt the performance now.', 'interestingly, when stacking the linguistic features, there is no improvement over only using the lemma of the source words.', 'we now compare our best models to previous parsers 4  #TAUTHOR_TAG and two baseline systems, spar and sim - spar.', 'as previously indicated,  #TAUTHOR_TAG used a similar sequence - to - sequence model as our current approach, but implemented in opennmt and without the linguistic features.', 'boxer  #AUTHOR_TAG ( bos,, 2015 ) is a drs parser that uses a statistical ccg parser for syntactic analysis and a compositional semantics based on λ - calculus, followed by pronoun and presupposition resolution.', '']",4
['##s 4  #TAUTHOR_TAG and'],['parsers 4  #TAUTHOR_TAG and'],"['previous parsers 4  #TAUTHOR_TAG and two baseline systems, spar']","['', 'if we also employ silver data, we again observe that the multi - encoder setup is preferable over a single encoder, for both isolating and stacking the linguistic features.', 'on isolation, the results are similar to only using gold data, with the exception of the semantic tags, which even hurt the performance now.', 'interestingly, when stacking the linguistic features, there is no improvement over only using the lemma of the source words.', 'we now compare our best models to previous parsers 4  #TAUTHOR_TAG and two baseline systems, spar and sim - spar.', 'as previously indicated,  #TAUTHOR_TAG used a similar sequence - to - sequence model as our current approach, but implemented in opennmt and without the linguistic features.', 'boxer  #AUTHOR_TAG ( bos,, 2015 ) is a drs parser that uses a statistical ccg parser for syntactic analysis and a compositional semantics based on λ - calculus, followed by pronoun and presupposition resolution.', '']",4
['##s 4  #TAUTHOR_TAG and'],['parsers 4  #TAUTHOR_TAG and'],"['previous parsers 4  #TAUTHOR_TAG and two baseline systems, spar']","['', 'if we also employ silver data, we again observe that the multi - encoder setup is preferable over a single encoder, for both isolating and stacking the linguistic features.', 'on isolation, the results are similar to only using gold data, with the exception of the semantic tags, which even hurt the performance now.', 'interestingly, when stacking the linguistic features, there is no improvement over only using the lemma of the source words.', 'we now compare our best models to previous parsers 4  #TAUTHOR_TAG and two baseline systems, spar and sim - spar.', 'as previously indicated,  #TAUTHOR_TAG used a similar sequence - to - sequence model as our current approach, but implemented in opennmt and without the linguistic features.', 'boxer  #AUTHOR_TAG ( bos,, 2015 ) is a drs parser that uses a statistical ccg parser for syntactic analysis and a compositional semantics based on λ - calculus, followed by pronoun and presupposition resolution.', '']",3
"['wise alignment algorithms based on symmetric alignments from a hmm alignment model  #AUTHOR_TAG or edit distance alignments allowing shifts  #TAUTHOR_TAG.', 'the alignment method described in this paper extends the latter by incremental']","['based multiple string alignment ( msa )  #AUTHOR_TAG to build the confusion networks.', 'the recent approaches used pair - wise alignment algorithms based on symmetric alignments from a hmm alignment model  #AUTHOR_TAG or edit distance alignments allowing shifts  #TAUTHOR_TAG.', 'the alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in msa but also allowing shifts as in']","['wise alignment algorithms based on symmetric alignments from a hmm alignment model  #AUTHOR_TAG or edit distance alignments allowing shifts  #TAUTHOR_TAG.', 'the alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in msa but also allowing shifts as in the ter alignment.', 'the confusion networks are built']","['network decoding has been applied in combining outputs from multiple machine translation systems.', 'the earliest approach in  #AUTHOR_TAG used edit distance based multiple string alignment ( msa )  #AUTHOR_TAG to build the confusion networks.', 'the recent approaches used pair - wise alignment algorithms based on symmetric alignments from a hmm alignment model  #AUTHOR_TAG or edit distance alignments allowing shifts  #TAUTHOR_TAG.', 'the alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in msa but also allowing shifts as in the ter alignment.', 'the confusion networks are built around a "" skeleton "" hypothesis.', 'the skeleton hypothesis defines the word order of the decoding output.', 'usually, the 1 - best hypotheses from each system are considered as possible skeletons.', 'using the pair - wise hypothesis alignment, the confusion networks are built in two steps.', 'first, all hypotheses are aligned against the skeleton independently.', 'second, the confusion networks are created from the union of these alignments.', 'the incremental hypothesis alignment algorithm combines these two steps.', 'all words from the previously aligned hypotheses are available, even if not present in the skeleton hypothesis, when aligning the following hypotheses.', '']",0
"['wise alignment algorithms based on symmetric alignments from a hmm alignment model  #AUTHOR_TAG or edit distance alignments allowing shifts  #TAUTHOR_TAG.', 'the alignment method described in this paper extends the latter by incremental']","['based multiple string alignment ( msa )  #AUTHOR_TAG to build the confusion networks.', 'the recent approaches used pair - wise alignment algorithms based on symmetric alignments from a hmm alignment model  #AUTHOR_TAG or edit distance alignments allowing shifts  #TAUTHOR_TAG.', 'the alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in msa but also allowing shifts as in']","['wise alignment algorithms based on symmetric alignments from a hmm alignment model  #AUTHOR_TAG or edit distance alignments allowing shifts  #TAUTHOR_TAG.', 'the alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in msa but also allowing shifts as in the ter alignment.', 'the confusion networks are built']","['network decoding has been applied in combining outputs from multiple machine translation systems.', 'the earliest approach in  #AUTHOR_TAG used edit distance based multiple string alignment ( msa )  #AUTHOR_TAG to build the confusion networks.', 'the recent approaches used pair - wise alignment algorithms based on symmetric alignments from a hmm alignment model  #AUTHOR_TAG or edit distance alignments allowing shifts  #TAUTHOR_TAG.', 'the alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in msa but also allowing shifts as in the ter alignment.', 'the confusion networks are built around a "" skeleton "" hypothesis.', 'the skeleton hypothesis defines the word order of the decoding output.', 'usually, the 1 - best hypotheses from each system are considered as possible skeletons.', 'using the pair - wise hypothesis alignment, the confusion networks are built in two steps.', 'first, all hypotheses are aligned against the skeleton independently.', 'second, the confusion networks are created from the union of these alignments.', 'the incremental hypothesis alignment algorithm combines these two steps.', 'all words from the previously aligned hypotheses are available, even if not present in the skeleton hypothesis, when aligning the following hypotheses.', '']",6
"['wise alignment algorithms based on symmetric alignments from a hmm alignment model  #AUTHOR_TAG or edit distance alignments allowing shifts  #TAUTHOR_TAG.', 'the alignment method described in this paper extends the latter by incremental']","['based multiple string alignment ( msa )  #AUTHOR_TAG to build the confusion networks.', 'the recent approaches used pair - wise alignment algorithms based on symmetric alignments from a hmm alignment model  #AUTHOR_TAG or edit distance alignments allowing shifts  #TAUTHOR_TAG.', 'the alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in msa but also allowing shifts as in']","['wise alignment algorithms based on symmetric alignments from a hmm alignment model  #AUTHOR_TAG or edit distance alignments allowing shifts  #TAUTHOR_TAG.', 'the alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in msa but also allowing shifts as in the ter alignment.', 'the confusion networks are built']","['network decoding has been applied in combining outputs from multiple machine translation systems.', 'the earliest approach in  #AUTHOR_TAG used edit distance based multiple string alignment ( msa )  #AUTHOR_TAG to build the confusion networks.', 'the recent approaches used pair - wise alignment algorithms based on symmetric alignments from a hmm alignment model  #AUTHOR_TAG or edit distance alignments allowing shifts  #TAUTHOR_TAG.', 'the alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in msa but also allowing shifts as in the ter alignment.', 'the confusion networks are built around a "" skeleton "" hypothesis.', 'the skeleton hypothesis defines the word order of the decoding output.', 'usually, the 1 - best hypotheses from each system are considered as possible skeletons.', 'using the pair - wise hypothesis alignment, the confusion networks are built in two steps.', 'first, all hypotheses are aligned against the skeleton independently.', 'second, the confusion networks are created from the union of these alignments.', 'the incremental hypothesis alignment algorithm combines these two steps.', 'all words from the previously aligned hypotheses are available, even if not present in the skeleton hypothesis, when aligning the following hypotheses.', '']",3
['word arc are set as in  #TAUTHOR_TAG'],['two consecutive nodes. other scores for the word arc are set as in  #TAUTHOR_TAG'],['word arc are set as in  #TAUTHOR_TAG'],"['', 'confidence for the system is set to and the confidences for', 'other systems are set to zeros. each deletion will generate a new null word', 'arc unless one exists at the corresponding position in the network. the null word arc', 'confidences are adjusted as in the case of a', 'match or a substitution depending on whether the null word arc exists', 'or not. finally, each insertion will generate a new node and two word arcs at the corresponding position', 'in the network. the first word arc will have the inserted word with the confidence set as in the case of a substitution and the second word arc', 'will have a null word with confidences set by assuming all previously aligned hypotheses and the skeleton generated the null', 'word arc. after all hypotheses have been added into the confusion network, the system specific word arc confidences are scaled to sum to one over all arcs between 1 2 3 4 5 6 i ( 3 ) like ( 3 ) kites ( 1', ') null ( 2 ) null ( 1 ) big ( 1 ) blue ( 2 ) balloons ( 2 ) figure 2 : network after incremental ter alignment. each set of two consecutive nodes. other scores for the word arc are set as in  #TAUTHOR_TAG']",3
"['wise alignment algorithms based on symmetric alignments from a hmm alignment model  #AUTHOR_TAG or edit distance alignments allowing shifts  #TAUTHOR_TAG.', 'the alignment method described in this paper extends the latter by incremental']","['based multiple string alignment ( msa )  #AUTHOR_TAG to build the confusion networks.', 'the recent approaches used pair - wise alignment algorithms based on symmetric alignments from a hmm alignment model  #AUTHOR_TAG or edit distance alignments allowing shifts  #TAUTHOR_TAG.', 'the alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in msa but also allowing shifts as in']","['wise alignment algorithms based on symmetric alignments from a hmm alignment model  #AUTHOR_TAG or edit distance alignments allowing shifts  #TAUTHOR_TAG.', 'the alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in msa but also allowing shifts as in the ter alignment.', 'the confusion networks are built']","['network decoding has been applied in combining outputs from multiple machine translation systems.', 'the earliest approach in  #AUTHOR_TAG used edit distance based multiple string alignment ( msa )  #AUTHOR_TAG to build the confusion networks.', 'the recent approaches used pair - wise alignment algorithms based on symmetric alignments from a hmm alignment model  #AUTHOR_TAG or edit distance alignments allowing shifts  #TAUTHOR_TAG.', 'the alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in msa but also allowing shifts as in the ter alignment.', 'the confusion networks are built around a "" skeleton "" hypothesis.', 'the skeleton hypothesis defines the word order of the decoding output.', 'usually, the 1 - best hypotheses from each system are considered as possible skeletons.', 'using the pair - wise hypothesis alignment, the confusion networks are built in two steps.', 'first, all hypotheses are aligned against the skeleton independently.', 'second, the confusion networks are created from the union of these alignments.', 'the incremental hypothesis alignment algorithm combines these two steps.', 'all words from the previously aligned hypotheses are available, even if not present in the skeleton hypothesis, when aligning the following hypotheses.', '']",5
['word arc are set as in  #TAUTHOR_TAG'],['two consecutive nodes. other scores for the word arc are set as in  #TAUTHOR_TAG'],['word arc are set as in  #TAUTHOR_TAG'],"['', 'confidence for the system is set to and the confidences for', 'other systems are set to zeros. each deletion will generate a new null word', 'arc unless one exists at the corresponding position in the network. the null word arc', 'confidences are adjusted as in the case of a', 'match or a substitution depending on whether the null word arc exists', 'or not. finally, each insertion will generate a new node and two word arcs at the corresponding position', 'in the network. the first word arc will have the inserted word with the confidence set as in the case of a substitution and the second word arc', 'will have a null word with confidences set by assuming all previously aligned hypotheses and the skeleton generated the null', 'word arc. after all hypotheses have been added into the confusion network, the system specific word arc confidences are scaled to sum to one over all arcs between 1 2 3 4 5 6 i ( 3 ) like ( 3 ) kites ( 1', ') null ( 2 ) null ( 1 ) big ( 1 ) blue ( 2 ) balloons ( 2 ) figure 2 : network after incremental ter alignment. each set of two consecutive nodes. other scores for the word arc are set as in  #TAUTHOR_TAG']",5
['##er of  #TAUTHOR_TAG'],['network ( iornn ) reranker of  #TAUTHOR_TAG'],"['##rnn ) reranker of  #TAUTHOR_TAG.', 'replacing the original softmax function with a hierarchical softmax using']","['propose solutions to enhance the inside - outside recursive neural network ( iornn ) reranker of  #TAUTHOR_TAG.', ""replacing the original softmax function with a hierarchical softmax using a binary tree constructed by combining output of the brown clustering algorithm and frequency - based huffman codes, we significantly reduce the reranker's computational complexity."", '']",5
"[' #TAUTHOR_TAG is among the top systems, including the  #AUTHOR_TAG']","["" #TAUTHOR_TAG is among the top systems, including the  #AUTHOR_TAG's extremely fast transition - based""]","[' #TAUTHOR_TAG is among the top systems, including the  #AUTHOR_TAG']",[' #TAUTHOR_TAG'],5
['##er  #TAUTHOR_TAG'],['firstly introduce the iornn reranker  #TAUTHOR_TAG'],['firstly introduce the iornn reranker  #TAUTHOR_TAG'],['firstly introduce the iornn reranker  #TAUTHOR_TAG'],5
"['.', ' #TAUTHOR_TAG where']","['outer representations.', ' #TAUTHOR_TAG where']","['outer representations.', ' #TAUTHOR_TAG where']","['', ' #TAUTHOR_TAG']",5
['##er of  #TAUTHOR_TAG'],['network ( iornn ) reranker of  #TAUTHOR_TAG'],"['##rnn ) reranker of  #TAUTHOR_TAG.', 'replacing the original softmax function with a hierarchical softmax using']","['propose solutions to enhance the inside - outside recursive neural network ( iornn ) reranker of  #TAUTHOR_TAG.', ""replacing the original softmax function with a hierarchical softmax using a binary tree constructed by combining output of the brown clustering algorithm and frequency - based huffman codes, we significantly reduce the reranker's computational complexity."", '']",6
"[' #TAUTHOR_TAG is among the top systems, including the  #AUTHOR_TAG']","["" #TAUTHOR_TAG is among the top systems, including the  #AUTHOR_TAG's extremely fast transition - based""]","[' #TAUTHOR_TAG is among the top systems, including the  #AUTHOR_TAG']",[' #TAUTHOR_TAG'],6
"['##er of  #TAUTHOR_TAG were proposed.', 'we showed that, by replacing']","['iornn reranker of  #TAUTHOR_TAG were proposed.', 'we showed that, by replacing']","['to enhance the iornn reranker of  #TAUTHOR_TAG were proposed.', ""we showed that, by replacing the original softmax function with a hierarchical softmax, the reranker's computational complexity significantly""]","['to enhance the iornn reranker of  #TAUTHOR_TAG were proposed.', ""we showed that, by replacing the original softmax function with a hierarchical softmax, the reranker's computational complexity significantly decreases."", ""the cost of this, which is drop on accuracy, is avoided by enriching contexts with subtrees rooted at ( ancestors') cousin nodes."", 'the new reranker, according to experimental results on the penn wsj treebank, has even higher accuracy than the old one']",6
"[' #TAUTHOR_TAG is among the top systems, including the  #AUTHOR_TAG']","["" #TAUTHOR_TAG is among the top systems, including the  #AUTHOR_TAG's extremely fast transition - based""]","[' #TAUTHOR_TAG is among the top systems, including the  #AUTHOR_TAG']",[' #TAUTHOR_TAG'],0
"[' #TAUTHOR_TAG is among the top systems, including the  #AUTHOR_TAG']","["" #TAUTHOR_TAG is among the top systems, including the  #AUTHOR_TAG's extremely fast transition - based""]","[' #TAUTHOR_TAG is among the top systems, including the  #AUTHOR_TAG']",[' #TAUTHOR_TAG'],0
"['.', ' #TAUTHOR_TAG where']","['outer representations.', ' #TAUTHOR_TAG where']","['outer representations.', ' #TAUTHOR_TAG where']","['', ' #TAUTHOR_TAG']",0
['.  #TAUTHOR_TAG proposed a method to detect the set of suitable wordnet senses able to ev'],['al.  #TAUTHOR_TAG proposed a method to detect the set of suitable wordnet senses able to evoke'],['.  #TAUTHOR_TAG proposed a method to detect the set of suitable wordnet senses able to ev'],"['', 'crespo and buitelaar [ 3 ] carried out an automatic mapping of medical - oriented frames to wordnet synsets, trying to select synsets attached to a lu that were statistically significant in a given reference corpus.', 'de cao et al.  #TAUTHOR_TAG proposed a method to detect the set of suitable wordnet senses able to evoke the same frame by exploiting the hypernym hierarchies that capture the largest number of lus in the frame.', 'for all above mentioned approaches, a real evaluation on randomly selected frames is missing, and accuracy was mainly computed over the new lexical units obtained for a frame, not on a gold standard where one or more synsets are assigned to every lexical unit in a frame.', 'besides, it seems that the most common approach to carry out the mapping relies on some similarity measures that perform better on richer sets of lexical units']",0
['.  #TAUTHOR_TAG reported a better'],"['available is reported in [ 5 ], and shows that our results are promising.', 'de cao at al.  #TAUTHOR_TAG reported a better performance, particularly']","['.  #TAUTHOR_TAG reported a better performance, particularly']","['proposed a new method to map framenet lus to wordnet synsets by computing a similarity measure between lu definitions and wordnet glosses.', 'to our knowledge, this is the only approach to the task based on this kind of similarity.', 'the only comparable evaluation available is reported in [ 5 ], and shows that our results are promising.', 'de cao at al.  #TAUTHOR_TAG reported a better performance, particularly for recall, but evaluation of their mapping algorithm relied on a gold standard of 4 selected frames having at least 10 lus and a given number of corpus instantiations.', 'in the future, we plan to improve the algorithm by shallow parsing the lu definitions and the wordnet glosses.', 'besides, we will exploit information extracted from the wordnet hierarchy.', 'we also want to evaluate the effectiveness of the approach focusing on the new lus to be included in the existing frames']",4
"['simplifications  #TAUTHOR_TAG, with translationbased approaches using an aligned version  #AUTHOR_TAG.', 'however, learning simplifications from wikipedia is not well suited to lexical simplification of scientific terms']","['simplifications  #TAUTHOR_TAG, with translationbased approaches using an aligned version  #AUTHOR_TAG.', 'however, learning simplifications from wikipedia is not well suited to lexical simplification of scientific terms.', 'though generic']","['simplifications  #TAUTHOR_TAG, with translationbased approaches using an aligned version  #AUTHOR_TAG.', 'however, learning simplifications from wikipedia is not well suited to lexical simplification of scientific terms.', 'though generic']","['simplification, the process of reducing the complexity of words by replacing them with simpler substitutes ( e. g., sodium in place of na ; insects in place of lepidopterans ) can make scientific texts more accessible to general audiences.', 'human - inthe - loop interfaces present multiple possible simplifications to a reader ( on demand ) in place of jargon and give the reader familiar access points to understanding jargon  #AUTHOR_TAG.', 'unfortunately, simplification techniques are not yet of high enough quality for fully automated scenarios.', 'currently lexical simplification pipelines for scientific texts are rare.', 'the vast majority of prior methods assume a domain independent context, and rely on wikipedia and simple english wikipedia, a subset of wikipedia using simplified grammar and terminology, to learn simplifications  #TAUTHOR_TAG, with translationbased approaches using an aligned version  #AUTHOR_TAG.', 'however, learning simplifications from wikipedia is not well suited to lexical simplification of scientific terms.', 'though generic or established terms may appear in wikipedia, novel terms associated with new advances may not be reflected.', ""wikipedia's editing rules also favor generality over specificity and eliminate redundancy, both of which are problematic in providing a rich training set that matches simple and complex terms."", 'further, some approaches work by detecting all pairs of words in a corpus and filtering to isolate synonym or hypernym - relationship pairs using wordnet  #TAUTHOR_TAG.', 'like wikipedia, wordnet is a general purpose semantic database  #AUTHOR_TAG, and does not cover all branches of science nor integrate new terminology quickly.', 'word embeddings do not require the use of prebuilt ontologies to identify associated terms like simplifications.', 'recent work indicates that they may improve results for simplification selection : determining which simplifications for a given complex word can be used without altering the meaning of the text  #AUTHOR_TAG.', 'embeddings have also been explored to extract hypernym relations from general corpora  #AUTHOR_TAG.', 'however, word embeddings have not been used for generating lexical simplifications.', 'we provide a novel demonstration of how using embeddings on a scientific corpus is better suited to learning scientific term simplifications than prior approaches that use wordnet as a filter and wikipedia as a corpus.', 'input : finally we show that the transient immune activation that renders mosquitoes resistant to the human malaria parasite has little to no effect on mosquito fitness as a measure of survival or fecundity under laboratory conditions.', 'candidate rules : { fecundity→fertility } { fecundity→productivity } output : finally we show that the transient immune activation that renders mosquitoes resistant to the human malaria parasite has little to no effect on mosquito fitness as a measure of survival or ( fertility ; productivity ) under laboratory conditions.', '']",0
"['simplifications  #TAUTHOR_TAG, with translationbased approaches using an aligned version  #AUTHOR_TAG.', 'however, learning simplifications from wikipedia is not well suited to lexical simplification of scientific terms']","['simplifications  #TAUTHOR_TAG, with translationbased approaches using an aligned version  #AUTHOR_TAG.', 'however, learning simplifications from wikipedia is not well suited to lexical simplification of scientific terms.', 'though generic']","['simplifications  #TAUTHOR_TAG, with translationbased approaches using an aligned version  #AUTHOR_TAG.', 'however, learning simplifications from wikipedia is not well suited to lexical simplification of scientific terms.', 'though generic']","['simplification, the process of reducing the complexity of words by replacing them with simpler substitutes ( e. g., sodium in place of na ; insects in place of lepidopterans ) can make scientific texts more accessible to general audiences.', 'human - inthe - loop interfaces present multiple possible simplifications to a reader ( on demand ) in place of jargon and give the reader familiar access points to understanding jargon  #AUTHOR_TAG.', 'unfortunately, simplification techniques are not yet of high enough quality for fully automated scenarios.', 'currently lexical simplification pipelines for scientific texts are rare.', 'the vast majority of prior methods assume a domain independent context, and rely on wikipedia and simple english wikipedia, a subset of wikipedia using simplified grammar and terminology, to learn simplifications  #TAUTHOR_TAG, with translationbased approaches using an aligned version  #AUTHOR_TAG.', 'however, learning simplifications from wikipedia is not well suited to lexical simplification of scientific terms.', 'though generic or established terms may appear in wikipedia, novel terms associated with new advances may not be reflected.', ""wikipedia's editing rules also favor generality over specificity and eliminate redundancy, both of which are problematic in providing a rich training set that matches simple and complex terms."", 'further, some approaches work by detecting all pairs of words in a corpus and filtering to isolate synonym or hypernym - relationship pairs using wordnet  #TAUTHOR_TAG.', 'like wikipedia, wordnet is a general purpose semantic database  #AUTHOR_TAG, and does not cover all branches of science nor integrate new terminology quickly.', 'word embeddings do not require the use of prebuilt ontologies to identify associated terms like simplifications.', 'recent work indicates that they may improve results for simplification selection : determining which simplifications for a given complex word can be used without altering the meaning of the text  #AUTHOR_TAG.', 'embeddings have also been explored to extract hypernym relations from general corpora  #AUTHOR_TAG.', 'however, word embeddings have not been used for generating lexical simplifications.', 'we provide a novel demonstration of how using embeddings on a scientific corpus is better suited to learning scientific term simplifications than prior approaches that use wordnet as a filter and wikipedia as a corpus.', 'input : finally we show that the transient immune activation that renders mosquitoes resistant to the human malaria parasite has little to no effect on mosquito fitness as a measure of survival or fecundity under laboratory conditions.', 'candidate rules : { fecundity→fertility } { fecundity→productivity } output : finally we show that the transient immune activation that renders mosquitoes resistant to the human malaria parasite has little to no effect on mosquito fitness as a measure of survival or ( fertility ; productivity ) under laboratory conditions.', '']",0
"['are not simplifications  #TAUTHOR_TAG.', 'we adopt a similar']","['are not simplifications  #TAUTHOR_TAG.', 'we adopt a similar']","['are not simplifications  #TAUTHOR_TAG.', 'we adopt a similar pipeline']","['goal is to learn simplification rules in the form complex word→simple word.', ""one approach identifies all pairwise permutations of'content'terms and then applies semantic ( i. e., wordnet ) and simplicity filters to eliminate pairs that are not simplifications  #TAUTHOR_TAG."", 'we adopt a similar pipeline but leverage distance metrics on word embeddings and a simpler frequency filter in place of wordnet.', 'embeddings identify words that share context in an unsupervised, scalable way and are more efficient than constructing co - occurrence matrices  #TAUTHOR_TAG.', 'as our experiments demonstrate, our approach improves performance on a scientific test set over prior work']",0
"['are not simplifications  #TAUTHOR_TAG.', 'we adopt a similar']","['are not simplifications  #TAUTHOR_TAG.', 'we adopt a similar']","['are not simplifications  #TAUTHOR_TAG.', 'we adopt a similar pipeline']","['goal is to learn simplification rules in the form complex word→simple word.', ""one approach identifies all pairwise permutations of'content'terms and then applies semantic ( i. e., wordnet ) and simplicity filters to eliminate pairs that are not simplifications  #TAUTHOR_TAG."", 'we adopt a similar pipeline but leverage distance metrics on word embeddings and a simpler frequency filter in place of wordnet.', 'embeddings identify words that share context in an unsupervised, scalable way and are more efficient than constructing co - occurrence matrices  #TAUTHOR_TAG.', 'as our experiments demonstrate, our approach improves performance on a scientific test set over prior work']",0
"['is complex, involving several similarity operations on word co - occurrence matrices  #TAUTHOR_TAG or using embeddings']","['is complex, involving several similarity operations on word co - occurrence matrices  #TAUTHOR_TAG or using embeddings']","['is complex, involving several similarity operations on word co - occurrence matrices  #TAUTHOR_TAG or using embeddings']","['prior context - aware simplification systems, the decision of whether to apply a simplification rule in an input sentence is complex, involving several similarity operations on word co - occurrence matrices  #TAUTHOR_TAG or using embeddings to incorporate co - occurrence context for pairs generated using other means  #AUTHOR_TAG.', 'however, the simplescience pipline already considers the context of appearance for each word in deriving simplifications via word embeddings learned from a large corpus.', 'we see no additional improvements in f - measure when we apply two variants of context similarity thresholds to decide whether to apply a rule to an input sentence.', 'the first is the cosine similarity between the distributed representation of the simple word and the sum of the distributed representations of all words within a window l surrounding the complex word in the input sentence  #AUTHOR_TAG.', 'the second is the cosine similarity of a minimum shared frequency co - occurrence matrix for the words in the pair and the co - occurrence matrix for the input sentence  #TAUTHOR_TAG.', 'in fully automated applications, the top rule from the ranked candidate rules is used.', 'we find that ranking by the cosine similarity between the word embeddings for the complex and simple word in the rule leads to the best performance at the top slot ( full results in supplementary material )']",0
"['is complex, involving several similarity operations on word co - occurrence matrices  #TAUTHOR_TAG or using embeddings']","['is complex, involving several similarity operations on word co - occurrence matrices  #TAUTHOR_TAG or using embeddings']","['is complex, involving several similarity operations on word co - occurrence matrices  #TAUTHOR_TAG or using embeddings']","['prior context - aware simplification systems, the decision of whether to apply a simplification rule in an input sentence is complex, involving several similarity operations on word co - occurrence matrices  #TAUTHOR_TAG or using embeddings to incorporate co - occurrence context for pairs generated using other means  #AUTHOR_TAG.', 'however, the simplescience pipline already considers the context of appearance for each word in deriving simplifications via word embeddings learned from a large corpus.', 'we see no additional improvements in f - measure when we apply two variants of context similarity thresholds to decide whether to apply a rule to an input sentence.', 'the first is the cosine similarity between the distributed representation of the simple word and the sum of the distributed representations of all words within a window l surrounding the complex word in the input sentence  #AUTHOR_TAG.', 'the second is the cosine similarity of a minimum shared frequency co - occurrence matrix for the words in the pair and the co - occurrence matrix for the input sentence  #TAUTHOR_TAG.', 'in fully automated applications, the top rule from the ranked candidate rules is used.', 'we find that ranking by the cosine similarity between the word embeddings for the complex and simple word in the rule leads to the best performance at the top slot ( full results in supplementary material )']",0
"['are not simplifications  #TAUTHOR_TAG.', 'we adopt a similar']","['are not simplifications  #TAUTHOR_TAG.', 'we adopt a similar']","['are not simplifications  #TAUTHOR_TAG.', 'we adopt a similar pipeline']","['goal is to learn simplification rules in the form complex word→simple word.', ""one approach identifies all pairwise permutations of'content'terms and then applies semantic ( i. e., wordnet ) and simplicity filters to eliminate pairs that are not simplifications  #TAUTHOR_TAG."", 'we adopt a similar pipeline but leverage distance metrics on word embeddings and a simpler frequency filter in place of wordnet.', 'embeddings identify words that share context in an unsupervised, scalable way and are more efficient than constructing co - occurrence matrices  #TAUTHOR_TAG.', 'as our experiments demonstrate, our approach improves performance on a scientific test set over prior work']",1
[' #TAUTHOR_TAG of'],[' #TAUTHOR_TAG of'],"['simple word we calculate the corpus complexity, c  #TAUTHOR_TAG of']",[' #TAUTHOR_TAG'],3
['by  #TAUTHOR_TAG applied to the wikipedia and sew corpus as well as to the scientific corpus ( table 1 )'],['by  #TAUTHOR_TAG applied to the wikipedia and sew corpus as well as to the scientific corpus ( table 1 )'],['by  #TAUTHOR_TAG applied to the wikipedia and sew corpus as well as to the scientific corpus ( table 1 )'],"[""compare our word embedding generation process ( applied to our corpora ) to biran et al.'s ( 2011 ) approach ( applied to the wikipedia and simple english wikipedia corpus as well as our scientific corpora )."", 'following the evaluation method used in  #AUTHOR_TAG, we calculate potential as the proportion of instances for which at least one of the substitutions generated is present in the gold standard set, precision as the proportion of generated instances which are present in the gold standard set, and f - measure as their harmonic mean.', 'our simplescience approach outperforms the original approach by  #TAUTHOR_TAG applied to the wikipedia and sew corpus as well as to the scientific corpus ( table 1 )']",3
[' #TAUTHOR_TAG of'],[' #TAUTHOR_TAG of'],"['simple word we calculate the corpus complexity, c  #TAUTHOR_TAG of']",[' #TAUTHOR_TAG'],4
['by  #TAUTHOR_TAG applied to the wikipedia and sew corpus as well as to the scientific corpus ( table 1 )'],['by  #TAUTHOR_TAG applied to the wikipedia and sew corpus as well as to the scientific corpus ( table 1 )'],['by  #TAUTHOR_TAG applied to the wikipedia and sew corpus as well as to the scientific corpus ( table 1 )'],"[""compare our word embedding generation process ( applied to our corpora ) to biran et al.'s ( 2011 ) approach ( applied to the wikipedia and simple english wikipedia corpus as well as our scientific corpora )."", 'following the evaluation method used in  #AUTHOR_TAG, we calculate potential as the proportion of instances for which at least one of the substitutions generated is present in the gold standard set, precision as the proportion of generated instances which are present in the gold standard set, and f - measure as their harmonic mean.', 'our simplescience approach outperforms the original approach by  #TAUTHOR_TAG applied to the wikipedia and sew corpus as well as to the scientific corpus ( table 1 )']",5
"['using co - reference chains  #TAUTHOR_TAG, is important in']","['using co - reference chains  #TAUTHOR_TAG, is important in']","['using co - reference chains  #TAUTHOR_TAG, is important in']","['risk of using word2vec to find related terms, rather than querying a lexical database like wordnet, is that generated rules may include antonyms.', 'adding techniques to filter antonym rules, such as using co - reference chains  #TAUTHOR_TAG, is important in future work.', 'we achieve a precision of 0. 389 at the top slot on our simplescigold standard set when we apply our generation method and rank candidates by cosine similarity.', 'this level of precision is higher than that achieved by various prior ranking methods used in lexenstein  #AUTHOR_TAG, with the exception of using machine learning techniques like svm  #AUTHOR_TAG.', 'future work should explore how much the precision of our simplescience pipeline can be improved by adopting more sophisticated ranking methods.', 'however, we suspect that even the highest precision obtained on general corpora and gold standard sets in prior work is not sufficient for fully automated simplification.', 'an exciting area for future work is in applying the simplescience pipeline in interactive simplification suggestion interfaces for those reading or writing about science  #AUTHOR_TAG']",2
"['##tabille participation to the hyperpartisan news detection task.', 'we propose the use of different text classification methods for this task.', 'preliminary experiments using a similar collection used in  #TAUTHOR_TAG show that neural - based classification methods reach']","['paper describes the rouletabille participation to the hyperpartisan news detection task.', 'we propose the use of different text classification methods for this task.', 'preliminary experiments using a similar collection used in  #TAUTHOR_TAG show that neural - based classification methods reach state - of - the']","['paper describes the rouletabille participation to the hyperpartisan news detection task.', 'we propose the use of different text classification methods for this task.', 'preliminary experiments using a similar collection used in  #TAUTHOR_TAG show that neural - based classification methods reach']","['paper describes the rouletabille participation to the hyperpartisan news detection task.', 'we propose the use of different text classification methods for this task.', 'preliminary experiments using a similar collection used in  #TAUTHOR_TAG show that neural - based classification methods reach state - of - the art results.', 'our final submission is composed of a unique run that ranks among all runs at 3 / 49 position for the by - publisher test dataset and 43 / 96 for the by - article test dataset in terms of accuracy']",0
"[""of an article : writer's preferences and affiliation of the editor house."", 'identifying partisan preferences in news, based only on text content, has been shown to be a challenging task  #TAUTHOR_TAG.', 'this']","[""of an article : writer's preferences and affiliation of the editor house."", 'identifying partisan preferences in news, based only on text content, has been shown to be a challenging task  #TAUTHOR_TAG.', 'this']","[""of an article : writer's preferences and affiliation of the editor house."", 'identifying partisan preferences in news, based only on text content, has been shown to be a challenging task  #TAUTHOR_TAG.', 'this problem requires to identify if']","['press have been in the last decades the main way to access to news in written format.', 'this tendency is changing with the appearance of online channels but usually the main factors of the journalistic content generation are still there : events, journalists, and editors.', 'one of the problems of the generation of this content is the influence of each factor in the veracity of the generated content.', ""two main factors may influence the final view of an article : writer's preferences and affiliation of the editor house."", 'identifying partisan preferences in news, based only on text content, has been shown to be a challenging task  #TAUTHOR_TAG.', 'this problem requires to identify if a news article was written in such a way that it includes an overrated appreciation of one of the participants in the news ( a political party, a person, a company, etc. ).', 'despite the fact that sharply polarized documents are not necessarily fake, it is an early problem to solve for the identification of fake content.', 'a recent paper  #TAUTHOR_TAG claims that stylometric features are a key factor to tackle this task.', 'in this paper, we present the description of our participation to the hyperpartisan classification task at semeval - 2019  #AUTHOR_TAG.', '']",0
"[""of an article : writer's preferences and affiliation of the editor house."", 'identifying partisan preferences in news, based only on text content, has been shown to be a challenging task  #TAUTHOR_TAG.', 'this']","[""of an article : writer's preferences and affiliation of the editor house."", 'identifying partisan preferences in news, based only on text content, has been shown to be a challenging task  #TAUTHOR_TAG.', 'this']","[""of an article : writer's preferences and affiliation of the editor house."", 'identifying partisan preferences in news, based only on text content, has been shown to be a challenging task  #TAUTHOR_TAG.', 'this problem requires to identify if']","['press have been in the last decades the main way to access to news in written format.', 'this tendency is changing with the appearance of online channels but usually the main factors of the journalistic content generation are still there : events, journalists, and editors.', 'one of the problems of the generation of this content is the influence of each factor in the veracity of the generated content.', ""two main factors may influence the final view of an article : writer's preferences and affiliation of the editor house."", 'identifying partisan preferences in news, based only on text content, has been shown to be a challenging task  #TAUTHOR_TAG.', 'this problem requires to identify if a news article was written in such a way that it includes an overrated appreciation of one of the participants in the news ( a political party, a person, a company, etc. ).', 'despite the fact that sharply polarized documents are not necessarily fake, it is an early problem to solve for the identification of fake content.', 'a recent paper  #TAUTHOR_TAG claims that stylometric features are a key factor to tackle this task.', 'in this paper, we present the description of our participation to the hyperpartisan classification task at semeval - 2019  #AUTHOR_TAG.', '']",0
"[""of an article : writer's preferences and affiliation of the editor house."", 'identifying partisan preferences in news, based only on text content, has been shown to be a challenging task  #TAUTHOR_TAG.', 'this']","[""of an article : writer's preferences and affiliation of the editor house."", 'identifying partisan preferences in news, based only on text content, has been shown to be a challenging task  #TAUTHOR_TAG.', 'this']","[""of an article : writer's preferences and affiliation of the editor house."", 'identifying partisan preferences in news, based only on text content, has been shown to be a challenging task  #TAUTHOR_TAG.', 'this problem requires to identify if']","['press have been in the last decades the main way to access to news in written format.', 'this tendency is changing with the appearance of online channels but usually the main factors of the journalistic content generation are still there : events, journalists, and editors.', 'one of the problems of the generation of this content is the influence of each factor in the veracity of the generated content.', ""two main factors may influence the final view of an article : writer's preferences and affiliation of the editor house."", 'identifying partisan preferences in news, based only on text content, has been shown to be a challenging task  #TAUTHOR_TAG.', 'this problem requires to identify if a news article was written in such a way that it includes an overrated appreciation of one of the participants in the news ( a political party, a person, a company, etc. ).', 'despite the fact that sharply polarized documents are not necessarily fake, it is an early problem to solve for the identification of fake content.', 'a recent paper  #TAUTHOR_TAG claims that stylometric features are a key factor to tackle this task.', 'in this paper, we present the description of our participation to the hyperpartisan classification task at semeval - 2019  #AUTHOR_TAG.', '']",0
"['presented in  #TAUTHOR_TAG, showing that stylometric features are probably not necessary for the task']","['presented in  #TAUTHOR_TAG, showing that stylometric features are probably not necessary for the task']","['presented in  #TAUTHOR_TAG, showing that stylometric features are probably not necessary for the task']","['of the three f - measures were calculated with sklearn 7.', 'note that in binary classification, micro f - measure values are equivalent to accuracy values.', 'two state - of - the - art models ( spa  #AUTHOR_TAG ) outperform the approach presented in  #TAUTHOR_TAG, showing that stylometric features are probably not necessary for the task']",0
"[""of an article : writer's preferences and affiliation of the editor house."", 'identifying partisan preferences in news, based only on text content, has been shown to be a challenging task  #TAUTHOR_TAG.', 'this']","[""of an article : writer's preferences and affiliation of the editor house."", 'identifying partisan preferences in news, based only on text content, has been shown to be a challenging task  #TAUTHOR_TAG.', 'this']","[""of an article : writer's preferences and affiliation of the editor house."", 'identifying partisan preferences in news, based only on text content, has been shown to be a challenging task  #TAUTHOR_TAG.', 'this problem requires to identify if']","['press have been in the last decades the main way to access to news in written format.', 'this tendency is changing with the appearance of online channels but usually the main factors of the journalistic content generation are still there : events, journalists, and editors.', 'one of the problems of the generation of this content is the influence of each factor in the veracity of the generated content.', ""two main factors may influence the final view of an article : writer's preferences and affiliation of the editor house."", 'identifying partisan preferences in news, based only on text content, has been shown to be a challenging task  #TAUTHOR_TAG.', 'this problem requires to identify if a news article was written in such a way that it includes an overrated appreciation of one of the participants in the news ( a political party, a person, a company, etc. ).', 'despite the fact that sharply polarized documents are not necessarily fake, it is an early problem to solve for the identification of fake content.', 'a recent paper  #TAUTHOR_TAG claims that stylometric features are a key factor to tackle this task.', 'in this paper, we present the description of our participation to the hyperpartisan classification task at semeval - 2019  #AUTHOR_TAG.', '']",1
[' #TAUTHOR_TAG and'],[' #TAUTHOR_TAG and'],[' #TAUTHOR_TAG and'],"['were performed using two collections, the acl2018 collection  #TAUTHOR_TAG and the semeval19 collection.', 'the first collection is composed of 1627 articles including 801 hyperpartisan and 826 2 different to the classical training of the involved classifiers.', '3 further experiments were performed using networkbased models but as results did not show improvement in an existing collection, we decided to not include these results.', 'mainstream manually annotated documents.', 'as this collection is not originally split in training - test sets, results are presented using cross - validation.', 'the second collection was split in train, validation, and test sets for the by - publisher category, and in train and test for the by - article category as presented in table 1.', 'results in this second collection are exclusively calculated using the tira evaluation system.', 'in order to determine the best configuration to our participation using the semeval collection, we decided to perform experiments and fix hyperparameters using the acl2018 collection.', ""we only used the first fold produced by the authors'code 6."", 'as our results are not directly comparable with the values reported in  #TAUTHOR_TAG, we re - evaluated their approach on this single fold']",5
[' #TAUTHOR_TAG and'],[' #TAUTHOR_TAG and'],[' #TAUTHOR_TAG and'],"['were performed using two collections, the acl2018 collection  #TAUTHOR_TAG and the semeval19 collection.', 'the first collection is composed of 1627 articles including 801 hyperpartisan and 826 2 different to the classical training of the involved classifiers.', '3 further experiments were performed using networkbased models but as results did not show improvement in an existing collection, we decided to not include these results.', 'mainstream manually annotated documents.', 'as this collection is not originally split in training - test sets, results are presented using cross - validation.', 'the second collection was split in train, validation, and test sets for the by - publisher category, and in train and test for the by - article category as presented in table 1.', 'results in this second collection are exclusively calculated using the tira evaluation system.', 'in order to determine the best configuration to our participation using the semeval collection, we decided to perform experiments and fix hyperparameters using the acl2018 collection.', ""we only used the first fold produced by the authors'code 6."", 'as our results are not directly comparable with the values reported in  #TAUTHOR_TAG, we re - evaluated their approach on this single fold']",5
"['classifiers to validate the work of  #TAUTHOR_TAG.', '• a state - of - the -']","['combined with strong representations / classifiers to validate the work of  #TAUTHOR_TAG.', '• a state - of - the - art classification model in its default configuration ( spacy )']","['combined with strong representations / classifiers to validate the work of  #TAUTHOR_TAG.', '• a state - of - the - art classification model in its default configuration ( spacy ) can be considered as a strong baseline']","['experiments and participation to the hyperpartisan task led us to conclude that :', '• stylometric features seem not to be necessary to achieve state - of - the - art results for hyperpartisan detection in the acl2018 collection.', 'this deserves a set of extra experiments to better understand the real contribution of stylometric features when combined with strong representations / classifiers to validate the work of  #TAUTHOR_TAG.', '• a state - of - the - art classification model in its default configuration ( spacy ) can be considered as a strong baseline for next experiments.', 'indeed, spacy is top - ranked according to the f1 metric on the by - publisher dataset.', 'one question is thus now if other top - ranked approaches are also from the text classification literature or dedicated ones']",5
[' #TAUTHOR_TAG and'],[' #TAUTHOR_TAG and'],[' #TAUTHOR_TAG and'],"['were performed using two collections, the acl2018 collection  #TAUTHOR_TAG and the semeval19 collection.', 'the first collection is composed of 1627 articles including 801 hyperpartisan and 826 2 different to the classical training of the involved classifiers.', '3 further experiments were performed using networkbased models but as results did not show improvement in an existing collection, we decided to not include these results.', 'mainstream manually annotated documents.', 'as this collection is not originally split in training - test sets, results are presented using cross - validation.', 'the second collection was split in train, validation, and test sets for the by - publisher category, and in train and test for the by - article category as presented in table 1.', 'results in this second collection are exclusively calculated using the tira evaluation system.', 'in order to determine the best configuration to our participation using the semeval collection, we decided to perform experiments and fix hyperparameters using the acl2018 collection.', ""we only used the first fold produced by the authors'code 6."", 'as our results are not directly comparable with the values reported in  #TAUTHOR_TAG, we re - evaluated their approach on this single fold']",4
['system presented in  #TAUTHOR_TAG. we also investigated the'],['system presented in  #TAUTHOR_TAG. we also investigated the'],['system presented in  #TAUTHOR_TAG. we also investigated the'],"['g., who attempts to shift topics ). in, we explored this dimension and found that candidates with higher power', 'introduce significantly more topics in the debates, but attempt to shift topics significantly less often while responding to a moderator. we used the basic lda topic modeling method ( with a filter', 'for substantivity of turns ) to assign topics to turns, which were then used to detect shifts in topics. however, segmenting interactions into coherent topic segments is an', 'active area of research and a variety of topic modeling approaches have been proposed for that purpose. in this paper, we explore the utility of one such topic modeling approach to tackle this problem. while most of the early approaches for topic segmenting', ""in interactions have focused on the content of the contribution,  #AUTHOR_TAG introduced a system called speaker identity for topic segmentation ( sits ) which also takes into account the topic shifting tendencies of the participants of the conversation. in later work,  #AUTHOR_TAG demonstrated the sits system's utility in"", 'detecting influencers in crossfire debates and wikipedia discussions. they also applied the sits system to the domain of political debates', '. however they were able to perform only a qualitative analysis of its utility in the debates domain since the debates data did not have influence annotations. in this paper, we', 'use the sits system to assign topics to turns and perform a quantitative analysis of how the topic shift features calculated using the sits system relate to the notion of power as captured by', ' #AUTHOR_TAG a ). the sits system associates each debate participant with a constant scalar value that captures his or her tendency to shift topics. however, since we want to investigate', ""how each candidate's topic shifting tendency relates to his or her changing power over the course of the campaign, we introduce a variation of the sit"", '##s analysis in which we represent a different "" persona "" for each candidate in each debate.', 'once equipped with this notion of "" persona "", we find that the topic shifting tendency of a candidate does indeed show a great deal of fluctuation during the election campaign period. we also find that this fluctuation in topic shifting tendencies is significantly correlated with the candidates', ""' power. as an additional contribution of this paper, we demonstrate the utility of our topic shift features extracted using both types of"", 'sits - based analyses in improving the performance of the automatic power rank', '##er system presented in  #TAUTHOR_TAG. we also investigated the utility of topic shifting features described in  #AUTHOR_TAG extracted using lda based topic modeling. however,', 'they did not improve the performance of the ranker, and hence we do not discuss them in detail in this paper']",1
['system presented in  #TAUTHOR_TAG. we also investigated the'],['system presented in  #TAUTHOR_TAG. we also investigated the'],['system presented in  #TAUTHOR_TAG. we also investigated the'],"['g., who attempts to shift topics ). in, we explored this dimension and found that candidates with higher power', 'introduce significantly more topics in the debates, but attempt to shift topics significantly less often while responding to a moderator. we used the basic lda topic modeling method ( with a filter', 'for substantivity of turns ) to assign topics to turns, which were then used to detect shifts in topics. however, segmenting interactions into coherent topic segments is an', 'active area of research and a variety of topic modeling approaches have been proposed for that purpose. in this paper, we explore the utility of one such topic modeling approach to tackle this problem. while most of the early approaches for topic segmenting', ""in interactions have focused on the content of the contribution,  #AUTHOR_TAG introduced a system called speaker identity for topic segmentation ( sits ) which also takes into account the topic shifting tendencies of the participants of the conversation. in later work,  #AUTHOR_TAG demonstrated the sits system's utility in"", 'detecting influencers in crossfire debates and wikipedia discussions. they also applied the sits system to the domain of political debates', '. however they were able to perform only a qualitative analysis of its utility in the debates domain since the debates data did not have influence annotations. in this paper, we', 'use the sits system to assign topics to turns and perform a quantitative analysis of how the topic shift features calculated using the sits system relate to the notion of power as captured by', ' #AUTHOR_TAG a ). the sits system associates each debate participant with a constant scalar value that captures his or her tendency to shift topics. however, since we want to investigate', ""how each candidate's topic shifting tendency relates to his or her changing power over the course of the campaign, we introduce a variation of the sit"", '##s analysis in which we represent a different "" persona "" for each candidate in each debate.', 'once equipped with this notion of "" persona "", we find that the topic shifting tendency of a candidate does indeed show a great deal of fluctuation during the election campaign period. we also find that this fluctuation in topic shifting tendencies is significantly correlated with the candidates', ""' power. as an additional contribution of this paper, we demonstrate the utility of our topic shift features extracted using both types of"", 'sits - based analyses in improving the performance of the automatic power rank', '##er system presented in  #TAUTHOR_TAG. we also investigated the utility of topic shifting features described in  #AUTHOR_TAG extracted using lda based topic modeling. however,', 'they did not improve the performance of the ranker, and hence we do not discuss them in detail in this paper']",0
['system presented in  #TAUTHOR_TAG. we also investigated the'],['system presented in  #TAUTHOR_TAG. we also investigated the'],['system presented in  #TAUTHOR_TAG. we also investigated the'],"['g., who attempts to shift topics ). in, we explored this dimension and found that candidates with higher power', 'introduce significantly more topics in the debates, but attempt to shift topics significantly less often while responding to a moderator. we used the basic lda topic modeling method ( with a filter', 'for substantivity of turns ) to assign topics to turns, which were then used to detect shifts in topics. however, segmenting interactions into coherent topic segments is an', 'active area of research and a variety of topic modeling approaches have been proposed for that purpose. in this paper, we explore the utility of one such topic modeling approach to tackle this problem. while most of the early approaches for topic segmenting', ""in interactions have focused on the content of the contribution,  #AUTHOR_TAG introduced a system called speaker identity for topic segmentation ( sits ) which also takes into account the topic shifting tendencies of the participants of the conversation. in later work,  #AUTHOR_TAG demonstrated the sits system's utility in"", 'detecting influencers in crossfire debates and wikipedia discussions. they also applied the sits system to the domain of political debates', '. however they were able to perform only a qualitative analysis of its utility in the debates domain since the debates data did not have influence annotations. in this paper, we', 'use the sits system to assign topics to turns and perform a quantitative analysis of how the topic shift features calculated using the sits system relate to the notion of power as captured by', ' #AUTHOR_TAG a ). the sits system associates each debate participant with a constant scalar value that captures his or her tendency to shift topics. however, since we want to investigate', ""how each candidate's topic shifting tendency relates to his or her changing power over the course of the campaign, we introduce a variation of the sit"", '##s analysis in which we represent a different "" persona "" for each candidate in each debate.', 'once equipped with this notion of "" persona "", we find that the topic shifting tendency of a candidate does indeed show a great deal of fluctuation during the election campaign period. we also find that this fluctuation in topic shifting tendencies is significantly correlated with the candidates', ""' power. as an additional contribution of this paper, we demonstrate the utility of our topic shift features extracted using both types of"", 'sits - based analyses in improving the performance of the automatic power rank', '##er system presented in  #TAUTHOR_TAG. we also investigated the utility of topic shifting features described in  #AUTHOR_TAG extracted using lda based topic modeling. however,', 'they did not improve the performance of the ranker, and hence we do not discuss them in detail in this paper']",4
"['.', 'as we do in  #TAUTHOR_TAG, we here report kend']","['to estimate this ranking function.', 'as we do in  #TAUTHOR_TAG, we here report']","[':', 'for our experiments, we use the svm rank based supervised learned power ranker presented in that work to estimate this ranking function.', 'as we do in  #TAUTHOR_TAG, we here report kend']",[' #TAUTHOR_TAG'],5
"['.', 'as we do in  #TAUTHOR_TAG, we here report kend']","['to estimate this ranking function.', 'as we do in  #TAUTHOR_TAG, we here report']","[':', 'for our experiments, we use the svm rank based supervised learned power ranker presented in that work to estimate this ranking function.', 'as we do in  #TAUTHOR_TAG, we here report kend']",[' #TAUTHOR_TAG'],5
['the transformer  #TAUTHOR_TAG'],"['one of the latest neural mt architectures called the transformer  #TAUTHOR_TAG. this architecture', 'is an encoderdecoder']",['the transformer  #TAUTHOR_TAG'],"['', 'able to learn non - resourced pairs  #AUTHOR_TAG. another related research area for this study is precisely training translation systems domain - specific tasks, where there are scarce in - domain', 'translation resources. a common approach in these cases consists in training a system with a generic corpus and then, use a small in - domain corpus to adapt the system to that particular domain. in this direction, there is a huge amount of research in', 'the statistical approach ( costa - jussa, 2015 ) and also starting in the neural approach  #AUTHOR_TAG. finally, there is am emerging line of research in the topic of unsupervised neural mt  #AUTHOR_TAG. this study designs and details an experiment for testing the standard cascade pivot architecture which has been employed in standard', 'statistical machine translation ( costajussa et al., 2012 ). the system that we propose builds on top of one of the latest neural mt architectures called the transformer  #TAUTHOR_TAG. this architecture', 'is an encoderdecoder structure which uses attention - based mechanisms as an alternative to recurrent neural networks proposed in initial architectures  #AUTHOR_TAG. this new architecture has been proven more efficient and better than all', 'previous proposed so far  #AUTHOR_TAG']",6
"['strongest systems presented recently  #TAUTHOR_TAG, as well as a gl']","['strongest systems presented recently  #TAUTHOR_TAG, as well as a glance of its differences with other popular neural machine translation architectures.', 'sequence - to - sequence recurrent models  #AUTHOR_TAG have been']","['of the strongest systems presented recently  #TAUTHOR_TAG, as well as a glance of its differences with other popular neural machine translation architectures.', 'sequence - to - sequence recurrent models  #AUTHOR_TAG have been the standard approach for neural machine translation,']","['section provides a brief high - level explanation of the neural mt approach that we are using as a baseline system, which is one of the strongest systems presented recently  #TAUTHOR_TAG, as well as a glance of its differences with other popular neural machine translation architectures.', 'sequence - to - sequence recurrent models  #AUTHOR_TAG have been the standard approach for neural machine translation, especially since the incorporation of attention mechanisms  #AUTHOR_TAG, which enables the system to learn to identify the information which is relevant for producing each word in the translation.', 'convolutional networks  #AUTHOR_TAG were the second paradigm to effectively approach sequence transduction tasks like machine translation.', 'in this paper we make use of the third paradigm for neural machine translation, proposed in  #TAUTHOR_TAG, namely the transformer architecture, which is based on a feed - forward encoder - decoder scheme with attention mechanisms.', 'the type of attention mechanism used in the system, referred to as multi - head attention, allows to train several attention modules in parallel, combining also self - attention with standard attention.', 'self - attention differs from standard attention in the use of the same sentence as input and trains over it allowing to solve issues as coreference resolution.', 'equations and details about the transformer system can be found in the original paper  #TAUTHOR_TAG and are out of the scope of this paper.', 'for the definition of the vocabulary to be used as input for the neural network, we used the sub - word mechanism from tensor2tensor package, which is similar to bytepair encoding ( bpe ) from  #AUTHOR_TAG.', 'for the english - spanish language pair, two separate 32k sub - word vocabularies where extracted, while for spanish - catalan we extracted a single shared 32k sub - word vocabulary for both languages']",0
"['strongest systems presented recently  #TAUTHOR_TAG, as well as a gl']","['strongest systems presented recently  #TAUTHOR_TAG, as well as a glance of its differences with other popular neural machine translation architectures.', 'sequence - to - sequence recurrent models  #AUTHOR_TAG have been']","['of the strongest systems presented recently  #TAUTHOR_TAG, as well as a glance of its differences with other popular neural machine translation architectures.', 'sequence - to - sequence recurrent models  #AUTHOR_TAG have been the standard approach for neural machine translation,']","['section provides a brief high - level explanation of the neural mt approach that we are using as a baseline system, which is one of the strongest systems presented recently  #TAUTHOR_TAG, as well as a glance of its differences with other popular neural machine translation architectures.', 'sequence - to - sequence recurrent models  #AUTHOR_TAG have been the standard approach for neural machine translation, especially since the incorporation of attention mechanisms  #AUTHOR_TAG, which enables the system to learn to identify the information which is relevant for producing each word in the translation.', 'convolutional networks  #AUTHOR_TAG were the second paradigm to effectively approach sequence transduction tasks like machine translation.', 'in this paper we make use of the third paradigm for neural machine translation, proposed in  #TAUTHOR_TAG, namely the transformer architecture, which is based on a feed - forward encoder - decoder scheme with attention mechanisms.', 'the type of attention mechanism used in the system, referred to as multi - head attention, allows to train several attention modules in parallel, combining also self - attention with standard attention.', 'self - attention differs from standard attention in the use of the same sentence as input and trains over it allowing to solve issues as coreference resolution.', 'equations and details about the transformer system can be found in the original paper  #TAUTHOR_TAG and are out of the scope of this paper.', 'for the definition of the vocabulary to be used as input for the neural network, we used the sub - word mechanism from tensor2tensor package, which is similar to bytepair encoding ( bpe ) from  #AUTHOR_TAG.', 'for the english - spanish language pair, two separate 32k sub - word vocabularies where extracted, while for spanish - catalan we extracted a single shared 32k sub - word vocabulary for both languages']",0
"['strongest systems presented recently  #TAUTHOR_TAG, as well as a gl']","['strongest systems presented recently  #TAUTHOR_TAG, as well as a glance of its differences with other popular neural machine translation architectures.', 'sequence - to - sequence recurrent models  #AUTHOR_TAG have been']","['of the strongest systems presented recently  #TAUTHOR_TAG, as well as a glance of its differences with other popular neural machine translation architectures.', 'sequence - to - sequence recurrent models  #AUTHOR_TAG have been the standard approach for neural machine translation,']","['section provides a brief high - level explanation of the neural mt approach that we are using as a baseline system, which is one of the strongest systems presented recently  #TAUTHOR_TAG, as well as a glance of its differences with other popular neural machine translation architectures.', 'sequence - to - sequence recurrent models  #AUTHOR_TAG have been the standard approach for neural machine translation, especially since the incorporation of attention mechanisms  #AUTHOR_TAG, which enables the system to learn to identify the information which is relevant for producing each word in the translation.', 'convolutional networks  #AUTHOR_TAG were the second paradigm to effectively approach sequence transduction tasks like machine translation.', 'in this paper we make use of the third paradigm for neural machine translation, proposed in  #TAUTHOR_TAG, namely the transformer architecture, which is based on a feed - forward encoder - decoder scheme with attention mechanisms.', 'the type of attention mechanism used in the system, referred to as multi - head attention, allows to train several attention modules in parallel, combining also self - attention with standard attention.', 'self - attention differs from standard attention in the use of the same sentence as input and trains over it allowing to solve issues as coreference resolution.', 'equations and details about the transformer system can be found in the original paper  #TAUTHOR_TAG and are out of the scope of this paper.', 'for the definition of the vocabulary to be used as input for the neural network, we used the sub - word mechanism from tensor2tensor package, which is similar to bytepair encoding ( bpe ) from  #AUTHOR_TAG.', 'for the english - spanish language pair, two separate 32k sub - word vocabularies where extracted, while for spanish - catalan we extracted a single shared 32k sub - word vocabulary for both languages']",1
"['strongest systems presented recently  #TAUTHOR_TAG, as well as a gl']","['strongest systems presented recently  #TAUTHOR_TAG, as well as a glance of its differences with other popular neural machine translation architectures.', 'sequence - to - sequence recurrent models  #AUTHOR_TAG have been']","['of the strongest systems presented recently  #TAUTHOR_TAG, as well as a glance of its differences with other popular neural machine translation architectures.', 'sequence - to - sequence recurrent models  #AUTHOR_TAG have been the standard approach for neural machine translation,']","['section provides a brief high - level explanation of the neural mt approach that we are using as a baseline system, which is one of the strongest systems presented recently  #TAUTHOR_TAG, as well as a glance of its differences with other popular neural machine translation architectures.', 'sequence - to - sequence recurrent models  #AUTHOR_TAG have been the standard approach for neural machine translation, especially since the incorporation of attention mechanisms  #AUTHOR_TAG, which enables the system to learn to identify the information which is relevant for producing each word in the translation.', 'convolutional networks  #AUTHOR_TAG were the second paradigm to effectively approach sequence transduction tasks like machine translation.', 'in this paper we make use of the third paradigm for neural machine translation, proposed in  #TAUTHOR_TAG, namely the transformer architecture, which is based on a feed - forward encoder - decoder scheme with attention mechanisms.', 'the type of attention mechanism used in the system, referred to as multi - head attention, allows to train several attention modules in parallel, combining also self - attention with standard attention.', 'self - attention differs from standard attention in the use of the same sentence as input and trains over it allowing to solve issues as coreference resolution.', 'equations and details about the transformer system can be found in the original paper  #TAUTHOR_TAG and are out of the scope of this paper.', 'for the definition of the vocabulary to be used as input for the neural network, we used the sub - word mechanism from tensor2tensor package, which is similar to bytepair encoding ( bpe ) from  #AUTHOR_TAG.', 'for the english - spanish language pair, two separate 32k sub - word vocabularies where extracted, while for spanish - catalan we extracted a single shared 32k sub - word vocabulary for both languages']",5
"['of the translations of the attentional architecture from  #TAUTHOR_TAG.', 'the english -']","['of the translations of the attentional architecture from  #TAUTHOR_TAG.', 'the english - to - spanish']","['are summarized in table 4, which shows the high quality of the translations of the attentional architecture from  #TAUTHOR_TAG.', 'the english - to - spanish translation obtains a bleu score of']","['results of each of the pivotal translation systems as well as the combined cascaded translation are summarized in table 4, which shows the high quality of the translations of the attentional architecture from  #TAUTHOR_TAG.', 'the english - to - spanish translation obtains a bleu score of 46. 55 in the test set of the wmt biomedical test set while the spanish - to - catalan translation obtains a bleu score of 86. 89 in the el periodico test set.', 'the cascaded translation achives a bleu score of 41. 38 in the translated wmt biometical test set.', 'all bleu scores are case - sensitive and where obtained with script t2t - bleu from the tensor2tensor framework, whose results are equivalent to those from mteval - v14. pl from the moses package.', 'in order to illustrate the quality of the cascaded translations quality, some sample translations are shown in table 3']",7
"[') coherence theories  #TAUTHOR_TAG, e. g.,']","['of textual cues in sentences  #AUTHOR_TAG, ( 3 ) the topic of the sentences  #AUTHOR_TAG, ( 4 ) coherence theories  #TAUTHOR_TAG, e. g., centering theory, ( 5 ) content models  #AUTHOR_TAG, and ( 6 ) ordering ( s ) in the underlying']","[') coherence theories  #TAUTHOR_TAG, e. g.,']","['', 'past research has investigated a wide range of aspects pertaining to the ordering of sentences in text.', 'the most prominent approaches include : ( 1 ) temporal ordering in terms of publication date  #AUTHOR_TAG, ( 2 ) temporal ordering in terms of textual cues in sentences  #AUTHOR_TAG, ( 3 ) the topic of the sentences  #AUTHOR_TAG, ( 4 ) coherence theories  #TAUTHOR_TAG, e. g., centering theory, ( 5 ) content models  #AUTHOR_TAG, and ( 6 ) ordering ( s ) in the underlying documents in the case of summarisation  #AUTHOR_TAG']",0
"[', paralleling  #TAUTHOR_TAG, our model has the following structure.', 'the data']","['into a ranking problem.', 'hence, paralleling  #TAUTHOR_TAG, our model has the following structure.', 'the data']","[', paralleling  #TAUTHOR_TAG, our model has the following structure.', 'the data']","['view coherence assessment, which we recast as a sentence ordering problem, as a machine learning problem using the feature representation discussed in section 2. 1.', 'it can be viewed as a ranking task because a text can only be more or less coherent than some other text.', 'the sentence ordering task used in this paper can easily be transformed into a ranking problem.', 'hence, paralleling  #TAUTHOR_TAG, our model has the following structure.', 'the data consists of alternative orderings ( x ij, x ik ) of the sentences of the same document d i.', 'in the training data, the preference ranking of the alternative orderings is known.', 'as a result, training consists of determining a parameter vector w that minimizes the number of violations of pairwise rankings in the training set, a problem which can be solved using svm constraint optimization  #AUTHOR_TAG.', 'the following section explores the features available for this optimization']",3
"['+ syntax + salience + and hmm - based content models are reproduced from  #TAUTHOR_TAG.', 'unknown at the']","['datasets.', 'the results for coreference + syntax + salience + and hmm - based content models are reproduced from  #TAUTHOR_TAG.', 'unknown at the']","['+ syntax + salience + and hmm - based content models are reproduced from  #TAUTHOR_TAG.', 'unknown at the time of training.', 'as a result, model performance on out - of - domain texts is important']","['', 'the results for coreference + syntax + salience + and hmm - based content models are reproduced from  #TAUTHOR_TAG.', 'unknown at the time of training.', 'as a result, model performance on out - of - domain texts is important for summarization.', 'experiment 2 seeks to evaluate how well our model performs in such cases.', '']",3
"['+ syntax + salience + and hmm - based content models are reproduced from  #TAUTHOR_TAG.', 'unknown at the']","['datasets.', 'the results for coreference + syntax + salience + and hmm - based content models are reproduced from  #TAUTHOR_TAG.', 'unknown at the']","['+ syntax + salience + and hmm - based content models are reproduced from  #TAUTHOR_TAG.', 'unknown at the time of training.', 'as a result, model performance on out - of - domain texts is important']","['', 'the results for coreference + syntax + salience + and hmm - based content models are reproduced from  #TAUTHOR_TAG.', 'unknown at the time of training.', 'as a result, model performance on out - of - domain texts is important for summarization.', 'experiment 2 seeks to evaluate how well our model performs in such cases.', '']",3
"['allows us to straightforwardly discern the individual value of various features ( cf.', ' #TAUTHOR_TAG']","['allows us to straightforwardly discern the individual value of various features ( cf.', ' #TAUTHOR_TAG']","['', 'the advantage of this approach is that it allows us to straightforwardly discern the individual value of various features ( cf.', ' #TAUTHOR_TAG.', 'the methods used in this paper are mostly shallow with the exception of two aspects.', 'first,']","['', 'the advantage of this approach is that it allows us to straightforwardly discern the individual value of various features ( cf.', ' #TAUTHOR_TAG.', 'the methods used in this paper are mostly shallow with the exception of two aspects.', '']",7
"['features in this category are inspired by discourse entity - based accounts of local coherence.', 'yet, in contrast to  #TAUTHOR_TAG employ the syntactic properties of']","['features in this category are inspired by discourse entity - based accounts of local coherence.', 'yet, in contrast to  #TAUTHOR_TAG employ the syntactic properties of']","['features in this category are inspired by discourse entity - based accounts of local coherence.', 'yet, in contrast to  #TAUTHOR_TAG employ the syntactic properties of the respective occurrences, we reduce the accounts to']","['features in this category are inspired by discourse entity - based accounts of local coherence.', ""yet, in contrast to  #TAUTHOR_TAG employ the syntactic properties of the respective occurrences, we reduce the accounts to whether or not the entities occur in subsequent sentences ( similar to  #AUTHOR_TAG's nocb metric )."", 'we also investigate whether using only the information from the head of the noun group ( cf.', ' #TAUTHOR_TAG suffices, or whether performance is gained when allowing the whole noun group in order to determine similarity.', 'moreover, as indicated above, some of the noun group measures make use of wordnet synonym, hypernym, hyponym, antonym relationships.', 'for completeness, we also consider the effects of using verb groups and whole sentences as syntactic units of choice']",7
"['of content models and the baseline of the  #TAUTHOR_TAG model, the']","['of content models and the baseline of the  #TAUTHOR_TAG model, the']","['##ing.', 'taking into account the performance of content models and the baseline of the  #TAUTHOR_TAG model, the']","['paper investigated the effect of different features on sentence ordering.', 'while a set of features has been identified that works well individually as well as in combination on the accident dataset, the results on the earthquake and duc 2005 datasets are disappointing.', 'taking into account the performance of content models and the baseline of the  #TAUTHOR_TAG model, the most convincing explanation is that the sentence ordering in the earthquake datasets is based on some sort of topic notion, providing a variety of possible antecedents between which our model is thus far unable to distinguish without resorting to the original ( correct ) ordering.', 'future work will have to concentrate on this aspect of sentence ordering, as it appears to coincide with the structure of the summaries for the duc 2005 dataset']",7
"['features in this category are inspired by discourse entity - based accounts of local coherence.', 'yet, in contrast to  #TAUTHOR_TAG employ the syntactic properties of']","['features in this category are inspired by discourse entity - based accounts of local coherence.', 'yet, in contrast to  #TAUTHOR_TAG employ the syntactic properties of']","['features in this category are inspired by discourse entity - based accounts of local coherence.', 'yet, in contrast to  #TAUTHOR_TAG employ the syntactic properties of the respective occurrences, we reduce the accounts to']","['features in this category are inspired by discourse entity - based accounts of local coherence.', ""yet, in contrast to  #TAUTHOR_TAG employ the syntactic properties of the respective occurrences, we reduce the accounts to whether or not the entities occur in subsequent sentences ( similar to  #AUTHOR_TAG's nocb metric )."", 'we also investigate whether using only the information from the head of the noun group ( cf.', ' #TAUTHOR_TAG suffices, or whether performance is gained when allowing the whole noun group in order to determine similarity.', 'moreover, as indicated above, some of the noun group measures make use of wordnet synonym, hypernym, hyponym, antonym relationships.', 'for completeness, we also consider the effects of using verb groups and whole sentences as syntactic units of choice']",4
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","['three datasets used for the automatic evaluation in this paper are based on human - generated texts ( table 1 ).', 'the first two are the earthquake and accident datasets used by  #TAUTHOR_TAG.', 'each of these sets consists of 100 datasets in the training and test sets, respectively, as well as 20 random permutations for each text.', 'the third dataset is similar to the first two in that it contains original texts and random permutations.', 'in contrast to the other two sources, however, this dataset is based on the human summaries from duc 2005  #AUTHOR_TAG.', 'it comprises 300 human summaries on 50 document sets, resulting in a total of 6, 000 pairwise rankings split into training and test sets.', ""the source furthermore differs from  #TAUTHOR_TAG's datasets in that the content of each text is not based on one individual event ( an earthquake or accident ), but on more complex topics followed over a period of time ( e. g., the espionage case between gm and vw along with the various actions taken to resolve it )."", 'since the different document sets cover completely different topics the third dataset will mainly be used to evaluate the topic - independent properties of our model']",4
"['is below standard.', 'however, it seems that sentence ordering in that set is primarily defined by topics, as only content models perform well.', ' #TAUTHOR_TAG orderings towards']","['is below standard.', 'however, it seems that sentence ordering in that set is primarily defined by topics, as only content models perform well.', ' #TAUTHOR_TAG orderings towards']","['is below standard.', 'however, it seems that sentence ordering in that set is primarily defined by topics, as only content models perform well.', ' #TAUTHOR_TAG orderings towards the correct ordering']","['', 'to this end, we explore two different approaches.', 'the first set of features considers the distribution of entities within a fixed set of sentences, and captures in how many different sentences the entities occur.', 'the resulting score is the number of times the entities occur in n out of m sentences.', 'the second set only considers the similarity score from the current sentence and the other sentences within a certain range from the current sentence.', 'the score of this feature is the sum of the individual similarities.', 'table 5 clearly confirms that longer range relations are relevant to the assessment of the coherence of text.', 'an interesting difference between the two approaches is that sentence similarity only provides good results for neighboring sentences or sentences only one sentence apart, while the occurrence - counting method also works well over longer ranges.', 'having evaluated the potential contributions of the individual features and their modeling, we now use svms to combine the features into one comprehensive measure.', 'given the indications from the foregoing experiments, the results in table 6 are disappointing.', 'in particular, the performance on the earthquake dataset is below standard.', 'however, it seems that sentence ordering in that set is primarily defined by topics, as only content models perform well.', ' #TAUTHOR_TAG orderings towards the correct ordering']",4
"['.,  #TAUTHOR_TAG.', 'the features in this section explore the impact of such relations on the coherence of the overall document as well as']","['immediate successor.', 'however, the coherence of a text is clearly not only defined by direct relations, but also requires longer range relations between sentences ( e. g.,  #TAUTHOR_TAG.', 'the features in this section explore the impact of such relations on the coherence of the overall document as well as']","['.,  #TAUTHOR_TAG.', 'the features in this section explore the impact of such relations on the coherence of the overall document as well as the appropriate way of modeling them']","['group similarity features only capture the relation between a sentence and its immediate successor.', 'however, the coherence of a text is clearly not only defined by direct relations, but also requires longer range relations between sentences ( e. g.,  #TAUTHOR_TAG.', 'the features in this section explore the impact of such relations on the coherence of the overall document as well as the appropriate way of modeling them']",1
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","['three datasets used for the automatic evaluation in this paper are based on human - generated texts ( table 1 ).', 'the first two are the earthquake and accident datasets used by  #TAUTHOR_TAG.', 'each of these sets consists of 100 datasets in the training and test sets, respectively, as well as 20 random permutations for each text.', 'the third dataset is similar to the first two in that it contains original texts and random permutations.', 'in contrast to the other two sources, however, this dataset is based on the human summaries from duc 2005  #AUTHOR_TAG.', 'it comprises 300 human summaries on 50 document sets, resulting in a total of 6, 000 pairwise rankings split into training and test sets.', ""the source furthermore differs from  #TAUTHOR_TAG's datasets in that the content of each text is not based on one individual event ( an earthquake or accident ), but on more complex topics followed over a period of time ( e. g., the espionage case between gm and vw along with the various actions taken to resolve it )."", 'since the different document sets cover completely different topics the third dataset will mainly be used to evaluate the topic - independent properties of our model']",5
"['recently proposed composition model  #TAUTHOR_TAG that relies on both the negator and the argument', '. this model learns the syntax and semantics of the negator']","['recently proposed composition model  #TAUTHOR_TAG that relies on both the negator and the argument', '. this model learns the syntax and semantics of the negator']","['proposed composition model  #TAUTHOR_TAG that relies on both the negator and the argument', '. this model learns the syntax and semantics of the negator']","['', 'very good ′ ′ ). we first evaluate the modeling capabilities of two influential heuristics and show that they capture only very limited', ""regularity of negators'effect. we then extend the models to be dependent on the negators and demonstrate that such a simple extension can significantly improve the performance of fitting to the human"", 'annotated data. next, we evaluate a recently proposed composition model  #TAUTHOR_TAG that relies on both the negator and the argument', "". this model learns the syntax and semantics of the negator's argument with a"", 'recursive neural network. this approach performs significantly better than those mentioned above. in addition, we explicitly incorporate the prior sentiment of the argument and observe that this information helps reduce fitting errors.', '1 the sentiment values have been linearly rescaled from the original range [ 0, 1 ] to [ - 0. 5, 0', '. 5 ] ; in the figure a negative or positive value corresponds to a negative or a positive sentiment respectively ; zero means neutral. the negator list will be discussed later in the paper. 2 similar distribution is observed in other data such as tweets  #AUTHOR_TAG']",0
"['from a semantic - composition perspective  #TAUTHOR_TAG, which achieved']","['analysis from a semantic - composition perspective  #TAUTHOR_TAG, which achieved']","['a semantic - composition perspective  #TAUTHOR_TAG, which achieved the']","['', ' #AUTHOR_TAG and  #AUTHOR_TAG. negation modeling for sentiment an early yet influential reversing assumption conjectures that a negator', 'reverses the sign of the sentiment value of the modified text  #AUTHOR_TAG, e. g., from + 0', '. 5 to - 0. 5, or vice versa. a different hypothesis, called the shifting hypothesis in this', 'paper, assumes that negators change the sentiment values by a constant amount  #AUTHOR_TAG. other approaches to negation modeling have been', 'discussed in  #AUTHOR_TAG. in the process of semantic composition, the effect of negators could depend on the syntax and semantics of the text spans they modify. the approaches of modeling this include bag - of - wordbased models. for example, in the work of', ' #AUTHOR_TAG, a feature not good will be created if the word good is encountered within a predefined range after a negator. there exist', 'different ways of incorporating more complicated syntactic and semantic information.', 'much recent work considers sentiment analysis from a semantic - composition perspective  #TAUTHOR_TAG, which achieved the state - of - the - art performance.', ' #AUTHOR_TAG used a collection of hand - written compositional rules to assign sentiment values', 'to different granularities of text spans.  #AUTHOR_TAG proposed a learning - based framework. the more recent work of  #TAUTHOR_TAG proposed models based on recursive neural networks that do', 'not rely on any heuristic rules. such models work in a bottom', '- up fashion over the parse tree of a sentence to infer the sentiment label of the sentence as a composition of the sentiment expressed by', 'its constituting parts. the approach leverages a principled method, the forward and backward propagation, to learn a', '']",0
"['from a semantic - composition perspective  #TAUTHOR_TAG, which achieved']","['analysis from a semantic - composition perspective  #TAUTHOR_TAG, which achieved']","['a semantic - composition perspective  #TAUTHOR_TAG, which achieved the']","['', ' #AUTHOR_TAG and  #AUTHOR_TAG. negation modeling for sentiment an early yet influential reversing assumption conjectures that a negator', 'reverses the sign of the sentiment value of the modified text  #AUTHOR_TAG, e. g., from + 0', '. 5 to - 0. 5, or vice versa. a different hypothesis, called the shifting hypothesis in this', 'paper, assumes that negators change the sentiment values by a constant amount  #AUTHOR_TAG. other approaches to negation modeling have been', 'discussed in  #AUTHOR_TAG. in the process of semantic composition, the effect of negators could depend on the syntax and semantics of the text spans they modify. the approaches of modeling this include bag - of - wordbased models. for example, in the work of', ' #AUTHOR_TAG, a feature not good will be created if the word good is encountered within a predefined range after a negator. there exist', 'different ways of incorporating more complicated syntactic and semantic information.', 'much recent work considers sentiment analysis from a semantic - composition perspective  #TAUTHOR_TAG, which achieved the state - of - the - art performance.', ' #AUTHOR_TAG used a collection of hand - written compositional rules to assign sentiment values', 'to different granularities of text spans.  #AUTHOR_TAG proposed a learning - based framework. the more recent work of  #TAUTHOR_TAG proposed models based on recursive neural networks that do', 'not rely on any heuristic rules. such models work in a bottom', '- up fashion over the parse tree of a sentence to infer the sentiment label of the sentence as a composition of the sentiment expressed by', 'its constituting parts. the approach leverages a principled method, the forward and backward propagation, to learn a', '']",0
"['be found in  #TAUTHOR_TAG.', 'as shown in']","['be found in  #TAUTHOR_TAG.', 'as shown in']","['compositional sentiment analysis.', 'for completeness, we briefly review it here.', 'more details can be found in  #TAUTHOR_TAG.', 'as shown in']","['recursive neural tensor network ( rntn ) is a specific form of feed - forward neural network based on syntactic ( phrasal - structure ) parse tree to conduct compositional sentiment analysis.', 'for completeness, we briefly review it here.', 'more details can be found in  #TAUTHOR_TAG.', '']",0
"[' #TAUTHOR_TAG has manually annotated, real - valued sentiment values for']","[' #TAUTHOR_TAG has manually annotated, real - valued sentiment values for']","[' #TAUTHOR_TAG has manually annotated, real - valued sentiment values for all phrases in parse trees.', 'this provides us with the training and evaluation data to study the effect of negators with syntax and semantics of different complexity in a natural setting.', 'the data contain around']","['as described earlier, the stanford sentiment treebank  #TAUTHOR_TAG has manually annotated, real - valued sentiment values for all phrases in parse trees.', 'this provides us with the training and evaluation data to study the effect of negators with syntax and semantics of different complexity in a natural setting.', 'the data contain around 11, 800 sentences from movie reviews that were originally collected by  #AUTHOR_TAG.', '']",0
"['recently proposed composition model  #TAUTHOR_TAG that relies on both the negator and the argument', '. this model learns the syntax and semantics of the negator']","['recently proposed composition model  #TAUTHOR_TAG that relies on both the negator and the argument', '. this model learns the syntax and semantics of the negator']","['proposed composition model  #TAUTHOR_TAG that relies on both the negator and the argument', '. this model learns the syntax and semantics of the negator']","['', 'very good ′ ′ ). we first evaluate the modeling capabilities of two influential heuristics and show that they capture only very limited', ""regularity of negators'effect. we then extend the models to be dependent on the negators and demonstrate that such a simple extension can significantly improve the performance of fitting to the human"", 'annotated data. next, we evaluate a recently proposed composition model  #TAUTHOR_TAG that relies on both the negator and the argument', "". this model learns the syntax and semantics of the negator's argument with a"", 'recursive neural network. this approach performs significantly better than those mentioned above. in addition, we explicitly incorporate the prior sentiment of the argument and observe that this information helps reduce fitting errors.', '1 the sentiment values have been linearly rescaled from the original range [ 0, 1 ] to [ - 0. 5, 0', '. 5 ] ; in the figure a negative or positive value corresponds to a negative or a positive sentiment respectively ; zero means neutral. the negator list will be discussed later in the paper. 2 similar distribution is observed in other data such as tweets  #AUTHOR_TAG']",5
"['from a semantic - composition perspective  #TAUTHOR_TAG, which achieved']","['analysis from a semantic - composition perspective  #TAUTHOR_TAG, which achieved']","['a semantic - composition perspective  #TAUTHOR_TAG, which achieved the']","['', ' #AUTHOR_TAG and  #AUTHOR_TAG. negation modeling for sentiment an early yet influential reversing assumption conjectures that a negator', 'reverses the sign of the sentiment value of the modified text  #AUTHOR_TAG, e. g., from + 0', '. 5 to - 0. 5, or vice versa. a different hypothesis, called the shifting hypothesis in this', 'paper, assumes that negators change the sentiment values by a constant amount  #AUTHOR_TAG. other approaches to negation modeling have been', 'discussed in  #AUTHOR_TAG. in the process of semantic composition, the effect of negators could depend on the syntax and semantics of the text spans they modify. the approaches of modeling this include bag - of - wordbased models. for example, in the work of', ' #AUTHOR_TAG, a feature not good will be created if the word good is encountered within a predefined range after a negator. there exist', 'different ways of incorporating more complicated syntactic and semantic information.', 'much recent work considers sentiment analysis from a semantic - composition perspective  #TAUTHOR_TAG, which achieved the state - of - the - art performance.', ' #AUTHOR_TAG used a collection of hand - written compositional rules to assign sentiment values', 'to different granularities of text spans.  #AUTHOR_TAG proposed a learning - based framework. the more recent work of  #TAUTHOR_TAG proposed models based on recursive neural networks that do', 'not rely on any heuristic rules. such models work in a bottom', '- up fashion over the parse tree of a sentence to infer the sentiment label of the sentence as a composition of the sentiment expressed by', 'its constituting parts. the approach leverages a principled method, the forward and backward propagation, to learn a', '']",5
"['recently by  #TAUTHOR_TAG, which has showed to']","['recently by  #TAUTHOR_TAG, which has showed to']","['the former, we adopt the recursive neural tensor network ( rntn ) proposed recently by  #TAUTHOR_TAG, which has showed to']","['##ators can interact with arguments in complex ways.', 'figure 1 shows the distribution of the effect of negators on sentiment without considering further semantics of the arguments.', 'the question then is that whether and how much incorporating further syntax and semantic information can help better fit or predict the negation effect.', 'above, we have considered the semantics of the negators.', 'below, we further make the models to be dependent on the arguments.', 'this can be written as :', 'in the formula, r ( w ) is a certain type of representation for the argument w and it models the semantics or / and syntax of the argument.', 'there exist different ways of implementing r ( w ).', 'we consider two models in this study : one drops s ( w ) in equation 4 and directly models f ( w n, r ( w ) ).', 'that is, the non - uniform information shown in figure 1 is not directly modeled.', 'the other takes into account s ( w ) too.', 'for the former, we adopt the recursive neural tensor network ( rntn ) proposed recently by  #TAUTHOR_TAG, which has showed to achieve the state - of - the - art performance in sentiment analysis.', 'for the latter, we propose a prior sentimentenriched tensor network ( pstn ) to take into account the prior sentiment of the argument s ( w )']",5
"['negated phrase ; we map the output to real - valued scores based on the scale that  #TAUTHOR_TAG used to map real - valued sentiment scores to sentiment categories. specifically', ', we conduct the mapping with the']","['negated phrase ; we map the output to real - valued scores based on the scale that  #TAUTHOR_TAG used to map real - valued sentiment scores to sentiment categories. specifically', ', we conduct the mapping with the']","['are applied to predict the sentiment of the test data. the original rntn and the pstn predict', '5 - class sentiment for each negated phrase ; we map the output to real - valued scores based on the scale that  #TAUTHOR_TAG used to map real - valued sentiment scores to sentiment categories. specifically', ', we conduct the mapping with the formula : p real i = y i ·']","['', 'd ] and δ p 2, down [ d + 1 : 2d ],', 'respectively. following this notation, we have the error message for the two children of p 2, provided that we have the δ', 'p 2, down : the incoming error message of node a can be calculated similarly. finally, we can finish the above equations with the following formula for computing δ p 2, down : after the models are trained,', 'they are applied to predict the sentiment of the test data. the original rntn and the pstn predict', '5 - class sentiment for each negated phrase ; we map the output to real - valued scores based on the scale that  #TAUTHOR_TAG used to map real - valued sentiment scores to sentiment categories. specifically', ', we conduct the mapping with the formula : p real i = y i · [ 0', '. 1 0. 3 0. 5 0. 7 0. 9 ] ; i. e., we calculate the dot product of', 'the posterior probability y i and the scaling vector. for example, if y i = [ 0. 5 0. 5 0 0', '0 ], meaning this phrase has a 0. 5 probability to be in the first category ( strong negative ) and 0. 5 for the second category (', '']",5
"['negated phrase ; we map the output to real - valued scores based on the scale that  #TAUTHOR_TAG used to map real - valued sentiment scores to sentiment categories. specifically', ', we conduct the mapping with the']","['negated phrase ; we map the output to real - valued scores based on the scale that  #TAUTHOR_TAG used to map real - valued sentiment scores to sentiment categories. specifically', ', we conduct the mapping with the']","['are applied to predict the sentiment of the test data. the original rntn and the pstn predict', '5 - class sentiment for each negated phrase ; we map the output to real - valued scores based on the scale that  #TAUTHOR_TAG used to map real - valued sentiment scores to sentiment categories. specifically', ', we conduct the mapping with the formula : p real i = y i ·']","['', 'd ] and δ p 2, down [ d + 1 : 2d ],', 'respectively. following this notation, we have the error message for the two children of p 2, provided that we have the δ', 'p 2, down : the incoming error message of node a can be calculated similarly. finally, we can finish the above equations with the following formula for computing δ p 2, down : after the models are trained,', 'they are applied to predict the sentiment of the test data. the original rntn and the pstn predict', '5 - class sentiment for each negated phrase ; we map the output to real - valued scores based on the scale that  #TAUTHOR_TAG used to map real - valued sentiment scores to sentiment categories. specifically', ', we conduct the mapping with the formula : p real i = y i · [ 0', '. 1 0. 3 0. 5 0. 7 0. 9 ] ; i. e., we calculate the dot product of', 'the posterior probability y i and the scaling vector. for example, if y i = [ 0. 5 0. 5 0 0', '0 ], meaning this phrase has a 0. 5 probability to be in the first category ( strong negative ) and 0. 5 for the second category (', '']",5
"['negated phrase ; we map the output to real - valued scores based on the scale that  #TAUTHOR_TAG used to map real - valued sentiment scores to sentiment categories. specifically', ', we conduct the mapping with the']","['negated phrase ; we map the output to real - valued scores based on the scale that  #TAUTHOR_TAG used to map real - valued sentiment scores to sentiment categories. specifically', ', we conduct the mapping with the']","['are applied to predict the sentiment of the test data. the original rntn and the pstn predict', '5 - class sentiment for each negated phrase ; we map the output to real - valued scores based on the scale that  #TAUTHOR_TAG used to map real - valued sentiment scores to sentiment categories. specifically', ', we conduct the mapping with the formula : p real i = y i ·']","['', 'd ] and δ p 2, down [ d + 1 : 2d ],', 'respectively. following this notation, we have the error message for the two children of p 2, provided that we have the δ', 'p 2, down : the incoming error message of node a can be calculated similarly. finally, we can finish the above equations with the following formula for computing δ p 2, down : after the models are trained,', 'they are applied to predict the sentiment of the test data. the original rntn and the pstn predict', '5 - class sentiment for each negated phrase ; we map the output to real - valued scores based on the scale that  #TAUTHOR_TAG used to map real - valued sentiment scores to sentiment categories. specifically', ', we conduct the mapping with the formula : p real i = y i · [ 0', '. 1 0. 3 0. 5 0. 7 0. 9 ] ; i. e., we calculate the dot product of', 'the posterior probability y i and the scaling vector. for example, if y i = [ 0. 5 0. 5 0 0', '0 ], meaning this phrase has a 0. 5 probability to be in the first category ( strong negative ) and 0. 5 for the second category (', '']",5
"['negated phrase ; we map the output to real - valued scores based on the scale that  #TAUTHOR_TAG used to map real - valued sentiment scores to sentiment categories. specifically', ', we conduct the mapping with the']","['negated phrase ; we map the output to real - valued scores based on the scale that  #TAUTHOR_TAG used to map real - valued sentiment scores to sentiment categories. specifically', ', we conduct the mapping with the']","['are applied to predict the sentiment of the test data. the original rntn and the pstn predict', '5 - class sentiment for each negated phrase ; we map the output to real - valued scores based on the scale that  #TAUTHOR_TAG used to map real - valued sentiment scores to sentiment categories. specifically', ', we conduct the mapping with the formula : p real i = y i ·']","['', 'd ] and δ p 2, down [ d + 1 : 2d ],', 'respectively. following this notation, we have the error message for the two children of p 2, provided that we have the δ', 'p 2, down : the incoming error message of node a can be calculated similarly. finally, we can finish the above equations with the following formula for computing δ p 2, down : after the models are trained,', 'they are applied to predict the sentiment of the test data. the original rntn and the pstn predict', '5 - class sentiment for each negated phrase ; we map the output to real - valued scores based on the scale that  #TAUTHOR_TAG used to map real - valued sentiment scores to sentiment categories. specifically', ', we conduct the mapping with the formula : p real i = y i · [ 0', '. 1 0. 3 0. 5 0. 7 0. 9 ] ; i. e., we calculate the dot product of', 'the posterior probability y i and the scaling vector. for example, if y i = [ 0. 5 0. 5 0 0', '0 ], meaning this phrase has a 0. 5 probability to be in the first category ( strong negative ) and 0. 5 for the second category (', '']",5
"[' #TAUTHOR_TAG has manually annotated, real - valued sentiment values for']","[' #TAUTHOR_TAG has manually annotated, real - valued sentiment values for']","[' #TAUTHOR_TAG has manually annotated, real - valued sentiment values for all phrases in parse trees.', 'this provides us with the training and evaluation data to study the effect of negators with syntax and semantics of different complexity in a natural setting.', 'the data contain around']","['as described earlier, the stanford sentiment treebank  #TAUTHOR_TAG has manually annotated, real - valued sentiment values for all phrases in parse trees.', 'this provides us with the training and evaluation data to study the effect of negators with syntax and semantics of different complexity in a natural setting.', 'the data contain around 11, 800 sentences from movie reviews that were originally collected by  #AUTHOR_TAG.', '']",5
"[' #TAUTHOR_TAG has manually annotated, real - valued sentiment values for']","[' #TAUTHOR_TAG has manually annotated, real - valued sentiment values for']","[' #TAUTHOR_TAG has manually annotated, real - valued sentiment values for all phrases in parse trees.', 'this provides us with the training and evaluation data to study the effect of negators with syntax and semantics of different complexity in a natural setting.', 'the data contain around']","['as described earlier, the stanford sentiment treebank  #TAUTHOR_TAG has manually annotated, real - valued sentiment values for all phrases in parse trees.', 'this provides us with the training and evaluation data to study the effect of negators with syntax and semantics of different complexity in a natural setting.', 'the data contain around 11, 800 sentences from movie reviews that were originally collected by  #AUTHOR_TAG.', '']",5
"['corpora  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG introduced multi - instance learning ( mil )  #AUTHOR_TAG to treat multiple']","['corpora  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG introduced multi - instance learning ( mil )  #AUTHOR_TAG to treat multiple mentions of target entities in']","['- level corpora  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG introduced multi - instance learning ( mil )  #AUTHOR_TAG to treat multiple mentions of target entities in']","['', 'in order to extract inter - sentence relations, most approaches utilise distant supervision to automatically generate document - level corpora  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG introduced multi - instance learning ( mil )  #AUTHOR_TAG to treat multiple mentions of target entities in a document.', '']",0
"['', ' #TAUTHOR_TAG considered multi - instance']","['protein - drug - disease associations.', 'they restricted the relation candidates in up to two - span sentences.', ' #TAUTHOR_TAG considered multi - instance']","['drug - disease associations.', 'they restricted the relation candidates in up to two - span sentences.', ' #TAUTHOR_TAG considered multi - instance learning for']","['- sentence re is a recently introduced task.', ' #AUTHOR_TAG and  #AUTHOR_TAG used graph - based lstm networks for n - ary re in multiple sentences for protein - drug - disease associations.', 'they restricted the relation candidates in up to two - span sentences.', ' #TAUTHOR_TAG considered multi - instance learning for document - level re.', 'our work is different from  #TAUTHOR_TAG in that we replace transformer with a gcnn model for full - abstract encoding using non - local dependencies such as entity coreference.', 'gcnn was firstly proposed by  #AUTHOR_TAG and applied on citation networks and knowledge graph datasets.', 'it was later used for semantic role labelling  #AUTHOR_TAG, multi - document summarization  #AUTHOR_TAG and temporal relation extraction  #AUTHOR_TAG.', '']",0
"['corpora  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG introduced multi - instance learning ( mil )  #AUTHOR_TAG to treat multiple']","['corpora  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG introduced multi - instance learning ( mil )  #AUTHOR_TAG to treat multiple mentions of target entities in']","['- level corpora  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG introduced multi - instance learning ( mil )  #AUTHOR_TAG to treat multiple mentions of target entities in']","['', 'in order to extract inter - sentence relations, most approaches utilise distant supervision to automatically generate document - level corpora  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG introduced multi - instance learning ( mil )  #AUTHOR_TAG to treat multiple mentions of target entities in a document.', '']",3
['by applying mil  #TAUTHOR_TAG to'],['by applying mil  #TAUTHOR_TAG to'],['by applying mil  #TAUTHOR_TAG to'],"['', 'we describe the architecture of our proposed model in figure 2.', 'the model takes as input an entire abstract of scientific articles and two target entities with all their mentions in the input layer.', 'it then constructs a graph structure with words as nodes and labelled edges that correspond to local and non - local dependencies.', 'next, it encodes the graph structure using a stacked gcnn layer and classifies the relation between the target entities by applying mil  #TAUTHOR_TAG to aggregate all 1 the dataset is publicly available at http : / / nactem.', 'ac. uk / chr /. mention pair representations']",3
['affine pairwise scoring  #TAUTHOR_TAG'],['target mention pairs using bi - affine pairwise scoring  #TAUTHOR_TAG'],"['aggregate the predictions of all target mention pairs using bi - affine pairwise scoring  #TAUTHOR_TAG.', 'as shown in figure 2,']","['each target entity can have multiple mentions in a document, we employ a multi - instance learning ( mil ) - based classification scheme to aggregate the predictions of all target mention pairs using bi - affine pairwise scoring  #TAUTHOR_TAG.', '']",3
"['hypernym filtering similar to  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'in the chr dataset, both directions were generated for']","['hypernym filtering similar to  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'in the chr dataset, both directions were generated for']","['( self - relations ).', 'for the cdr dataset, we performed hypernym filtering similar to  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'in the chr dataset, both directions were generated for each candidate chemical pair as chemicals can be either a reactant ( first argument ) or a product ( second argument ) in an interaction']","['', 'this technique results in less negative pairs.', 'we ignored entities that were not grounded to a known kb id and removed relations between the same entity ( self - relations ).', 'for the cdr dataset, we performed hypernym filtering similar to  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'in the chr dataset, both directions were generated for each candidate chemical pair as chemicals can be either a reactant ( first argument ) or a product ( second argument ) in an interaction']",3
"['maximum entropy  #AUTHOR_TAG, piece - wise cnn  #AUTHOR_TAG and transformer  #TAUTHOR_TAG']","['maximum entropy  #AUTHOR_TAG, piece - wise cnn  #AUTHOR_TAG and transformer  #TAUTHOR_TAG']","['maximum entropy  #AUTHOR_TAG, piece - wise cnn  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we additionally prepare and evaluate the following models : cnn - re, a re - implementation from  #AUTHOR_TAG and  #AUTHOR_TAG a ) and rn']","['the cdr dataset, we compare with five stateof - the - art models : svm, ensemble of feature - based and neural - based models  #AUTHOR_TAG a ), cnn and maximum entropy  #AUTHOR_TAG, piece - wise cnn  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we additionally prepare and evaluate the following models : cnn - re, a re - implementation from  #AUTHOR_TAG and  #AUTHOR_TAG a ) and rnn - re, a reimplementation from  #AUTHOR_TAG.', 'in all models we use bi - affine pairwise scoring to detect relations']",3
"['corpora  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG introduced multi - instance learning ( mil )  #AUTHOR_TAG to treat multiple']","['corpora  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG introduced multi - instance learning ( mil )  #AUTHOR_TAG to treat multiple mentions of target entities in']","['- level corpora  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG introduced multi - instance learning ( mil )  #AUTHOR_TAG to treat multiple mentions of target entities in']","['', 'in order to extract inter - sentence relations, most approaches utilise distant supervision to automatically generate document - level corpora  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG introduced multi - instance learning ( mil )  #AUTHOR_TAG to treat multiple mentions of target entities in a document.', '']",5
['by applying mil  #TAUTHOR_TAG to'],['by applying mil  #TAUTHOR_TAG to'],['by applying mil  #TAUTHOR_TAG to'],"['', 'we describe the architecture of our proposed model in figure 2.', 'the model takes as input an entire abstract of scientific articles and two target entities with all their mentions in the input layer.', 'it then constructs a graph structure with words as nodes and labelled edges that correspond to local and non - local dependencies.', 'next, it encodes the graph structure using a stacked gcnn layer and classifies the relation between the target entities by applying mil  #TAUTHOR_TAG to aggregate all 1 the dataset is publicly available at http : / / nactem.', 'ac. uk / chr /. mention pair representations']",5
['affine pairwise scoring  #TAUTHOR_TAG'],['target mention pairs using bi - affine pairwise scoring  #TAUTHOR_TAG'],"['aggregate the predictions of all target mention pairs using bi - affine pairwise scoring  #TAUTHOR_TAG.', 'as shown in figure 2,']","['each target entity can have multiple mentions in a document, we employ a multi - instance learning ( mil ) - based classification scheme to aggregate the predictions of all target mention pairs using bi - affine pairwise scoring  #TAUTHOR_TAG.', '']",5
"['hypernym filtering similar to  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'in the chr dataset, both directions were generated for']","['hypernym filtering similar to  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'in the chr dataset, both directions were generated for']","['( self - relations ).', 'for the cdr dataset, we performed hypernym filtering similar to  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'in the chr dataset, both directions were generated for each candidate chemical pair as chemicals can be either a reactant ( first argument ) or a product ( second argument ) in an interaction']","['', 'this technique results in less negative pairs.', 'we ignored entities that were not grounded to a known kb id and removed relations between the same entity ( self - relations ).', 'for the cdr dataset, we performed hypernym filtering similar to  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'in the chr dataset, both directions were generated for each candidate chemical pair as chemicals can be either a reactant ( first argument ) or a product ( second argument ) in an interaction']",5
"['maximum entropy  #AUTHOR_TAG, piece - wise cnn  #AUTHOR_TAG and transformer  #TAUTHOR_TAG']","['maximum entropy  #AUTHOR_TAG, piece - wise cnn  #AUTHOR_TAG and transformer  #TAUTHOR_TAG']","['maximum entropy  #AUTHOR_TAG, piece - wise cnn  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we additionally prepare and evaluate the following models : cnn - re, a re - implementation from  #AUTHOR_TAG and  #AUTHOR_TAG a ) and rn']","['the cdr dataset, we compare with five stateof - the - art models : svm, ensemble of feature - based and neural - based models  #AUTHOR_TAG a ), cnn and maximum entropy  #AUTHOR_TAG, piece - wise cnn  #AUTHOR_TAG and transformer  #TAUTHOR_TAG.', 'we additionally prepare and evaluate the following models : cnn - re, a re - implementation from  #AUTHOR_TAG and  #AUTHOR_TAG a ) and rnn - re, a reimplementation from  #AUTHOR_TAG.', 'in all models we use bi - affine pairwise scoring to detect relations']",5
"['used 100 - dimentional word embeddings trained on pubmed with glove  #AUTHOR_TAG th et al., 2015 ).', 'unlike  #TAUTHOR_TAG,']","['used 100 - dimentional word embeddings trained on pubmed with glove  #AUTHOR_TAG th et al., 2015 ).', 'unlike  #TAUTHOR_TAG,']","['used 100 - dimentional word embeddings trained on pubmed with glove  #AUTHOR_TAG th et al., 2015 ).', 'unlike  #TAUTHOR_TAG, we used the pre - trained word embeddings in place of sub - word embeddings to align with our word graphs.', 'due to the size of the cdr dataset, we merged the training and development sets to']","['used 100 - dimentional word embeddings trained on pubmed with glove  #AUTHOR_TAG th et al., 2015 ).', 'unlike  #TAUTHOR_TAG, we used the pre - trained word embeddings in place of sub - word embeddings to align with our word graphs.', 'due to the size of the cdr dataset, we merged the training and development sets to train the models, similarly to  #AUTHOR_TAG a ) and  #AUTHOR_TAG.', 'we report the performance as the average of five runs with different parameter initialisation seeds in terms of precision ( p ), recall ( r ) and f1 - score.', 'we used the frequencies of the edge types in the training set to choose the top - n edges in section 2. 3.', 'we refer to the supplementary materials for the details of the training and hyper - parameter settings']",4
"['', ' #TAUTHOR_TAG considered multi - instance']","['protein - drug - disease associations.', 'they restricted the relation candidates in up to two - span sentences.', ' #TAUTHOR_TAG considered multi - instance']","['drug - disease associations.', 'they restricted the relation candidates in up to two - span sentences.', ' #TAUTHOR_TAG considered multi - instance learning for']","['- sentence re is a recently introduced task.', ' #AUTHOR_TAG and  #AUTHOR_TAG used graph - based lstm networks for n - ary re in multiple sentences for protein - drug - disease associations.', 'they restricted the relation candidates in up to two - span sentences.', ' #TAUTHOR_TAG considered multi - instance learning for document - level re.', 'our work is different from  #TAUTHOR_TAG in that we replace transformer with a gcnn model for full - abstract encoding using non - local dependencies such as entity coreference.', 'gcnn was firstly proposed by  #AUTHOR_TAG and applied on citation networks and knowledge graph datasets.', 'it was later used for semantic role labelling  #AUTHOR_TAG, multi - document summarization  #AUTHOR_TAG and temporal relation extraction  #AUTHOR_TAG.', '']",4
"['used 100 - dimentional word embeddings trained on pubmed with glove  #AUTHOR_TAG th et al., 2015 ).', 'unlike  #TAUTHOR_TAG,']","['used 100 - dimentional word embeddings trained on pubmed with glove  #AUTHOR_TAG th et al., 2015 ).', 'unlike  #TAUTHOR_TAG,']","['used 100 - dimentional word embeddings trained on pubmed with glove  #AUTHOR_TAG th et al., 2015 ).', 'unlike  #TAUTHOR_TAG, we used the pre - trained word embeddings in place of sub - word embeddings to align with our word graphs.', 'due to the size of the cdr dataset, we merged the training and development sets to']","['used 100 - dimentional word embeddings trained on pubmed with glove  #AUTHOR_TAG th et al., 2015 ).', 'unlike  #TAUTHOR_TAG, we used the pre - trained word embeddings in place of sub - word embeddings to align with our word graphs.', 'due to the size of the cdr dataset, we merged the training and development sets to train the models, similarly to  #AUTHOR_TAG a ) and  #AUTHOR_TAG.', 'we report the performance as the average of five runs with different parameter initialisation seeds in terms of precision ( p ), recall ( r ) and f1 - score.', 'we used the frequencies of the edge types in the training set to choose the top - n edges in section 2. 3.', 'we refer to the supplementary materials for the details of the training and hyper - parameter settings']",6
"['', ' #TAUTHOR_TAG considered multi - instance']","['protein - drug - disease associations.', 'they restricted the relation candidates in up to two - span sentences.', ' #TAUTHOR_TAG considered multi - instance']","['drug - disease associations.', 'they restricted the relation candidates in up to two - span sentences.', ' #TAUTHOR_TAG considered multi - instance learning for']","['- sentence re is a recently introduced task.', ' #AUTHOR_TAG and  #AUTHOR_TAG used graph - based lstm networks for n - ary re in multiple sentences for protein - drug - disease associations.', 'they restricted the relation candidates in up to two - span sentences.', ' #TAUTHOR_TAG considered multi - instance learning for document - level re.', 'our work is different from  #TAUTHOR_TAG in that we replace transformer with a gcnn model for full - abstract encoding using non - local dependencies such as entity coreference.', 'gcnn was firstly proposed by  #AUTHOR_TAG and applied on citation networks and knowledge graph datasets.', 'it was later used for semantic role labelling  #AUTHOR_TAG, multi - document summarization  #AUTHOR_TAG and temporal relation extraction  #AUTHOR_TAG.', '']",6
['mboshi - francais  #TAUTHOR_TAG dans'],"['', 'nous traduisons un corpus parallele bilingue mboshi - francais  #TAUTHOR_TAG dans']","['', 'nous traduisons un corpus parallele bilingue mboshi - francais  #TAUTHOR_TAG dans']","['', ""nous traduisons un corpus parallele bilingue mboshi - francais  #TAUTHOR_TAG dans quatre autres langues, et evaluons l'impact de la langue de traduction sur une tache de segmentation en mots non supervisee."", '']",6
['mboshi - francais  #TAUTHOR_TAG dans'],"['', 'nous traduisons un corpus parallele bilingue mboshi - francais  #TAUTHOR_TAG dans']","['', 'nous traduisons un corpus parallele bilingue mboshi - francais  #TAUTHOR_TAG dans']","['', ""nous traduisons un corpus parallele bilingue mboshi - francais  #TAUTHOR_TAG dans quatre autres langues, et evaluons l'impact de la langue de traduction sur une tache de segmentation en mots non supervisee."", '']",6
"['parallel corpus  #TAUTHOR_TAG, fruit of the documentation process of mboshi ( bantu c25 ), an endangered language spoken in congo - brazzaville.', 'the corpus contains 5, 130 utterances, for which it provides audio, transcriptions and translations in french.', 'we translate the french into four other well - resource']","['multilingual mboshi parallel corpus : in this work we extend the bilingual mboshi - french parallel corpus  #TAUTHOR_TAG, fruit of the documentation process of mboshi ( bantu c25 ), an endangered language spoken in congo - brazzaville.', 'the corpus contains 5, 130 utterances, for which it provides audio, transcriptions and translations in french.', 'we translate the french into four other well - resourced languages through the use of the deepl translator.', '1 the languages added to the dataset are : english, german, portuguese and spanish.', '']","['multilingual mboshi parallel corpus : in this work we extend the bilingual mboshi - french parallel corpus  #TAUTHOR_TAG, fruit of the documentation process of mboshi ( bantu c25 ), an endangered language spoken in congo - brazzaville.', 'the corpus contains 5, 130 utterances, for which it provides audio, transcriptions and translations in french.', 'we translate the french into four other well - resourced languages through the use of the deepl translator.', '1 the languages added to the dataset are : english, german, portuguese and spanish.', '']","['multilingual mboshi parallel corpus : in this work we extend the bilingual mboshi - french parallel corpus  #TAUTHOR_TAG, fruit of the documentation process of mboshi ( bantu c25 ), an endangered language spoken in congo - brazzaville.', 'the corpus contains 5, 130 utterances, for which it provides audio, transcriptions and translations in french.', 'we translate the french into four other well - resourced languages through the use of the deepl translator.', '1 the languages added to the dataset are : english, german, portuguese and spanish.', 'table 1 shows some statistics for the produced multilingual mboshi parallel corpus.', '2 bilingual unsupervised word segmentation / discovery approach : we use the bilingual neuralbased unsupervised word segmentation ( uws ) approach from  #AUTHOR_TAG to discover words in mboshi.', 'in this approach, neural machine translation ( nmt ) models are trained between language pairs, using as source language the translation ( word - level ) and as target, the language to document ( unsegmented phonemic sequence ).', 'due to the attention mechanism present in these networks  #AUTHOR_TAG, posterior to training, it is possible to retrieve soft - alignment probability matrices between source and target sequences.', 'these matrices give us sentence - level source - to - target alignment information, and by using it for clustering neighbor phonemes aligned to the same translation word, we are able to create segmentation in the target side.', 'the product of this approach is a set of ( discovered - units, translation words ) pairs.', 'multilingual leveraging : in this work we apply two simple methods for including multilingual information into the bilingual models from  #AUTHOR_TAG.', 'the first one, multilingual voting, consists of merging the information learned by models trained with different language pairs by performing a voting over the final discovered boundaries.', 'the voting is performed by applying an agreement threshold t over the output boundaries.', 'this threshold balances between accepting all boundaries from all the bilingual models ( zero agreement ) and accepting only input boundaries discovered by all these models ( total agreement ).', 'the second method is ane selection.', 'for every language pair and aligned sentence in the dataset, a soft - alignment probability matrix is generated.', 'we use average normalized entropy ( ane )  #AUTHOR_TAG a ) computed over these matrices for selecting the most confident one for segmenting each phoneme sequence.', 'this exploits the idea that models trained on different language pairs will have language - related behavior, thus differing on the resulting alignment and segmentation over the same phoneme sequence']",6
"['language  #TAUTHOR_TAG.', 'moreover,']","['the collected recordings, and such parallel corpora have been recently created by aligning the collected audio with translations in a well - resourced language  #TAUTHOR_TAG.', 'moreover,']","['the collected recordings, and such parallel corpora have been recently created by aligning the collected audio with translations in a well - resourced language  #TAUTHOR_TAG.', 'moreover,']","['cambridge handbook of endangered languages  #AUTHOR_TAG estimates that at least half of the 7, 000 languages currently spoken worldwide will no longer exist by the end of this century.', 'for these endangered languages, data collection campaigns have to accommodate the challenge that many of them are from oral tradition, and producing transcriptions is costly.', 'this transcription bottleneck problem can be handled by translating into a widely spoken language to ensure subsequent interpretability of the collected recordings, and such parallel corpora have been recently created by aligning the collected audio with translations in a well - resourced language  #TAUTHOR_TAG.', 'moreover, some linguists suggested that more than one translation should be collected to capture deeper layers of meaning  #AUTHOR_TAG.', 'this work is a contribution to the computational language documentation ( cld ) research field, that aims to replace part of the manual steps performed by linguists during language documentation initiatives by automatic approaches.', 'here we investigate the unsupervised word discovery and segmentation task, using the bilingual - rooted approach from  #AUTHOR_TAG.', 'there, words in the well - resourced language are aligned to unsegmented phonemes in the endangered language in order to identify group of phonemes, and to cluster them into word - like units.', 'we experiment with the mboshi - french parallel corpus, translating the french text into four other well - resourced languages in order to investigate language impact in this cld approach.', 'our results hint that this language impact exists, and that models based on different languages will output different word - like units']",0
"['iteratively  #TAUTHOR_TAG.', 'this']","['iteratively  #TAUTHOR_TAG.', 'this']","['refine model output iteratively  #TAUTHOR_TAG.', 'this work pursues this iterative approach to non - autoreg']","['- of - the - art neural machine translation systems use autoregressive decoding where words are predicted one - byone conditioned on all previous words  #AUTHOR_TAG.', 'non - autoregressive machine translation ( nat,  #AUTHOR_TAG ), on the other hand, generates all words in one shot and speeds up decoding at the expense of performance drop.', 'parallel decoding results in conditional independence and prevents the model from properly capturing highly multimodal distribution of target translations  #AUTHOR_TAG.', 'one way to remedy this fundamental problem is to refine model output iteratively  #TAUTHOR_TAG.', 'this work pursues this iterative approach to non - autoregressive translation.', '1 in this work, we propose a transformer - based architecture with attention masking, which we call disentangled context ( disco ) transformer, and use it for non - autoregressive decoding.', 'specifically, our disco transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words.', 'unlike the masked language models  #TAUTHOR_TAG where the model only predicts the masked words, the disco transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large.', 'we also introduce a new inference algorithm for iterative parallel decoding, parallel easy - first, where each word is predicted by attending to the words that the model is more confident about.', '']",0
"['iteratively  #TAUTHOR_TAG.', 'this']","['iteratively  #TAUTHOR_TAG.', 'this']","['refine model output iteratively  #TAUTHOR_TAG.', 'this work pursues this iterative approach to non - autoreg']","['- of - the - art neural machine translation systems use autoregressive decoding where words are predicted one - byone conditioned on all previous words  #AUTHOR_TAG.', 'non - autoregressive machine translation ( nat,  #AUTHOR_TAG ), on the other hand, generates all words in one shot and speeds up decoding at the expense of performance drop.', 'parallel decoding results in conditional independence and prevents the model from properly capturing highly multimodal distribution of target translations  #AUTHOR_TAG.', 'one way to remedy this fundamental problem is to refine model output iteratively  #TAUTHOR_TAG.', 'this work pursues this iterative approach to non - autoregressive translation.', '1 in this work, we propose a transformer - based architecture with attention masking, which we call disentangled context ( disco ) transformer, and use it for non - autoregressive decoding.', 'specifically, our disco transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words.', 'unlike the masked language models  #TAUTHOR_TAG where the model only predicts the masked words, the disco transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large.', 'we also introduce a new inference algorithm for iterative parallel decoding, parallel easy - first, where each word is predicted by attending to the words that the model is more confident about.', '']",0
"['iteratively  #TAUTHOR_TAG.', 'this']","['iteratively  #TAUTHOR_TAG.', 'this']","['refine model output iteratively  #TAUTHOR_TAG.', 'this work pursues this iterative approach to non - autoreg']","['- of - the - art neural machine translation systems use autoregressive decoding where words are predicted one - byone conditioned on all previous words  #AUTHOR_TAG.', 'non - autoregressive machine translation ( nat,  #AUTHOR_TAG ), on the other hand, generates all words in one shot and speeds up decoding at the expense of performance drop.', 'parallel decoding results in conditional independence and prevents the model from properly capturing highly multimodal distribution of target translations  #AUTHOR_TAG.', 'one way to remedy this fundamental problem is to refine model output iteratively  #TAUTHOR_TAG.', 'this work pursues this iterative approach to non - autoregressive translation.', '1 in this work, we propose a transformer - based architecture with attention masking, which we call disentangled context ( disco ) transformer, and use it for non - autoregressive decoding.', 'specifically, our disco transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words.', 'unlike the masked language models  #TAUTHOR_TAG where the model only predicts the masked words, the disco transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large.', 'we also introduce a new inference algorithm for iterative parallel decoding, parallel easy - first, where each word is predicted by attending to the words that the model is more confident about.', '']",0
"['con - ditional masked language model ( cmlm,  #TAUTHOR_TAG predicts randomly masked target tokens']","['con - ditional masked language model ( cmlm,  #TAUTHOR_TAG predicts randomly masked target tokens']","['contextual word representations  #AUTHOR_TAG, a con - ditional masked language model ( cmlm,  #TAUTHOR_TAG predicts randomly masked target tokens y mask given']","['to masked language models for contextual word representations  #AUTHOR_TAG, a con - ditional masked language model ( cmlm,  #TAUTHOR_TAG predicts randomly masked target tokens y mask given a source text x and the rest of the target tokens y obs.', 'namely, for every sentence pair in bitext x and y,', 'where rs denotes random sampling of masked tokens.', '2 cmlms have proven successful in parallel decoding for machine translation  #TAUTHOR_TAG, video captioning  #AUTHOR_TAG a ), and speech recognition  #AUTHOR_TAG.', 'however, the fundamental inefficiency with this masked language modeling objective is that the model can only be trained to predict a subset of the reference tokens ( y mask ) for each network pass unlike a normal autoregressive model where we predict all y from left to right.', 'to address this limitation, we propose a disentangled context ( disco ) objective.', 'the objective involves prediction of every token given an arbitrary ( thus disentangled ) subset of the other tokens.', 'for every 1 ≤ n ≤ n where | y | = n, we predict']",0
"['con - ditional masked language model ( cmlm,  #TAUTHOR_TAG predicts randomly masked target tokens']","['con - ditional masked language model ( cmlm,  #TAUTHOR_TAG predicts randomly masked target tokens']","['contextual word representations  #AUTHOR_TAG, a con - ditional masked language model ( cmlm,  #TAUTHOR_TAG predicts randomly masked target tokens y mask given']","['to masked language models for contextual word representations  #AUTHOR_TAG, a con - ditional masked language model ( cmlm,  #TAUTHOR_TAG predicts randomly masked target tokens y mask given a source text x and the rest of the target tokens y obs.', 'namely, for every sentence pair in bitext x and y,', 'where rs denotes random sampling of masked tokens.', '2 cmlms have proven successful in parallel decoding for machine translation  #TAUTHOR_TAG, video captioning  #AUTHOR_TAG a ), and speech recognition  #AUTHOR_TAG.', 'however, the fundamental inefficiency with this masked language modeling objective is that the model can only be trained to predict a subset of the reference tokens ( y mask ) for each network pass unlike a normal autoregressive model where we predict all y from left to right.', 'to address this limitation, we propose a disentangled context ( disco ) objective.', 'the objective involves prediction of every token given an arbitrary ( thus disentangled ) subset of the other tokens.', 'for every 1 ≤ n ≤ n where | y | = n, we predict']",0
['- predict is an iterative inference algorithm introduced in  #TAUTHOR_TAG'],['- predict is an iterative inference algorithm introduced in  #TAUTHOR_TAG'],['- predict is an iterative inference algorithm introduced in  #TAUTHOR_TAG'],"['- predict is an iterative inference algorithm introduced in  #TAUTHOR_TAG to decode a conditional masked language model ( cmlm ).', 'the target length n is first predicted, and then the algorithm iterates over two steps : mask where i t tokens with lowest probability are masked and predict where those masked tokens are updated given the other n − i t tokens.', 'the number of masked tokens i t decays from n with a constant rate over a fixed number of iterations t.', 'specifically, at iteration t,', 'this method is directly applicable to our disco transformer by fixing y n, t obs regardless of the position n']",0
"['iteratively  #TAUTHOR_TAG.', 'this']","['iteratively  #TAUTHOR_TAG.', 'this']","['refine model output iteratively  #TAUTHOR_TAG.', 'this work pursues this iterative approach to non - autoreg']","['- of - the - art neural machine translation systems use autoregressive decoding where words are predicted one - byone conditioned on all previous words  #AUTHOR_TAG.', 'non - autoregressive machine translation ( nat,  #AUTHOR_TAG ), on the other hand, generates all words in one shot and speeds up decoding at the expense of performance drop.', 'parallel decoding results in conditional independence and prevents the model from properly capturing highly multimodal distribution of target translations  #AUTHOR_TAG.', 'one way to remedy this fundamental problem is to refine model output iteratively  #TAUTHOR_TAG.', 'this work pursues this iterative approach to non - autoregressive translation.', '1 in this work, we propose a transformer - based architecture with attention masking, which we call disentangled context ( disco ) transformer, and use it for non - autoregressive decoding.', 'specifically, our disco transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words.', 'unlike the masked language models  #TAUTHOR_TAG where the model only predicts the masked words, the disco transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large.', 'we also introduce a new inference algorithm for iterative parallel decoding, parallel easy - first, where each word is predicted by attending to the words that the model is more confident about.', '']",1
"['iteratively  #TAUTHOR_TAG.', 'this']","['iteratively  #TAUTHOR_TAG.', 'this']","['refine model output iteratively  #TAUTHOR_TAG.', 'this work pursues this iterative approach to non - autoreg']","['- of - the - art neural machine translation systems use autoregressive decoding where words are predicted one - byone conditioned on all previous words  #AUTHOR_TAG.', 'non - autoregressive machine translation ( nat,  #AUTHOR_TAG ), on the other hand, generates all words in one shot and speeds up decoding at the expense of performance drop.', 'parallel decoding results in conditional independence and prevents the model from properly capturing highly multimodal distribution of target translations  #AUTHOR_TAG.', 'one way to remedy this fundamental problem is to refine model output iteratively  #TAUTHOR_TAG.', 'this work pursues this iterative approach to non - autoregressive translation.', '1 in this work, we propose a transformer - based architecture with attention masking, which we call disentangled context ( disco ) transformer, and use it for non - autoregressive decoding.', 'specifically, our disco transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words.', 'unlike the masked language models  #TAUTHOR_TAG where the model only predicts the masked words, the disco transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large.', 'we also introduce a new inference algorithm for iterative parallel decoding, parallel easy - first, where each word is predicted by attending to the words that the model is more confident about.', '']",4
"['iteratively  #TAUTHOR_TAG.', 'this']","['iteratively  #TAUTHOR_TAG.', 'this']","['refine model output iteratively  #TAUTHOR_TAG.', 'this work pursues this iterative approach to non - autoreg']","['- of - the - art neural machine translation systems use autoregressive decoding where words are predicted one - byone conditioned on all previous words  #AUTHOR_TAG.', 'non - autoregressive machine translation ( nat,  #AUTHOR_TAG ), on the other hand, generates all words in one shot and speeds up decoding at the expense of performance drop.', 'parallel decoding results in conditional independence and prevents the model from properly capturing highly multimodal distribution of target translations  #AUTHOR_TAG.', 'one way to remedy this fundamental problem is to refine model output iteratively  #TAUTHOR_TAG.', 'this work pursues this iterative approach to non - autoregressive translation.', '1 in this work, we propose a transformer - based architecture with attention masking, which we call disentangled context ( disco ) transformer, and use it for non - autoregressive decoding.', 'specifically, our disco transformer predicts every word in a sentence conditioned on an arbitrary subset of the rest of the words.', 'unlike the masked language models  #TAUTHOR_TAG where the model only predicts the masked words, the disco transformer can predict all words simultaneously, leading to faster inference as well as a substantial performance gain when training data are relatively large.', 'we also introduce a new inference algorithm for iterative parallel decoding, parallel easy - first, where each word is predicted by attending to the words that the model is more confident about.', '']",4
"['15 while cmlms  #TAUTHOR_TAG sample the number of masked tokens uniformly from [ 1, n ].', 'disc']","['n obs.', 'we introduce the 2 bert  #AUTHOR_TAG masks a token with probability 0. 15 while cmlms  #TAUTHOR_TAG sample the number of masked tokens uniformly from [ 1, n ].', 'disco transformer to compute these n contexts in one shot :', 'in particular, our disco transformer']","['n | x, y n obs ) with a vanilla transformer decoder will necessitate n separate transformer passes for each y n obs.', 'we introduce the 2 bert  #AUTHOR_TAG masks a token with probability 0. 15 while cmlms  #TAUTHOR_TAG sample the number of masked tokens uniformly from [ 1, n ].', 'disco transformer to compute these n contexts in one shot :', 'in particular, our disco transformer']","['computing conditional probabilities p ( y n | x, y n obs ) with a vanilla transformer decoder will necessitate n separate transformer passes for each y n obs.', 'we introduce the 2 bert  #AUTHOR_TAG masks a token with probability 0. 15 while cmlms  #TAUTHOR_TAG sample the number of masked tokens uniformly from [ 1, n ].', 'disco transformer to compute these n contexts in one shot :', 'in particular, our disco transformer makes crucial use of attention masking to achieve this computational efficiency.', 'denote input word and positional embeddings at position n by w n and p n.', 'for each position n in y, the vanilla transformer computes self - attention : 3 k n, v n, q n = proj ( w n + p n )', 'where k and v denote concatenated matricies of k n and v n for 1 ≤ n ≤ n.', 'we modify this attention computation in two aspects.', 'first, we separate query input from key and value input to avoid feeding the token we predict.', 'then we only attend to keys and values that correspond to observed tokens ( k n obs, v n obs ) and mask out the connection to the other tokens ( y n mask and y n itself, dashed lines in fig. 1 ).', 'k n, v n = proj ( w n + p n ) q n = proj ( p n ) h n = attention ( k n obs, v n obs, q n']",4
[' #TAUTHOR_TAG'],"['n obs, similarly to cmlms  #TAUTHOR_TAG']",[' #TAUTHOR_TAG'],"['use a standard transformer as an encoder and stacked disco layers as a decoder.', 'for each y n in y where | y | = n, we uniformly sample the number of visible tokens from [ 0, n − 1 ], and then we randomly choose that number of tokens from y \\ y n as y n obs, similarly to cmlms  #TAUTHOR_TAG.', 'we optimize the negative log likelihood loss from p ( y n | x, y n obs ) ( 1 ≤ n ≤ n ).', 'again following cmlms, we append a special token to the encoder and project the vector to predict the target length for parallel decoding.', 'we add the negative log likelihood loss from this length prediction to the loss from word predictions']",3
"['predictions  #TAUTHOR_TAG.', 'other approaches']","['predictions  #TAUTHOR_TAG.', 'other approaches']","['work on non - autoregressive translation developed ways to mitigate the trade - off between decoding parallelism and performance.', 'as in this work, several prior work proposed methods to iteratively refine output predictions  #TAUTHOR_TAG.', 'other approaches include adding a lite autoregressive module to parallel decoding  #AUTHOR_TAG, partially decoding autoregressively  #AUTHOR_TAG rescoring output candidates autoregressively (']","['work on non - autoregressive translation developed ways to mitigate the trade - off between decoding parallelism and performance.', 'as in this work, several prior work proposed methods to iteratively refine output predictions  #TAUTHOR_TAG.', 'other approaches include adding a lite autoregressive module to parallel decoding  #AUTHOR_TAG, partially decoding autoregressively  #AUTHOR_TAG rescoring output candidates autoregressively ( e. g.  #AUTHOR_TAG ), mimicking hidden states of an autoregressive teacher, training with different objectives than vanilla negative log likelihood ( libovicky &  #AUTHOR_TAG, reordering input sentences  #AUTHOR_TAG, and modeling with latent variables  #AUTHOR_TAG.', 'while this work took iterative decoding methods, our disco transformer can be combined with other approaches for efficient training.', 'for example,  #AUTHOR_TAG trained two separate non - autoregressive and autoregressive models, but it is possible to train a single disco transformer with both autoregressive and random masking and use hidden states from autoregressive masking as a teacher.', 'we leave integration of the disco transformer with more approaches to non - autoregressive translation for future.', 'we also note that our disco transformer can be used for general - purpose representation learning.', 'in particular,  #AUTHOR_TAG found that masking different tokens in every epoch outperforms static masking in bert  #AUTHOR_TAG.', 'our disco transformer would allow for making a prediction at every position given arbitrary context, providing even more flexibility for large - scale pretraining']",3
"[' #TAUTHOR_TAG, we']","[' #TAUTHOR_TAG, we']","[' #TAUTHOR_TAG, we apply length beam.', 'in particular, we predict top k lengths from the distribution in length prediction and run parallel easy - first simultaneously.', 'in']","[' #TAUTHOR_TAG, we apply length beam.', 'in particular, we predict top k lengths from the distribution in length prediction and run parallel easy - first simultaneously.', 'in order to speed up decoding, we terminate if the one with the highest average log score n n = 1 log ( p t n ) / n converges.', 'it should be noted that for parallel easy - first,', 'obs for all positions n while mask - predict may keep updating tokens even after because y t obs changes over iterations.', 'see alg.', '1 for full pseudo - code.', 'notice that all for - loops are parallelizable except the one over iterations t. in the subsequent experiments, we use length beam size of 5  #TAUTHOR_TAG unless otherwise noted.', 'in sec. 5. 2, we algorithm 1 parallel easy - first with length beam source sentence : x predicted lengths : n1, · · ·, nk max number of iterations : t for k ∈ { 1, 2,..., k } do for n ∈ { 1, 2,..., n k } do y 1, k n, p k n = ( arg ) max w p ( yn = w | x ) end for get the easy - first order z k by sorting p k and let z k ( i ) be the rank of the ith position.', 'end for', 'will illustrate that length beam facilitates decoding both the cmlm and disco transformer']",5
"[' #TAUTHOR_TAG, we']","[' #TAUTHOR_TAG, we']","[' #TAUTHOR_TAG, we apply length beam.', 'in particular, we predict top k lengths from the distribution in length prediction and run parallel easy - first simultaneously.', 'in']","[' #TAUTHOR_TAG, we apply length beam.', 'in particular, we predict top k lengths from the distribution in length prediction and run parallel easy - first simultaneously.', 'in order to speed up decoding, we terminate if the one with the highest average log score n n = 1 log ( p t n ) / n converges.', 'it should be noted that for parallel easy - first,', 'obs for all positions n while mask - predict may keep updating tokens even after because y t obs changes over iterations.', 'see alg.', '1 for full pseudo - code.', 'notice that all for - loops are parallelizable except the one over iterations t. in the subsequent experiments, we use length beam size of 5  #TAUTHOR_TAG unless otherwise noted.', 'in sec. 5. 2, we algorithm 1 parallel easy - first with length beam source sentence : x predicted lengths : n1, · · ·, nk max number of iterations : t for k ∈ { 1, 2,..., k } do for n ∈ { 1, 2,..., n k } do y 1, k n, p k n = ( arg ) max w p ( yn = w | x ) end for get the easy - first order z k by sorting p k and let z k ( i ) be the rank of the ith position.', 'end for', 'will illustrate that length beam facilitates decoding both the cmlm and disco transformer']",5
"[' #TAUTHOR_TAG.', 'for']","[' #TAUTHOR_TAG.', 'for']","['fair comparison with prior work  #TAUTHOR_TAG.', 'for all autoregressive models, we use beam search with b = 5  #AUTHOR_TAG and tune length penalty of α ∈']","[""datasets we evaluate on 7 directions from four standard datasets with various training data sizes : wmt'14 en - de ( 4. 5m pairs ), wmt'16 en - ro ( 610k pairs ), wmt'17 en - zh ( 20m pairs ), and wmt'14 en - fr ( 36m pairs, en→fr only )."", 'these datasets are all encoded into subword units by bpe  #AUTHOR_TAG.', '4 we use the same preprocessed data and train / dev / test splits as prior work for fair comparisons ( en - de : vaswani et al. 2017 ) ; 4 we run joint bpe on all language pairs except en - zh.', ' #AUTHOR_TAG ).', 'we evaluate performance with bleu scores  #AUTHOR_TAG for all directions except that we use sacrebleu  #AUTHOR_TAG 5 in en→zh again for fair comparison with prior work  #TAUTHOR_TAG.', 'for all autoregressive models, we use beam search with b = 5  #AUTHOR_TAG and tune length penalty of α ∈ [ 0. 0, 0. 2, · · ·, 2. 0 ] in validation.', 'for parallel easy - first, we set the max number of iterations t = 10 and use t = 4, 10 for constant - time mask - predict']",5
"['translation with mask - predict from a cmlm  #TAUTHOR_TAG.', 'we can directly compare our disco transformer with this method by the number of iterations required']","['translation with mask - predict from a cmlm  #TAUTHOR_TAG.', 'we can directly compare our disco transformer with this method by the number of iterations required.', '6 we provide results']","['to the similar model structure.', 'see sec. 6 for descriptions of more work on nat.', 'cmlm as discussed earlier, we can generate a translation with mask - predict from a cmlm  #TAUTHOR_TAG.', 'we can directly compare our disco transformer with this method by the number of iterations required']","['has been a flurry of recent work on non - autoregressive machine translation ( nat ) that finds a balance between parallelism and performance.', 'performance can be measured using automatic evaluation such as bleu scores  #AUTHOR_TAG.', 'latency is, however, challenging to compare across different methods.', 'for models that have an autoregressive component ( e. g.  #AUTHOR_TAG ;  #AUTHOR_TAG ), we can speed up sequential computation by caching states.', 'further, many of prior nat approaches generate varying numbers of translation candidates and rescore them using an autoregressive model.', 'the rescoring process typically costs overhead of one parallel pass of a transformer encoder followed by a decoder.', 'given this complexity in latency comparison, we highlight two state - of - the - art iteration - based nat models whose latency is comparable to our disco transformer due to the similar model structure.', 'see sec. 6 for descriptions of more work on nat.', 'cmlm as discussed earlier, we can generate a translation with mask - predict from a cmlm  #TAUTHOR_TAG.', 'we can directly compare our disco transformer with this method by the number of iterations required.', '6 we provide results obtained by running their code.', '7', 'levenshtein transformer levenshtein transformer ( levt ) is a transformer - based iterative model for parallel sequence generation  #AUTHOR_TAG.', 'its iteration consists of three sequential steps : deletion, placeholder prediction, and token prediction.', 'unlike the cmlm with the constant - time mask - predict inference, decoding in levt terminates adaptively under certain condition.', 'its latency is roughly comparable by the average number of sequential transformer runs.', 'each iteration consists of three transformer runs except that the first iteration skips the deletion step.', ' #AUTHOR_TAG  #AUTHOR_TAG.', 'unfortunately, we lack consensus in evaluation  #AUTHOR_TAG.', 'hyperparameters we generally follow the hyperparameters for a transformer base  #TAUTHOR_TAG : 6 layers for both the encoder and decoder, 8 attention heads, 512 model dimensions, and 2048 hidden dimensions.', 'we sample weights from n ( 0, 0. 02 ), initialize biases to zero, and set layer normalization parameters to β = 0, γ = 1  #AUTHOR_TAG.', 'for regularization, we tune the dropout rate from [ 0. 1, 0. 2, 0. 3 ] based on dev performance in each direction, and use 0. 01 l 2 weight decay and label smoothing with ε = 0. 1.', 'we train batches of 128k tokens using adam  #AUTHOR_TAG with β = ( 0. 9, 0. 999 ) and ε = 10 −6.', 'the learning rate warms up to 5 · 10 −4 in the first 10k steps, and then decays']",5
"['translation with mask - predict from a cmlm  #TAUTHOR_TAG.', 'we can directly compare our disco transformer with this method by the number of iterations required']","['translation with mask - predict from a cmlm  #TAUTHOR_TAG.', 'we can directly compare our disco transformer with this method by the number of iterations required.', '6 we provide results']","['to the similar model structure.', 'see sec. 6 for descriptions of more work on nat.', 'cmlm as discussed earlier, we can generate a translation with mask - predict from a cmlm  #TAUTHOR_TAG.', 'we can directly compare our disco transformer with this method by the number of iterations required']","['has been a flurry of recent work on non - autoregressive machine translation ( nat ) that finds a balance between parallelism and performance.', 'performance can be measured using automatic evaluation such as bleu scores  #AUTHOR_TAG.', 'latency is, however, challenging to compare across different methods.', 'for models that have an autoregressive component ( e. g.  #AUTHOR_TAG ;  #AUTHOR_TAG ), we can speed up sequential computation by caching states.', 'further, many of prior nat approaches generate varying numbers of translation candidates and rescore them using an autoregressive model.', 'the rescoring process typically costs overhead of one parallel pass of a transformer encoder followed by a decoder.', 'given this complexity in latency comparison, we highlight two state - of - the - art iteration - based nat models whose latency is comparable to our disco transformer due to the similar model structure.', 'see sec. 6 for descriptions of more work on nat.', 'cmlm as discussed earlier, we can generate a translation with mask - predict from a cmlm  #TAUTHOR_TAG.', 'we can directly compare our disco transformer with this method by the number of iterations required.', '6 we provide results obtained by running their code.', '7', 'levenshtein transformer levenshtein transformer ( levt ) is a transformer - based iterative model for parallel sequence generation  #AUTHOR_TAG.', 'its iteration consists of three sequential steps : deletion, placeholder prediction, and token prediction.', 'unlike the cmlm with the constant - time mask - predict inference, decoding in levt terminates adaptively under certain condition.', 'its latency is roughly comparable by the average number of sequential transformer runs.', 'each iteration consists of three transformer runs except that the first iteration skips the deletion step.', ' #AUTHOR_TAG  #AUTHOR_TAG.', 'unfortunately, we lack consensus in evaluation  #AUTHOR_TAG.', 'hyperparameters we generally follow the hyperparameters for a transformer base  #TAUTHOR_TAG : 6 layers for both the encoder and decoder, 8 attention heads, 512 model dimensions, and 2048 hidden dimensions.', 'we sample weights from n ( 0, 0. 02 ), initialize biases to zero, and set layer normalization parameters to β = 0, γ = 1  #AUTHOR_TAG.', 'for regularization, we tune the dropout rate from [ 0. 1, 0. 2, 0. 3 ] based on dev performance in each direction, and use 0. 01 l 2 weight decay and label smoothing with ε = 0. 1.', 'we train batches of 128k tokens using adam  #AUTHOR_TAG with β = ( 0. 9, 0. 999 ) and ε = 10 −6.', 'the learning rate warms up to 5 · 10 −4 in the first 10k steps, and then decays']",5
[' #TAUTHOR_TAG ('],"[' #TAUTHOR_TAG ( e. g. 31. 24 vs. 30. 53 in de→en with 10 steps ).', '']",['re - implementations of cmlm + mask - predict outperform  #TAUTHOR_TAG ('],"[""in table 1 are the results in the four directions from the wmt'14 en - de and wmt'16 en - ro datasets."", 'first, our re - implementations of cmlm + mask - predict outperform  #TAUTHOR_TAG ( e. g. 31. 24 vs. 30. 53 in de→en with 10 steps ).', 'this is probably due to our tuning on the dropout rate and weight averaging of the 5 best epochs based on the validation bleu performance ( sec. 4. 1 ).', 'our disco transformer with the parallel easy - first inference achieves at least comparable performance to the cmlm with 10 steps despite the significantly fewer steps on average ( e. g. 4. 82 steps in en→de ).', 'the one exception is ro→en ( 33. 25 vs. 33. 67 ), but disco + easy - first requires only 3. 10 steps, and cmlm + mask - predict with 4 steps achieves similar performance of 33. 27.', 'the limited advantage of our disco transformer on the en - ro dataset suggests that we benefit less from the training efficiency of the disco transformer on the small dataset ( 610k sentence pairs ).', 'disco + mask - predict generally underperforms disco + easy - first, implying that the mask - predict inference, which fixes y n obs across all positions n, fails to utilize the flexibility of the disco transformer.', 'disco + easy - first also accomplishes significant reduction in the average number of steps as compared to the adaptive decoding in levt  #AUTHOR_TAG while performing competitively.', 'as discussed earlier, each iteration in inference on levt involves three sequential transformer runs, which undermine the latency improvement.', 'overall, we outperform other nat models from prior work.', 'we achieve competitive performance to the standard autoregressive models with the same transformer base configuration on the en - de dataset except that the autoregressive model with distillation performs comparably to the transformer large teacher in en→de ( 28. 24 vs. 28. 60 ).', 'nonetheless, we still see a large gap between the autoregressive teachers and our nat results in both directions from en - ro, illustrating a limitation of our remedy for the trade - off between decoding parallelism and performance']",5
"['more accurate, even if pruning is necessary or decoding needs to be approximate  #TAUTHOR_TAG.', 'the']","['more accurate, even if pruning is necessary or decoding needs to be approximate  #TAUTHOR_TAG.', 'the']","['##ll 2006 - 7  #AUTHOR_TAG and, more recently, spmrl 2013 - 14  #AUTHOR_TAG.', 'as a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate  #TAUTHOR_TAG.', '']","['last years have witnessed a continuous progress in statistical multilingual models for syntax, thanks to shared tasks such as conll 2006 - 7  #AUTHOR_TAG and, more recently, spmrl 2013 - 14  #AUTHOR_TAG.', 'as a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate  #TAUTHOR_TAG.', 'the same rationale applies to semantic dependency parsing, also a structured prediction problem, but where the output variable is a semantic graph, rather than a syntactic tree.', 'indeed, the best performing systems in last year shared task on broad - coverage semantic dependency parsing follow this principle  #AUTHOR_TAG.', 'this year, a new challenge was put forth : how to handle multiple languages and out - ofdomain data?', 'our proposed parser ( § 2 ) is essentially the same that we submitted in the previous year to the same semeval task  #AUTHOR_TAG, where we scored top in the open challenge and second in the closed track.', 'this year, we report results using new out - of - domain and multilingual data ( namely, czech and chinese, in addition to english ).', 'for the english language, we participated in the closed and open tracks, using as additional resources the syntactic dependency annotations provided by the organizers.', 'for czech and chinese, we only addressed the closed track, since no companion data were provided for these languages.', 'we did not participate in the gold track that uses gold - standard syntactic annotations ; and we did not address the prediction of predicate senses']",0
"['more accurate, even if pruning is necessary or decoding needs to be approximate  #TAUTHOR_TAG.', 'the']","['more accurate, even if pruning is necessary or decoding needs to be approximate  #TAUTHOR_TAG.', 'the']","['##ll 2006 - 7  #AUTHOR_TAG and, more recently, spmrl 2013 - 14  #AUTHOR_TAG.', 'as a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate  #TAUTHOR_TAG.', '']","['last years have witnessed a continuous progress in statistical multilingual models for syntax, thanks to shared tasks such as conll 2006 - 7  #AUTHOR_TAG and, more recently, spmrl 2013 - 14  #AUTHOR_TAG.', 'as a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate  #TAUTHOR_TAG.', 'the same rationale applies to semantic dependency parsing, also a structured prediction problem, but where the output variable is a semantic graph, rather than a syntactic tree.', 'indeed, the best performing systems in last year shared task on broad - coverage semantic dependency parsing follow this principle  #AUTHOR_TAG.', 'this year, a new challenge was put forth : how to handle multiple languages and out - ofdomain data?', 'our proposed parser ( § 2 ) is essentially the same that we submitted in the previous year to the same semeval task  #AUTHOR_TAG, where we scored top in the open challenge and second in the closed track.', 'this year, we report results using new out - of - domain and multilingual data ( namely, czech and chinese, in addition to english ).', 'for the english language, we participated in the closed and open tracks, using as additional resources the syntactic dependency annotations provided by the organizers.', 'for czech and chinese, we only addressed the closed track, since no companion data were provided for these languages.', 'we did not participate in the gold track that uses gold - standard syntactic annotations ; and we did not address the prediction of predicate senses']",1
"[' #TAUTHOR_TAG, with the goal of performing semantic']","[' #TAUTHOR_TAG, with the goal of performing semantic']","['##ser  #TAUTHOR_TAG, with the goal of performing semantic parsing using']","['', 'our second - order model looks at some pairs of arcs : arcs bearing a grandparent relationship, arguments of the same predicate, predicates sharing the same argument, and consecutive versions of these two.', ' #AUTHOR_TAG for further details.', 'the parser was built as an extension of a recent dependency parser, turboparser  #TAUTHOR_TAG, with the goal of performing semantic parsing using any of the three formalisms considered in the shared task ( dm, pas, and psd ).', 'we have followed prior work in semantic role labeling  #AUTHOR_TAG, by adding constraints and modeling interactions among arguments within the same frame ; however, we went beyond such sibling interactions to consider more complex grandparent and co - parent structures, effectively correlating different predicates.', 'the overall set of parts used by our parser is illustrated in figure 1 ; note that by using only a subset of the parts ( predicate, arc, labeled arc, and sibling parts ), the semantic parser decodes each predicate frame independently from other predicates ; it is the co - parent and grandparent parts that have the effect of creating inter - dependence among predicates ; we will analyze the effect of these dependencies in the experimental section ( § 3 ).', 'for each part in our model ( shown in figure 1 ), we computed binary features based on various combination of lexical forms, lemmas, pos tags and syntactic dependency relations of words related to the corresponding predicates and arguments.', 'most of these features were taken from turboparser  #TAUTHOR_TAG, and others were inspired by the semantic parser of  #AUTHOR_TAG.', 'to tackle all the parts, we formulate parsing as a global optimization problem and solve a relaxation through ad 3  #AUTHOR_TAG, a fast dual decomposition algorithm in which several simple local subproblems are solved iteratively.', 'through a rich set of features, we arrive at top accuracies at parsing speeds around 1, 000 tokens']",5
"[' #TAUTHOR_TAG, with the goal of performing semantic']","[' #TAUTHOR_TAG, with the goal of performing semantic']","['##ser  #TAUTHOR_TAG, with the goal of performing semantic parsing using']","['', 'our second - order model looks at some pairs of arcs : arcs bearing a grandparent relationship, arguments of the same predicate, predicates sharing the same argument, and consecutive versions of these two.', ' #AUTHOR_TAG for further details.', 'the parser was built as an extension of a recent dependency parser, turboparser  #TAUTHOR_TAG, with the goal of performing semantic parsing using any of the three formalisms considered in the shared task ( dm, pas, and psd ).', 'we have followed prior work in semantic role labeling  #AUTHOR_TAG, by adding constraints and modeling interactions among arguments within the same frame ; however, we went beyond such sibling interactions to consider more complex grandparent and co - parent structures, effectively correlating different predicates.', 'the overall set of parts used by our parser is illustrated in figure 1 ; note that by using only a subset of the parts ( predicate, arc, labeled arc, and sibling parts ), the semantic parser decodes each predicate frame independently from other predicates ; it is the co - parent and grandparent parts that have the effect of creating inter - dependence among predicates ; we will analyze the effect of these dependencies in the experimental section ( § 3 ).', 'for each part in our model ( shown in figure 1 ), we computed binary features based on various combination of lexical forms, lemmas, pos tags and syntactic dependency relations of words related to the corresponding predicates and arguments.', 'most of these features were taken from turboparser  #TAUTHOR_TAG, and others were inspired by the semantic parser of  #AUTHOR_TAG.', 'to tackle all the parts, we formulate parsing as a global optimization problem and solve a relaxation through ad 3  #AUTHOR_TAG, a fast dual decomposition algorithm in which several simple local subproblems are solved iteratively.', 'through a rich set of features, we arrive at top accuracies at parsing speeds around 1, 000 tokens']",5
['based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],"['sets with respect to interpretants. rtms pioneer a language independent approach to all', 'similarity tasks and remove the need to access any task or domain specific information or resource. rtms become the 2nd system out of 13 systems participating', 'in paraphrase and semantic similarity in twitter, 6th out of 16 submissions in semantic textual similarity spanish, and 50th out', 'of 73 submissions in semantic textual similarity english. we present positive results from a fully automated judge for semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at semeval - 2015, semantic evaluation exercises - international workshop on semantic', 'evaluation  #AUTHOR_TAG. referential translation machine ( rtm ) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. an', 'rtm model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. each rt', '##m model is a data translation and translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation. rtms present an', 'accurate and language independent solution for making semantic similarity judgments. rtms pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training', 'data ( bicici and  #AUTHOR_TAG as interpretants for reaching shared semantics. rt', '##ms achieve ( i ) top performance when predicting the quality of translations ( bicici, 2013 ; bicici and  #AUTHOR_TAG a ) ; ( ii ) top performance when predicting monolingual cross - level', 'semantic similarity ; ( iii ) second performance when predicting paraphrase and semantic similarity in twitter ( iv ) good performance when judging the semantic similarity of sentences ; ( iv ) good performance when evaluating the semantic relatedness of sentences and their entailment  #TAUTHOR_TAG. rtms use machine translation performance prediction ( mtpp ) system  #TAUTHOR_TAG, which is a state - of - the - art ( soa ) performance predictor of translation even without using the translation. mtpp', 'system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and', 'the presence of acts of translation for data transformation. mtpp features for translation acts are provided in  #TAUTHOR_TAG. rtms become the 2nd system out of 13 systems participating in paraphrase and semantic similarity in twitter ( task 1 )  #AUTHOR_TAG and achieve', 'good results in semantic tex -']",0
['based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],"['sets with respect to interpretants. rtms pioneer a language independent approach to all', 'similarity tasks and remove the need to access any task or domain specific information or resource. rtms become the 2nd system out of 13 systems participating', 'in paraphrase and semantic similarity in twitter, 6th out of 16 submissions in semantic textual similarity spanish, and 50th out', 'of 73 submissions in semantic textual similarity english. we present positive results from a fully automated judge for semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at semeval - 2015, semantic evaluation exercises - international workshop on semantic', 'evaluation  #AUTHOR_TAG. referential translation machine ( rtm ) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. an', 'rtm model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. each rt', '##m model is a data translation and translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation. rtms present an', 'accurate and language independent solution for making semantic similarity judgments. rtms pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training', 'data ( bicici and  #AUTHOR_TAG as interpretants for reaching shared semantics. rt', '##ms achieve ( i ) top performance when predicting the quality of translations ( bicici, 2013 ; bicici and  #AUTHOR_TAG a ) ; ( ii ) top performance when predicting monolingual cross - level', 'semantic similarity ; ( iii ) second performance when predicting paraphrase and semantic similarity in twitter ( iv ) good performance when judging the semantic similarity of sentences ; ( iv ) good performance when evaluating the semantic relatedness of sentences and their entailment  #TAUTHOR_TAG. rtms use machine translation performance prediction ( mtpp ) system  #TAUTHOR_TAG, which is a state - of - the - art ( soa ) performance predictor of translation even without using the translation. mtpp', 'system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and', 'the presence of acts of translation for data transformation. mtpp features for translation acts are provided in  #TAUTHOR_TAG. rtms become the 2nd system out of 13 systems participating in paraphrase and semantic similarity in twitter ( task 1 )  #AUTHOR_TAG and achieve', 'good results in semantic tex -']",0
['based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],"['sets with respect to interpretants. rtms pioneer a language independent approach to all', 'similarity tasks and remove the need to access any task or domain specific information or resource. rtms become the 2nd system out of 13 systems participating', 'in paraphrase and semantic similarity in twitter, 6th out of 16 submissions in semantic textual similarity spanish, and 50th out', 'of 73 submissions in semantic textual similarity english. we present positive results from a fully automated judge for semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at semeval - 2015, semantic evaluation exercises - international workshop on semantic', 'evaluation  #AUTHOR_TAG. referential translation machine ( rtm ) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. an', 'rtm model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. each rt', '##m model is a data translation and translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation. rtms present an', 'accurate and language independent solution for making semantic similarity judgments. rtms pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training', 'data ( bicici and  #AUTHOR_TAG as interpretants for reaching shared semantics. rt', '##ms achieve ( i ) top performance when predicting the quality of translations ( bicici, 2013 ; bicici and  #AUTHOR_TAG a ) ; ( ii ) top performance when predicting monolingual cross - level', 'semantic similarity ; ( iii ) second performance when predicting paraphrase and semantic similarity in twitter ( iv ) good performance when judging the semantic similarity of sentences ; ( iv ) good performance when evaluating the semantic relatedness of sentences and their entailment  #TAUTHOR_TAG. rtms use machine translation performance prediction ( mtpp ) system  #TAUTHOR_TAG, which is a state - of - the - art ( soa ) performance predictor of translation even without using the translation. mtpp', 'system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and', 'the presence of acts of translation for data transformation. mtpp features for translation acts are provided in  #TAUTHOR_TAG. rtms become the 2nd system out of 13 systems participating in paraphrase and semantic similarity in twitter ( task 1 )  #AUTHOR_TAG and achieve', 'good results in semantic tex -']",0
['based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],"['sets with respect to interpretants. rtms pioneer a language independent approach to all', 'similarity tasks and remove the need to access any task or domain specific information or resource. rtms become the 2nd system out of 13 systems participating', 'in paraphrase and semantic similarity in twitter, 6th out of 16 submissions in semantic textual similarity spanish, and 50th out', 'of 73 submissions in semantic textual similarity english. we present positive results from a fully automated judge for semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at semeval - 2015, semantic evaluation exercises - international workshop on semantic', 'evaluation  #AUTHOR_TAG. referential translation machine ( rtm ) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. an', 'rtm model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. each rt', '##m model is a data translation and translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation. rtms present an', 'accurate and language independent solution for making semantic similarity judgments. rtms pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training', 'data ( bicici and  #AUTHOR_TAG as interpretants for reaching shared semantics. rt', '##ms achieve ( i ) top performance when predicting the quality of translations ( bicici, 2013 ; bicici and  #AUTHOR_TAG a ) ; ( ii ) top performance when predicting monolingual cross - level', 'semantic similarity ; ( iii ) second performance when predicting paraphrase and semantic similarity in twitter ( iv ) good performance when judging the semantic similarity of sentences ; ( iv ) good performance when evaluating the semantic relatedness of sentences and their entailment  #TAUTHOR_TAG. rtms use machine translation performance prediction ( mtpp ) system  #TAUTHOR_TAG, which is a state - of - the - art ( soa ) performance predictor of translation even without using the translation. mtpp', 'system measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and', 'the presence of acts of translation for data transformation. mtpp features for translation acts are provided in  #TAUTHOR_TAG. rtms become the 2nd system out of 13 systems participating in paraphrase and semantic similarity in twitter ( task 1 )  #AUTHOR_TAG and achieve', 'good results in semantic tex -']",0
['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],"['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at semeval - 2015, semantic']",['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],"['present positive results from a fully automated judge for semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at semeval - 2015, semantic evaluation exercises - international workshop on semantic evaluation  #AUTHOR_TAG.', 'referential translation machine ( rtm ) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain.', 'an rtm model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments.', 'each rtm model is a data translation and translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation.', 'rtms present an accurate and language independent solution for making semantic similarity judgments.', 'rtms pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data ( bicici and  #AUTHOR_TAG as interpretants for reaching shared semantics.', '']",0
['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],"['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at semeval - 2015, semantic']",['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],"['present positive results from a fully automated judge for semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at semeval - 2015, semantic evaluation exercises - international workshop on semantic evaluation  #AUTHOR_TAG.', 'referential translation machine ( rtm ) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain.', 'an rtm model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments.', 'each rtm model is a data translation and translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation.', 'rtms present an accurate and language independent solution for making semantic similarity judgments.', 'rtms pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data ( bicici and  #AUTHOR_TAG as interpretants for reaching shared semantics.', '']",0
['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],"['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at semeval - 2015, semantic']",['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],"['present positive results from a fully automated judge for semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at semeval - 2015, semantic evaluation exercises - international workshop on semantic evaluation  #AUTHOR_TAG.', 'referential translation machine ( rtm ) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain.', 'an rtm model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments.', 'each rtm model is a data translation and translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation.', 'rtms present an accurate and language independent solution for making semantic similarity judgments.', 'rtms pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data ( bicici and  #AUTHOR_TAG as interpretants for reaching shared semantics.', '']",0
"['the learning parameters, the number of dimensions used for pls, and the parameters for parallel fda5.', 'more details about the optimization processes are in  #TAUTHOR_TAG ; bic']","['the learning parameters, the number of dimensions used for pls, and the parameters for parallel fda5.', 'more details about the optimization processes are in  #TAUTHOR_TAG ; bicici et al., 2014 ).', '']","['mapping step with partial least squares ( pls )  #AUTHOR_TAG.', 'we optimize the learning parameters, the number of dimensions used for pls, and the parameters for parallel fda5.', 'more details about the optimization processes are in  #TAUTHOR_TAG ; bic']","['use ridge regression ( rr ), support vector regression ( svr ), and extremely randomized trees ( tree )  #AUTHOR_TAG as the learning models.', 'these models learn a regression function using the features to estimate a numerical target value.', 'we also use them after a dimensionality reduction and mapping step with partial least squares ( pls )  #AUTHOR_TAG.', 'we optimize the learning parameters, the number of dimensions used for pls, and the parameters for parallel fda5.', 'more details about the optimization processes are in  #TAUTHOR_TAG ; bicici et al., 2014 ).', 'we optimize the learning parameters by selecting ε close to the standard deviation of the noise in the training set ( bicici, 2013 ) since the optimal value for ε is shown to have linear dependence to the noise level for different noise models  #AUTHOR_TAG.', 'at testing time, the predictions are bounded to obtain scores in the corresponding ranges.', '']",0
"['in those domains  #TAUTHOR_TAG.', 'sts english test set contains 2000, 1500, 2000, 1500, and 1500 sentences respectively from']","['in those domains  #TAUTHOR_TAG.', 'sts english test set contains 2000, 1500, 2000, 1500, and 1500 sentences respectively from']","['improved performance in those domains  #TAUTHOR_TAG.', 'sts english test set contains 2000, 1500, 2000, 1500, and 1500 sentences respectively from the specified domains however']","['contains sentence pairs from different domains : answers - forums, answers - students, belief, headlines, and images for english and wikipedia and newswire for spanish.', ""official evaluation metric in sts is the pearson's correlation score."", 'we build separate rtm models for headlines and images domains for sts english.', 'domain specific rtm models obtain improved performance in those domains  #TAUTHOR_TAG.', 'sts english test set contains 2000, 1500, 2000, 1500, and 1500 sentences respectively from the specified domains however for evaluation, sts use a subset of the test set, 375, 750, 375, 750, and 750 instances respectively from the corresponding domains.', 'this may lower the performance of rtms by causing fda5 to select more domain specific data and less task specific since rtms use the test set to select interpretants and build a task specific rtm prediction model.', '( bicici and  #AUTHOR_TAG b ) are presented in table 6, where we have used the top results from domain specific rtm models for headlines and images domains in the overall model results.', 'top 3 individual rtm model performance on the training set with further optimized learning model parameters after the challenge are presented in table 7.', 'better r p, rae, and mraer on the test set than on the training set in sts 2015 english may be attributed to rtms']",0
"['2014  #TAUTHOR_TAG, and']","[' #TAUTHOR_TAG, and']","['2014  #TAUTHOR_TAG, and']",[' #TAUTHOR_TAG'],0
['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],"['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at semeval - 2015, semantic']",['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],"['present positive results from a fully automated judge for semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at semeval - 2015, semantic evaluation exercises - international workshop on semantic evaluation  #AUTHOR_TAG.', 'referential translation machine ( rtm ) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain.', 'an rtm model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments.', 'each rtm model is a data translation and translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation.', 'rtms present an accurate and language independent solution for making semantic similarity judgments.', 'rtms pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data ( bicici and  #AUTHOR_TAG as interpretants for reaching shared semantics.', '']",5
['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],"['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at semeval - 2015, semantic']",['semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at sem'],"['present positive results from a fully automated judge for semantic similarity based on referential translation machines  #TAUTHOR_TAG in two semantic similarity tasks at semeval - 2015, semantic evaluation exercises - international workshop on semantic evaluation  #AUTHOR_TAG.', 'referential translation machine ( rtm ) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain.', 'an rtm model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments.', 'each rtm model is a data translation and translation prediction model between the instances in the training set and the test set and translation acts are indicators of the data transformation and translation.', 'rtms present an accurate and language independent solution for making semantic similarity judgments.', 'rtms pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data ( bicici and  #AUTHOR_TAG as interpretants for reaching shared semantics.', '']",3
"['depends on elmo inherits its bias and demonstrates significant bias on the  #TAUTHOR_TAG.', '']","['depends on elmo inherits its bias and demonstrates significant bias on the  #TAUTHOR_TAG.', 'finally, we explore two methods to mitigate such gender']","['depends on elmo inherits its bias and demonstrates significant bias on the  #TAUTHOR_TAG.', '']","[""this paper, we quantify, analyze and mitigate gender bias exhibited in elmo's contextualized word vectors."", 'first, we conduct several intrinsic analyses and find that ( 1 ) training data for elmo contains significantly more male than female entities, ( 2 ) the trained elmo embeddings systematically encode gender information and ( 3 ) elmo unequally encodes gender information about male and female entities.', 'then, we show that a state - of - the - art coreference system that depends on elmo inherits its bias and demonstrates significant bias on the  #TAUTHOR_TAG.', 'finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on  #TAUTHOR_TAG can be eliminated']",5
"[""' s contextual embeddings on  #TAUTHOR_TAG evaluates""]","[""use of elmo's contextual embeddings on  #TAUTHOR_TAG evaluates""]","[""' s contextual embeddings on  #TAUTHOR_TAG evaluates""]","['representations of words in the form of word embeddings  #AUTHOR_TAG and contextualized word embeddings  #AUTHOR_TAG mc  #AUTHOR_TAG have led to huge performance improvement on many nlp tasks.', 'however, several recent studies show that training word embeddings in large corpora could lead to encoding societal biases present in these human - produced data  #AUTHOR_TAG.', 'in this work, we extend these analyses to the elmo contextualized word embeddings.', 'our work provides a new intrinsic analysis of how elmo represents gender in biased ways.', 'first, the corpus used for training elmo has a significant gender skew : male entities are nearly three times more common than female entities, which leads to gender bias in the downloadable pre - trained contextualized embeddings.', 'then, we apply principal component analysis ( pca ) to show that after training on such biased corpora, there exists a lowdimensional subspace that captures much of the gender information in the contextualized embeddings.', 'finally, we evaluate how faithfully elmo preserves gender information in sentences by measuring how predictable gender is from elmo representations of occupation words that co - occur with gender revealing pronouns.', 'our results show that elmo embeddings perform unequally on male and female pronouns : male entities can be predicted from occupation words 14 % more accurately than female entities.', 'in addition, we examine how gender bias in elmo propagates to the downstream applications.', ""specifically, we evaluate a state - of - the - art coreference resolution system ) that makes use of elmo's contextual embeddings on  #TAUTHOR_TAG evaluates whether systems behave differently on decisions involving male and female entities of stereotyped or anti - stereotyped occupations."", 'we find that in the most challenging setting, the elmo - based system has a disparity in accuracy between pro - and anti - stereotypical predictions, which is nearly 30 % higher than a similar system based on glove  #AUTHOR_TAG.', '']",5
"[""' s contextual embeddings on  #TAUTHOR_TAG evaluates""]","[""use of elmo's contextual embeddings on  #TAUTHOR_TAG evaluates""]","[""' s contextual embeddings on  #TAUTHOR_TAG evaluates""]","['representations of words in the form of word embeddings  #AUTHOR_TAG and contextualized word embeddings  #AUTHOR_TAG mc  #AUTHOR_TAG have led to huge performance improvement on many nlp tasks.', 'however, several recent studies show that training word embeddings in large corpora could lead to encoding societal biases present in these human - produced data  #AUTHOR_TAG.', 'in this work, we extend these analyses to the elmo contextualized word embeddings.', 'our work provides a new intrinsic analysis of how elmo represents gender in biased ways.', 'first, the corpus used for training elmo has a significant gender skew : male entities are nearly three times more common than female entities, which leads to gender bias in the downloadable pre - trained contextualized embeddings.', 'then, we apply principal component analysis ( pca ) to show that after training on such biased corpora, there exists a lowdimensional subspace that captures much of the gender information in the contextualized embeddings.', 'finally, we evaluate how faithfully elmo preserves gender information in sentences by measuring how predictable gender is from elmo representations of occupation words that co - occur with gender revealing pronouns.', 'our results show that elmo embeddings perform unequally on male and female pronouns : male entities can be predicted from occupation words 14 % more accurately than female entities.', 'in addition, we examine how gender bias in elmo propagates to the downstream applications.', ""specifically, we evaluate a state - of - the - art coreference resolution system ) that makes use of elmo's contextual embeddings on  #TAUTHOR_TAG evaluates whether systems behave differently on decisions involving male and female entities of stereotyped or anti - stereotyped occupations."", 'we find that in the most challenging setting, the elmo - based system has a disparity in accuracy between pro - and anti - stereotypical predictions, which is nearly 30 % higher than a similar system based on glove  #AUTHOR_TAG.', '']",5
"['co - occurrence of occupation words with those pronouns.', 'we use the set of occupation words defined in the  #TAUTHOR_TAG.', 'the analysis shows that the  #TAUTHOR_TAG contains a significant skew with']","['co - occurrence of occupation words with those pronouns.', 'we use the set of occupation words defined in the  #TAUTHOR_TAG.', 'the analysis shows that the  #TAUTHOR_TAG contains a significant skew with']","['##mo.', 'we show counts for the number of occurrences of male pronouns ( he, his and him ) and female pronouns ( she and her ) in the corpus as well as the co - occurrence of occupation words with those pronouns.', 'we use the set of occupation words defined in the  #TAUTHOR_TAG.', 'the analysis shows that the  #TAUTHOR_TAG contains a significant skew with']","['', 'm 5, 300, 000 170, 000 81, 000 f 1, 600, 000 33, 000 36, 000 nificantly more male entities compared to female entities leading to gender bias in the pre - trained contextual word embeddings ( 2 ) the geometry of trained elmo embeddings systematically encodes gender information and ( 3 ) elmo propagates gender information about male and female entities unequally.', 'table 1 lists the data analysis on the one billion word benchmark  #AUTHOR_TAG corpus, the training corpus for elmo.', 'we show counts for the number of occurrences of male pronouns ( he, his and him ) and female pronouns ( she and her ) in the corpus as well as the co - occurrence of occupation words with those pronouns.', 'we use the set of occupation words defined in the  #TAUTHOR_TAG.', 'the analysis shows that the  #TAUTHOR_TAG contains a significant skew with respect to gender : ( 1 ) male pronouns occur three times more than female pronouns and ( 2 ) male pronouns co - occur more frequently with occupation words, irrespective of whether they are prototypically male or female']",5
"['itself ( occupational gender ).', 'to visualize the gender subspace, we pick a few sentence pairs from  #TAUTHOR_TAG.', '']","['itself ( occupational gender ).', 'to visualize the gender subspace, we pick a few sentence pairs from  #TAUTHOR_TAG.', '']","['itself ( occupational gender ).', 'to visualize the gender subspace, we pick a few sentence pairs from  #TAUTHOR_TAG.', '']","[', we analyze the gender subspace in elmo.', 'we first sample 400 sentences with at least one gendered word ( e. g., he or she from the ontonotes 5. 0 dataset  #AUTHOR_TAG and generate the corresponding gender - swapped variants ( changing he to she and vice - versa ).', 'we then calculate the difference of elmo embeddings between occupation words in corresponding sentences and conduct principal component analysis for all pairs of sentences.', 'figure 1 shows there are two principal components for gender in elmo, in contrast to glove which only has one  #AUTHOR_TAG.', 'the two principal components in elmo seem to represent the gender from the contextual information ( contextual gender ) as well as the gender embedded in the word itself ( occupational gender ).', 'to visualize the gender subspace, we pick a few sentence pairs from  #TAUTHOR_TAG.', 'each sentence in the corpus contains one gendered pronoun and two occupation words, such as "" the developer corrected the secretary because she made a mistake "" and also the same sentence with the opposite pronoun ( he ).', 'in figure 1 on the right, we project the elmo embeddings of occupation words that are co - referent with the pronoun ( e. g. secretary in the above example ) for when the pronoun is male ( blue dots ) and female ( orange dots ) on the two principal components from the pca analysis.', 'qualitatively, we can see the first component separates male and female contexts while the second component groups male related words such as lawyer and developer and female related words such as cashier and nurse']",5
['numbers of male 1 we use the list collected in  #TAUTHOR_TAG and'],['numbers of male 1 we use the list collected in  #TAUTHOR_TAG and'],['of the opposite gender such that the numbers of male 1 we use the list collected in  #TAUTHOR_TAG and female entities are balanced'],"['test how elmo embeds gender information in contextualized word embeddings, we train a classifier to predict the gender of entities from occupation words in the same sentence.', 'we collect sentences containing gendered words ( e. g., he - she, father - mother ) and occupation words ( e. g., doctor ) 1 from the ontonotes 5. 0 corpus  #AUTHOR_TAG, where we treat occupation words as a mention to an entity, and the gender of that entity is taken to the gender of a co - referring gendered word, if one exists.', 'for example, in the sentence "" the engineer went back to her home, "" we take engineer to be a female mention.', 'then we split all such instances into training and test, with 539 and 62 instances, respectively and augment these sentences by swapping all the gendered words with words of the opposite gender such that the numbers of male 1 we use the list collected in  #TAUTHOR_TAG and female entities are balanced.', 'we first test if elmo embedding vectors carry gender information.', 'we train an svm classifier with an rbf kernel 2 to predict the gender of a mention ( i. e., an occupation word ) based on its elmo embedding.', '']",5
"['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are']","['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are']","['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are associated with occupations predom']","['evaluate bias with respect to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are associated with occupations predominately associated with the gender of the pronoun, or anti - stereotype, when the opposite relation is true.', 'table 2 : f1 on ontonotes and  #TAUTHOR_TAG development sets.', ' #TAUTHOR_TAG is split semantics only and w / syntactic cues subsets.', 'elmo improves the performance on the ontonotes dataset by 5 % but shows stronger bias on the  #TAUTHOR_TAG', 'avg. stands for averaged f1 score on the pro - and anti - stereotype subsets while "" diff. "" is the absolute difference between these two subsets.', '* indicates the difference between pro / anti stereotypical conditions is significant ( p <. 05 ) under an approximate randomized test  #AUTHOR_TAG.', 'mitigating bias by data augmentation reduces all the bias from the coreference model to a neglect level.', 'however, the neutralizing elmo approach only mitigates bias when there are other strong learning signals for the task.', 'each subset consists of two types of sentences : one that requires semantic understanding of the sentence to make coreference resolution ( semantics only ) and another that relies on syntactic cues ( w / syntactic cues ).', 'gender bias is measured by taking the difference of the performance in pro - and antistereotypical subsets.', 'previous work  #TAUTHOR_TAG evaluated the systems based on glove embeddings but here we evaluate a state - of - the - art system that trained on the ontonotes corpus with elmo embeddings']",5
"['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are']","['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are']","['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are associated with occupations predom']","['evaluate bias with respect to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are associated with occupations predominately associated with the gender of the pronoun, or anti - stereotype, when the opposite relation is true.', 'table 2 : f1 on ontonotes and  #TAUTHOR_TAG development sets.', ' #TAUTHOR_TAG is split semantics only and w / syntactic cues subsets.', 'elmo improves the performance on the ontonotes dataset by 5 % but shows stronger bias on the  #TAUTHOR_TAG', 'avg. stands for averaged f1 score on the pro - and anti - stereotype subsets while "" diff. "" is the absolute difference between these two subsets.', '* indicates the difference between pro / anti stereotypical conditions is significant ( p <. 05 ) under an approximate randomized test  #AUTHOR_TAG.', 'mitigating bias by data augmentation reduces all the bias from the coreference model to a neglect level.', 'however, the neutralizing elmo approach only mitigates bias when there are other strong learning signals for the task.', 'each subset consists of two types of sentences : one that requires semantic understanding of the sentence to make coreference resolution ( semantics only ) and another that relies on syntactic cues ( w / syntactic cues ).', 'gender bias is measured by taking the difference of the performance in pro - and antistereotypical subsets.', 'previous work  #TAUTHOR_TAG evaluated the systems based on glove embeddings but here we evaluate a state - of - the - art system that trained on the ontonotes corpus with elmo embeddings']",5
"['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are']","['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are']","['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are associated with occupations predom']","['evaluate bias with respect to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are associated with occupations predominately associated with the gender of the pronoun, or anti - stereotype, when the opposite relation is true.', 'table 2 : f1 on ontonotes and  #TAUTHOR_TAG development sets.', ' #TAUTHOR_TAG is split semantics only and w / syntactic cues subsets.', 'elmo improves the performance on the ontonotes dataset by 5 % but shows stronger bias on the  #TAUTHOR_TAG', 'avg. stands for averaged f1 score on the pro - and anti - stereotype subsets while "" diff. "" is the absolute difference between these two subsets.', '* indicates the difference between pro / anti stereotypical conditions is significant ( p <. 05 ) under an approximate randomized test  #AUTHOR_TAG.', 'mitigating bias by data augmentation reduces all the bias from the coreference model to a neglect level.', 'however, the neutralizing elmo approach only mitigates bias when there are other strong learning signals for the task.', 'each subset consists of two types of sentences : one that requires semantic understanding of the sentence to make coreference resolution ( semantics only ) and another that relies on syntactic cues ( w / syntactic cues ).', 'gender bias is measured by taking the difference of the performance in pro - and antistereotypical subsets.', 'previous work  #TAUTHOR_TAG evaluated the systems based on glove embeddings but here we evaluate a state - of - the - art system that trained on the ontonotes corpus with elmo embeddings']",5
"['', ' #TAUTHOR_TAG propose a method to']","['and ( 2 ) a test - time neutralization approach.', ' #TAUTHOR_TAG propose a method to']","['and ( 2 ) a test - time neutralization approach.', ' #TAUTHOR_TAG propose a method to']","['', ' #TAUTHOR_TAG propose a method to reduce gender bias in coreference resolution by augmenting the training corpus for this task.', 'data augmentation is performed by replacing gender revealing entities in the ontonotes dataset with words indicating the opposite gender and then training on the union of the original data and this swapped data.', 'in addition,  #TAUTHOR_TAG find it useful to also mitigate bias in supporting resources and therefore replace standard glove embeddings with bias mitigated word embeddings from  #AUTHOR_TAG.', 'we evaluate the performance of both aspects of  #TAUTHOR_TAG']",5
"['representations.', 'table 2 summarizes our results on  #TAUTHOR_TAG']","['representations.', 'table 2 summarizes our results on  #TAUTHOR_TAG']","['the final representations.', 'table 2 summarizes our results on  #TAUTHOR_TAG']","['we also investigate an approach to mitigate bias induced by elmo embeddings without retraining the coreference model.', 'instead of augmenting training corpus by swapping gender words, we generate a gender - swapped version of the test instances.', 'we then apply elmo to obtain contextualized word representations of the original and the gender - swapped sentences and use their average as the final representations.', 'table 2 summarizes our results on  #TAUTHOR_TAG']",5
"['the elmo based coreference system on  #TAUTHOR_TAG.', 'while elmo']","['the elmo based coreference system on  #TAUTHOR_TAG.', 'while elmo']","['the elmo based coreference system on  #TAUTHOR_TAG.', 'while elmo']","['##mo bias transfers to coreference row 3 in table 2 summarizes performance of the elmo based coreference system on  #TAUTHOR_TAG.', 'while elmo helps to boost the coreference resolution f1 score ( ontonotes ) it also propagates bias to the task.', 'it exhibits large differences between pro - and anti - stereotyped sets ( | diff | ) on both semantic and syntactic examples in  #TAUTHOR_TAG.', 'bias mitigation rows 4 - 6 in table 2 summarize the effectiveness of the two bias mitigation approaches we consider.', 'data augmentation is largely effective at mitigating bias in the coreference resolution system with elmo ( reducing | diff | to insignificant levels ) but requires retraining the system.', 'neutralization is less effective than augmentation and cannot fully remove gender bias on the semantics only portion of  #TAUTHOR_TAG, indicating it is effective only for simpler cases.', 'this observation is consistent with  #AUTHOR_TAG, where they show that entirely removing bias from an embedding is difficult and depends on the manner, by which one measures the bias']",5
"['the elmo based coreference system on  #TAUTHOR_TAG.', 'while elmo']","['the elmo based coreference system on  #TAUTHOR_TAG.', 'while elmo']","['the elmo based coreference system on  #TAUTHOR_TAG.', 'while elmo']","['##mo bias transfers to coreference row 3 in table 2 summarizes performance of the elmo based coreference system on  #TAUTHOR_TAG.', 'while elmo helps to boost the coreference resolution f1 score ( ontonotes ) it also propagates bias to the task.', 'it exhibits large differences between pro - and anti - stereotyped sets ( | diff | ) on both semantic and syntactic examples in  #TAUTHOR_TAG.', 'bias mitigation rows 4 - 6 in table 2 summarize the effectiveness of the two bias mitigation approaches we consider.', 'data augmentation is largely effective at mitigating bias in the coreference resolution system with elmo ( reducing | diff | to insignificant levels ) but requires retraining the system.', 'neutralization is less effective than augmentation and cannot fully remove gender bias on the semantics only portion of  #TAUTHOR_TAG, indicating it is effective only for simpler cases.', 'this observation is consistent with  #AUTHOR_TAG, where they show that entirely removing bias from an embedding is difficult and depends on the manner, by which one measures the bias']",5
"['depends on elmo inherits its bias and demonstrates significant bias on the  #TAUTHOR_TAG.', '']","['depends on elmo inherits its bias and demonstrates significant bias on the  #TAUTHOR_TAG.', 'finally, we explore two methods to mitigate such gender']","['depends on elmo inherits its bias and demonstrates significant bias on the  #TAUTHOR_TAG.', '']","[""this paper, we quantify, analyze and mitigate gender bias exhibited in elmo's contextualized word vectors."", 'first, we conduct several intrinsic analyses and find that ( 1 ) training data for elmo contains significantly more male than female entities, ( 2 ) the trained elmo embeddings systematically encode gender information and ( 3 ) elmo unequally encodes gender information about male and female entities.', 'then, we show that a state - of - the - art coreference system that depends on elmo inherits its bias and demonstrates significant bias on the  #TAUTHOR_TAG.', 'finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on  #TAUTHOR_TAG can be eliminated']",6
"['example,  #TAUTHOR_TAG and  #AUTHOR_TAG show that coreference']","['example,  #TAUTHOR_TAG and  #AUTHOR_TAG show that coreference']","['example,  #TAUTHOR_TAG and  #AUTHOR_TAG show that coreference resolution systems relying on word embeddings encode such occupational stereotypes.', 'in concurrent work,  #AUTHOR_TAG measure gender bias in sentence embeddings, but']","['bias has been shown to affect several realworld applications relying on automatic language analysis, including online news  #AUTHOR_TAG, advertisements  #AUTHOR_TAG, abusive language detection  #AUTHOR_TAG, machine translation ( font and costa - jussa, 2019 ;  #AUTHOR_TAG, and web search  #AUTHOR_TAG.', 'in many cases, a model not only replicates bias in the training data but also amplifies it  #AUTHOR_TAG.', 'for word representations,  #AUTHOR_TAG and  #AUTHOR_TAG show that word embeddings encode societal biases about gender roles and occupations, e. g. engineers are stereotypically men, and nurses are stereotypically women.', 'as a consequence, downstream applications that use these pretrained word embeddings also reflect this bias.', 'for example,  #TAUTHOR_TAG and  #AUTHOR_TAG show that coreference resolution systems relying on word embeddings encode such occupational stereotypes.', 'in concurrent work,  #AUTHOR_TAG measure gender bias in sentence embeddings, but their evaluation is on the aggregation of word representations.', 'in contrast, we analyze bias in contextualized word representations and its effect on a downstream task.', 'to mitigate bias from word embeddings,  #AUTHOR_TAG propose a post - processing method to project out the bias subspace from the pre - trained embeddings.', 'their method is shown to reduce the gender information from the embeddings of gender - neutral words, and, remarkably, maintains the same level of performance on different downstream nlp tasks.', ' #AUTHOR_TAG b ) further propose a training mechanism to separate gender information from other factors.', ' #AUTHOR_TAG argue that entirely removing bias is difficult, if not impossible, and the gender bias information can be often recovered.', 'this paper investigates a natural follow - up question : what are effective bias mitigation techniques for contextualized embeddings']",0
"['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are']","['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are']","['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are associated with occupations predom']","['evaluate bias with respect to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are associated with occupations predominately associated with the gender of the pronoun, or anti - stereotype, when the opposite relation is true.', 'table 2 : f1 on ontonotes and  #TAUTHOR_TAG development sets.', ' #TAUTHOR_TAG is split semantics only and w / syntactic cues subsets.', 'elmo improves the performance on the ontonotes dataset by 5 % but shows stronger bias on the  #TAUTHOR_TAG', 'avg. stands for averaged f1 score on the pro - and anti - stereotype subsets while "" diff. "" is the absolute difference between these two subsets.', '* indicates the difference between pro / anti stereotypical conditions is significant ( p <. 05 ) under an approximate randomized test  #AUTHOR_TAG.', 'mitigating bias by data augmentation reduces all the bias from the coreference model to a neglect level.', 'however, the neutralizing elmo approach only mitigates bias when there are other strong learning signals for the task.', 'each subset consists of two types of sentences : one that requires semantic understanding of the sentence to make coreference resolution ( semantics only ) and another that relies on syntactic cues ( w / syntactic cues ).', 'gender bias is measured by taking the difference of the performance in pro - and antistereotypical subsets.', 'previous work  #TAUTHOR_TAG evaluated the systems based on glove embeddings but here we evaluate a state - of - the - art system that trained on the ontonotes corpus with elmo embeddings']",0
"['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are']","['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are']","['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are associated with occupations predom']","['evaluate bias with respect to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are associated with occupations predominately associated with the gender of the pronoun, or anti - stereotype, when the opposite relation is true.', 'table 2 : f1 on ontonotes and  #TAUTHOR_TAG development sets.', ' #TAUTHOR_TAG is split semantics only and w / syntactic cues subsets.', 'elmo improves the performance on the ontonotes dataset by 5 % but shows stronger bias on the  #TAUTHOR_TAG', 'avg. stands for averaged f1 score on the pro - and anti - stereotype subsets while "" diff. "" is the absolute difference between these two subsets.', '* indicates the difference between pro / anti stereotypical conditions is significant ( p <. 05 ) under an approximate randomized test  #AUTHOR_TAG.', 'mitigating bias by data augmentation reduces all the bias from the coreference model to a neglect level.', 'however, the neutralizing elmo approach only mitigates bias when there are other strong learning signals for the task.', 'each subset consists of two types of sentences : one that requires semantic understanding of the sentence to make coreference resolution ( semantics only ) and another that relies on syntactic cues ( w / syntactic cues ).', 'gender bias is measured by taking the difference of the performance in pro - and antistereotypical subsets.', 'previous work  #TAUTHOR_TAG evaluated the systems based on glove embeddings but here we evaluate a state - of - the - art system that trained on the ontonotes corpus with elmo embeddings']",0
"['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are']","['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are']","['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are associated with occupations predom']","['evaluate bias with respect to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are associated with occupations predominately associated with the gender of the pronoun, or anti - stereotype, when the opposite relation is true.', 'table 2 : f1 on ontonotes and  #TAUTHOR_TAG development sets.', ' #TAUTHOR_TAG is split semantics only and w / syntactic cues subsets.', 'elmo improves the performance on the ontonotes dataset by 5 % but shows stronger bias on the  #TAUTHOR_TAG', 'avg. stands for averaged f1 score on the pro - and anti - stereotype subsets while "" diff. "" is the absolute difference between these two subsets.', '* indicates the difference between pro / anti stereotypical conditions is significant ( p <. 05 ) under an approximate randomized test  #AUTHOR_TAG.', 'mitigating bias by data augmentation reduces all the bias from the coreference model to a neglect level.', 'however, the neutralizing elmo approach only mitigates bias when there are other strong learning signals for the task.', 'each subset consists of two types of sentences : one that requires semantic understanding of the sentence to make coreference resolution ( semantics only ) and another that relies on syntactic cues ( w / syntactic cues ).', 'gender bias is measured by taking the difference of the performance in pro - and antistereotypical subsets.', 'previous work  #TAUTHOR_TAG evaluated the systems based on glove embeddings but here we evaluate a state - of - the - art system that trained on the ontonotes corpus with elmo embeddings']",0
"['', ' #TAUTHOR_TAG propose a method to']","['and ( 2 ) a test - time neutralization approach.', ' #TAUTHOR_TAG propose a method to']","['and ( 2 ) a test - time neutralization approach.', ' #TAUTHOR_TAG propose a method to']","['', ' #TAUTHOR_TAG propose a method to reduce gender bias in coreference resolution by augmenting the training corpus for this task.', 'data augmentation is performed by replacing gender revealing entities in the ontonotes dataset with words indicating the opposite gender and then training on the union of the original data and this swapped data.', 'in addition,  #TAUTHOR_TAG find it useful to also mitigate bias in supporting resources and therefore replace standard glove embeddings with bias mitigated word embeddings from  #AUTHOR_TAG.', 'we evaluate the performance of both aspects of  #TAUTHOR_TAG']",0
"['', ' #TAUTHOR_TAG propose a method to']","['and ( 2 ) a test - time neutralization approach.', ' #TAUTHOR_TAG propose a method to']","['and ( 2 ) a test - time neutralization approach.', ' #TAUTHOR_TAG propose a method to']","['', ' #TAUTHOR_TAG propose a method to reduce gender bias in coreference resolution by augmenting the training corpus for this task.', 'data augmentation is performed by replacing gender revealing entities in the ontonotes dataset with words indicating the opposite gender and then training on the union of the original data and this swapped data.', 'in addition,  #TAUTHOR_TAG find it useful to also mitigate bias in supporting resources and therefore replace standard glove embeddings with bias mitigated word embeddings from  #AUTHOR_TAG.', 'we evaluate the performance of both aspects of  #TAUTHOR_TAG']",0
"['example,  #TAUTHOR_TAG and  #AUTHOR_TAG show that coreference']","['example,  #TAUTHOR_TAG and  #AUTHOR_TAG show that coreference']","['example,  #TAUTHOR_TAG and  #AUTHOR_TAG show that coreference resolution systems relying on word embeddings encode such occupational stereotypes.', 'in concurrent work,  #AUTHOR_TAG measure gender bias in sentence embeddings, but']","['bias has been shown to affect several realworld applications relying on automatic language analysis, including online news  #AUTHOR_TAG, advertisements  #AUTHOR_TAG, abusive language detection  #AUTHOR_TAG, machine translation ( font and costa - jussa, 2019 ;  #AUTHOR_TAG, and web search  #AUTHOR_TAG.', 'in many cases, a model not only replicates bias in the training data but also amplifies it  #AUTHOR_TAG.', 'for word representations,  #AUTHOR_TAG and  #AUTHOR_TAG show that word embeddings encode societal biases about gender roles and occupations, e. g. engineers are stereotypically men, and nurses are stereotypically women.', 'as a consequence, downstream applications that use these pretrained word embeddings also reflect this bias.', 'for example,  #TAUTHOR_TAG and  #AUTHOR_TAG show that coreference resolution systems relying on word embeddings encode such occupational stereotypes.', 'in concurrent work,  #AUTHOR_TAG measure gender bias in sentence embeddings, but their evaluation is on the aggregation of word representations.', 'in contrast, we analyze bias in contextualized word representations and its effect on a downstream task.', 'to mitigate bias from word embeddings,  #AUTHOR_TAG propose a post - processing method to project out the bias subspace from the pre - trained embeddings.', 'their method is shown to reduce the gender information from the embeddings of gender - neutral words, and, remarkably, maintains the same level of performance on different downstream nlp tasks.', ' #AUTHOR_TAG b ) further propose a training mechanism to separate gender information from other factors.', ' #AUTHOR_TAG argue that entirely removing bias is difficult, if not impossible, and the gender bias information can be often recovered.', 'this paper investigates a natural follow - up question : what are effective bias mitigation techniques for contextualized embeddings']",4
"['co - occurrence of occupation words with those pronouns.', 'we use the set of occupation words defined in the  #TAUTHOR_TAG.', 'the analysis shows that the  #TAUTHOR_TAG contains a significant skew with']","['co - occurrence of occupation words with those pronouns.', 'we use the set of occupation words defined in the  #TAUTHOR_TAG.', 'the analysis shows that the  #TAUTHOR_TAG contains a significant skew with']","['##mo.', 'we show counts for the number of occurrences of male pronouns ( he, his and him ) and female pronouns ( she and her ) in the corpus as well as the co - occurrence of occupation words with those pronouns.', 'we use the set of occupation words defined in the  #TAUTHOR_TAG.', 'the analysis shows that the  #TAUTHOR_TAG contains a significant skew with']","['', 'm 5, 300, 000 170, 000 81, 000 f 1, 600, 000 33, 000 36, 000 nificantly more male entities compared to female entities leading to gender bias in the pre - trained contextual word embeddings ( 2 ) the geometry of trained elmo embeddings systematically encodes gender information and ( 3 ) elmo propagates gender information about male and female entities unequally.', 'table 1 lists the data analysis on the one billion word benchmark  #AUTHOR_TAG corpus, the training corpus for elmo.', 'we show counts for the number of occurrences of male pronouns ( he, his and him ) and female pronouns ( she and her ) in the corpus as well as the co - occurrence of occupation words with those pronouns.', 'we use the set of occupation words defined in the  #TAUTHOR_TAG.', 'the analysis shows that the  #TAUTHOR_TAG contains a significant skew with respect to gender : ( 1 ) male pronouns occur three times more than female pronouns and ( 2 ) male pronouns co - occur more frequently with occupation words, irrespective of whether they are prototypically male or female']",1
"['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are']","['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are']","['to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are associated with occupations predom']","['evaluate bias with respect to the  #TAUTHOR_TAG, a benchmark of paired male and female coreference resolution examples following the winograd format  #AUTHOR_TAG.', ' #TAUTHOR_TAG contains two different subsets, pro - stereotype, where pronouns are associated with occupations predominately associated with the gender of the pronoun, or anti - stereotype, when the opposite relation is true.', 'table 2 : f1 on ontonotes and  #TAUTHOR_TAG development sets.', ' #TAUTHOR_TAG is split semantics only and w / syntactic cues subsets.', 'elmo improves the performance on the ontonotes dataset by 5 % but shows stronger bias on the  #TAUTHOR_TAG', 'avg. stands for averaged f1 score on the pro - and anti - stereotype subsets while "" diff. "" is the absolute difference between these two subsets.', '* indicates the difference between pro / anti stereotypical conditions is significant ( p <. 05 ) under an approximate randomized test  #AUTHOR_TAG.', 'mitigating bias by data augmentation reduces all the bias from the coreference model to a neglect level.', 'however, the neutralizing elmo approach only mitigates bias when there are other strong learning signals for the task.', 'each subset consists of two types of sentences : one that requires semantic understanding of the sentence to make coreference resolution ( semantics only ) and another that relies on syntactic cues ( w / syntactic cues ).', 'gender bias is measured by taking the difference of the performance in pro - and antistereotypical subsets.', 'previous work  #TAUTHOR_TAG evaluated the systems based on glove embeddings but here we evaluate a state - of - the - art system that trained on the ontonotes corpus with elmo embeddings']",1
"['the elmo based coreference system on  #TAUTHOR_TAG.', 'while elmo']","['the elmo based coreference system on  #TAUTHOR_TAG.', 'while elmo']","['the elmo based coreference system on  #TAUTHOR_TAG.', 'while elmo']","['##mo bias transfers to coreference row 3 in table 2 summarizes performance of the elmo based coreference system on  #TAUTHOR_TAG.', 'while elmo helps to boost the coreference resolution f1 score ( ontonotes ) it also propagates bias to the task.', 'it exhibits large differences between pro - and anti - stereotyped sets ( | diff | ) on both semantic and syntactic examples in  #TAUTHOR_TAG.', 'bias mitigation rows 4 - 6 in table 2 summarize the effectiveness of the two bias mitigation approaches we consider.', 'data augmentation is largely effective at mitigating bias in the coreference resolution system with elmo ( reducing | diff | to insignificant levels ) but requires retraining the system.', 'neutralization is less effective than augmentation and cannot fully remove gender bias on the semantics only portion of  #TAUTHOR_TAG, indicating it is effective only for simpler cases.', 'this observation is consistent with  #AUTHOR_TAG, where they show that entirely removing bias from an embedding is difficult and depends on the manner, by which one measures the bias']",1
"['the role of the local language catalan in * equal contributions.', 'this setting, we apply the methodology used by  #TAUTHOR_TAG in the context of']","['the role of the local language catalan in * equal contributions.', 'this setting, we apply the methodology used by  #TAUTHOR_TAG in the context of']","['the role of the local language catalan in * equal contributions.', 'this setting, we apply the methodology used by  #TAUTHOR_TAG in the context of the 2014 scottish independence referendum to a dataset of tweets']","['identity is often constructed through language use, and variation in language therefore reflects social differences within the population  #AUTHOR_TAG.', ""in a multilingual setting, an individual's preference to use a local language rather than the national one may reflect their political stance, as the local language can have strong ties to cultural and political identity  #AUTHOR_TAG."", 'the role of linguistic identity is enhanced in extreme situations such as referenda, where the voting decision may be driven by identification with a local culture or language  #AUTHOR_TAG.', 'in october 2017, the semi - autonomous region of catalonia held a referendum on independence from spain, where 92 % of respondents voted for independence  #AUTHOR_TAG.', 'to determine the role of the local language catalan in * equal contributions.', 'this setting, we apply the methodology used by  #TAUTHOR_TAG in the context of the 2014 scottish independence referendum to a dataset of tweets related to the catalonian referendum.', 'we use the phenomenon of code - switching between catalan and spanish to pursue the following research questions in order to understand the choice of language in the context of the referendum :', '']",3
"['to political separatism,  #TAUTHOR_TAG studied the use of scots, a language local to scotland, in the context of the 2014 scotland independence referendum.', 'they']","['to political separatism,  #TAUTHOR_TAG studied the use of scots, a language local to scotland, in the context of the 2014 scotland independence referendum.', 'they']","['to political separatism,  #TAUTHOR_TAG studied the use of scots, a language local to scotland, in the context of the 2014 scotland independence referendum.', 'they found that twitter users who openly supported scottish independence were more likely to incorporate words from scots in']","['- switching, the alternation between languages within conversation  #AUTHOR_TAG, has been shown to be the product of grammatical factors, such as syntax  #AUTHOR_TAG, and social factors, such as intended audience  #AUTHOR_TAG.', 'while many studies have examined codeswitching in the spoken context  #AUTHOR_TAG social media platforms such as twitter provide an opportunity to study code - switching in online discussions  #AUTHOR_TAG.', ""in the online context, choice of language may reflect the writer's intended audience  #AUTHOR_TAG or identity  #AUTHOR_TAG, and the explicit social signals in online discussions such as @ - replies can be leveraged to test claims about code - switching at a large scale  #AUTHOR_TAG."", 'a relatively unexplored area of code - switching behavior is politically - motivated code - switching, which we assume has a different set of constraints compared to everyday code - switching.', 'with respect to political separatism,  #TAUTHOR_TAG studied the use of scots, a language local to scotland, in the context of the 2014 scotland independence referendum.', 'they found that twitter users who openly supported scottish independence were more likely to incorporate words from scots in their tweets.', 'they also found that twitter users who tweeted about the referendum were less likely to use scots in referendum - related tweets than in non - referendum tweets.', 'this study considers the similar scenario which took place in 2017 vis - a - vis the semi - autonomous region of catalonia.', 'our main methodological divergence from  #TAUTHOR_TAG relates to the linguistic phenomenon at hand : while scots is mainly manifested as interleaving individual words within english text ( code - mixing ), catalan is a distinct language which, when used, usually replaces spanish altogether for the entire tweet ( code - switching )']",3
"['users in  #TAUTHOR_TAG.', '36 referen']","['users in  #TAUTHOR_TAG.', '36 referendum - related hashtags appear in the filtered dataset.', '']","['##9 users in  #TAUTHOR_TAG.', '36 referendum - related hashtags appear in the filtered dataset.', 'they are shown with']","['initial set of tweets for this study, t, was drawn from a 1 % twitter sample mined between january 1 and october 31, 2017, covering nearly a year of activity before the referendum, as well as its immediate aftermath.', '2 the first step in building this dataset was to manually develop a seed set of hashtags related to the referendum.', 'through browsing referendum content on twitter, the following seed hashtags were selected : # catalunalibre, # independenci - acataluna, # catalunaesespana, # espanaunida, and # catalanreferendum.', 'all tweets containing at least one of these hashtags were extracted from t, and the top 1, 000 hashtags appearing in the resulting dataset were manually inspected for relevance to the referendum.', 'from these co - occurring hashtags, we selected a set of 46 hashtags and divided it into pro - independence, anti - independence, and neutral hashtags, based on translations of associated tweet content.', '3 after including ascii - equivalent variants of special characters, as well as lowercased variants, our final hashtag set comprises 111 unique strings.', 'next, all tweets containing any referendum hashtag were extracted from t, yielding 190, 061 tweets.', 'after removing retweets and tweets from users whose tweets frequently contained urls ( i. e., likely bots ), our final "" catalonian independence tweets "" ( ct ) dataset is made up of 11, 670 tweets from 10, 498 users ( cf. the scottish referendum set it with 59, 664 tweets and 18, 589 users in  #TAUTHOR_TAG.', '36 referendum - related hashtags appear in the filtered dataset.', 'they are shown with their frequencies ( including variants ) in table 1 ( cf. the 47 hashtags and similar frequency distribution in table 1 of  #TAUTHOR_TAG.', 'to address the control condition, all authors of tweets in the ct dataset were collected to form a set u, and all other tweets in t written by these users were extracted into a control dataset ( xt ) of 45, 222 tweets ( cf.', 'the 693, 815 control tweets in table 6 of  #TAUTHOR_TAG.', 'the ct dataset is very balanced with respect to the number of tweets per user : only four users contribute over ten tweets ( max = 14 ) and only 16 have more than five.', 'the xt dataset also has only a few "" power "" users, such that nine']",3
"['for anti - independence users ( p anti ).', 'this is consistent with  #TAUTHOR_TAG, who found more scots usage among proindependence users (']","['for anti - independence users ( p anti ).', 'this is consistent with  #TAUTHOR_TAG, who found more scots usage among proindependence users ( d = 0. 00555']","['- tweet conditions.', 'table 3 shows that the proportion of tweets in catalan for proindependence users ( p pro ) is significantly higher than the proportion for anti - independence users ( p anti ).', 'this is consistent with  #TAUTHOR_TAG, who found more scots usage among proindependence users (']","['first research question concerns political stance : do pro - independence users tweet in catalan at a higher rate than anti - independence users?', 'we analyze the relationship between language use and stance on independence under two conditions, comparing the use of catalan among pro - independence users vs. anti - independence users in ( 1 ) opinionated referendum - related tweets ( tweets with pro / anti hashtags ) ; and ( 2 ) all tweets.', 'these conditions address the possibilities that the language distinction is relevant for pro / antiindependence twitter users in political discourse and outside of political discourse, respectively.', 'method.', 'the first step is to divide the twitter users in u into pro - independence ( pro ) and antiindependence ( anti ) groups.', 'first, the proportion of tweets from each user that include a pro - independence hashtag is computed as', 'anti ) is the count of tweets from user u that contain a pro - ( anti - ) independence hashtag.', 'the pro user set ( u pro ) includes all users whose pro - independence proportion was above or equal to 75 %, and the anti user set ( u anti ) includes all users whose pro - independence proportion was below or equal to 25 %.', 'the counts of users and tweets identified as either spanish or catalan are presented in table 2.', 'to measure catalan usage, let n to determine significance, the users are randomly shuffled between the two groups to recompute d over 100, 000 iterations.', 'the p - value is the proportion of permutations in which the randomized test statistic was greater than or equal to the original test statistic from the unpermuted data.', 'results. catalan is used more often among the pro - independence users compared to the antiindependence users, across both the hashtagonly and all - tweet conditions.', 'table 3 shows that the proportion of tweets in catalan for proindependence users ( p pro ) is significantly higher than the proportion for anti - independence users ( p anti ).', 'this is consistent with  #TAUTHOR_TAG, who found more scots usage among proindependence users ( d = 0. 00555 for pro / anti tweets, d = 0. 00709 for all tweets ).', 'the relative differences between the groups are large : in the all - tweet condition, p pro is five times greater than p anti, whereas shoemark et al. found a twofold difference ( p pro = 0. 01443 versusp anti = 0. 00734 for all - tweet condition ).', 'all']",3
"['the role of the local language catalan in * equal contributions.', 'this setting, we apply the methodology used by  #TAUTHOR_TAG in the context of']","['the role of the local language catalan in * equal contributions.', 'this setting, we apply the methodology used by  #TAUTHOR_TAG in the context of']","['the role of the local language catalan in * equal contributions.', 'this setting, we apply the methodology used by  #TAUTHOR_TAG in the context of the 2014 scottish independence referendum to a dataset of tweets']","['identity is often constructed through language use, and variation in language therefore reflects social differences within the population  #AUTHOR_TAG.', ""in a multilingual setting, an individual's preference to use a local language rather than the national one may reflect their political stance, as the local language can have strong ties to cultural and political identity  #AUTHOR_TAG."", 'the role of linguistic identity is enhanced in extreme situations such as referenda, where the voting decision may be driven by identification with a local culture or language  #AUTHOR_TAG.', 'in october 2017, the semi - autonomous region of catalonia held a referendum on independence from spain, where 92 % of respondents voted for independence  #AUTHOR_TAG.', 'to determine the role of the local language catalan in * equal contributions.', 'this setting, we apply the methodology used by  #TAUTHOR_TAG in the context of the 2014 scottish independence referendum to a dataset of tweets related to the catalonian referendum.', 'we use the phenomenon of code - switching between catalan and spanish to pursue the following research questions in order to understand the choice of language in the context of the referendum :', '']",5
"['users in  #TAUTHOR_TAG.', '36 referen']","['users in  #TAUTHOR_TAG.', '36 referendum - related hashtags appear in the filtered dataset.', '']","['##9 users in  #TAUTHOR_TAG.', '36 referendum - related hashtags appear in the filtered dataset.', 'they are shown with']","['initial set of tweets for this study, t, was drawn from a 1 % twitter sample mined between january 1 and october 31, 2017, covering nearly a year of activity before the referendum, as well as its immediate aftermath.', '2 the first step in building this dataset was to manually develop a seed set of hashtags related to the referendum.', 'through browsing referendum content on twitter, the following seed hashtags were selected : # catalunalibre, # independenci - acataluna, # catalunaesespana, # espanaunida, and # catalanreferendum.', 'all tweets containing at least one of these hashtags were extracted from t, and the top 1, 000 hashtags appearing in the resulting dataset were manually inspected for relevance to the referendum.', 'from these co - occurring hashtags, we selected a set of 46 hashtags and divided it into pro - independence, anti - independence, and neutral hashtags, based on translations of associated tweet content.', '3 after including ascii - equivalent variants of special characters, as well as lowercased variants, our final hashtag set comprises 111 unique strings.', 'next, all tweets containing any referendum hashtag were extracted from t, yielding 190, 061 tweets.', 'after removing retweets and tweets from users whose tweets frequently contained urls ( i. e., likely bots ), our final "" catalonian independence tweets "" ( ct ) dataset is made up of 11, 670 tweets from 10, 498 users ( cf. the scottish referendum set it with 59, 664 tweets and 18, 589 users in  #TAUTHOR_TAG.', '36 referendum - related hashtags appear in the filtered dataset.', 'they are shown with their frequencies ( including variants ) in table 1 ( cf. the 47 hashtags and similar frequency distribution in table 1 of  #TAUTHOR_TAG.', 'to address the control condition, all authors of tweets in the ct dataset were collected to form a set u, and all other tweets in t written by these users were extracted into a control dataset ( xt ) of 45, 222 tweets ( cf.', 'the 693, 815 control tweets in table 6 of  #TAUTHOR_TAG.', 'the ct dataset is very balanced with respect to the number of tweets per user : only four users contribute over ten tweets ( max = 14 ) and only 16 have more than five.', 'the xt dataset also has only a few "" power "" users, such that nine']",5
"['to political separatism,  #TAUTHOR_TAG studied the use of scots, a language local to scotland, in the context of the 2014 scotland independence referendum.', 'they']","['to political separatism,  #TAUTHOR_TAG studied the use of scots, a language local to scotland, in the context of the 2014 scotland independence referendum.', 'they']","['to political separatism,  #TAUTHOR_TAG studied the use of scots, a language local to scotland, in the context of the 2014 scotland independence referendum.', 'they found that twitter users who openly supported scottish independence were more likely to incorporate words from scots in']","['- switching, the alternation between languages within conversation  #AUTHOR_TAG, has been shown to be the product of grammatical factors, such as syntax  #AUTHOR_TAG, and social factors, such as intended audience  #AUTHOR_TAG.', 'while many studies have examined codeswitching in the spoken context  #AUTHOR_TAG social media platforms such as twitter provide an opportunity to study code - switching in online discussions  #AUTHOR_TAG.', ""in the online context, choice of language may reflect the writer's intended audience  #AUTHOR_TAG or identity  #AUTHOR_TAG, and the explicit social signals in online discussions such as @ - replies can be leveraged to test claims about code - switching at a large scale  #AUTHOR_TAG."", 'a relatively unexplored area of code - switching behavior is politically - motivated code - switching, which we assume has a different set of constraints compared to everyday code - switching.', 'with respect to political separatism,  #TAUTHOR_TAG studied the use of scots, a language local to scotland, in the context of the 2014 scotland independence referendum.', 'they found that twitter users who openly supported scottish independence were more likely to incorporate words from scots in their tweets.', 'they also found that twitter users who tweeted about the referendum were less likely to use scots in referendum - related tweets than in non - referendum tweets.', 'this study considers the similar scenario which took place in 2017 vis - a - vis the semi - autonomous region of catalonia.', 'our main methodological divergence from  #TAUTHOR_TAG relates to the linguistic phenomenon at hand : while scots is mainly manifested as interleaving individual words within english text ( code - mixing ), catalan is a distinct language which, when used, usually replaces spanish altogether for the entire tweet ( code - switching )']",0
"['users in  #TAUTHOR_TAG.', '36 referen']","['users in  #TAUTHOR_TAG.', '36 referendum - related hashtags appear in the filtered dataset.', '']","['##9 users in  #TAUTHOR_TAG.', '36 referendum - related hashtags appear in the filtered dataset.', 'they are shown with']","['initial set of tweets for this study, t, was drawn from a 1 % twitter sample mined between january 1 and october 31, 2017, covering nearly a year of activity before the referendum, as well as its immediate aftermath.', '2 the first step in building this dataset was to manually develop a seed set of hashtags related to the referendum.', 'through browsing referendum content on twitter, the following seed hashtags were selected : # catalunalibre, # independenci - acataluna, # catalunaesespana, # espanaunida, and # catalanreferendum.', 'all tweets containing at least one of these hashtags were extracted from t, and the top 1, 000 hashtags appearing in the resulting dataset were manually inspected for relevance to the referendum.', 'from these co - occurring hashtags, we selected a set of 46 hashtags and divided it into pro - independence, anti - independence, and neutral hashtags, based on translations of associated tweet content.', '3 after including ascii - equivalent variants of special characters, as well as lowercased variants, our final hashtag set comprises 111 unique strings.', 'next, all tweets containing any referendum hashtag were extracted from t, yielding 190, 061 tweets.', 'after removing retweets and tweets from users whose tweets frequently contained urls ( i. e., likely bots ), our final "" catalonian independence tweets "" ( ct ) dataset is made up of 11, 670 tweets from 10, 498 users ( cf. the scottish referendum set it with 59, 664 tweets and 18, 589 users in  #TAUTHOR_TAG.', '36 referendum - related hashtags appear in the filtered dataset.', 'they are shown with their frequencies ( including variants ) in table 1 ( cf. the 47 hashtags and similar frequency distribution in table 1 of  #TAUTHOR_TAG.', 'to address the control condition, all authors of tweets in the ct dataset were collected to form a set u, and all other tweets in t written by these users were extracted into a control dataset ( xt ) of 45, 222 tweets ( cf.', 'the 693, 815 control tweets in table 6 of  #TAUTHOR_TAG.', 'the ct dataset is very balanced with respect to the number of tweets per user : only four users contribute over ten tweets ( max = 14 ) and only 16 have more than five.', 'the xt dataset also has only a few "" power "" users, such that nine']",0
"['users in  #TAUTHOR_TAG.', '36 referen']","['users in  #TAUTHOR_TAG.', '36 referendum - related hashtags appear in the filtered dataset.', '']","['##9 users in  #TAUTHOR_TAG.', '36 referendum - related hashtags appear in the filtered dataset.', 'they are shown with']","['initial set of tweets for this study, t, was drawn from a 1 % twitter sample mined between january 1 and october 31, 2017, covering nearly a year of activity before the referendum, as well as its immediate aftermath.', '2 the first step in building this dataset was to manually develop a seed set of hashtags related to the referendum.', 'through browsing referendum content on twitter, the following seed hashtags were selected : # catalunalibre, # independenci - acataluna, # catalunaesespana, # espanaunida, and # catalanreferendum.', 'all tweets containing at least one of these hashtags were extracted from t, and the top 1, 000 hashtags appearing in the resulting dataset were manually inspected for relevance to the referendum.', 'from these co - occurring hashtags, we selected a set of 46 hashtags and divided it into pro - independence, anti - independence, and neutral hashtags, based on translations of associated tweet content.', '3 after including ascii - equivalent variants of special characters, as well as lowercased variants, our final hashtag set comprises 111 unique strings.', 'next, all tweets containing any referendum hashtag were extracted from t, yielding 190, 061 tweets.', 'after removing retweets and tweets from users whose tweets frequently contained urls ( i. e., likely bots ), our final "" catalonian independence tweets "" ( ct ) dataset is made up of 11, 670 tweets from 10, 498 users ( cf. the scottish referendum set it with 59, 664 tweets and 18, 589 users in  #TAUTHOR_TAG.', '36 referendum - related hashtags appear in the filtered dataset.', 'they are shown with their frequencies ( including variants ) in table 1 ( cf. the 47 hashtags and similar frequency distribution in table 1 of  #TAUTHOR_TAG.', 'to address the control condition, all authors of tweets in the ct dataset were collected to form a set u, and all other tweets in t written by these users were extracted into a control dataset ( xt ) of 45, 222 tweets ( cf.', 'the 693, 815 control tweets in table 6 of  #TAUTHOR_TAG.', 'the ct dataset is very balanced with respect to the number of tweets per user : only four users contribute over ten tweets ( max = 14 ) and only 16 have more than five.', 'the xt dataset also has only a few "" power "" users, such that nine']",0
['in  #TAUTHOR_TAG ;'],['in  #TAUTHOR_TAG ;'],['in  #TAUTHOR_TAG ;'],"['.', 'we extract all users in u who have posted at least one referendum - related tweet and at least one tweet unrelated to the referendum into a new set, u r.', 'tweet and user counts for all conditions are provided in table 4.', 'the small numbers are a result of the condition requirement and the language constraint ( tweets must be identified as spanish or catalan with 90 % confidence ).', ""for a user u, we denote the proportion of u's referendum - related tweets written in catalan byp ( u ) c, and the proportion of u's control tweets written in catalan byp ( u ) x."", 'we are interested in the difference between these two propor -', 'x and its average across all u ).', 'under the null hypothesis that catalan usage is unrelated to topic, d u r would be equal to 0, which we test for significance using a one - sample t - test.', 'results.', 'our results, presented in the middle columns of table 5, show that users tweet in catalan at a significantly higher rate in referendum tweets than in all control tweets ( first results column ), but no significant difference was observed in the control condition where tweets include at least one hashtag ( second results column ).', ""the lack of a significant difference between referendum - related hashtags and other hashtags suggests that the topic being discussed is not as central in choosing one's language, compared with the audience being targeted."", 'our second result is the opposite of the prior finding that there were significantly fewer scots words in referendum - related tweets than in control tweets ( cf.', 'table 7 in  #TAUTHOR_TAG ; d u = −0. 0015 for all controls ).', 'this suggests that catalan may serve a different function than scots in terms of political identity expression.', 'rather than suppressing their use of catalan in broadcast tweets, users increase their catalan use, perhaps to signal their catalonian identity to a broader audience.', 'this is supported by literature highlighting the integral role catalan plays in the catalonian national narrative  #AUTHOR_TAG, as well as the relatively high proportion of catalan speakers in catalonia : 80. 4 % of the population has speaking knowledge of catalan ( government of  #AUTHOR_TAG, versus 30 % population of scotland with speaking knowledge of scots  #AUTHOR_TAG.', 'there are also systemic differences between the political settings of the two cases : the catalonian referendum had much larger support for separation among those who voted ( 92 % in catalonia vs. 45']",4
['grid  #TAUTHOR_TAG uses information'],['grid  #TAUTHOR_TAG uses information'],['grid  #TAUTHOR_TAG uses information'],"['', 'the third type of projection, p acc, integrates syntactic information in the edge weights calculated by the following formula : while the entity grid  #TAUTHOR_TAG uses information about sentences which do not share entities by means of the "" - - "" transition, the entity graph cannot employ this negative information.', 'here, we propose a normalization for the entity graph and its corresponding one - mode projections which is based on the relative importance of entities and, in turn, the relative importance of sentences.', 'including negative information allows to normalize the importance of entities according to sentence length ( measured in terms of entity mentions ), and hence to capture distance information between mentions of the same entity.', 'this brings the entity graph closer to  #AUTHOR_TAG, p. 30 ) notion of cohesion : "" the relative cohesiveness of a text depends on the number of cohesive ties [... ] and on the distance between the nodes and their associated cohesive elements.', '"" by using this information, edge weights are set less arbitrary which leads to the more sound method and higher performance in all tasks']",0
"['s, are reproduced, results for  #TAUTHOR_TAG, b & l, and  #AUTHOR_TAG, e']","['1 shows the results.', 'results for  #AUTHOR_TAG, g & s, are reproduced, results for  #TAUTHOR_TAG, b & l, and  #AUTHOR_TAG, e & c, were reproduced by  #AUTHOR_TAG.', '']","['s, are reproduced, results for  #TAUTHOR_TAG, b & l, and  #AUTHOR_TAG, e']","['task consists of two subtasks : discrimination and insertion.', 'in both subtasks we evaluate whether our model can distinguish between the correct order of sentences in a document and an incorrect one.', 'experimental setup and data follow  #AUTHOR_TAG ( 61 documents from the english test part of the conll 2012 shared task  #AUTHOR_TAG ).', 'for discrimination we use 20 permutations of each text.', 'table 1 shows the results.', 'results for  #AUTHOR_TAG, g & s, are reproduced, results for  #TAUTHOR_TAG, b & l, and  #AUTHOR_TAG, e & c, were reproduced by  #AUTHOR_TAG.', '']",0
"['##ability assessment aims to distinguish texts which are difficult to read from texts which are easier to read.', 'in experiments,  #TAUTHOR_TAG assume that articles taken from']","['##ability assessment aims to distinguish texts which are difficult to read from texts which are easier to read.', 'in experiments,  #TAUTHOR_TAG assume that articles taken from encyclopedia britannica are more difficult to read ( less coherent ) than the corresponding articles']","['##ability assessment aims to distinguish texts which are difficult to read from texts which are easier to read.', 'in experiments,  #TAUTHOR_TAG assume that articles taken from']","['##ability assessment aims to distinguish texts which are difficult to read from texts which are easier to read.', 'in experiments,  #TAUTHOR_TAG assume that articles taken from encyclopedia britannica are more difficult to read ( less coherent ) than the corresponding articles from encyclopedia britannica elementary, its version for children.', 'we follow them with regard to data ( 107 article pairs ), experimental setup and evaluation.', 'sentences in the britannica elementary are simpler and shorter than in the encyclopedia britannica.', 'the entity graph does not take into account the effect of entities not shared between sentences while the normalized entity graph assigns a lower weight if there are more of these entities.', 'hence, britannica elementary receives a higher cohesion score than encyclopedia britannica in our model.', 'adding grammatical information, does not help, because of the influence of the number of entities ( shared and not shared ) outweighs the influence of syntactic roles.', 'the normalized entity graph ( p w, dist ) does not only outperform the entity graph ( significantly ) and b & l but also s & o and the combination b & l + s & o']",0
['grid  #TAUTHOR_TAG uses information'],['grid  #TAUTHOR_TAG uses information'],['grid  #TAUTHOR_TAG uses information'],"['', 'the third type of projection, p acc, integrates syntactic information in the edge weights calculated by the following formula : while the entity grid  #TAUTHOR_TAG uses information about sentences which do not share entities by means of the "" - - "" transition, the entity graph cannot employ this negative information.', 'here, we propose a normalization for the entity graph and its corresponding one - mode projections which is based on the relative importance of entities and, in turn, the relative importance of sentences.', 'including negative information allows to normalize the importance of entities according to sentence length ( measured in terms of entity mentions ), and hence to capture distance information between mentions of the same entity.', 'this brings the entity graph closer to  #AUTHOR_TAG, p. 30 ) notion of cohesion : "" the relative cohesiveness of a text depends on the number of cohesive ties [... ] and on the distance between the nodes and their associated cohesive elements.', '"" by using this information, edge weights are set less arbitrary which leads to the more sound method and higher performance in all tasks']",1
"['##ability assessment aims to distinguish texts which are difficult to read from texts which are easier to read.', 'in experiments,  #TAUTHOR_TAG assume that articles taken from']","['##ability assessment aims to distinguish texts which are difficult to read from texts which are easier to read.', 'in experiments,  #TAUTHOR_TAG assume that articles taken from encyclopedia britannica are more difficult to read ( less coherent ) than the corresponding articles']","['##ability assessment aims to distinguish texts which are difficult to read from texts which are easier to read.', 'in experiments,  #TAUTHOR_TAG assume that articles taken from']","['##ability assessment aims to distinguish texts which are difficult to read from texts which are easier to read.', 'in experiments,  #TAUTHOR_TAG assume that articles taken from encyclopedia britannica are more difficult to read ( less coherent ) than the corresponding articles from encyclopedia britannica elementary, its version for children.', 'we follow them with regard to data ( 107 article pairs ), experimental setup and evaluation.', 'sentences in the britannica elementary are simpler and shorter than in the encyclopedia britannica.', 'the entity graph does not take into account the effect of entities not shared between sentences while the normalized entity graph assigns a lower weight if there are more of these entities.', 'hence, britannica elementary receives a higher cohesion score than encyclopedia britannica in our model.', 'adding grammatical information, does not help, because of the influence of the number of entities ( shared and not shared ) outweighs the influence of syntactic roles.', 'the normalized entity graph ( p w, dist ) does not only outperform the entity graph ( significantly ) and b & l but also s & o and the combination b & l + s & o']",1
['follow  #TAUTHOR_TAG'],['follow  #TAUTHOR_TAG'],['follow  #TAUTHOR_TAG'],"['follow  #TAUTHOR_TAG for evaluating whether the normalized entity graph can decide whether automatic or human summaries are more coherent ( 80 pairs of summaries extracted from duc 2003 ).', 'human coherence scores are associated with each pair of summarized documents  #TAUTHOR_TAG.', 'table 3 displays reported results of b & l and reproduced results of the entity graph and our normalized entity graph.', 'normalizing significantly improves the results for p w and p acc.', 'p u is still slightly better than both, but in contrast to the entity graph, this difference is not statistically significant.', 'we believe that better weighting schemes based on linguistic insights eventually will outperform p u and b & l ( left for future work ).', 'distance information always degrades the results for this task ( see  #AUTHOR_TAG )']",5
['follow  #TAUTHOR_TAG'],['follow  #TAUTHOR_TAG'],['follow  #TAUTHOR_TAG'],"['follow  #TAUTHOR_TAG for evaluating whether the normalized entity graph can decide whether automatic or human summaries are more coherent ( 80 pairs of summaries extracted from duc 2003 ).', 'human coherence scores are associated with each pair of summarized documents  #TAUTHOR_TAG.', 'table 3 displays reported results of b & l and reproduced results of the entity graph and our normalized entity graph.', 'normalizing significantly improves the results for p w and p acc.', 'p u is still slightly better than both, but in contrast to the entity graph, this difference is not statistically significant.', 'we believe that better weighting schemes based on linguistic insights eventually will outperform p u and b & l ( left for future work ).', 'distance information always degrades the results for this task ( see  #AUTHOR_TAG )']",5
"['##ability assessment aims to distinguish texts which are difficult to read from texts which are easier to read.', 'in experiments,  #TAUTHOR_TAG assume that articles taken from']","['##ability assessment aims to distinguish texts which are difficult to read from texts which are easier to read.', 'in experiments,  #TAUTHOR_TAG assume that articles taken from encyclopedia britannica are more difficult to read ( less coherent ) than the corresponding articles']","['##ability assessment aims to distinguish texts which are difficult to read from texts which are easier to read.', 'in experiments,  #TAUTHOR_TAG assume that articles taken from']","['##ability assessment aims to distinguish texts which are difficult to read from texts which are easier to read.', 'in experiments,  #TAUTHOR_TAG assume that articles taken from encyclopedia britannica are more difficult to read ( less coherent ) than the corresponding articles from encyclopedia britannica elementary, its version for children.', 'we follow them with regard to data ( 107 article pairs ), experimental setup and evaluation.', 'sentences in the britannica elementary are simpler and shorter than in the encyclopedia britannica.', 'the entity graph does not take into account the effect of entities not shared between sentences while the normalized entity graph assigns a lower weight if there are more of these entities.', 'hence, britannica elementary receives a higher cohesion score than encyclopedia britannica in our model.', 'adding grammatical information, does not help, because of the influence of the number of entities ( shared and not shared ) outweighs the influence of syntactic roles.', 'the normalized entity graph ( p w, dist ) does not only outperform the entity graph ( significantly ) and b & l but also s & o and the combination b & l + s & o']",5
['entity grid  #TAUTHOR_TAG and showed that normalization'],['entity grid  #TAUTHOR_TAG and showed that normalization'],"['proposed a normalization method for the entity graph  #AUTHOR_TAG.', 'we compared our model to the entity graph and to the entity grid  #TAUTHOR_TAG and showed that normalization']","['proposed a normalization method for the entity graph  #AUTHOR_TAG.', 'we compared our model to the entity graph and to the entity grid  #TAUTHOR_TAG and showed that normalization improves the results significantly in most tasks.', 'future work will include adding more linguistic information, stronger weighting schemes and application to other readability datasets  #AUTHOR_TAG']",4
"['has been widely used in the context of dialog policy learning  #TAUTHOR_TAG.', 'however according to a recent comparison  #TAUTHOR_TAG in']","['has been widely used in the context of dialog policy learning  #TAUTHOR_TAG.', 'however according to a recent comparison  #TAUTHOR_TAG in']","['##n )  #AUTHOR_TAG introducing the idea of using neural networks as a q - function approximator.', 'it has been widely used in the context of dialog policy learning  #TAUTHOR_TAG.', 'however according to a recent comparison  #TAUTHOR_TAG in the context of dialog policy learning, it performed worse than other rl methods such']","['', 'depending on the ontology of the task, e. g. the restaurant search, the size of the input space for the policy can quickly become very large.', 'furthermore, the belief state might be wrong due to noisy inputs, e. g. the user could be misunderstood because of nlu errors or in general, language ambiguity.', 'therefore, building such policies by hand is rather time consuming.', 'reinforcement learning ( rl ) can alleviate this task by allowing to learn such policies automatically  #AUTHOR_TAG with a user simulator such as proposed in  #AUTHOR_TAG within a task  #AUTHOR_TAG, between task and non - task  #AUTHOR_TAG and also in multimodal dialog systems  #AUTHOR_TAG.', 'deep rl has been proven to be successful with deep q - learning ( dqn )  #AUTHOR_TAG introducing the idea of using neural networks as a q - function approximator.', 'it has been widely used in the context of dialog policy learning  #TAUTHOR_TAG.', 'however according to a recent comparison  #TAUTHOR_TAG in the context of dialog policy learning, it performed worse than other rl methods such as gaussian process in many testing conditions.', 'recently, several advances in deep rl such as distributional rl  #AUTHOR_TAG, dueling network architectures  #AUTHOR_TAG and their combination  #AUTHOR_TAG a rainbow agent - have been shown to be promising for further improvements of deep rl agents in benchmark environments, e. g. atari 2600.', 'however, it is still unclear whether these methods could advance dialog policies.', 'this paper attempts to provide insights motivated from dialog policy modeling perspectives how to use state - of - the - art deep rl methods such as prioritized experience replay  #AUTHOR_TAG, double dqn  #AUTHOR_TAG, dueling network architecture, distributional learning method and how to combine them to train the rainbow agent for dialog policy learning 1.', 'moreover, we explore the influence of each method w. r. t the resulting rewards and the number of successful dialogs, highlighting methods with the biggest and the smallest impact.', 'env.', '1 env.', '2 env.', '3 env.', '4']",0
"['has been widely used in the context of dialog policy learning  #TAUTHOR_TAG.', 'however according to a recent comparison  #TAUTHOR_TAG in']","['has been widely used in the context of dialog policy learning  #TAUTHOR_TAG.', 'however according to a recent comparison  #TAUTHOR_TAG in']","['##n )  #AUTHOR_TAG introducing the idea of using neural networks as a q - function approximator.', 'it has been widely used in the context of dialog policy learning  #TAUTHOR_TAG.', 'however according to a recent comparison  #TAUTHOR_TAG in the context of dialog policy learning, it performed worse than other rl methods such']","['', 'depending on the ontology of the task, e. g. the restaurant search, the size of the input space for the policy can quickly become very large.', 'furthermore, the belief state might be wrong due to noisy inputs, e. g. the user could be misunderstood because of nlu errors or in general, language ambiguity.', 'therefore, building such policies by hand is rather time consuming.', 'reinforcement learning ( rl ) can alleviate this task by allowing to learn such policies automatically  #AUTHOR_TAG with a user simulator such as proposed in  #AUTHOR_TAG within a task  #AUTHOR_TAG, between task and non - task  #AUTHOR_TAG and also in multimodal dialog systems  #AUTHOR_TAG.', 'deep rl has been proven to be successful with deep q - learning ( dqn )  #AUTHOR_TAG introducing the idea of using neural networks as a q - function approximator.', 'it has been widely used in the context of dialog policy learning  #TAUTHOR_TAG.', 'however according to a recent comparison  #TAUTHOR_TAG in the context of dialog policy learning, it performed worse than other rl methods such as gaussian process in many testing conditions.', 'recently, several advances in deep rl such as distributional rl  #AUTHOR_TAG, dueling network architectures  #AUTHOR_TAG and their combination  #AUTHOR_TAG a rainbow agent - have been shown to be promising for further improvements of deep rl agents in benchmark environments, e. g. atari 2600.', 'however, it is still unclear whether these methods could advance dialog policies.', 'this paper attempts to provide insights motivated from dialog policy modeling perspectives how to use state - of - the - art deep rl methods such as prioritized experience replay  #AUTHOR_TAG, double dqn  #AUTHOR_TAG, dueling network architecture, distributional learning method and how to combine them to train the rainbow agent for dialog policy learning 1.', 'moreover, we explore the influence of each method w. r. t the resulting rewards and the number of successful dialogs, highlighting methods with the biggest and the smallest impact.', 'env.', '1 env.', '2 env.', '3 env.', '4']",0
"['##able slot  #AUTHOR_TAG.', ' #TAUTHOR_TAG propose six different environmental models, varying in user friendliness, simulated input channel noise and the presence or absence of action masks, which']","['requestable slot  #AUTHOR_TAG.', ' #TAUTHOR_TAG propose six different environmental models, varying in user friendliness, simulated input channel noise and the presence or absence of action masks, which, when enabled, simplify learning by masking some of the possible actions.', 'an overview of all these environmental configurations']","['# values of each requestable slot  #AUTHOR_TAG.', ' #TAUTHOR_TAG propose six different environmental models, varying in user friendliness, simulated input channel noise and the presence or absence of action masks, which']","['used pydial toolkit as a test - bed for experiments and evaluation.', 'it includes a configurable user simulator and provides multiple dialog ontologies like cambridge restaurants ( cr ), laptops ( lap ) and san francisco restaurants ( sfr ).', 'the ontologies used for the benchmarks in this paper together with their properties are listed in table 2.', 'cr 3 9 268 sfr 6 11 636 lap 11 21 257 table 2 : benchmark domains with # slots the user can provide or # request from the system as well as # values of each requestable slot  #AUTHOR_TAG.', ' #TAUTHOR_TAG propose six different environmental models, varying in user friendliness, simulated input channel noise and the presence or absence of action masks, which, when enabled, simplify learning by masking some of the possible actions.', 'an overview of all these environmental configurations and their assignment to tasks is given in table 1.', 'evaluation results in  #TAUTHOR_TAG with several dialog policy types, e. g. a handcrafted policy and the best reported policies serve as baselines in our experiments']",0
"['##able slot  #AUTHOR_TAG.', ' #TAUTHOR_TAG propose six different environmental models, varying in user friendliness, simulated input channel noise and the presence or absence of action masks, which']","['requestable slot  #AUTHOR_TAG.', ' #TAUTHOR_TAG propose six different environmental models, varying in user friendliness, simulated input channel noise and the presence or absence of action masks, which, when enabled, simplify learning by masking some of the possible actions.', 'an overview of all these environmental configurations']","['# values of each requestable slot  #AUTHOR_TAG.', ' #TAUTHOR_TAG propose six different environmental models, varying in user friendliness, simulated input channel noise and the presence or absence of action masks, which']","['used pydial toolkit as a test - bed for experiments and evaluation.', 'it includes a configurable user simulator and provides multiple dialog ontologies like cambridge restaurants ( cr ), laptops ( lap ) and san francisco restaurants ( sfr ).', 'the ontologies used for the benchmarks in this paper together with their properties are listed in table 2.', 'cr 3 9 268 sfr 6 11 636 lap 11 21 257 table 2 : benchmark domains with # slots the user can provide or # request from the system as well as # values of each requestable slot  #AUTHOR_TAG.', ' #TAUTHOR_TAG propose six different environmental models, varying in user friendliness, simulated input channel noise and the presence or absence of action masks, which, when enabled, simplify learning by masking some of the possible actions.', 'an overview of all these environmental configurations and their assignment to tasks is given in table 1.', 'evaluation results in  #TAUTHOR_TAG with several dialog policy types, e. g. a handcrafted policy and the best reported policies serve as baselines in our experiments']",5
"['and evaluation with the pydial user simulator follows the pydial benchmarking tasks  #TAUTHOR_TAG, where each task ( see table 1 ) is']","['and evaluation with the pydial user simulator follows the pydial benchmarking tasks  #TAUTHOR_TAG, where each task ( see table 1 ) is']","['and evaluation with the pydial user simulator follows the pydial benchmarking tasks  #TAUTHOR_TAG, where each task ( see table 1 ) is trained on 10000 dialogs split into ten training iterations of 1000 dialogs each.', 'we evaluate policies after each training iteration on 1000 test dialogs.', 'all of the following results were obtained by averaging over the outcome of ten different random seeds using the parameters described in appendix a']","['and evaluation with the pydial user simulator follows the pydial benchmarking tasks  #TAUTHOR_TAG, where each task ( see table 1 ) is trained on 10000 dialogs split into ten training iterations of 1000 dialogs each.', 'we evaluate policies after each training iteration on 1000 test dialogs.', 'all of the following results were obtained by averaging over the outcome of ten different random seeds using the parameters described in appendix a']",5
['benchmark  #TAUTHOR_TAG to serve as'],['highest scoring policy from the pydial benchmark  #TAUTHOR_TAG to serve as'],['first row of table 3 and 4 show the results of the highest scoring policy from the pydial benchmark  #TAUTHOR_TAG to serve as'],"['first row of table 3 and 4 show the results of the highest scoring policy from the pydial benchmark  #TAUTHOR_TAG to serve as baselines.', 'evaluations of the handcrafted policies follow in the last line.', 'the results show that rainbow agent outperforms reward of the best pydial agents in all 18 conditions and success rate in 16 out of 18 setting.', '']",5
"['to 1.', 'following the pydial benchmarking process, we leave all hyperparameters constant across all environments and dialog domains  #TAUTHOR_TAG, thus also evaluating the generalization']","['to 1.', 'following the pydial benchmarking process, we leave all hyperparameters constant across all environments and dialog domains  #TAUTHOR_TAG, thus also evaluating the generalization']","['. 99, minibatch size 256 and the huber loss κ is set to 1.', 'following the pydial benchmarking process, we leave all hyperparameters constant across all environments and dialog domains  #TAUTHOR_TAG, thus also evaluating the generalization capabilities of the agents']","['neural network layers are fully connected linear layers with relus as activation functions.', 'in case of the dueling network architecture, the shared layer consists of 256 neurons, followed by two value layers, each with 300 neurons, and two advantage layers with 400 neurons per layer.', 'distributional agents use an atom count of 50.', 'where the dueling architecture is replaced by a standard architecture in the evaluation process, three layers of sizes 256, 700 and 700 are used to guarantee a fair comparison to the dueling case by providing the same model capacity.', 'for prioritized replay, the prioritization exponent α is set to 0. 525 and importance sampling exponent β to 0. 4  #AUTHOR_TAG.', 'to train the networks we use the adam optimizer with a learning rate of 10 −4.', 'exploration is performed - greedy with linear decay, starting at 0. 3.', 'whenever an agent makes use of double q - learning, it updates its target network after 6 dialogs.', 'all agents use an experience replay buffer capacity of 16384 transitions, a discount factor γ = 0. 99, minibatch size 256 and the huber loss κ is set to 1.', 'following the pydial benchmarking process, we leave all hyperparameters constant across all environments and dialog domains  #TAUTHOR_TAG, thus also evaluating the generalization capabilities of the agents']",5
"[""item'gay'with the meaning dimension of homosexuality  #TAUTHOR_TAG."", 'neural word embeddings  #AUTHOR_TAG are probably']","[""item'gay'with the meaning dimension of homosexuality  #TAUTHOR_TAG."", 'neural word embeddings  #AUTHOR_TAG are probably']","[""the lexical item'gay'with the meaning dimension of homosexuality  #TAUTHOR_TAG."", 'neural word embeddings  #AUTHOR_TAG are probably']","['methods applied to large - sized, often temporally stratified corpora have markedly enhanced the methodological repertoire of both synchronic and diachronic computational linguistics and are getting more and more popular in the digital humanities ( see section 2. 2 ).', 'however, using such quantitative data as a basis for qualitative, empirically - grounded theories requires that measurements should not only be accurate, but also reliable.', 'only under such a guarantee, quantitative data can be assembled from different experiments as a foundation for trustful theories.', 'measuring word similarity by word neighborhoods in embedding space can be used to detect diachronic shifts or domain specific usage, by training word embeddings on suited corpora and comparing these representations.', 'additionally, lexical items near in the embedding space to the lexical item under scrutiny can be considered as approximating its meaning at a given point in time or in a specific domain.', ""these two lines of research converge in prior work to show, e. g., the increasing association of the lexical item'gay'with the meaning dimension of homosexuality  #TAUTHOR_TAG."", 'neural word embeddings  #AUTHOR_TAG are probably the most influential among all embedding types ( see section 2. 1 ).', 'yet, we gathered evidence that the inherent randomness involved in their generation affects the reliability of word neighborhood judgments and demonstrate how this hampers qualitative conclusions based on such models.', 'our investigation was performed on both historical ( for the time span of 1900 to 1904 ) and contemporary texts ( for the time span of 2005 to 2009 ) in two languages, english and german.', 'it is thus a continuation of prior work, in which we investigated historical english texts only  #AUTHOR_TAG a ), and also influenced by the design decisions of  #AUTHOR_TAG and  #TAUTHOR_TAG which were the first to use word embeddings in diachronic studies.', 'our results cast doubt on the reproducibility of such experiments where neighborhoods between words in embedding space are taken as a computationally valid indicator for properly capturing lexical meaning ( and, consequently, meaning shifts ).', 'linguistics.', 'the word2vec family of algorithms, developed from heavily trimmed artificial neural networks, is a widely used and robust way to generate such embeddings  #AUTHOR_TAG.', 'its skip - gram variant predicts plausible contexts for a given word, whereas the alternative continuous bag - of - words variant tries to predict words from contexts ; we focus on the former as it is generally reported to be superior ( see e. g.,  #AUTHOR_TAG ).', 'there are two strategies for managing the huge number of potential contexts a word can appear in.', 'skip - gram hierarchical softmax ( sghs ) uses a binary tree to more efficiently represent the vocabulary,']",0
"[""item'gay'with the meaning dimension of homosexuality  #TAUTHOR_TAG."", 'neural word embeddings  #AUTHOR_TAG are probably']","[""item'gay'with the meaning dimension of homosexuality  #TAUTHOR_TAG."", 'neural word embeddings  #AUTHOR_TAG are probably']","[""the lexical item'gay'with the meaning dimension of homosexuality  #TAUTHOR_TAG."", 'neural word embeddings  #AUTHOR_TAG are probably']","['methods applied to large - sized, often temporally stratified corpora have markedly enhanced the methodological repertoire of both synchronic and diachronic computational linguistics and are getting more and more popular in the digital humanities ( see section 2. 2 ).', 'however, using such quantitative data as a basis for qualitative, empirically - grounded theories requires that measurements should not only be accurate, but also reliable.', 'only under such a guarantee, quantitative data can be assembled from different experiments as a foundation for trustful theories.', 'measuring word similarity by word neighborhoods in embedding space can be used to detect diachronic shifts or domain specific usage, by training word embeddings on suited corpora and comparing these representations.', 'additionally, lexical items near in the embedding space to the lexical item under scrutiny can be considered as approximating its meaning at a given point in time or in a specific domain.', ""these two lines of research converge in prior work to show, e. g., the increasing association of the lexical item'gay'with the meaning dimension of homosexuality  #TAUTHOR_TAG."", 'neural word embeddings  #AUTHOR_TAG are probably the most influential among all embedding types ( see section 2. 1 ).', 'yet, we gathered evidence that the inherent randomness involved in their generation affects the reliability of word neighborhood judgments and demonstrate how this hampers qualitative conclusions based on such models.', 'our investigation was performed on both historical ( for the time span of 1900 to 1904 ) and contemporary texts ( for the time span of 2005 to 2009 ) in two languages, english and german.', 'it is thus a continuation of prior work, in which we investigated historical english texts only  #AUTHOR_TAG a ), and also influenced by the design decisions of  #AUTHOR_TAG and  #TAUTHOR_TAG which were the first to use word embeddings in diachronic studies.', 'our results cast doubt on the reproducibility of such experiments where neighborhoods between words in embedding space are taken as a computationally valid indicator for properly capturing lexical meaning ( and, consequently, meaning shifts ).', 'linguistics.', 'the word2vec family of algorithms, developed from heavily trimmed artificial neural networks, is a widely used and robust way to generate such embeddings  #AUTHOR_TAG.', 'its skip - gram variant predicts plausible contexts for a given word, whereas the alternative continuous bag - of - words variant tries to predict words from contexts ; we focus on the former as it is generally reported to be superior ( see e. g.,  #AUTHOR_TAG ).', 'there are two strategies for managing the huge number of potential contexts a word can appear in.', 'skip - gram hierarchical softmax ( sghs ) uses a binary tree to more efficiently represent the vocabulary,']",0
"['be calculated  #TAUTHOR_TAG.', 'the first approach cannot be performed in parallel and is']","['be calculated  #TAUTHOR_TAG.', 'the first approach cannot be performed in parallel and is']","['predecessor  #AUTHOR_TAG b ), or a mapping between models for different points in time must be calculated  #TAUTHOR_TAG.', 'the first approach cannot be performed in parallel and is thus rather time - consuming, if texts are not subsampled.', 'we nevertheless discourage using samples instead of full corpora,']","['embeddings can be used rather directly for tracking semantic changes, namely by measuring the similarity of word representations generated for one word at different points in time - words which underwent semantic shifts will be dissimilar with themselves.', 'these models must either be trained in a continuous manner where the model for each time span is initialized with its predecessor  #AUTHOR_TAG b ), or a mapping between models for different points in time must be calculated  #TAUTHOR_TAG.', 'the first approach cannot be performed in parallel and is thus rather time - consuming, if texts are not subsampled.', 'we nevertheless discourage using samples instead of full corpora, as we observed extremely low reliability values between different samples  #AUTHOR_TAG a ).', 'word embeddings can also be used in diachronic studies without any kind of mapping to track clusters of similar words over time and, thus, model the evolution of topics  #AUTHOR_TAG or compare neighborhoods in embedding space for preselected words  #AUTHOR_TAG.', 'besides temporal variations, word embeddings can also used to analyze geographic ones, e. g., the distinction between us american and british english variants  #AUTHOR_TAG.', 'most of these studies were performed with algorithms from the word2vec family, respectively glove in  #AUTHOR_TAG, and are thus likely to be affected by the same systematic reliability problems on which we focus here.', ' #AUTHOR_TAG used svd ppmi in some of their very recent experiments and showed it to be adequate for exploring historical semantics.', 'the google books ngram corpus ( gbn ;  #AUTHOR_TAG,  #AUTHOR_TAG is used in most of the studies we already mentioned, including our current study and its predecessor  #AUTHOR_TAG a ).', 'it contains about 6 % of all books published between 1500 and 2009 in the form of n - grams ( up to pentagrams ), together with their frequency for each year.', 'this corpus has often been criticized for its opaque sampling strategy, as its constituent books remain unknown and can be shown to form an unbalanced collection  #AUTHOR_TAG.', 'gbn is multilingual, with its english part being subdivided into regional segments ( british, us ) and topic categories ( general language and fiction texts ).', 'diachronic research focuses on the english fiction part, with the exception of some work relating to german data  #AUTHOR_TAG b )']",0
"[""item'gay'with the meaning dimension of homosexuality  #TAUTHOR_TAG."", 'neural word embeddings  #AUTHOR_TAG are probably']","[""item'gay'with the meaning dimension of homosexuality  #TAUTHOR_TAG."", 'neural word embeddings  #AUTHOR_TAG are probably']","[""the lexical item'gay'with the meaning dimension of homosexuality  #TAUTHOR_TAG."", 'neural word embeddings  #AUTHOR_TAG are probably']","['methods applied to large - sized, often temporally stratified corpora have markedly enhanced the methodological repertoire of both synchronic and diachronic computational linguistics and are getting more and more popular in the digital humanities ( see section 2. 2 ).', 'however, using such quantitative data as a basis for qualitative, empirically - grounded theories requires that measurements should not only be accurate, but also reliable.', 'only under such a guarantee, quantitative data can be assembled from different experiments as a foundation for trustful theories.', 'measuring word similarity by word neighborhoods in embedding space can be used to detect diachronic shifts or domain specific usage, by training word embeddings on suited corpora and comparing these representations.', 'additionally, lexical items near in the embedding space to the lexical item under scrutiny can be considered as approximating its meaning at a given point in time or in a specific domain.', ""these two lines of research converge in prior work to show, e. g., the increasing association of the lexical item'gay'with the meaning dimension of homosexuality  #TAUTHOR_TAG."", 'neural word embeddings  #AUTHOR_TAG are probably the most influential among all embedding types ( see section 2. 1 ).', 'yet, we gathered evidence that the inherent randomness involved in their generation affects the reliability of word neighborhood judgments and demonstrate how this hampers qualitative conclusions based on such models.', 'our investigation was performed on both historical ( for the time span of 1900 to 1904 ) and contemporary texts ( for the time span of 2005 to 2009 ) in two languages, english and german.', 'it is thus a continuation of prior work, in which we investigated historical english texts only  #AUTHOR_TAG a ), and also influenced by the design decisions of  #AUTHOR_TAG and  #TAUTHOR_TAG which were the first to use word embeddings in diachronic studies.', 'our results cast doubt on the reproducibility of such experiments where neighborhoods between words in embedding space are taken as a computationally valid indicator for properly capturing lexical meaning ( and, consequently, meaning shifts ).', 'linguistics.', 'the word2vec family of algorithms, developed from heavily trimmed artificial neural networks, is a widely used and robust way to generate such embeddings  #AUTHOR_TAG.', 'its skip - gram variant predicts plausible contexts for a given word, whereas the alternative continuous bag - of - words variant tries to predict words from contexts ; we focus on the former as it is generally reported to be superior ( see e. g.,  #AUTHOR_TAG ).', 'there are two strategies for managing the huge number of potential contexts a word can appear in.', 'skip - gram hierarchical softmax ( sghs ) uses a binary tree to more efficiently represent the vocabulary,']",5
"['of both world wars on book production.', 'following  #TAUTHOR_TAG, we trained our models on all 5 - grams occurring']","['of both world wars on book production.', 'following  #TAUTHOR_TAG, we trained our models on all 5 - grams occurring']","['of both world wars on book production.', 'following  #TAUTHOR_TAG, we trained our models on all 5 - grams occurring']","['experiments 4 were performed on the german part and the english fiction part of the gbn ; the latter is known to be less unbalanced than the general english part  #AUTHOR_TAG.', 'both corpus splits differ in size and contain mainly contemporary texts ( from the past fifty years ), as is evident from figure 1 ; note the logarithmic axis and the negative impact of both world wars on book production.', 'following  #TAUTHOR_TAG, we trained our models on all 5 - grams occurring during five consecutive years for the two time spans, 5 1900 - 1904 and 2005 - 2009 ; the number of 5 - grams 6 for each time span is listed in table 1.', 'the two languages share a similar number of 5 - grams for 1900 - 1904, yet not for [ 2005 ] [ 2006 ] [ 2007 ] [ 2008 ] [ 2009 ].', '5 - grams from both corpus parts were lower cased for training.', 'the german part was not only taken as is, but also orthographically normalized using the cab service  #AUTHOR_TAG.', '7 we incorporated this step because major changes in german orthography occurred during the 20th century, an issue that could hamper diachronic comparisons, e. g., archaic\'gemuth\'( in english : "" mind, emotional disposition "" ) became modern\'gemut \'.', 'table 1 shows the resulting reduction in the number of types, bringing the morphologically richer german to levels below english ( yet this reduction is in line with the respective corpus sizes )']",5
"['by  #TAUTHOR_TAG,']","['by  #TAUTHOR_TAG, i. e', '., c = 0.', '9999, was']","['by  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],5
"['by  #TAUTHOR_TAG,']","['by  #TAUTHOR_TAG, i. e', '., c = 0.', '9999, was']","['by  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],5
"['be calculated  #TAUTHOR_TAG.', 'the first approach cannot be performed in parallel and is']","['be calculated  #TAUTHOR_TAG.', 'the first approach cannot be performed in parallel and is']","['predecessor  #AUTHOR_TAG b ), or a mapping between models for different points in time must be calculated  #TAUTHOR_TAG.', 'the first approach cannot be performed in parallel and is thus rather time - consuming, if texts are not subsampled.', 'we nevertheless discourage using samples instead of full corpora,']","['embeddings can be used rather directly for tracking semantic changes, namely by measuring the similarity of word representations generated for one word at different points in time - words which underwent semantic shifts will be dissimilar with themselves.', 'these models must either be trained in a continuous manner where the model for each time span is initialized with its predecessor  #AUTHOR_TAG b ), or a mapping between models for different points in time must be calculated  #TAUTHOR_TAG.', 'the first approach cannot be performed in parallel and is thus rather time - consuming, if texts are not subsampled.', 'we nevertheless discourage using samples instead of full corpora, as we observed extremely low reliability values between different samples  #AUTHOR_TAG a ).', 'word embeddings can also be used in diachronic studies without any kind of mapping to track clusters of similar words over time and, thus, model the evolution of topics  #AUTHOR_TAG or compare neighborhoods in embedding space for preselected words  #AUTHOR_TAG.', 'besides temporal variations, word embeddings can also used to analyze geographic ones, e. g., the distinction between us american and british english variants  #AUTHOR_TAG.', 'most of these studies were performed with algorithms from the word2vec family, respectively glove in  #AUTHOR_TAG, and are thus likely to be affected by the same systematic reliability problems on which we focus here.', ' #AUTHOR_TAG used svd ppmi in some of their very recent experiments and showed it to be adequate for exploring historical semantics.', 'the google books ngram corpus ( gbn ;  #AUTHOR_TAG,  #AUTHOR_TAG is used in most of the studies we already mentioned, including our current study and its predecessor  #AUTHOR_TAG a ).', 'it contains about 6 % of all books published between 1500 and 2009 in the form of n - grams ( up to pentagrams ), together with their frequency for each year.', 'this corpus has often been criticized for its opaque sampling strategy, as its constituent books remain unknown and can be shown to form an unbalanced collection  #AUTHOR_TAG.', 'gbn is multilingual, with its english part being subdivided into regional segments ( british, us ) and topic categories ( general language and fiction texts ).', 'diachronic research focuses on the english fiction part, with the exception of some work relating to german data  #AUTHOR_TAG b )']",1
"['by  #TAUTHOR_TAG,']","['by  #TAUTHOR_TAG, i. e', '., c = 0.', '9999, was']","['by  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],4
"['by  #TAUTHOR_TAG,']","['by  #TAUTHOR_TAG, i. e', '., c = 0.', '9999, was']","['by  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],4
"['', 'recently,  #TAUTHOR_TAG extended']","['translation task.', 'recently,  #TAUTHOR_TAG extended']","['', 'recently,  #TAUTHOR_TAG extended']","['machine translation ( nmt ) models are typically trained using a fixed - size lexical vocabulary.', 'in addition to controlling the computational load, this limitation also serves to maintain better distributed representations for the most frequent set of words included in the vocabulary.', 'on the other hand, rare words in the long tail of the lexical distribution are often discarded during translation since they are not found in the vocabulary.', 'the prominent approach to overcome this limitation is to segment words into subword units  #AUTHOR_TAG and perform translation based on a vocabulary composed of these units.', 'however, subword segmentation methods generally rely on statistical heuristics that lack any linguistic notion.', 'moreover, they are typically deployed as a pre - processing step before training the nmt model, hence, the predicted set of subword units are essentially not optimized for the translation task.', 'recently,  #TAUTHOR_TAG extended the approach of nmt based on subword units to implement the translation model directly at the level of characters, which could reach comparable performance to the subword - based model, although this would require much larger networks which may be more difficult to train.', 'the major reason to this requirement may lie behind the fact that treating the characters as individual tokens at the same level and processing the input sequences in linear time increases the difficulty of the learning task, where translation would then be modeled as a mapping between the characters in two languages.', 'the increased sequence lengths due to processing sentences as sequences of characters also augments the computational cost, and a possible limitation, since sequence models typically have limited capacity in remembering longdistance context.', 'in many languages, words are the core atomic units of semantic and syntactic structure, and their explicit modeling should be beneficial in learning distributed representations for translation.', 'there have been early studies in nmt which proposed to perform translation at the level of characters while also regarding the word boundaries in the translation model through a hierarchical decoding procedure, although these approaches were generally deployed through hybrid systems, either as a back - off solution to translate unknown words  #AUTHOR_TAG, or as pre - trained components  #AUTHOR_TAG.', 'in this paper, we explore the benefit of achieving character - level nmt by processing sentences at multi - level dynamic time steps defined by the word boundaries, integrating a notion of explicit hierarchy into the decoder.', 'in our model, all word representations are learned compositionally from character embeddings using bi - directional recurrent neural networks ( bi - rnns )  #AUTHOR_TAG, and decoding is performed by generating each word character by character based on the predicted word representation through a hierarchical beam search algorithm which takes advantage of the hierarchical architecture while generating translations.', 'we present the results of']",0
"['computational cost of the model, defined by the network parameters  #TAUTHOR_TAG. as given in']","['computational cost of the model, defined by the network parameters  #TAUTHOR_TAG. as given in']","['segmentation. although character - level nmt models have shown the potential to obtain comparable performance with subwordbased nmt', 'models, this would require increasing the computational cost of the model, defined by the network parameters  #TAUTHOR_TAG. as given in figure 1a implementing the nmt decoder directly']","['training set via stochastic gradientdescent  #AUTHOR_TAG, where the gradients are computed with the', 'back propagation through time  #AUTHOR_TAG algorithm. due to the softmax function in equation 2, the size of the target vocabulary plays an important role in defining the computational complexity of the model. in the standard architecture', ', the embedding matrices account for the vast majority of the network', 'parameters, thus, the amount of embeddings that could be learned and stored efficiently needs to be limited', '. moreover, for many words corresponding to the long tail of the lexical distribution, the model fails in learning accurate embeddings, as they are rarely observed in varying context, leading the model vocabulary to', 'typically include the most frequent set of words in the target language. this creates an important bottleneck over the vocabulary coverage of the model, which is especially crucial when translating', 'into low - resource and morphologically - rich languages, which often have a high level of sparsity in the lexical distribution. the standard approach to overcome this limitation has now become applying a statistical segmentation algorithm on the training corpus which splits words into smaller and more frequent subword units', ', and building the model vocabulary composed of these units. the translation problem is then modeled as a mapping between sequences of subword units in the source and target languages  #AUTHOR_TAG. the most popular statistical segmentation method is byte - pair encoding ( bpe', ')  #AUTHOR_TAG, which finds the optimal description of a corpus vocabulary by iteratively merging the most frequent character sequences. one problem related to the subwordbased nmt approach is that segmentation methods are typically implemented as pre - processing steps to nmt, thus, they are not optimized simultaneously with the translation task in an end - to - end fashion. this can lead to morphological errors at different levels, and', 'cause loss of semantic or syntactic information  #AUTHOR_TAG, due to the ambiguity in subword embeddings. in fact, recent studies have shown that the same approach can', 'be extended to implement the nmt model directly at the level of characters,', 'which could alleviate potential morphological errors due to subword segmentation. although character - level nmt models have shown the potential to obtain comparable performance with subwordbased nmt', 'models, this would require increasing the computational cost of the model, defined by the network parameters  #TAUTHOR_TAG. as given in figure 1a implementing the nmt decoder directly at the level', 'of characters leads to repetitive passes over the attention mechanism and the rnns modeling the target language for each character in the sentence. since the distributed representations', 'of characters are shared among different word and sentence - level context, the translation task requires a network with high capacity to learn this vastly dynamic context']",0
"[', including  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, and']","[', including  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, and']","['work, including  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, and leave implementation of', 'hierarchical decoding with feed - forward architectures to future work']","['', 'the hierarchical decoder is the only model which can generate a meaningful and grammatically - correct sentence, suggesting that modeling translation based on a context defined at the lexical level might help to learn better grammatical and contextual dependencies, and remembering', 'longer history. although current methodology in nmt allows more efficient processing by implementing feed - forward architectures  #AUTHOR_TAG, our approach can conceptually be applied within these frameworks.', 'in this paper, we limit the evaluation to recurrent architectures for comparison to previous work, including  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, and leave implementation of', 'hierarchical decoding with feed - forward architectures to future work']",0
"[', including  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, and']","[', including  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, and']","['work, including  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, and leave implementation of', 'hierarchical decoding with feed - forward architectures to future work']","['', 'the hierarchical decoder is the only model which can generate a meaningful and grammatically - correct sentence, suggesting that modeling translation based on a context defined at the lexical level might help to learn better grammatical and contextual dependencies, and remembering', 'longer history. although current methodology in nmt allows more efficient processing by implementing feed - forward architectures  #AUTHOR_TAG, our approach can conceptually be applied within these frameworks.', 'in this paper, we limit the evaluation to recurrent architectures for comparison to previous work, including  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, and leave implementation of', 'hierarchical decoding with feed - forward architectures to future work']",0
"[' #AUTHOR_TAG or fully character - level  #TAUTHOR_TAG units, which']","['implemented either with subword  #AUTHOR_TAG or fully character - level  #TAUTHOR_TAG units, which']","[' #AUTHOR_TAG or fully character - level  #TAUTHOR_TAG units, which']","['evaluate decoding architectures using different levels of granularity in the vocabulary units and the attention mechanism, including the standard decoding architecture implemented either with subword  #AUTHOR_TAG or fully character - level  #TAUTHOR_TAG units, which constitute the baseline approaches, and the hierarchical decoding architecture, by implementing all in pytorch  #AUTHOR_TAG within the opennmt - py framework  #AUTHOR_TAG.', 'in order to evaluate how each generative method performs in languages with different morphological typology, we model the machine translation task from english into five languages from different language families and exhibiting distinct morphological typology : arabic ( templatic ), czech ( mostly fusional, partially agglutinative ), german ( fusional ), italian ( fusional ) and turkish ( agglutinative ).', 'we use the ted talks corpora  #AUTHOR_TAG for training the nmt models, which range from 110k to 240k sentences, and the official development and test sets from iwslt 1  #AUTHOR_TAG.', 'the low - resource settings for the training data allows us to examine the quality of the internal representations learned by each decoder under high data sparseness.', ""in order to evaluate how the performance of each method scales with increasing data size, we evaluate the models also by training with a multi - domain training data using the public data sets from wmt 2  #AUTHOR_TAG in the english - to - german direction, followed by an analysis on each model's capability in generalizing to morphological variations in the target language, using the morpheval  #AUTHOR_TAG evaluation sets."", '']",5
"[', including  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, and']","[', including  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, and']","['work, including  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, and leave implementation of', 'hierarchical decoding with feed - forward architectures to future work']","['', 'the hierarchical decoder is the only model which can generate a meaningful and grammatically - correct sentence, suggesting that modeling translation based on a context defined at the lexical level might help to learn better grammatical and contextual dependencies, and remembering', 'longer history. although current methodology in nmt allows more efficient processing by implementing feed - forward architectures  #AUTHOR_TAG, our approach can conceptually be applied within these frameworks.', 'in this paper, we limit the evaluation to recurrent architectures for comparison to previous work, including  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, and leave implementation of', 'hierarchical decoding with feed - forward architectures to future work']",5
"[', including  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, and']","[', including  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, and']","['work, including  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, and leave implementation of', 'hierarchical decoding with feed - forward architectures to future work']","['', 'the hierarchical decoder is the only model which can generate a meaningful and grammatically - correct sentence, suggesting that modeling translation based on a context defined at the lexical level might help to learn better grammatical and contextual dependencies, and remembering', 'longer history. although current methodology in nmt allows more efficient processing by implementing feed - forward architectures  #AUTHOR_TAG, our approach can conceptually be applied within these frameworks.', 'in this paper, we limit the evaluation to recurrent architectures for comparison to previous work, including  #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG, and leave implementation of', 'hierarchical decoding with feed - forward architectures to future work']",3
['##z ) data  #TAUTHOR_TAG. we'],['##z ) data  #TAUTHOR_TAG. we'],['woz ) data  #TAUTHOR_TAG. we'],"['automatic evaluation of dialogue strategies, e. g. the paradise framework  #AUTHOR_TAG, and meta - evaluation of dialogue metrics, e. g.', '( engelbrecht and moller, 2007 ;  #AUTHOR_TAG. clearly, automatic optimisation and evaluation of dialogue policies, as well as quality control of', 'the objective function, are closely inter - related problems :', ""how can we make sure that we optimise a system according to real users'preferences? in particular, we construct a data - driven objective function using the paradise framework, and use it for automatic dialogue strategy optimisation following pioneering work by  #AUTHOR_TAG. however, it"", 'is not clear how reliable such a predictive model is, i. e. if it indeed estimates real user preferences. the models obtained with parad', '##ise usually fit the data poorly ( engelbrecht and moller, 2007 ).', 'it is also not clear how general they are across different systems and user groups  #AUTHOR_TAG,  #AUTHOR_TAG.', 'furthermore, it is not clear how they perform when being used for automatic strategy optimisation within the', 'rl framework. in the following we evaluate different aspects of an objective function obtained from wi', '##zard - of - oz ( woz ) data  #TAUTHOR_TAG. we proceed as follows : the next section shortly summarises the overall dialogue system design. in section 3. we test the model stability in a test - retest comparison across different user populations and data sets. in section 4. we measure prediction accuracy. in section 5. we conduct a detailed error analysis where we test the relationship', 'between improved user ratings and dialogue behaviour, i. e. we investigate which factors lead the users to give higher scores, and whether this was correctly reflected in the original objective function. 2. overall framework 2. 1. dialogue', 'system design our application domains are multimodal information seeking dialogue systems as an interface to an in - car mp3 player. the structure of information seeking dialogues consists of', 'an information acquisition dialogue and an information presentation sub - dialogue ( see figure 1 ). for information acquisition the task of the dialogue policy', ""is to gather'enough'search constraints from the user, and then,'at the right time ',"", ""to start the information presentation phase where the task is to present'the right amount'of information"", '- either on the screen or listing the items verbally. what this actually means depends on the dialogue context', 'and the preferences of our users as reflected in the objective function. we therefore formulate dialogue learning as a hierarchical optimisation problem  #TAUTHOR_TAG. the applied', 'objective function follows this structure as well. figure 1 : hierarchical dialogue structure for information seeking', 'multimodal systems']",5
['##z ) data  #TAUTHOR_TAG. we'],['##z ) data  #TAUTHOR_TAG. we'],['woz ) data  #TAUTHOR_TAG. we'],"['automatic evaluation of dialogue strategies, e. g. the paradise framework  #AUTHOR_TAG, and meta - evaluation of dialogue metrics, e. g.', '( engelbrecht and moller, 2007 ;  #AUTHOR_TAG. clearly, automatic optimisation and evaluation of dialogue policies, as well as quality control of', 'the objective function, are closely inter - related problems :', ""how can we make sure that we optimise a system according to real users'preferences? in particular, we construct a data - driven objective function using the paradise framework, and use it for automatic dialogue strategy optimisation following pioneering work by  #AUTHOR_TAG. however, it"", 'is not clear how reliable such a predictive model is, i. e. if it indeed estimates real user preferences. the models obtained with parad', '##ise usually fit the data poorly ( engelbrecht and moller, 2007 ).', 'it is also not clear how general they are across different systems and user groups  #AUTHOR_TAG,  #AUTHOR_TAG.', 'furthermore, it is not clear how they perform when being used for automatic strategy optimisation within the', 'rl framework. in the following we evaluate different aspects of an objective function obtained from wi', '##zard - of - oz ( woz ) data  #TAUTHOR_TAG. we proceed as follows : the next section shortly summarises the overall dialogue system design. in section 3. we test the model stability in a test - retest comparison across different user populations and data sets. in section 4. we measure prediction accuracy. in section 5. we conduct a detailed error analysis where we test the relationship', 'between improved user ratings and dialogue behaviour, i. e. we investigate which factors lead the users to give higher scores, and whether this was correctly reflected in the original objective function. 2. overall framework 2. 1. dialogue', 'system design our application domains are multimodal information seeking dialogue systems as an interface to an in - car mp3 player. the structure of information seeking dialogues consists of', 'an information acquisition dialogue and an information presentation sub - dialogue ( see figure 1 ). for information acquisition the task of the dialogue policy', ""is to gather'enough'search constraints from the user, and then,'at the right time ',"", ""to start the information presentation phase where the task is to present'the right amount'of information"", '- either on the screen or listing the items verbally. what this actually means depends on the dialogue context', 'and the preferences of our users as reflected in the objective function. we therefore formulate dialogue learning as a hierarchical optimisation problem  #TAUTHOR_TAG. the applied', 'objective function follows this structure as well. figure 1 : hierarchical dialogue structure for information seeking', 'multimodal systems']",5
['##z ) data  #TAUTHOR_TAG. we'],['##z ) data  #TAUTHOR_TAG. we'],['woz ) data  #TAUTHOR_TAG. we'],"['automatic evaluation of dialogue strategies, e. g. the paradise framework  #AUTHOR_TAG, and meta - evaluation of dialogue metrics, e. g.', '( engelbrecht and moller, 2007 ;  #AUTHOR_TAG. clearly, automatic optimisation and evaluation of dialogue policies, as well as quality control of', 'the objective function, are closely inter - related problems :', ""how can we make sure that we optimise a system according to real users'preferences? in particular, we construct a data - driven objective function using the paradise framework, and use it for automatic dialogue strategy optimisation following pioneering work by  #AUTHOR_TAG. however, it"", 'is not clear how reliable such a predictive model is, i. e. if it indeed estimates real user preferences. the models obtained with parad', '##ise usually fit the data poorly ( engelbrecht and moller, 2007 ).', 'it is also not clear how general they are across different systems and user groups  #AUTHOR_TAG,  #AUTHOR_TAG.', 'furthermore, it is not clear how they perform when being used for automatic strategy optimisation within the', 'rl framework. in the following we evaluate different aspects of an objective function obtained from wi', '##zard - of - oz ( woz ) data  #TAUTHOR_TAG. we proceed as follows : the next section shortly summarises the overall dialogue system design. in section 3. we test the model stability in a test - retest comparison across different user populations and data sets. in section 4. we measure prediction accuracy. in section 5. we conduct a detailed error analysis where we test the relationship', 'between improved user ratings and dialogue behaviour, i. e. we investigate which factors lead the users to give higher scores, and whether this was correctly reflected in the original objective function. 2. overall framework 2. 1. dialogue', 'system design our application domains are multimodal information seeking dialogue systems as an interface to an in - car mp3 player. the structure of information seeking dialogues consists of', 'an information acquisition dialogue and an information presentation sub - dialogue ( see figure 1 ). for information acquisition the task of the dialogue policy', ""is to gather'enough'search constraints from the user, and then,'at the right time ',"", ""to start the information presentation phase where the task is to present'the right amount'of information"", '- either on the screen or listing the items verbally. what this actually means depends on the dialogue context', 'and the preferences of our users as reflected in the objective function. we therefore formulate dialogue learning as a hierarchical optimisation problem  #TAUTHOR_TAG. the applied', 'objective function follows this structure as well. figure 1 : hierarchical dialogue structure for information seeking', 'multimodal systems']",5
['user tests  #TAUTHOR_TAG running'],['user tests  #TAUTHOR_TAG running'],"['the user tests  #TAUTHOR_TAG running the supervised baseline policy and the rl - based policy.', 'by replicating the regression model on different data sets we test']","['the information acquisition phase we applied stepwise multivariate linear regression to select the dialogue features which are most predictive for perceived task ease.', 'task ease is a measure from the user questionnaires obtained by taking the average of two user ratings on a 5 - point likert scale.', '1. the task was easy to solve.', '2. i had no problems finding the information i wanted.', 'we choose task ease as the ultimate measure to be optimised following  #AUTHOR_TAG\'s principle of the least effort which says : "" all things being equal, agents try to minimize their effort in doing what they intend to do "".', 'the paradise regression model is constructed from 3 different corpora : the sammie woz experiment  #AUTHOR_TAG, and the italk system used for the user tests  #TAUTHOR_TAG running the supervised baseline policy and the rl - based policy.', 'by replicating the regression model on different data sets we test whether the automatic estimate of task ease generalises beyond the conditions and assumptions of a particular experimental design.', '']",5
['the following the overall method is shortly summarised. please see  #TAUTHOR_TAG for details'],"['the following the overall method is shortly summarised. please see  #TAUTHOR_TAG for details.', '1. we obtain an objective']",['the following the overall method is shortly summarised. please see  #TAUTHOR_TAG for details'],"['the following the overall method is shortly summarised. please see  #TAUTHOR_TAG for details.', '1. we obtain an objective function from the woz data of  #AUTHOR_TAG according to the paradise framework.', 'in paradise multivariate linear regression is applied to experimental dialogue data in order to develop predictive models of user preferences ( obtained from questionnaires ) as a linear weighted function of dialogue performance measures ( such as dialogue length ).', 'this predictive model is used to automatically evaluate dialogues.', 'for rl this function is used as the "" reward "" for training.', '2. we train an rl - based dialogue system with the obtained model.', 'the hypothesis is that, by using the obtained quality measures as a reward function for rl, we will be able to learn an improved strategy over a policy which simply mimics observed patterns ( i. e. the human wizard behaviour ) in the data.', 'the baseline policy is therefore constructed using supervised learning ( sl ) on the woz data.', 'we then test both strategies ( sl and rl ) with real users using the same objective / evaluation function.', '3. since the objective function plays such a central role in automatic dialogue design, we need to find methods that ensure its quality.', 'in this paper, we evaluate the obtained function in a test - retest comparison between the model obtained from the woz study and the one obtained when testing the real system as described in the following']",0
"['dialogue performance measures  #TAUTHOR_TAG.', 'here, we test the']","['dialogue performance measures  #TAUTHOR_TAG.', 'here, we test the']","['dialogue performance measures  #TAUTHOR_TAG.', 'here, we test the relationship between improved user ratings and dialogue behaviour, i. e. we investigate']","['previous work we showed that the rl - based policy significantly outperforms the supervised policy in terms of improved user ratings and dialogue performance measures  #TAUTHOR_TAG.', 'here, we test the relationship between improved user ratings and dialogue behaviour, i. e. we investigate which factors lead the users to give higher scores, and whether this was correctly reflected in the original reward function.', 'we concentrate on the information presentation phase, since there is a simple two - way relationship between user scores and the number of presented items.', 'to estimate this relationship we use curve fitting, which is used as an alternative model to linear regression in cases where the relationship between two variables can also be non - linear.', 'for each presentation mode ( verbal vs. multimodal ) we select the ( simplest ) model with the closest fit to the data ( r 2 )']",0
"['##rds did not follow a specific pattern,  #TAUTHOR_TAG.', 'when relating number of items to user scores, the rl']","['wizards did not follow a specific pattern,  #TAUTHOR_TAG.', 'when relating number of items to user scores, the rl']","['when to show items on the screen ( since the wizards did not follow a specific pattern,  #TAUTHOR_TAG.', 'when relating number of items to user scores, the rl policy produces a linear ( slightly declining ) line between']","['', 'the rl - based policy learned to maximise the returned reward by displaying no more than 15 items.', 'the sl policy, in contrast, did not learn an upper boundary for when to show items on the screen ( since the wizards did not follow a specific pattern,  #TAUTHOR_TAG.', ""when relating number of items to user scores, the rl policy produces a linear ( slightly declining ) line between 7 and 6 ( table 3, bottom right ), indicating that the applied policy reflected the users'preferences."", 'hence, we conclude that the objective function derived from the woz data gave the right feedback to the learner.', 'for the sl policy the logarithmic function best describes the data.', 'it function indicates that the multimodal presentation strategy received the highest scores if the number of items presented were just under 15 ( table 3, top right ), which is the turning point of the woz objective function.', 'this again indicates that, for the italk users the preferred multimodal policy was indeed the one reflected in the woz objective function']",4
"['identifying the sense of implicit relations, including linguistically informed', 'features like polarity tags, levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features  #TAUTHOR_TAG. for']","['identifying the sense of implicit relations, including linguistically informed', 'features like polarity tags, levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features  #TAUTHOR_TAG. for some of second -']","['identifying the sense of implicit relations, including linguistically informed', 'features like polarity tags, levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features  #TAUTHOR_TAG. for']","['greatly help with classification of the relation and can be disambiguated with 0. 93 accuracy ( 4 - ways ) solely on the discourse relation', 'connectives  #AUTHOR_TAG. in implicit relations, no such strong cue is available and the discourse relation instead needs', 'to be inferred based on the two textual arguments. in recent studies, various classes of features are explored to capture lexical and semantic regularities for identifying the sense of implicit relations, including linguistically informed', ""features like polarity tags, levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features  #TAUTHOR_TAG. for some of second - level relations ( a level of granularity that should be much more meaningful to downstream tasks than the four - way distinction ), there are only a dozen in - stances, so that it's important to"", 'make maximal use of both the data set for training and testing. the test', 'set that is currently most often used for 11 way classification is section 23  #TAUTHOR_TAG, which contains only about 761 implicit relations. this small size implies that a gain of 1', '']",0
"['identifying the sense of implicit relations, including linguistically informed', 'features like polarity tags, levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features  #TAUTHOR_TAG. for']","['identifying the sense of implicit relations, including linguistically informed', 'features like polarity tags, levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features  #TAUTHOR_TAG. for some of second -']","['identifying the sense of implicit relations, including linguistically informed', 'features like polarity tags, levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features  #TAUTHOR_TAG. for']","['greatly help with classification of the relation and can be disambiguated with 0. 93 accuracy ( 4 - ways ) solely on the discourse relation', 'connectives  #AUTHOR_TAG. in implicit relations, no such strong cue is available and the discourse relation instead needs', 'to be inferred based on the two textual arguments. in recent studies, various classes of features are explored to capture lexical and semantic regularities for identifying the sense of implicit relations, including linguistically informed', ""features like polarity tags, levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features  #TAUTHOR_TAG. for some of second - level relations ( a level of granularity that should be much more meaningful to downstream tasks than the four - way distinction ), there are only a dozen in - stances, so that it's important to"", 'make maximal use of both the data set for training and testing. the test', 'set that is currently most often used for 11 way classification is section 23  #TAUTHOR_TAG, which contains only about 761 implicit relations. this small size implies that a gain of 1', '']",0
"[' #TAUTHOR_TAG.', 'the']","[' #TAUTHOR_TAG.', 'the']","['level 11 - way classification  #TAUTHOR_TAG.', 'the distribution of']","['penn discourse treebank ( pdtb ) we use the penn discourse treebank  #AUTHOR_TAG, the largest available manually annotated corpora of discourse on top of one million word tokens from the wall street journal ( wsj ).', 'the pdtb provides annotations for explicit and implicit discourse relations.', 'by definition, an explicit relation contains an explicit discourse connective while the implicit one does not.', 'the pdtb provides a three level hierarchy of relation tags for its annotation.', 'previous work in this task has been done over two schemes of evaluation : first - level 4 - ways classification  #AUTHOR_TAG, second - level 11 - way classification  #TAUTHOR_TAG.', '']",0
"[' #TAUTHOR_TAG.', 'the']","[' #TAUTHOR_TAG.', 'the']","['level 11 - way classification  #TAUTHOR_TAG.', 'the distribution of']","['penn discourse treebank ( pdtb ) we use the penn discourse treebank  #AUTHOR_TAG, the largest available manually annotated corpora of discourse on top of one million word tokens from the wall street journal ( wsj ).', 'the pdtb provides annotations for explicit and implicit discourse relations.', 'by definition, an explicit relation contains an explicit discourse connective while the implicit one does not.', 'the pdtb provides a three level hierarchy of relation tags for its annotation.', 'previous work in this task has been done over two schemes of evaluation : first - level 4 - ways classification  #AUTHOR_TAG, second - level 11 - way classification  #TAUTHOR_TAG.', '']",5
"['given the two arguments of an implicit instance.', 'as a label set, we use 11 - way distinction as proposed in  #TAUTHOR_TAG ;  #AUTHOR_TAG.', 'word embeddings']","['given the two arguments of an implicit instance.', 'as a label set, we use 11 - way distinction as proposed in  #TAUTHOR_TAG ;  #AUTHOR_TAG.', 'word embeddings']","['task is to predict the discourse relation given the two arguments of an implicit instance.', 'as a label set, we use 11 - way distinction as proposed in  #TAUTHOR_TAG ;  #AUTHOR_TAG.', 'word embeddings']","['task is to predict the discourse relation given the two arguments of an implicit instance.', 'as a label set, we use 11 - way distinction as proposed in  #TAUTHOR_TAG ;  #AUTHOR_TAG.', 'word embeddings are trained with the skip - gram architecture in word2vec  #AUTHOR_TAG, which is able to capture semantic and syntactic patterns with an unsupervised method, on the training sections of wsj data.', 'our model is illustrated in figure 1.', 'each word is represented as a vector, which is found through a look - up word embedding.', 'then we get the representations of argument 1 and argument 2 separately after transforming semantic word vectors into distributed continuous - value features by lstm recurrent neural network.', '']",5
"['as dependency labels or part - of - speech tag features trees  #TAUTHOR_TAG.', 'even']","['as dependency labels or part - of - speech tag features trees  #TAUTHOR_TAG.', 'even']","['as dependency labels or part - of - speech tag features trees  #TAUTHOR_TAG.', 'even']","['compression can be applied to various tasks such as summarization, text editing and even data augmentation where the compressed text can be employed as additional training examples.', 'however, almost all existing approaches require either parallel data, hand - crafted rules, or extra syntactic information such as dependency labels or part - of - speech tag features trees  #TAUTHOR_TAG.', 'even for one model that achieves this task in an unsupervised setting, its architecture necessitates a task - specific "" sequence - to - sequenceto - sequence "" autoencoder  #AUTHOR_TAG.', 'moreover, these models only generate one compressed sentence for each source input, making adaption to different style requirements difficult.', '']",0
"['sentence compression by two annotators  #TAUTHOR_TAG.', '5  #AUTHOR_TAG,']","['extractive sentence compression by two annotators  #TAUTHOR_TAG.', '5  #AUTHOR_TAG,']","['extractive sentence compression by two annotators  #TAUTHOR_TAG.', '5  #AUTHOR_TAG, we use the ground truth compressed sentences to']","['use a pretrained bert uncased model implemented by huggingface.', '2 to score any token in a sentence, we use the special token [ mask ] to mask out the target token, and then prepend a [CLS] and append a [SEP], which function as [ start ] and [ end ] tokens.', 'we use both tokens to help the model evaluate whether the current first or last token can function as a sentence start or end.', 'this is also the main reason that we did not choose other competitive language models such as gpt - 2  #AUTHOR_TAG : because these models are not bidirectional, they will be much less sensitive to deletions from the very end of a sentence.', 'however, note that since bert is not intended to be a language model, 3 we mask one token at a time to obtain the negative log probability of each.', 'the maximum lookahead length in our work is set to 3 to facilitate deletion of phrases which contain multiple tokens, such as "" by the way "" in table 1.', 'datasets we experiment on two datasets.', 'google sentence compression dataset  #AUTHOR_TAG ( 2015 ), we used the first 1, 000 sentences of evaluation set as our test set.', 'the gigaword dataset  #AUTHOR_TAG with 1. 02 million examples, where the first 200 are labeled for extractive sentence compression by two annotators  #TAUTHOR_TAG.', '5  #AUTHOR_TAG, we use the ground truth compressed sentences to compute f1 scores.', 'although f1 can be indicative of how well a compression model performs, we note that their could be multiple viable compressions for the same sentence, which single - reference ground - truth cannot cover  #AUTHOR_TAG.', 'thus to faithfully evaluate the quality of the compressions generated by our model, we follow  #TAUTHOR_TAG and conducted human studies on the gigaword dataset with amazon mechanical turk ( mturk ).', '6 specifically, we sample 150 examples from the test set, and put our model output side - by - side with the two compressions created by the two annotators.', '7 the three compressions were randomly shuffled to anonymize model identity.', 'we hire annotators who have an approval rate of at least 98 % and 10, 000 approved hits.', 'following  #TAUTHOR_TAG, we employed readability and informativeness as criteria on a five - point likert scale']",5
"['sentence compression by two annotators  #TAUTHOR_TAG.', '5  #AUTHOR_TAG,']","['extractive sentence compression by two annotators  #TAUTHOR_TAG.', '5  #AUTHOR_TAG,']","['extractive sentence compression by two annotators  #TAUTHOR_TAG.', '5  #AUTHOR_TAG, we use the ground truth compressed sentences to']","['use a pretrained bert uncased model implemented by huggingface.', '2 to score any token in a sentence, we use the special token [ mask ] to mask out the target token, and then prepend a [CLS] and append a [SEP], which function as [ start ] and [ end ] tokens.', 'we use both tokens to help the model evaluate whether the current first or last token can function as a sentence start or end.', 'this is also the main reason that we did not choose other competitive language models such as gpt - 2  #AUTHOR_TAG : because these models are not bidirectional, they will be much less sensitive to deletions from the very end of a sentence.', 'however, note that since bert is not intended to be a language model, 3 we mask one token at a time to obtain the negative log probability of each.', 'the maximum lookahead length in our work is set to 3 to facilitate deletion of phrases which contain multiple tokens, such as "" by the way "" in table 1.', 'datasets we experiment on two datasets.', 'google sentence compression dataset  #AUTHOR_TAG ( 2015 ), we used the first 1, 000 sentences of evaluation set as our test set.', 'the gigaword dataset  #AUTHOR_TAG with 1. 02 million examples, where the first 200 are labeled for extractive sentence compression by two annotators  #TAUTHOR_TAG.', '5  #AUTHOR_TAG, we use the ground truth compressed sentences to compute f1 scores.', 'although f1 can be indicative of how well a compression model performs, we note that their could be multiple viable compressions for the same sentence, which single - reference ground - truth cannot cover  #AUTHOR_TAG.', 'thus to faithfully evaluate the quality of the compressions generated by our model, we follow  #TAUTHOR_TAG and conducted human studies on the gigaword dataset with amazon mechanical turk ( mturk ).', '6 specifically, we sample 150 examples from the test set, and put our model output side - by - side with the two compressions created by the two annotators.', '7 the three compressions were randomly shuffled to anonymize model identity.', 'we hire annotators who have an approval rate of at least 98 % and 10, 000 approved hits.', 'following  #TAUTHOR_TAG, we employed readability and informativeness as criteria on a five - point likert scale']",5
"['sentence compression by two annotators  #TAUTHOR_TAG.', '5  #AUTHOR_TAG,']","['extractive sentence compression by two annotators  #TAUTHOR_TAG.', '5  #AUTHOR_TAG,']","['extractive sentence compression by two annotators  #TAUTHOR_TAG.', '5  #AUTHOR_TAG, we use the ground truth compressed sentences to']","['use a pretrained bert uncased model implemented by huggingface.', '2 to score any token in a sentence, we use the special token [ mask ] to mask out the target token, and then prepend a [CLS] and append a [SEP], which function as [ start ] and [ end ] tokens.', 'we use both tokens to help the model evaluate whether the current first or last token can function as a sentence start or end.', 'this is also the main reason that we did not choose other competitive language models such as gpt - 2  #AUTHOR_TAG : because these models are not bidirectional, they will be much less sensitive to deletions from the very end of a sentence.', 'however, note that since bert is not intended to be a language model, 3 we mask one token at a time to obtain the negative log probability of each.', 'the maximum lookahead length in our work is set to 3 to facilitate deletion of phrases which contain multiple tokens, such as "" by the way "" in table 1.', 'datasets we experiment on two datasets.', 'google sentence compression dataset  #AUTHOR_TAG ( 2015 ), we used the first 1, 000 sentences of evaluation set as our test set.', 'the gigaword dataset  #AUTHOR_TAG with 1. 02 million examples, where the first 200 are labeled for extractive sentence compression by two annotators  #TAUTHOR_TAG.', '5  #AUTHOR_TAG, we use the ground truth compressed sentences to compute f1 scores.', 'although f1 can be indicative of how well a compression model performs, we note that their could be multiple viable compressions for the same sentence, which single - reference ground - truth cannot cover  #AUTHOR_TAG.', 'thus to faithfully evaluate the quality of the compressions generated by our model, we follow  #TAUTHOR_TAG and conducted human studies on the gigaword dataset with amazon mechanical turk ( mturk ).', '6 specifically, we sample 150 examples from the test set, and put our model output side - by - side with the two compressions created by the two annotators.', '7 the three compressions were randomly shuffled to anonymize model identity.', 'we hire annotators who have an approval rate of at least 98 % and 10, 000 approved hits.', 'following  #TAUTHOR_TAG, we employed readability and informativeness as criteria on a five - point likert scale']",5
"[""2, respectively ), we report two f1's following  #TAUTHOR_TAG""]","[""2, respectively ), we report two f1's following  #TAUTHOR_TAG""]","[""2, respectively ), we report two f1's following  #TAUTHOR_TAG."", 'as shown in']","['evaluation we report f1 scores on both google and gigaword dataset.', ""since each of the 200 sentences from giga test set has two references ( by annotator 1 and 2, respectively ), we report two f1's following  #TAUTHOR_TAG."", 'as shown in table 2, our model is competitive ( esp.', 'for annotator # 2 ) with state - of - the - art supervised models trained on 1. 02 million examples with similar compression ratio.', 'we also report f1 results on the google dataset ( table 4 ).', '']",5
"['.', 'similar to our avgppl,  #TAUTHOR_TAG also employed average perplexity']","['then reconstruct it.', 'similar to our avgppl,  #TAUTHOR_TAG also employed average perplexity ( though without our length correction terms ) as the reward to a policy network trained with reinforcement learning.', 'another characteristic of our model is that we']","['.', 'similar to our avgppl,  #TAUTHOR_TAG also employed average perplexity']","['compression task has been investigated by various previous work  #AUTHOR_TAG mc  #AUTHOR_TAG berg -  #AUTHOR_TAG, where the more recent work tend to adopt a neural approach  #AUTHOR_TAG cifka et al., 2018 ).', 'our model is also neural - based since it leverages a neural language model.', 'our work differs from previous work in that it does not require any syntactic information such as pos tags or dependency parse tree, which is an advantage especially for low - resource language where training data for tagger or parser is scarce.', 'we note that  #AUTHOR_TAG also built an unsupervised model for abstractive sentence compression.', 'they trained a "" sequence - to - sequence - tosequence "" autoencoder to first compress the original sentence, and then reconstruct it.', 'similar to our avgppl,  #TAUTHOR_TAG also employed average perplexity ( though without our length correction terms ) as the reward to a policy network trained with reinforcement learning.', 'another characteristic of our model is that we obtain a sequence of sentences which are all valid compressions of the original one, while other models usually generate only one compression']",3
"['approaches  #TAUTHOR_TAG.', 'while']","['approaches  #TAUTHOR_TAG.', 'while']","['supervised machine learning, particularly deep learning approaches  #TAUTHOR_TAG.', 'while']","['morphology of polysynthetic languages is an emerging field of research.', 'polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations  #AUTHOR_TAG d ;  #AUTHOR_TAG a ).', 'previous approaches include rule - based methods based on finite state transducers  #AUTHOR_TAG, hybrid models  #AUTHOR_TAG b ;  #AUTHOR_TAG, and supervised machine learning, particularly deep learning approaches  #TAUTHOR_TAG.', 'while each rule - based method is developed for a specific language ( inuktitut  #AUTHOR_TAG, or arapaho  #AUTHOR_TAG ), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages.', 'we propose an unsupervised approach for morphological segmentation of polysynthetic languages based on adaptor grammars  #AUTHOR_TAG.', 'we experiment with four utoaztecan languages : mexicanero ( mx ), nahuatl ( nh ), wixarika ( wx ) and yorem nokki ( yn )  #TAUTHOR_TAG.', 'adaptor grammars ( ags ) are nonparametric bayesian models that generalize probabilistic context free grammars ( pcfg ), and have proven to be successful for unsupervised morphological segmentation, where a pcfg is a morphological grammar that specifies word structure  #AUTHOR_TAG eskander et al.,, 2018.', 'our main goal is to examine the success of adaptor grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex ( not simply agglutinative ), and where resources are minimal.', 'we use the datasets introduced by  #TAUTHOR_TAG in an unsupervised fashion ( unsegmented words ).', 'we design several ag learning setups : 1 ) use the best - on - average ag setup from  #AUTHOR_TAG ; 2 ) optimize for language using just the small training vocabulary ( unsegmented ) and dev vocabulary ( segmented ) from  #TAUTHOR_TAG ; 3 ) approximate the effect of having some linguistic knowledge ; 4 ) learn from all languages at once and 5 ) add additional unsupervised data for nh and wx ( section 3 ).', 'we show that the ag - based approaches outperform other unsupervised methods - m orf essor  #AUTHOR_TAG and m orphochain  #AUTHOR_TAG ) -, and that for two of the languages ( nh and yn ), the best ag - based approaches outperform the best supervised methods ( section 4 )']",0
"['approaches  #TAUTHOR_TAG.', 'while']","['approaches  #TAUTHOR_TAG.', 'while']","['supervised machine learning, particularly deep learning approaches  #TAUTHOR_TAG.', 'while']","['morphology of polysynthetic languages is an emerging field of research.', 'polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations  #AUTHOR_TAG d ;  #AUTHOR_TAG a ).', 'previous approaches include rule - based methods based on finite state transducers  #AUTHOR_TAG, hybrid models  #AUTHOR_TAG b ;  #AUTHOR_TAG, and supervised machine learning, particularly deep learning approaches  #TAUTHOR_TAG.', 'while each rule - based method is developed for a specific language ( inuktitut  #AUTHOR_TAG, or arapaho  #AUTHOR_TAG ), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages.', 'we propose an unsupervised approach for morphological segmentation of polysynthetic languages based on adaptor grammars  #AUTHOR_TAG.', 'we experiment with four utoaztecan languages : mexicanero ( mx ), nahuatl ( nh ), wixarika ( wx ) and yorem nokki ( yn )  #TAUTHOR_TAG.', 'adaptor grammars ( ags ) are nonparametric bayesian models that generalize probabilistic context free grammars ( pcfg ), and have proven to be successful for unsupervised morphological segmentation, where a pcfg is a morphological grammar that specifies word structure  #AUTHOR_TAG eskander et al.,, 2018.', 'our main goal is to examine the success of adaptor grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex ( not simply agglutinative ), and where resources are minimal.', 'we use the datasets introduced by  #TAUTHOR_TAG in an unsupervised fashion ( unsegmented words ).', 'we design several ag learning setups : 1 ) use the best - on - average ag setup from  #AUTHOR_TAG ; 2 ) optimize for language using just the small training vocabulary ( unsegmented ) and dev vocabulary ( segmented ) from  #TAUTHOR_TAG ; 3 ) approximate the effect of having some linguistic knowledge ; 4 ) learn from all languages at once and 5 ) add additional unsupervised data for nh and wx ( section 3 ).', 'we show that the ag - based approaches outperform other unsupervised methods - m orf essor  #AUTHOR_TAG and m orphochain  #AUTHOR_TAG ) -, and that for two of the languages ( nh and yn ), the best ag - based approaches outperform the best supervised methods ( section 4 )']",0
"['and lopez, 1999 ), and yn  #AUTHOR_TAG. they were constructed so they include both segmentable as well as non -  #TAUTHOR_TAG, for training we']","['and lopez, 1999 ), and yn  #AUTHOR_TAG. they were constructed so they include both segmentable as well as non -  #TAUTHOR_TAG, for training we do not use the segmented version of the data ( our approach is unsupervised )', '. in addition to the datasets, for nh and wx we also have available the bible  #AUTHOR_TAG a ), which we consider for', 'one of our experimental setups as additional training data. in the dataset from  #TAUTHOR_TAG, the maximum number of morphemes per word for mx is seven with an average of 2']","['and lopez, 1999 ), and yn  #AUTHOR_TAG. they were constructed so they include both segmentable as well as non -  #TAUTHOR_TAG, for training we do not use the segmented version of the data ( our approach is unsupervised )', '. in addition to the datasets, for nh and wx we also have available the bible  #AUTHOR_TAG a ), which we consider for', 'one of our experimental setups as additional training data. in the dataset from  #TAUTHOR_TAG, the maximum number of morphemes per word for mx is seven with an average of 2. 13 ;', 'for nh, six with']","['- p + - we - iwa an - two - ns 1sg : s - asi - 2pl : o - brother i have two brothers. in linguistic typ', '##ology, the broader gradient is : isolating / analytic to synthetic to polysynthetic. agglutinating refers to the clarity of boundaries between morphemes. this more specific gradation is : agglutinating to mildly fusional to fusion', '##al. thus a language might be characterized overall as polysynthetic and agglutinating, i. e. generally a high number of morphemes per word, with clear boundaries between morphemes and thus easily segmentable. another language might be characterized as polysynthetic', 'and fusional, so again, many morphemes per word, but many phonological and other processes so it is difficult to segment morph', '##emes. thus, morphological analysis of polysynthetic languages is challenging due to the rootmorpheme complexity and to word class gradations. linguists recognize a gradience in word classes, known as "" squ', '##ishiness "", a term first discussed in  #AUTHOR_TAG who argued that, instead of a fixed, distinct', 'inventory of syntactic categories, a quasi - continuum from verb,', 'adjective and noun best reflects most lexical distinctions. the rootmorpheme complexity and the', 'word class "" squish "" makes developing segmented training data with reliability across annotators difficult to achieve.  #AUTHOR_TAG have made a first step by releasing a small set of morphologically segmented datasets although even in these carefully', 'curated datasets, the distinction between affix and clitic is not always indicated. we use these datasets in an unsupervised fashion ( i. e., we use the unsegm', '##ented words ). these datasets were taken from detailed descriptions in the archive of indigenous languages collection for mx  #AUTHOR_TAG, nh (', 'de suarez, 1980 ), wx ( gomez and lopez, 1999 ), and yn  #AUTHOR_TAG. they were constructed so they include both segmentable as well as non -  #TAUTHOR_TAG, for training we do not use the segmented version of the data ( our approach is unsupervised )', '. in addition to the datasets, for nh and wx we also have available the bible  #AUTHOR_TAG a ), which we consider for', 'one of our experimental setups as additional training data. in the dataset from  #TAUTHOR_TAG, the maximum number of morphemes per word for mx is seven with an average of 2. 13 ;', 'for nh, six with an average of 2. 2 ; for wx, maximum of ten with an average of', '3. 3 ; and for yn, the maximum is ten, with an average of 2. 13']",0
"['and stems are not distinguished in the training annotations from  #TAUTHOR_TAG,']","['affixes and use them as scholar - seeded knowledge added to the grammars ( before the learning happens ).', 'however, since affixes and stems are not distinguished in the training annotations from  #TAUTHOR_TAG,']","['and stems are not distinguished in the training annotations from  #TAUTHOR_TAG,']","['', 'using seeded knowledge.', 'to approximate the effect of scholar - seeded - knowledge in  #AUTHOR_TAG, we used the training set to derive affixes and use them as scholar - seeded knowledge added to the grammars ( before the learning happens ).', 'however, since affixes and stems are not distinguished in the training annotations from  #TAUTHOR_TAG, we only consider the first and last morphemes that appear at least five times.', 'we call this setup ag scholar bestl.', 'multilingual training.', 'since the vocabulary in  #TAUTHOR_TAG for each language is small, and the languages are from the same language family, one data augmentation approach is to train on all languages and test then on each language individually.', 'we call this setup ag m ulti.', 'data augmentation.', 'in this setup, we examine the performance of the best ag configuration per language ( ag bestl ) when more data is available.', 'we merge the training corpus with unique words in the new testament of the bible ( train bible ).', 'we run this only on nh and wx since the bible text is only available for these two languages.', 'we denote this setup as ag aug']",1
"['and stems are not distinguished in the training annotations from  #TAUTHOR_TAG,']","['affixes and use them as scholar - seeded knowledge added to the grammars ( before the learning happens ).', 'however, since affixes and stems are not distinguished in the training annotations from  #TAUTHOR_TAG,']","['and stems are not distinguished in the training annotations from  #TAUTHOR_TAG,']","['', 'using seeded knowledge.', 'to approximate the effect of scholar - seeded - knowledge in  #AUTHOR_TAG, we used the training set to derive affixes and use them as scholar - seeded knowledge added to the grammars ( before the learning happens ).', 'however, since affixes and stems are not distinguished in the training annotations from  #TAUTHOR_TAG, we only consider the first and last morphemes that appear at least five times.', 'we call this setup ag scholar bestl.', 'multilingual training.', 'since the vocabulary in  #TAUTHOR_TAG for each language is small, and the languages are from the same language family, one data augmentation approach is to train on all languages and test then on each language individually.', 'we call this setup ag m ulti.', 'data augmentation.', 'in this setup, we examine the performance of the best ag configuration per language ( ag bestl ) when more data is available.', 'we merge the training corpus with unique words in the new testament of the bible ( train bible ).', 'we run this only on nh and wx since the bible text is only available for these two languages.', 'we denote this setup as ag aug']",1
,,,,7
,,,,7
,,,,7
,,,,4
[' #TAUTHOR_TAG only generated bag - of - words from'],[' #TAUTHOR_TAG only generated bag - of - words from'],[' #TAUTHOR_TAG only generated bag - of - words from'],[' #TAUTHOR_TAG'],1
[': in  #TAUTHOR_TAG'],"[""in  #TAUTHOR_TAG work, the source document's sentences were""]",[': in  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
['method of  #TAUTHOR_TAG and then our guided nlg approach'],['first briefly describe the amr - based summarization method of  #TAUTHOR_TAG and then our guided nlg approach'],['first briefly describe the amr - based summarization method of  #TAUTHOR_TAG and then our guided nlg approach'],['first briefly describe the amr - based summarization method of  #TAUTHOR_TAG and then our guided nlg approach'],0
"["" #TAUTHOR_TAG's work,""]","["" #TAUTHOR_TAG's work,""]","["" #TAUTHOR_TAG's work,""]","["" #TAUTHOR_TAG's work, each of the sentence of the source document was parsed into an amr graph, and combined into a source graph, g = ( v, e ) where v ∈ v and e ∈ e are the unique concepts and the relations between pairs of concepts."", 'they then extracted a summary graph, g ′ using the following sub - graph prediction :', 'where f ( v ) and f ( e ) are the feature representations of node v and edge e respectively.', 'the final summary produced was a bag of concept words extracted from g ′.', 'this output we will be replacing with our proposed guided nlg']",0
"[""##r graph, g ′ from  #TAUTHOR_TAG, following  #AUTHOR_TAG's preprocessing steps."", 'following this, the decoder generates the']","[""linearized summary amr graph, g ′ from  #TAUTHOR_TAG, following  #AUTHOR_TAG's preprocessing steps."", 'following this, the decoder generates the']","[""1, z 2,..., z k }, which is the linearized summary amr graph, g ′ from  #TAUTHOR_TAG, following  #AUTHOR_TAG's preprocessing steps."", 'following this, the decoder generates the target words, {']","['baseline is a standard ( unguided ) seq2seq model with attention  #AUTHOR_TAG which consists of an encoder and a decoder.', ""the encoder computes the hidden representation of the input, { z 1, z 2,..., z k }, which is the linearized summary amr graph, g ′ from  #TAUTHOR_TAG, following  #AUTHOR_TAG's preprocessing steps."", 'following this, the decoder generates the target words, { y 1, y 2,..., y m }, using the conditional probability p s2s ( y j | y < j, z ), which is calculated using the equation', ', where the attentional hidden state, h t is calculated using the equation', ', where c t is the source context vector, and h t is the target rnn hidden state.', 'the source context vector is defined as the weighted average over all the source rnn hidden states, h s, given the alignment vector, a t where a t is defined']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[""##eval 2016 task 8  #AUTHOR_TAG. we compare our result against  #TAUTHOR_TAG's bag of words 1, the unguided"", 'amr - to - text model from § 3. 2, and a seq2seq summarization model ( open - nmt brnn ) 2, 3 which summarizes directly', 'from the source document to summary sentence without using amr as an interlingua and is trained on cnn / dm corpus  #AUTHOR_TAG using the same settings as  #AUTHOR_TAG. in table 3, we can see that our', '']",3
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[""##eval 2016 task 8  #AUTHOR_TAG. we compare our result against  #TAUTHOR_TAG's bag of words 1, the unguided"", 'amr - to - text model from § 3. 2, and a seq2seq summarization model ( open - nmt brnn ) 2, 3 which summarizes directly', 'from the source document to summary sentence without using amr as an interlingua and is trained on cnn / dm corpus  #AUTHOR_TAG using the same settings as  #AUTHOR_TAG. in table 3, we can see that our', '']",3
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[""##eval 2016 task 8  #AUTHOR_TAG. we compare our result against  #TAUTHOR_TAG's bag of words 1, the unguided"", 'amr - to - text model from § 3. 2, and a seq2seq summarization model ( open - nmt brnn ) 2, 3 which summarizes directly', 'from the source document to summary sentence without using amr as an interlingua and is trained on cnn / dm corpus  #AUTHOR_TAG using the same settings as  #AUTHOR_TAG. in table 3, we can see that our', '']",3
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[""##eval 2016 task 8  #AUTHOR_TAG. we compare our result against  #TAUTHOR_TAG's bag of words 1, the unguided"", 'amr - to - text model from § 3. 2, and a seq2seq summarization model ( open - nmt brnn ) 2, 3 which summarizes directly', 'from the source document to summary sentence without using amr as an interlingua and is trained on cnn / dm corpus  #AUTHOR_TAG using the same settings as  #AUTHOR_TAG. in table 3, we can see that our', '']",5
['##s  #AUTHOR_TAG 2018 ;  #TAUTHOR_TAG'],['tree or graph - structured decoders  #AUTHOR_TAG 2018 ;  #TAUTHOR_TAG'],"['structured decoders  #AUTHOR_TAG 2018 ;  #TAUTHOR_TAG.', 'whereas']","['parsing is a task of transducing natural language to meaning representations, which in turn can be expressed through many different semantic formalisms including lambda calculus  #AUTHOR_TAG, dcs  #AUTHOR_TAG, discourse representation theory ( drt )  #AUTHOR_TAG, amr  #AUTHOR_TAG and so on.', 'this availability of annotated data in english has translated into the development of a plethora of models, including encoder - decoders  #AUTHOR_TAG as well as tree or graph - structured decoders  #AUTHOR_TAG 2018 ;  #TAUTHOR_TAG.', 'whereas the majority of semantic banks focus on english, recent effort has focussed on * work done when jingfeng yang was an intern and federico fancellu a post - doc at the university of edinburgh building multilingual representations, e. g. pmb  #AUTHOR_TAG, mrs  #AUTHOR_TAG and framenet ( pado and  #AUTHOR_TAG.', 'however, manually annotating meaning representations in a new language is a painstaking process which explains why there are only a few datasets available for different formalisms in languages other than english.', 'as a consequence, whereas the field has made great advances for english, little work has been done in other languages.', 'to answer this question, previous work have leveraged machine translation techniques to map the semantics from a language to another ( e. g.  #AUTHOR_TAG.', 'however, these methods require parallel corpora to extract automatic alignments which are often noisy or not available at all.', 'in this paper we explore parameter - shared models instead, where a model is trained on english using language independent features and tested in a target language.', 'to show how this approach performs, we focus on the parallel meaning bank ( pmb  #AUTHOR_TAG - a multilingual semantic bank, where sentences in english, german, italian and dutch have been annotated with their meaning representations.', 'the annotations in the pmb are based on discourse representation theory ( drt,  #AUTHOR_TAG, a popular theory of meaning representation designed to account for intra and inter - sentential phenomena, like temporal expressions and anaphora.', ""figure 1 shows an example drt for the sentence'i sat down and opened my laptop'in its canonical'box'representation."", ""a drs is a nested structure with the top part containing the discourse references and the bottom with unary and binary predicates, as well as semantic constants ( e. g.'speaker')."", 'drs can be linked to each other via logic operator ( e. g. ¬, →, [UNK] ) or, as in this case, discourse relations ( e. g. continuation, result, elabora - tion, etc. ).', 'to test our approach we leverage the drt parser of  #TAUTHOR_TAG']",0
['decoder of  #TAUTHOR_TAG reconstructs'],['decoder of  #TAUTHOR_TAG reconstructs'],"[""decoder of  #TAUTHOR_TAG reconstructs the drs in three steps, by first predicting the overall structure ( the'boxes')""]","[""decoder of  #TAUTHOR_TAG reconstructs the drs in three steps, by first predicting the overall structure ( the'boxes'), then the predicates and finally the referents, with each subsequent step being conditioned on the output of the previous."", ""during predicate prediction, the decoder uses a copying mechanism to predict those unary predicates that are also lemmas in the input sentence ( e. g.'eat')."", 'for the those that are not, soft attention is used instead.', 'no modifications were done to the decoder ; for more detail, we refer the reader to the original paper']",0
"['to be used as input to the parser,  #TAUTHOR_TAG first']","['to be used as input to the parser,  #TAUTHOR_TAG first']","['to be used as input to the parser,  #TAUTHOR_TAG first']","['use the pmb v. in order to be used as input to the parser,  #TAUTHOR_TAG first convert the drs into treebased representations, which are subsequently linearized into ptb - style bracketed sequences.', 'this transformation is lossless in that re - entrancies are duplicated to fit in the tree structure.', 'we use the same conversion in this work ; for further detail we refer the reader to the original paper.', 'finally, it is worth noting that lexical predicates in pmb are in english, even for non - english languages.', 'since this is not compatible with our copy mechanism, we revert predicates to their original language by substituting them with the lemmas of the tokens they are aligned to ( since gold alignment information is included in the pmb )']",0
"['. g. van  #AUTHOR_TAG,  #TAUTHOR_TAG']","['on the pmb ( e. g. van  #AUTHOR_TAG,  #TAUTHOR_TAG']","['. g. van  #AUTHOR_TAG,  #TAUTHOR_TAG does not deal with presupposition.', 'in the pmb, presupposed variables']","['use counter  #AUTHOR_TAG to evaluate the performance of our models.', 'counter looks for the best alignment between the predicted and gold drs and computes precision, recall and f1.', 'for further details about counter, the reader is referred to  #AUTHOR_TAG.', 'it is worth reminding that unlike other work on the pmb ( e. g. van  #AUTHOR_TAG,  #TAUTHOR_TAG does not deal with presupposition.', 'in the pmb, presupposed variables are extracted from a main box and included in a separate one.', 'in our work, we revert this process so to ignore presupposed boxes.', 'similarly, we also do not deal with sense tags which we aim to include in future work.', 'table 1 shows the performance of our crosslingual models in german, italian and dutch.', 'we summarize the results as follows : dependency features are crucial for zeroshot cross - lingual semantic parsing.', 'adding dependency features dramatically improves the performance in all three languages, when compared to using multilingual word - embedding and universal pos embeddings alone.', 'we hypothesize that the quality of the multilingual word - embeddings is poor, given that models using embeddings for the dependency relations alone outperform those using the other two features']",0
[' #AUTHOR_TAG 2018 ;  #TAUTHOR_TAG'],"['as multilingual word embeddings, universal pos tags and ud  #AUTHOR_TAG.', 'for semantic parsing, encoder - decoder mod - els have achieved great success.', 'amongst these, tree or graph - structured decoders have recently shown to be state - of - the - art  #AUTHOR_TAG 2018 ;  #TAUTHOR_TAG']",[' #AUTHOR_TAG 2018 ;  #TAUTHOR_TAG'],"['work have explored two main methods for cross - lingual semantic parsing.', 'one method requires parallel corpora to extract alignments between source and target languages using machine translation ( pado and  #AUTHOR_TAG the other method is to use parameter - shared models in the target language and the source language by leveraging language - independent features such as multilingual word embeddings, universal pos tags and ud  #AUTHOR_TAG.', 'for semantic parsing, encoder - decoder mod - els have achieved great success.', 'amongst these, tree or graph - structured decoders have recently shown to be state - of - the - art  #AUTHOR_TAG 2018 ;  #TAUTHOR_TAG']",0
['##s  #AUTHOR_TAG 2018 ;  #TAUTHOR_TAG'],['tree or graph - structured decoders  #AUTHOR_TAG 2018 ;  #TAUTHOR_TAG'],"['structured decoders  #AUTHOR_TAG 2018 ;  #TAUTHOR_TAG.', 'whereas']","['parsing is a task of transducing natural language to meaning representations, which in turn can be expressed through many different semantic formalisms including lambda calculus  #AUTHOR_TAG, dcs  #AUTHOR_TAG, discourse representation theory ( drt )  #AUTHOR_TAG, amr  #AUTHOR_TAG and so on.', 'this availability of annotated data in english has translated into the development of a plethora of models, including encoder - decoders  #AUTHOR_TAG as well as tree or graph - structured decoders  #AUTHOR_TAG 2018 ;  #TAUTHOR_TAG.', 'whereas the majority of semantic banks focus on english, recent effort has focussed on * work done when jingfeng yang was an intern and federico fancellu a post - doc at the university of edinburgh building multilingual representations, e. g. pmb  #AUTHOR_TAG, mrs  #AUTHOR_TAG and framenet ( pado and  #AUTHOR_TAG.', 'however, manually annotating meaning representations in a new language is a painstaking process which explains why there are only a few datasets available for different formalisms in languages other than english.', 'as a consequence, whereas the field has made great advances for english, little work has been done in other languages.', 'to answer this question, previous work have leveraged machine translation techniques to map the semantics from a language to another ( e. g.  #AUTHOR_TAG.', 'however, these methods require parallel corpora to extract automatic alignments which are often noisy or not available at all.', 'in this paper we explore parameter - shared models instead, where a model is trained on english using language independent features and tested in a target language.', 'to show how this approach performs, we focus on the parallel meaning bank ( pmb  #AUTHOR_TAG - a multilingual semantic bank, where sentences in english, german, italian and dutch have been annotated with their meaning representations.', 'the annotations in the pmb are based on discourse representation theory ( drt,  #AUTHOR_TAG, a popular theory of meaning representation designed to account for intra and inter - sentential phenomena, like temporal expressions and anaphora.', ""figure 1 shows an example drt for the sentence'i sat down and opened my laptop'in its canonical'box'representation."", ""a drs is a nested structure with the top part containing the discourse references and the bottom with unary and binary predicates, as well as semantic constants ( e. g.'speaker')."", 'drs can be linked to each other via logic operator ( e. g. ¬, →, [UNK] ) or, as in this case, discourse relations ( e. g. continuation, result, elabora - tion, etc. ).', 'to test our approach we leverage the drt parser of  #TAUTHOR_TAG']",6
"['of  #TAUTHOR_TAG ; for more detail, we refer']","['of  #TAUTHOR_TAG ; for more detail, we refer']","['this section, we describe the modifications to the coarse - to - fine encoder - decoder architecture of  #TAUTHOR_TAG ; for more detail, we refer the reader to the original paper']","['this section, we describe the modifications to the coarse - to - fine encoder - decoder architecture of  #TAUTHOR_TAG ; for more detail, we refer the reader to the original paper']",6
"['##stm.', ""we use  #TAUTHOR_TAG's bi - lstm as baseline""]","['##stm.', ""we use  #TAUTHOR_TAG's bi - lstm as baseline."", 'however, whereas']","['##stm.', ""we use  #TAUTHOR_TAG's bi - lstm as baseline""]","['##stm.', ""we use  #TAUTHOR_TAG's bi - lstm as baseline."", 'however, whereas the original model represents each token in the input sentence as the concatenation of word ( e w i ) and lemma embeddings, we discard the latter and add a pos tag embedding ( e p i ) and dependency relation embedding ( e d i ) feature.', '']",5
"['multiple predicates  #TAUTHOR_TAG.', 'in']","['multiple predicates  #TAUTHOR_TAG.', 'in']","['multiple predicates  #TAUTHOR_TAG.', 'in figure 1, for']",[' #TAUTHOR_TAG'],0
"['multiple predicates  #TAUTHOR_TAG.', 'in']","['multiple predicates  #TAUTHOR_TAG.', 'in']","['multiple predicates  #TAUTHOR_TAG.', 'in figure 1, for']",[' #TAUTHOR_TAG'],0
['of  #TAUTHOR_TAG is a'],['of  #TAUTHOR_TAG is a'],"['of  #TAUTHOR_TAG is a state - of - the - art end - to - end model, designed']","['third set of rows in table 1 shows the reported performance of related studies.', 'grid rnn of  #TAUTHOR_TAG is a state - of - the - art end - to - end model, designed to capture interactions among multiple predicate - argument relations.', 'a comparison between their model and the proposed models was somewhat tricky because our replication of grid rnn did not reproduce the reported performance on the same dataset ( see the row of grid in table 1 ).', 'unlike the results reported in  #TAUTHOR_TAG, the grid model in our experiment did not clearly outperform the model without the grid architecture, i. e., the base model.', '']",0
"['end neural model,  #TAUTHOR_TAG used a grid rn']","['neural model,  #TAUTHOR_TAG used a grid rnn to capture multiple predicate interactions.', 'through experiments, we demonstrated that']","['to other predicates.', "" #AUTHOR_TAG adapted a nn framework to  #AUTHOR_TAG's model using a feedforward network."", 'for an end - to - end neural model,  #TAUTHOR_TAG used a grid rnn to capture multiple predicate interactions.', 'through experiments, we demonstrated that']","['', "" #AUTHOR_TAG adapted a nn framework to  #AUTHOR_TAG's model using a feedforward network."", 'for an end - to - end neural model,  #TAUTHOR_TAG used a grid rnn to capture multiple predicate interactions.', 'through experiments, we demonstrated that our proposed models outperformed these models in terms of the overall f 1 on a standard benchmark corpus.', '4 to the best of our knowledge, there are few previous studies related to srl considering multiple predicate interactions for languages other than japanese.', ' #AUTHOR_TAG performed a discriminative reranking in the role classification of shared arguments.', ' #AUTHOR_TAG proposed an srl model based on the dimensionality reduction on a tensor representation to capture meaningful interactions between the argument, predicate, corresponding features, and role label.', 'it is not straightforward to compare these methods with our models ; however, it is an intriguing future issue to consider how well the techniques devised for japanese pas analysis work for other languages.', 'other approaches to argument omission in order to perform robust prediction for arguments with fewer syntactic clues, several previous studies have explored various types of selectional preference scores that consider the semantic relations between a predicate and its arguments  #AUTHOR_TAG.', '']",0
"['multiple predicates  #TAUTHOR_TAG.', 'in']","['multiple predicates  #TAUTHOR_TAG.', 'in']","['multiple predicates  #TAUTHOR_TAG.', 'in figure 1, for']",[' #TAUTHOR_TAG'],1
"['multiple predicates  #TAUTHOR_TAG.', 'in']","['multiple predicates  #TAUTHOR_TAG.', 'in']","['multiple predicates  #TAUTHOR_TAG.', 'in figure 1, for']",[' #TAUTHOR_TAG'],1
"[' #TAUTHOR_TAG, and  #AUTHOR_TAG,']","[' #TAUTHOR_TAG, and  #AUTHOR_TAG,']","[' #TAUTHOR_TAG, and  #AUTHOR_TAG,']","['this paper, we employ a task definition based on the naist text corpus ( ntc )  #AUTHOR_TAG, a commonly used benchmark corpus annotated with nominative ( nom ), accusative ( acc ), and dative ( dat ) arguments for predicates.', 'given a tokenized sentence w = w 1,..., w n and its predicate positions p = p 1,..., p q, our task is to identify at most one head of the filler tokens for each argument slot of each predicate.', 'in this study, we follow the setting of  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG, and focus only on analyzing arguments in a target sentence.', 'in addition, we exclude argument instances that are in the same bunsetsu, a base phrase unit in japanese, as the target predicate, following  #TAUTHOR_TAG, which we will compare with the results in experiments.', 'the semantic labels used in ntc may seem to be rather syntactic as they are named nominative, accusative, etc.', 'however, this annotation task markedly differs from shallow syntactic parsing and is, in fact, more like a semantic role labeling ( srl ) task including implicit argument prediction.', 'first, the semantic labels in ntc generalize case alteration caused by voice alteration and thus represent semantic roles analogous to arg0, arg1, etc.', 'in the propbank - style annotation  #AUTHOR_TAG.', 'second, in the corpus, when an argument is omitted ( i. e., zero - anaphora ), the antecedent is identified with an appropriate semantic role, which is a prominent problem in japanese semantic analysis and is the primary target of this study']",5
"[' #TAUTHOR_TAG, and  #AUTHOR_TAG,']","[' #TAUTHOR_TAG, and  #AUTHOR_TAG,']","[' #TAUTHOR_TAG, and  #AUTHOR_TAG,']","['this paper, we employ a task definition based on the naist text corpus ( ntc )  #AUTHOR_TAG, a commonly used benchmark corpus annotated with nominative ( nom ), accusative ( acc ), and dative ( dat ) arguments for predicates.', 'given a tokenized sentence w = w 1,..., w n and its predicate positions p = p 1,..., p q, our task is to identify at most one head of the filler tokens for each argument slot of each predicate.', 'in this study, we follow the setting of  #AUTHOR_TAG,  #TAUTHOR_TAG, and  #AUTHOR_TAG, and focus only on analyzing arguments in a target sentence.', 'in addition, we exclude argument instances that are in the same bunsetsu, a base phrase unit in japanese, as the target predicate, following  #TAUTHOR_TAG, which we will compare with the results in experiments.', 'the semantic labels used in ntc may seem to be rather syntactic as they are named nominative, accusative, etc.', 'however, this annotation task markedly differs from shallow syntactic parsing and is, in fact, more like a semantic role labeling ( srl ) task including implicit argument prediction.', 'first, the semantic labels in ntc generalize case alteration caused by voice alteration and thus represent semantic roles analogous to arg0, arg1, etc.', 'in the propbank - style annotation  #AUTHOR_TAG.', 'second, in the corpus, when an argument is omitted ( i. e., zero - anaphora ), the antecedent is identified with an appropriate semantic role, which is a prominent problem in japanese semantic analysis and is the primary target of this study']",5
[' #TAUTHOR_TAG to combine mechanisms that consider multiple predicate interactions'],"['proposed models extend end - to - end style srl systems using deep bi - rnn  #TAUTHOR_TAG to combine mechanisms that consider multiple predicate interactions.', '']","['- to - end style srl systems using deep bi - rnn  #TAUTHOR_TAG to combine mechanisms that consider multiple predicate interactions.', 'figure 2a shows the network of']","['proposed models extend end - to - end style srl systems using deep bi - rnn  #TAUTHOR_TAG to combine mechanisms that consider multiple predicate interactions.', 'figure 2a shows the network of our base model.', 'formally, given a word sequence w = w 1,..., w n and a target predicate position p i in p, the model outputs a label probability for each word position :', 'here, c i, t ∈ { nom, acc, dat, none } represents the argument label of the word w t for the target predicate w p i.', 'the input layer creates a vector h 0 i, t ∈ r dw + 1 for each pair of a predicate w p i and a word w t by concatenating a word embedding e ( w t ) ∈ r dw and a binary value representing the target predicate position in a method similar to that of  #AUTHOR_TAG.', 'the obtained vectors are then input into the deep bi - rnn, where the directions of the layers alternate  #AUTHOR_TAG :', '']",5
[' #TAUTHOR_TAG to combine mechanisms that consider multiple predicate interactions'],"['proposed models extend end - to - end style srl systems using deep bi - rnn  #TAUTHOR_TAG to combine mechanisms that consider multiple predicate interactions.', '']","['- to - end style srl systems using deep bi - rnn  #TAUTHOR_TAG to combine mechanisms that consider multiple predicate interactions.', 'figure 2a shows the network of']","['proposed models extend end - to - end style srl systems using deep bi - rnn  #TAUTHOR_TAG to combine mechanisms that consider multiple predicate interactions.', 'figure 2a shows the network of our base model.', 'formally, given a word sequence w = w 1,..., w n and a target predicate position p i in p, the model outputs a label probability for each word position :', 'here, c i, t ∈ { nom, acc, dat, none } represents the argument label of the word w t for the target predicate w p i.', 'the input layer creates a vector h 0 i, t ∈ r dw + 1 for each pair of a predicate w p i and a word w t by concatenating a word embedding e ( w t ) ∈ r dw and a binary value representing the target predicate position in a method similar to that of  #AUTHOR_TAG.', 'the obtained vectors are then input into the deep bi - rnn, where the directions of the layers alternate  #AUTHOR_TAG :', '']",5
['replicated the method of  #TAUTHOR_TAG by modifying equations ( 1 ) of'],['replicated the method of  #TAUTHOR_TAG by modifying equations ( 1 ) of'],"['base model, we replicated the method of  #TAUTHOR_TAG by modifying equations ( 1 ) of']","['order to strictly compare the impact of our extensions to the method used for integrating multiple pieces of predicate information in the state - of - the - art end - to - end model, in addition to our base model, we replicated the method of  #TAUTHOR_TAG by modifying equations ( 1 ) of our base model as follows :', 'if 1 ≤ i ≤ q ; otherwise, h k i, t = 0.', 'the performance of this replicated model may not be strictly the same as that reported in  #TAUTHOR_TAG due to discrepancies in the embeddings of inputs, hyperparameters ( a training batch size, a hidden unit size, etc. ), and training strategy ( an optimizing algorithm, a regularization method, an early stopping method, etc. ).', 'the predicate positions p = p 1,..., p q are arranged in ascending order.', 'table 2 : p - values in one - sided permutation test using 10 overall f 1 scores for each model.', 'the bold values indicate that an average f 1 score of model a outperforms that of model b at the 5 % significance level']",5
['of  #TAUTHOR_TAG is a'],['of  #TAUTHOR_TAG is a'],"['of  #TAUTHOR_TAG is a state - of - the - art end - to - end model, designed']","['third set of rows in table 1 shows the reported performance of related studies.', 'grid rnn of  #TAUTHOR_TAG is a state - of - the - art end - to - end model, designed to capture interactions among multiple predicate - argument relations.', 'a comparison between their model and the proposed models was somewhat tricky because our replication of grid rnn did not reproduce the reported performance on the same dataset ( see the row of grid in table 1 ).', 'unlike the results reported in  #TAUTHOR_TAG, the grid model in our experiment did not clearly outperform the model without the grid architecture, i. e., the base model.', '']",5
[' #TAUTHOR_TAG to combine mechanisms that consider multiple predicate interactions'],"['proposed models extend end - to - end style srl systems using deep bi - rnn  #TAUTHOR_TAG to combine mechanisms that consider multiple predicate interactions.', '']","['- to - end style srl systems using deep bi - rnn  #TAUTHOR_TAG to combine mechanisms that consider multiple predicate interactions.', 'figure 2a shows the network of']","['proposed models extend end - to - end style srl systems using deep bi - rnn  #TAUTHOR_TAG to combine mechanisms that consider multiple predicate interactions.', 'figure 2a shows the network of our base model.', 'formally, given a word sequence w = w 1,..., w n and a target predicate position p i in p, the model outputs a label probability for each word position :', 'here, c i, t ∈ { nom, acc, dat, none } represents the argument label of the word w t for the target predicate w p i.', 'the input layer creates a vector h 0 i, t ∈ r dw + 1 for each pair of a predicate w p i and a word w t by concatenating a word embedding e ( w t ) ∈ r dw and a binary value representing the target predicate position in a method similar to that of  #AUTHOR_TAG.', 'the obtained vectors are then input into the deep bi - rnn, where the directions of the layers alternate  #AUTHOR_TAG :', '']",6
"['of  #TAUTHOR_TAG, where the information of multiple predicates propagates through the rn']","['of  #TAUTHOR_TAG, where the information of multiple predicates propagates through the rnns,']","['of  #TAUTHOR_TAG, where the information of multiple predicates propagates through the rnns,']","['base model independently predicts the arguments of each predicate.', 'in order to capture dependencies between the arguments of multiple predicates, we apply two extensions to our base model : a multi - predicate input layer and three variants of interaction layers on top of the deep bi - rnns.', 'figures 2b and 3 show the network structures of the extended models.', 'in contrast to the grid rnn model of  #TAUTHOR_TAG, where the information of multiple predicates propagates through the rnns, our interaction layers use pooling and attention mechanisms to directly associate the label prediction information for a target ( predicate, word ) pair with that for words strongly related to the target pair, without being disturbed by word order and distance']",4
['replicated the method of  #TAUTHOR_TAG by modifying equations ( 1 ) of'],['replicated the method of  #TAUTHOR_TAG by modifying equations ( 1 ) of'],"['base model, we replicated the method of  #TAUTHOR_TAG by modifying equations ( 1 ) of']","['order to strictly compare the impact of our extensions to the method used for integrating multiple pieces of predicate information in the state - of - the - art end - to - end model, in addition to our base model, we replicated the method of  #TAUTHOR_TAG by modifying equations ( 1 ) of our base model as follows :', 'if 1 ≤ i ≤ q ; otherwise, h k i, t = 0.', 'the performance of this replicated model may not be strictly the same as that reported in  #TAUTHOR_TAG due to discrepancies in the embeddings of inputs, hyperparameters ( a training batch size, a hidden unit size, etc. ), and training strategy ( an optimizing algorithm, a regularization method, an early stopping method, etc. ).', 'the predicate positions p = p 1,..., p q are arranged in ascending order.', 'table 2 : p - values in one - sided permutation test using 10 overall f 1 scores for each model.', 'the bold values indicate that an average f 1 score of model a outperforms that of model b at the 5 % significance level']",4
['of  #TAUTHOR_TAG is a'],['of  #TAUTHOR_TAG is a'],"['of  #TAUTHOR_TAG is a state - of - the - art end - to - end model, designed']","['third set of rows in table 1 shows the reported performance of related studies.', 'grid rnn of  #TAUTHOR_TAG is a state - of - the - art end - to - end model, designed to capture interactions among multiple predicate - argument relations.', 'a comparison between their model and the proposed models was somewhat tricky because our replication of grid rnn did not reproduce the reported performance on the same dataset ( see the row of grid in table 1 ).', 'unlike the results reported in  #TAUTHOR_TAG, the grid model in our experiment did not clearly outperform the model without the grid architecture, i. e., the base model.', '']",4
['of  #TAUTHOR_TAG is a'],['of  #TAUTHOR_TAG is a'],"['of  #TAUTHOR_TAG is a state - of - the - art end - to - end model, designed']","['third set of rows in table 1 shows the reported performance of related studies.', 'grid rnn of  #TAUTHOR_TAG is a state - of - the - art end - to - end model, designed to capture interactions among multiple predicate - argument relations.', 'a comparison between their model and the proposed models was somewhat tricky because our replication of grid rnn did not reproduce the reported performance on the same dataset ( see the row of grid in table 1 ).', 'unlike the results reported in  #TAUTHOR_TAG, the grid model in our experiment did not clearly outperform the model without the grid architecture, i. e., the base model.', '']",4
['of  #TAUTHOR_TAG is a'],['of  #TAUTHOR_TAG is a'],"['of  #TAUTHOR_TAG is a state - of - the - art end - to - end model, designed']","['third set of rows in table 1 shows the reported performance of related studies.', 'grid rnn of  #TAUTHOR_TAG is a state - of - the - art end - to - end model, designed to capture interactions among multiple predicate - argument relations.', 'a comparison between their model and the proposed models was somewhat tricky because our replication of grid rnn did not reproduce the reported performance on the same dataset ( see the row of grid in table 1 ).', 'unlike the results reported in  #TAUTHOR_TAG, the grid model in our experiment did not clearly outperform the model without the grid architecture, i. e., the base model.', '']",4
"['end neural model,  #TAUTHOR_TAG used a grid rn']","['neural model,  #TAUTHOR_TAG used a grid rnn to capture multiple predicate interactions.', 'through experiments, we demonstrated that']","['to other predicates.', "" #AUTHOR_TAG adapted a nn framework to  #AUTHOR_TAG's model using a feedforward network."", 'for an end - to - end neural model,  #TAUTHOR_TAG used a grid rnn to capture multiple predicate interactions.', 'through experiments, we demonstrated that']","['', "" #AUTHOR_TAG adapted a nn framework to  #AUTHOR_TAG's model using a feedforward network."", 'for an end - to - end neural model,  #TAUTHOR_TAG used a grid rnn to capture multiple predicate interactions.', 'through experiments, we demonstrated that our proposed models outperformed these models in terms of the overall f 1 on a standard benchmark corpus.', '4 to the best of our knowledge, there are few previous studies related to srl considering multiple predicate interactions for languages other than japanese.', ' #AUTHOR_TAG performed a discriminative reranking in the role classification of shared arguments.', ' #AUTHOR_TAG proposed an srl model based on the dimensionality reduction on a tensor representation to capture meaningful interactions between the argument, predicate, corresponding features, and role label.', 'it is not straightforward to compare these methods with our models ; however, it is an intriguing future issue to consider how well the techniques devised for japanese pas analysis work for other languages.', 'other approaches to argument omission in order to perform robust prediction for arguments with fewer syntactic clues, several previous studies have explored various types of selectional preference scores that consider the semantic relations between a predicate and its arguments  #AUTHOR_TAG.', '']",4
"[' #TAUTHOR_TAG model, by which']","['in this paper, we explicitly encode lexical', 'semantics, both corpus - based and knowledge - based information, in the  #TAUTHOR_TAG model, by which we are able to achieve even better results in ss task']","['sentential semantics. in this paper, we explicitly encode lexical', 'semantics, both corpus - based and knowledge - based information, in the  #TAUTHOR_TAG model, by which we are able to achieve even better results in ss task. the additional corpus - based information']","['reduction techniques for documents, which have abundant words for extracting the document level semantics. however, in the ss setting, it is crucial to make good use of each word, given the limited number of words in a sentence. we believe', 'a reasonable word generation story will avoid introducing noise in sentential semantics, encouraging robust lexical semantics which can further boost the sentential semantics. in this paper, we explicitly encode lexical', 'semantics, both corpus - based and knowledge - based information, in the  #TAUTHOR_TAG model, by which we are able to achieve even better results in ss task. the additional corpus - based information we exploit is selectional preference semantics  #AUTHOR_TAG, a feature already existing in the data', 'yet ignored by most latent variable models. selectional preference focuses on the admissible arguments for a word, thus capturing more nuanced semantics than the sentence ids ( when applied to a corpus of sentences as opposed to documents ). consider the following example : in  #TAUTHOR_TAG / lsa / lda, a word will receive semantics from all the other words', '']",5
"[' #TAUTHOR_TAG model, by which']","['in this paper, we explicitly encode lexical', 'semantics, both corpus - based and knowledge - based information, in the  #TAUTHOR_TAG model, by which we are able to achieve even better results in ss task']","['sentential semantics. in this paper, we explicitly encode lexical', 'semantics, both corpus - based and knowledge - based information, in the  #TAUTHOR_TAG model, by which we are able to achieve even better results in ss task. the additional corpus - based information']","['reduction techniques for documents, which have abundant words for extracting the document level semantics. however, in the ss setting, it is crucial to make good use of each word, given the limited number of words in a sentence. we believe', 'a reasonable word generation story will avoid introducing noise in sentential semantics, encouraging robust lexical semantics which can further boost the sentential semantics. in this paper, we explicitly encode lexical', 'semantics, both corpus - based and knowledge - based information, in the  #TAUTHOR_TAG model, by which we are able to achieve even better results in ss task. the additional corpus - based information we exploit is selectional preference semantics  #AUTHOR_TAG, a feature already existing in the data', 'yet ignored by most latent variable models. selectional preference focuses on the admissible arguments for a word, thus capturing more nuanced semantics than the sentence ids ( when applied to a corpus of sentences as opposed to documents ). consider the following example : in  #TAUTHOR_TAG / lsa / lda, a word will receive semantics from all the other words', '']",5
"[' #TAUTHOR_TAG model, by which']","['in this paper, we explicitly encode lexical', 'semantics, both corpus - based and knowledge - based information, in the  #TAUTHOR_TAG model, by which we are able to achieve even better results in ss task']","['sentential semantics. in this paper, we explicitly encode lexical', 'semantics, both corpus - based and knowledge - based information, in the  #TAUTHOR_TAG model, by which we are able to achieve even better results in ss task. the additional corpus - based information']","['reduction techniques for documents, which have abundant words for extracting the document level semantics. however, in the ss setting, it is crucial to make good use of each word, given the limited number of words in a sentence. we believe', 'a reasonable word generation story will avoid introducing noise in sentential semantics, encouraging robust lexical semantics which can further boost the sentential semantics. in this paper, we explicitly encode lexical', 'semantics, both corpus - based and knowledge - based information, in the  #TAUTHOR_TAG model, by which we are able to achieve even better results in ss task. the additional corpus - based information we exploit is selectional preference semantics  #AUTHOR_TAG, a feature already existing in the data', 'yet ignored by most latent variable models. selectional preference focuses on the admissible arguments for a word, thus capturing more nuanced semantics than the sentence ids ( when applied to a corpus of sentences as opposed to documents ). consider the following example : in  #TAUTHOR_TAG / lsa / lda, a word will receive semantics from all the other words', '']",5
['models the sentences in the weighted matrix factorization'],['models the sentences in the weighted matrix factorization'],"['models the sentences in the weighted matrix factorization framework ( figure 1 ).', 'the corpus is stored in an m × n matrix']","['models the sentences in the weighted matrix factorization framework ( figure 1 ).', 'the corpus is stored in an m × n matrix x, with each cell containing the tf - idf values of words.', 'the rows of x are m distinct words and columns are n sentences.', '']",5
[' #TAUTHOR_TAG use alternating least square'],[' #TAUTHOR_TAG use alternating least square'],"[' #TAUTHOR_TAG use alternating least square [ als ] for inference, which is to set the derivative of equation 1 for p / q to 0 and iteratively compute p / q by fixing the other matrix  #AUTHOR_TAG.', 'however, it is no longer applicable with the new term ( equation 2 ) involving the length of word vectors']","[' #TAUTHOR_TAG use alternating least square [ als ] for inference, which is to set the derivative of equation 1 for p / q to 0 and iteratively compute p / q by fixing the other matrix  #AUTHOR_TAG.', 'however, it is no longer applicable with the new term ( equation 2 ) involving the length of word vectors | p ·, i |.', 'therefore we approximate the objective function by treating the vector length | p ·, i | as fixed values during the als iterations :', 'where p ·, s ( i ) are the latent vectors of similar words of word i ; the length of these vectors in the current iteration are stored in l s ( i ) ( similarly l i is the current length of p ·, i ) ( cf.  #TAUTHOR_TAG for optimization details )']",5
"[' #TAUTHOR_TAG. we run gibbs sampling based lda', 'for 2000 iterations']","[""the system's answer"", 'and gold standard similarity scores is used as the evaluation metric. we include three baselines lsa, lda and  #TAUTHOR_TAG. we run gibbs sampling based lda', 'for 2000 iterations']","[', lda and  #TAUTHOR_TAG. we run gibbs sampling based lda', 'for 2000 iterations']","['##12 test data that comprises 3150 sentence pairs from msr - par, msr - vid, smt - eur, smt - news, on - wn. it is worth noting that smt - news and on - wn are not part of the', ""tuning data. we use cosine similarity to measure the similarity scores between two sentences. pearson correlation between the system's answer"", 'and gold standard similarity scores is used as the evaluation metric. we include three baselines lsa, lda and  #TAUTHOR_TAG. we run gibbs sampling based lda', '']",5
"[' #TAUTHOR_TAG. we run gibbs sampling based lda', 'for 2000 iterations']","[""the system's answer"", 'and gold standard similarity scores is used as the evaluation metric. we include three baselines lsa, lda and  #TAUTHOR_TAG. we run gibbs sampling based lda', 'for 2000 iterations']","[', lda and  #TAUTHOR_TAG. we run gibbs sampling based lda', 'for 2000 iterations']","['##12 test data that comprises 3150 sentence pairs from msr - par, msr - vid, smt - eur, smt - news, on - wn. it is worth noting that smt - news and on - wn are not part of the', ""tuning data. we use cosine similarity to measure the similarity scores between two sentences. pearson correlation between the system's answer"", 'and gold standard similarity scores is used as the evaluation metric. we include three baselines lsa, lda and  #TAUTHOR_TAG. we run gibbs sampling based lda', '']",5
"[' #TAUTHOR_TAG. we run gibbs sampling based lda', 'for 2000 iterations']","[""the system's answer"", 'and gold standard similarity scores is used as the evaluation metric. we include three baselines lsa, lda and  #TAUTHOR_TAG. we run gibbs sampling based lda', 'for 2000 iterations']","[', lda and  #TAUTHOR_TAG. we run gibbs sampling based lda', 'for 2000 iterations']","['##12 test data that comprises 3150 sentence pairs from msr - par, msr - vid, smt - eur, smt - news, on - wn. it is worth noting that smt - news and on - wn are not part of the', ""tuning data. we use cosine similarity to measure the similarity scores between two sentences. pearson correlation between the system's answer"", 'and gold standard similarity scores is used as the evaluation metric. we include three baselines lsa, lda and  #TAUTHOR_TAG. we run gibbs sampling based lda', '']",5
"['on  #TAUTHOR_TAG and wtmf + pk.', 'generally a larger k']","['of dimension k = { 50, 75, 100, 125, 150 } on  #TAUTHOR_TAG and wtmf + pk.', 'generally a larger k']","['on  #TAUTHOR_TAG and wtmf + pk.', 'generally a larger k']","['the performance using different values of weights in figure 3a and 3b, we can conclude that the selectional preference and similar word pairs yield very promising results.', 'the trends hold in different parameter conditions with a consistent improvement.', 'figure 3c illustrates the impact of dimension k = { 50, 75, 100, 125, 150 } on  #TAUTHOR_TAG and wtmf + pk.', 'generally a larger k leads to a higher pearson correlation, but the improvement is tiny when k ≥ 100 ( 0. 1 % increase ).', 'compared to all the unsupervised systems that participated in semeval sts 2012 task, wtmf + pk yields state - of - the - art performance ( 70. 70 % ).', '2 in  #AUTHOR_TAG c ) we also apply wtmf ( k = 100 ) on sts12, achieving a correlation of 69. 5 %.', 'however, additional data is incorporated in the training corpora : ( 1 ) sts12 tuning set ; ( 2 ) for wordnet and wiktionary data, the target words are also included in the definitions ( hence synonym pairs were used ) ; ( 3 ) the usage examples of target words were also appended to the definitions.', '3 while trained with this experimental setting, our model wtmf + pk ( γ = 2, δ = 0. 3, k = 100 ) is able to reach an even higher correlation of 72. 0 %.', 'figure 3d presents the results obtained on the li06 data set at different weight values for the corpusbased selectional preference semantics γ and for the knowledge - based semantics δ.', 'our previous experiments  #TAUTHOR_TAG is the state - of - the - art model on li06.', 'with lexical semantics explicitly modeled, wtmf + pk yields better results than  #TAUTHOR_TAG ( see table 1 ).', 'it should be noted that li06 prefers a smaller similar word pair weight ( a δ = 0. 1 yields the best performance around of 90. 75 % ), yet in almost all conditions wtmf + pk outperforms  #TAUTHOR_TAG as shown in figure 3d']",5
"[' #TAUTHOR_TAG. we run gibbs sampling based lda', 'for 2000 iterations']","[""the system's answer"", 'and gold standard similarity scores is used as the evaluation metric. we include three baselines lsa, lda and  #TAUTHOR_TAG. we run gibbs sampling based lda', 'for 2000 iterations']","[', lda and  #TAUTHOR_TAG. we run gibbs sampling based lda', 'for 2000 iterations']","['##12 test data that comprises 3150 sentence pairs from msr - par, msr - vid, smt - eur, smt - news, on - wn. it is worth noting that smt - news and on - wn are not part of the', ""tuning data. we use cosine similarity to measure the similarity scores between two sentences. pearson correlation between the system's answer"", 'and gold standard similarity scores is used as the evaluation metric. we include three baselines lsa, lda and  #TAUTHOR_TAG. we run gibbs sampling based lda', '']",3
"['google books, a new paradigm has been added to this research area, whereby the prime interest is in identifying the temporal scope of a sense [ 10, 14,  #TAUTHOR_TAG 25 ] which, in turn, can give further']","['google books, a new paradigm has been added to this research area, whereby the prime interest is in identifying the temporal scope of a sense [ 10, 14,  #TAUTHOR_TAG 25 ] which, in turn, can give further']","['google books, a new paradigm has been added to this research area, whereby the prime interest is in identifying the temporal scope of a sense [ 10, 14,  #TAUTHOR_TAG 25 ] which, in turn, can give further insights to the phenomenon of language evolution.', 'some recent attempts [ 5, 8, 11, 12, 15 ] also have been made to model the dynamics of language in']","[', with the arrival of large - scale collections of historic texts and online libraries such as google books, a new paradigm has been added to this research area, whereby the prime interest is in identifying the temporal scope of a sense [ 10, 14,  #TAUTHOR_TAG 25 ] which, in turn, can give further insights to the phenomenon of language evolution.', 'some recent attempts [ 5, 8, 11, 12, 15 ] also have been made to model the dynamics of language in terms of word senses.', ""one of the studies in this area has been presented by mitra et al. [ 19 ] where the authors show that at earlier times, the sense of the word'sick'was mostly associated to some form of illness ; however, over the years, a new sense associating the same word to something that is'cool'or'crazy'has emerged."", 'their study is based on a unique network representation of the corpus called a distributional thesauri ( dt ) network built using google books syntactic n - grams.', 'they have used unsupervised clustering techniques to induce a sense of a word and then compared the induced senses of two time periods to get the new sense for a particular target word']",0
,,,,0
"['.  #TAUTHOR_TAG over the same google books corpus 1, apply topic modeling']","['al.  #TAUTHOR_TAG over the same google books corpus 1, apply topic modeling']","['.  #TAUTHOR_TAG over the same google books corpus 1, apply topic modeling']","['mitra et al. [ 19 ] reported a precision close to 0. 6 over a random sample of 49 words, we take another random sample of 100 words separately and repeat manual evaluation.', 'when we extract the novel senses by comparing the dts from 1909 - 1953 and 2002 - 2005, the precision obtained for these 100 words is as low as 0. 32.', 'similarly if we extract the novel senses comparing the dts of 1909 - 1953 with 2006 - 2008, the precision stands at 0. 23.', 'we then explore another unsupervised approach presented in lau et al.  #TAUTHOR_TAG over the same google books corpus 1, apply topic modeling for sense induction and directly adapt their similarity measure to get the new senses.', 'using a set intersecting with the 100 random samples for mitra et al. [ 19 ], we obtain the precision values of 0. 21 and 0. 28, respectively.', 'clearly, none of the precision values are good enough for reliable novel sense detection.', 'this motivates us to devise a new approach to improve the precision of the existing approaches.', 'further, being inspired by the recent works of applying complex network theory in nlp applications like co - hyponymy detection [ 13 ], evaluating machine generated summaries [ 20 ], detection of ambiguity in a text [ 4 ], etc. we opt for a solution using complex network measures']",5
,,,,5
,,,,5
,,,,5
,,,,5
,,,,5
,,,,5
"['shows evidence to the contrary  #TAUTHOR_TAG.', 'in this paper, we focus on improving']","['shows evidence to the contrary  #TAUTHOR_TAG.', 'in this paper, we focus on improving']","['- occurrence matrices, recent work shows evidence to the contrary  #TAUTHOR_TAG.', 'in this paper, we focus on improving a state - ofthe - art counting model, lexvec  #TAUTHOR_TAG, which']","['word representations have become a mainstay in natural language processing, enjoying a slew of applications  #AUTHOR_TAG.', ' #AUTHOR_TAG suggested that predictive models which use neural networks to generate the distributed word representations ( also known as embeddings in this context ) outperform counting models which work on co - occurrence matrices, recent work shows evidence to the contrary  #TAUTHOR_TAG.', 'in this paper, we focus on improving a state - ofthe - art counting model, lexvec  #TAUTHOR_TAG, which performs factorization of the positive pointwise mutual information ( ppmi ) matrix using window sampling and negative sampling ( wsns ).', ' #AUTHOR_TAG suggest that lexvec matches and often outperforms competing models in word similarity and semantic analogy tasks.', 'here we show that using positional contexts to approximate syntactic dependencies yields state - of - the - art performance on syntactic analogy tasks as well.', 'we also show how it is possible to approximate wsns using aggregate data, eliminating random access to the ppmi matrix, enabling the use of external memory.', 'though not undertaken in this paper, this modification effectively allows lexvec to be trained on web - scale corpora.', 'this paper is organized as follows : we review the lexvec model ( § 2 ) and detail how positional contexts and external memory can be incorporated into the model ( § 3 ).', '']",1
"['suggested by  #AUTHOR_TAG and  #TAUTHOR_TAG, positional contexts ( introduced in  #AUTHOR_TAG ) are a potential']","['suggested by  #AUTHOR_TAG and  #TAUTHOR_TAG, positional contexts ( introduced in  #AUTHOR_TAG ) are a potential']","['suggested by  #AUTHOR_TAG and  #TAUTHOR_TAG, positional contexts ( introduced in  #AUTHOR_TAG ) are a potential solution to poor performance on syntactic analogy tasks.', 'rather than only accounting']","['suggested by  #AUTHOR_TAG and  #TAUTHOR_TAG, positional contexts ( introduced in  #AUTHOR_TAG ) are a potential solution to poor performance on syntactic analogy tasks.', 'rather than only accounting for which context words appear around a target word, positional contexts also account for their position relative to the target word.', 'for example, in the sentence "" the big dog barked loudly "", target word dog has contexts ( the −2, big −1, barked 1, loudly 2 ).', 'the cooccurrence matrix, before having dimensions | v | × | v |, takes on dimensions | v | × 2 * win * | v | when using positional contexts.', 'this can be incorporated into lexvec with two minor modifications : 1 ) the context embeddingw takes on dimensions 2 * win * | v | × d, 2 ) negative sampling must now sample positional contexts rather than simple contexts.', 'this latter point requires that the distribution from which negative samples are drawn become', 'without positional contexts, either w or w + w can be used as embeddings.', 'since positional contexts make the dimensions of both matrices incompatible, w cannot be used directly.', 'we propose using the sum of all positional context vectors as the context vector for a word ( w pos )']",1
"['shows evidence to the contrary  #TAUTHOR_TAG.', 'in this paper, we focus on improving']","['shows evidence to the contrary  #TAUTHOR_TAG.', 'in this paper, we focus on improving']","['- occurrence matrices, recent work shows evidence to the contrary  #TAUTHOR_TAG.', 'in this paper, we focus on improving a state - ofthe - art counting model, lexvec  #TAUTHOR_TAG, which']","['word representations have become a mainstay in natural language processing, enjoying a slew of applications  #AUTHOR_TAG.', ' #AUTHOR_TAG suggested that predictive models which use neural networks to generate the distributed word representations ( also known as embeddings in this context ) outperform counting models which work on co - occurrence matrices, recent work shows evidence to the contrary  #TAUTHOR_TAG.', 'in this paper, we focus on improving a state - ofthe - art counting model, lexvec  #TAUTHOR_TAG, which performs factorization of the positive pointwise mutual information ( ppmi ) matrix using window sampling and negative sampling ( wsns ).', ' #AUTHOR_TAG suggest that lexvec matches and often outperforms competing models in word similarity and semantic analogy tasks.', 'here we show that using positional contexts to approximate syntactic dependencies yields state - of - the - art performance on syntactic analogy tasks as well.', 'we also show how it is possible to approximate wsns using aggregate data, eliminating random access to the ppmi matrix, enabling the use of external memory.', 'though not undertaken in this paper, this modification effectively allows lexvec to be trained on web - scale corpora.', 'this paper is organized as follows : we review the lexvec model ( § 2 ) and detail how positional contexts and external memory can be incorporated into the model ( § 3 ).', '']",5
"['4  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'since the latter is more computationally efficient']","[' #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'since the latter is more computationally efficient']","['drawing negative samples, chosen to be', 'with α = 3 / 4  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'since the latter is more computationally efficient']","['##vec uses wsns to factorize the ppmi matrix into two lower rank matrices.', 'the co - occurrence matrix m is calculated from word - context pairs ( w, c ) obtained by sliding a symmetric window of size win over the training corpus c. the ppmi matrix is then calculated as follows', 'where * represents index summation.', 'the word and context embeddings w andw, with dimensions | v | × d ( where v is the vocabulary and d the embedding dimension ), are obtained by the minimization via stochastic gradient descent ( sgd ) of a combination of the loss functions', 'using wsns.', 'p n is the distribution used for drawing negative samples, chosen to be', 'with α = 3 / 4  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'since the latter is more computationally efficient and yields equivalent results, we adopt it in this paper.', 'the stochastic method extends the context window with noise words drawn using negative sampling according to eq. ( 4 ).', 'the key idea is that window sampling is likely to sample related words, approximating their vectors using eq. ( 2 ), while negative sampling is likely to select unrelated words, scattering their vectors using eq. ( 3 ).', 'the resulting global loss, where', 'given a word w and context word c, eq. ( 5 ) is proportional to # ( w ) and # ( c ).', 'this is the desired behaviour for the global loss function, since the more frequent w or c are in the corpus, the more confident we can be about the corpus estimated p p m i wc.', 'suppose both # ( w ) and # ( c ) are high, but p p m i wc is low.', 'this is unequivocal evidence of negative correlation between them, and so we should put more effort into approximating their p p m i. the argument is analogous for high p p m i. if on the other hand # ( w ) and # ( c ) are low, we cannot be too confident about the corpus estimated p p m i wc, and so less effort should be spent on its approximation']",5
['report results from  #TAUTHOR_TAG and use'],['report results from  #TAUTHOR_TAG and use'],['report results from  #TAUTHOR_TAG and use the same training corpus and parameters'],"['report results from  #TAUTHOR_TAG and use the same training corpus and parameters to train lexvec with positional contexts and external memory.', 'the corpus is a wikipedia dump from june 2015, tokenized, lowercased, and split into sentences, removing punctuation and converting numbers to words, for a final vocabulary of 302, 203 words.', 'all generated embeddings have dimensionality equal to 300.', 'as recommended in  #AUTHOR_TAG and used in  #TAUTHOR_TAG, the ppmi matrix used in all lexvec models and in ppmi - svd is transformed using context distribution smoothing exponentiating context frequencies to the power 0. 75.', '']",5
['report results from  #TAUTHOR_TAG and use'],['report results from  #TAUTHOR_TAG and use'],['report results from  #TAUTHOR_TAG and use the same training corpus and parameters'],"['report results from  #TAUTHOR_TAG and use the same training corpus and parameters to train lexvec with positional contexts and external memory.', 'the corpus is a wikipedia dump from june 2015, tokenized, lowercased, and split into sentences, removing punctuation and converting numbers to words, for a final vocabulary of 302, 203 words.', 'all generated embeddings have dimensionality equal to 300.', 'as recommended in  #AUTHOR_TAG and used in  #TAUTHOR_TAG, the ppmi matrix used in all lexvec models and in ppmi - svd is transformed using context distribution smoothing exponentiating context frequencies to the power 0. 75.', '']",5
['report results from  #TAUTHOR_TAG and use'],['report results from  #TAUTHOR_TAG and use'],['report results from  #TAUTHOR_TAG and use the same training corpus and parameters'],"['report results from  #TAUTHOR_TAG and use the same training corpus and parameters to train lexvec with positional contexts and external memory.', 'the corpus is a wikipedia dump from june 2015, tokenized, lowercased, and split into sentences, removing punctuation and converting numbers to words, for a final vocabulary of 302, 203 words.', 'all generated embeddings have dimensionality equal to 300.', 'as recommended in  #AUTHOR_TAG and used in  #TAUTHOR_TAG, the ppmi matrix used in all lexvec models and in ppmi - svd is transformed using context distribution smoothing exponentiating context frequencies to the power 0. 75.', '']",5
"['suggested by  #AUTHOR_TAG and  #TAUTHOR_TAG, positional contexts ( introduced in  #AUTHOR_TAG ) are a potential']","['suggested by  #AUTHOR_TAG and  #TAUTHOR_TAG, positional contexts ( introduced in  #AUTHOR_TAG ) are a potential']","['suggested by  #AUTHOR_TAG and  #TAUTHOR_TAG, positional contexts ( introduced in  #AUTHOR_TAG ) are a potential solution to poor performance on syntactic analogy tasks.', 'rather than only accounting']","['suggested by  #AUTHOR_TAG and  #TAUTHOR_TAG, positional contexts ( introduced in  #AUTHOR_TAG ) are a potential solution to poor performance on syntactic analogy tasks.', 'rather than only accounting for which context words appear around a target word, positional contexts also account for their position relative to the target word.', 'for example, in the sentence "" the big dog barked loudly "", target word dog has contexts ( the −2, big −1, barked 1, loudly 2 ).', 'the cooccurrence matrix, before having dimensions | v | × | v |, takes on dimensions | v | × 2 * win * | v | when using positional contexts.', 'this can be incorporated into lexvec with two minor modifications : 1 ) the context embeddingw takes on dimensions 2 * win * | v | × d, 2 ) negative sampling must now sample positional contexts rather than simple contexts.', 'this latter point requires that the distribution from which negative samples are drawn become', 'without positional contexts, either w or w + w can be used as embeddings.', 'since positional contexts make the dimensions of both matrices incompatible, w cannot be used directly.', 'we propose using the sum of all positional context vectors as the context vector for a word ( w pos )']",0
"['', 'another different context type is dependency - based word embedding  #TAUTHOR_TAG 12, 13 ], which considers syntactic contexts']","['side of the target words.', 'another different context type is dependency - based word embedding  #TAUTHOR_TAG 12, 13 ], which considers syntactic contexts']","['to an efficient and user - friendly implementation that learns high quality word embeddings from very large corpora.', 'the word2vec learns low dimensional continuous vector representations for words by considering window - based contexts, i. e., context words within some fixed distance of each side of the target words.', 'another different context type is dependency - based word embedding  #TAUTHOR_TAG 12, 13 ], which considers syntactic contexts']","['', 'learning knowledge from analyzing large - scaled unlabeled data is compulsory and proved useful in the previous works [ 4, 5, 6 ].', 'how to extract useful information from unannotated large scale corpus has been a research issue.', 'word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of nlp tasks [ 7, 8, 9 ].', 'the word2vec [ 10 ] is among the most widely used word embedding models today.', 'their success is largely due to an efficient and user - friendly implementation that learns high quality word embeddings from very large corpora.', 'the word2vec learns low dimensional continuous vector representations for words by considering window - based contexts, i. e., context words within some fixed distance of each side of the target words.', 'another different context type is dependency - based word embedding  #TAUTHOR_TAG 12, 13 ], which considers syntactic contexts']",0
"['embedding  #TAUTHOR_TAG 12, 13 ],']","['embedding  #TAUTHOR_TAG 12, 13 ],']","['each side of the target words.', 'another different context type is dependency - based word embedding  #TAUTHOR_TAG 12, 13 ],']","['from analyzing large - scaled unlabeled data is compulsory and proved useful in the previous works [ 4, 5, 6 ].', 'how to extract useful information from unannotated large scale corpus has been a research issue.', 'word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of nlp tasks [ 7, 8, 9 ].', 'the word2vec [ 10 ] is among the most widely used word embedding models today.', 'their success is largely due to an efficient and user - friendly implementation that learns high quality word embeddings from very large corpora.', 'the word2vec learns low dimensional continuous vector representations for words by considering window - based contexts, i. e., context words within some fixed distance of each side of the target words.', 'another different context type is dependency - based word embedding  #TAUTHOR_TAG 12, 13 ], which considers syntactic contexts rather', 'the 2016 conference on computational linguistics and speech processing rocling 2016, pp.', '100 - 102 the association for computational linguistics and chinese language processing 100 than window contexts in word2vec.', 'bansal et al. [ 8 ] and  #TAUTHOR_TAG show the benefits of such modified - context embeddings in dependency parsing task.', 'the dependency - based word embedding can relieve the problem of data sparseness, since even without occurrence of dependency word pairs in a corpus, dependency scores can be still calculated by word embeddings [ 12 ].', 'in this paper, we proposed a rescoring approach for parsing, based on a combination of original parsing scores and dependency word embedding scores to assist the determination of the best parse tree among the n - best parse trees.', 'there are three main steps in our rescoring approach.', 'the first step is to have the parser to produce n - best parse trees with their structural scores.', 'for each parsed tree including words, part - of - speech ( pos ) and semantic role labels.', 'second, we extract word - to - word associations ( or called word dependency, a dependency implies its close association with other words in either syntactic or semantic perspective ) from large amounts of auto - parsed data and adopt word2vecf [ 13 ] to train dependency - based word embeddings.', 'the last step is to build a structural rescoring method to find the best tree structure from the n - best candidates.', 'we conduct experiments on the standard data sets of the chinese treebank.', 'we also study how different types of embeddings influence on rescoring, including word, word with semantic role labels, and word senses ( concepts ).', 'experimental results show that using semantic role']",0
"['embedding  #TAUTHOR_TAG 12, 13 ],']","['embedding  #TAUTHOR_TAG 12, 13 ],']","['each side of the target words.', 'another different context type is dependency - based word embedding  #TAUTHOR_TAG 12, 13 ],']","['from analyzing large - scaled unlabeled data is compulsory and proved useful in the previous works [ 4, 5, 6 ].', 'how to extract useful information from unannotated large scale corpus has been a research issue.', 'word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of nlp tasks [ 7, 8, 9 ].', 'the word2vec [ 10 ] is among the most widely used word embedding models today.', 'their success is largely due to an efficient and user - friendly implementation that learns high quality word embeddings from very large corpora.', 'the word2vec learns low dimensional continuous vector representations for words by considering window - based contexts, i. e., context words within some fixed distance of each side of the target words.', 'another different context type is dependency - based word embedding  #TAUTHOR_TAG 12, 13 ], which considers syntactic contexts rather', 'the 2016 conference on computational linguistics and speech processing rocling 2016, pp.', '100 - 102 the association for computational linguistics and chinese language processing 100 than window contexts in word2vec.', 'bansal et al. [ 8 ] and  #TAUTHOR_TAG show the benefits of such modified - context embeddings in dependency parsing task.', 'the dependency - based word embedding can relieve the problem of data sparseness, since even without occurrence of dependency word pairs in a corpus, dependency scores can be still calculated by word embeddings [ 12 ].', 'in this paper, we proposed a rescoring approach for parsing, based on a combination of original parsing scores and dependency word embedding scores to assist the determination of the best parse tree among the n - best parse trees.', 'there are three main steps in our rescoring approach.', 'the first step is to have the parser to produce n - best parse trees with their structural scores.', 'for each parsed tree including words, part - of - speech ( pos ) and semantic role labels.', 'second, we extract word - to - word associations ( or called word dependency, a dependency implies its close association with other words in either syntactic or semantic perspective ) from large amounts of auto - parsed data and adopt word2vecf [ 13 ] to train dependency - based word embeddings.', 'the last step is to build a structural rescoring method to find the best tree structure from the n - best candidates.', 'we conduct experiments on the standard data sets of the chinese treebank.', 'we also study how different types of embeddings influence on rescoring, including word, word with semantic role labels, and word senses ( concepts ).', 'experimental results show that using semantic role']",0
"['selfexplaining method  #TAUTHOR_TAG, as shown in figure 1']","['work leverages the attention - based selfexplaining method  #TAUTHOR_TAG, as shown in figure 1']","['work leverages the attention - based selfexplaining method  #TAUTHOR_TAG, as shown in figure 1']","['work leverages the attention - based selfexplaining method  #TAUTHOR_TAG, as shown in figure 1.', 'first, our text encoder ( § 3 ) formulates an input text into a list of basic units, learning a vector representation for each, where the basic units can be words, phrases, or arbitrary ngrams.', 'then, the attention mechanism is leveraged over all basic units, and sums up all unit representations based on the attention weights { α 1,..., α n }.', 'eventually, the attention weight α i will be used to reveal how important a basic unit h i is.', 'the last prediction layer takes the fixed - length text representation t as input, and makes the final prediction']",5
"[' #TAUTHOR_TAG ( bilstm ),']","[' #TAUTHOR_TAG ( bilstm ),']","[' #TAUTHOR_TAG ( bilstm ),']","['compare two types of baseline text encoders in figure 1. ( 1 )  #TAUTHOR_TAG ( bilstm ), which formulates single word positions as basic units, and computes the vector h i for the i - th word position with a bilstm ; ( 2 ) extension of  #AUTHOR_TAG ( cnn ).', 'the original model in  #AUTHOR_TAG only utilizes 4 - grams.', 'here we extend this model to take all unigrams, bigrams, and up to n - grams as the basic units.', 'for a fair comparison, both our approach and the baselines share the same architecture, and the only difference is the text encoder used.', 'volving seven pathogens, discusses pitfalls of routine blood cultures and examines the role of the laboratory in microbiologic diagnosis.', 'structures for a sentence w 1 w 2 w 3 w 4, where each node corresponds to a phrase or ngram']",5
"['was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims']","['was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims.', '']","['was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims.', 'table 1 shows three instances from the data set with the claim, the evidence and']","['', 'the dataset for the shared task was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims.', 'table 1 shows three instances from the data set with the claim, the evidence and the verdict.', '']",0
"['was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims']","['was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims.', '']","['was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims.', 'table 1 shows three instances from the data set with the claim, the evidence and']","['', 'the dataset for the shared task was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims.', 'table 1 shows three instances from the data set with the claim, the evidence and the verdict.', '']",0
"['was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims']","['was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims.', '']","['was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims.', 'table 1 shows three instances from the data set with the claim, the evidence and']","['', 'the dataset for the shared task was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims.', 'table 1 shows three instances from the data set with the claim, the evidence and the verdict.', '']",0
"['was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims']","['was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims.', '']","['was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims.', 'table 1 shows three instances from the data set with the claim, the evidence and']","['', 'the dataset for the shared task was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims.', 'table 1 shows three instances from the data set with the claim, the evidence and the verdict.', '']",0
"['was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims']","['was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims.', '']","['was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims.', 'table 1 shows three instances from the data set with the claim, the evidence and']","['', 'the dataset for the shared task was introduced by  #TAUTHOR_TAG and consists of 185, 445 claims.', 'table 1 shows three instances from the data set with the claim, the evidence and the verdict.', '']",0
['baseline  #TAUTHOR_TAG of'],['baseline  #TAUTHOR_TAG of'],['( combined ) achieved a high coverage 94. 4 % compared to the baseline  #TAUTHOR_TAG of'],"['', 'table 2 shows the percentage of claims that can be fully supported or refuted by the retrieved docu - ments before sentence selection on the dev set.', 'we see that our best approach ( combined ) achieved a high coverage 94. 4 % compared to the baseline  #TAUTHOR_TAG of 55. 3 %.', 'because we do not have the gold evidences for the blind test set we cannot report']",4
"['with binning as proposed by  #TAUTHOR_TAG.', '']","[' #AUTHOR_TAG to select sentences using bigram tf - idf with binning as proposed by  #TAUTHOR_TAG.', '']","['with binning as proposed by  #TAUTHOR_TAG.', '']","['sentence selection, we used the modified document retrieval component of drqa  #AUTHOR_TAG to select sentences using bigram tf - idf with binning as proposed by  #TAUTHOR_TAG.', '']",4
"['', 'unlike  #TAUTHOR_TAG,']","['', 'unlike  #TAUTHOR_TAG,']","['', 'unlike  #TAUTHOR_TAG,']","['', 'unlike  #TAUTHOR_TAG, we did not concatenate evidences, but trained our model for each claim - evidence pair.', '']",4
"['with binning as proposed by  #TAUTHOR_TAG.', '']","[' #AUTHOR_TAG to select sentences using bigram tf - idf with binning as proposed by  #TAUTHOR_TAG.', '']","['with binning as proposed by  #TAUTHOR_TAG.', '']","['sentence selection, we used the modified document retrieval component of drqa  #AUTHOR_TAG to select sentences using bigram tf - idf with binning as proposed by  #TAUTHOR_TAG.', '']",3
"['with binning as proposed by  #TAUTHOR_TAG.', '']","[' #AUTHOR_TAG to select sentences using bigram tf - idf with binning as proposed by  #TAUTHOR_TAG.', '']","['with binning as proposed by  #TAUTHOR_TAG.', '']","['sentence selection, we used the modified document retrieval component of drqa  #AUTHOR_TAG to select sentences using bigram tf - idf with binning as proposed by  #TAUTHOR_TAG.', '']",5
"['of mt  #TAUTHOR_TAG, to assess']","['of mt  #TAUTHOR_TAG, to assess']","['of mt  #TAUTHOR_TAG, to assess']","['metrics, such as bleu  #AUTHOR_TAG, are widely used in machine translation ( mt ) as a substitute for human evaluation.', 'such metrics commonly take the form of an automatic comparison of mt output text with one or more human reference translations.', 'small differences in automatic metric scores can be difficult to interpret, however, and statistical significance testing provides a way of estimating the likelihood that a score difference has occurred simply by chance.', 'for several metrics, such as bleu, standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the use of randomized methods.', 'bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of mt  #TAUTHOR_TAG, to assess for a pair of systems how likely a difference in bleu scores occurred by chance.', 'empirical tests detailed in  #TAUTHOR_TAG show that even for test sets as small as 300 translations, bleu confidence intervals can be computed as accurately as if they had been computed on a test set 100 times as large.', 'approximate randomization was subsequently proposed as an alternate to bootstrap resampling  #AUTHOR_TAG.', 'theoretically speaking, approximate randomization has an advantage over bootstrap resampling, in that it does not make the assumption that samples are representative of the populations from which they are drawn.', '']",0
"['of mt  #TAUTHOR_TAG, to assess']","['of mt  #TAUTHOR_TAG, to assess']","['of mt  #TAUTHOR_TAG, to assess']","['metrics, such as bleu  #AUTHOR_TAG, are widely used in machine translation ( mt ) as a substitute for human evaluation.', 'such metrics commonly take the form of an automatic comparison of mt output text with one or more human reference translations.', 'small differences in automatic metric scores can be difficult to interpret, however, and statistical significance testing provides a way of estimating the likelihood that a score difference has occurred simply by chance.', 'for several metrics, such as bleu, standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the use of randomized methods.', 'bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of mt  #TAUTHOR_TAG, to assess for a pair of systems how likely a difference in bleu scores occurred by chance.', 'empirical tests detailed in  #TAUTHOR_TAG show that even for test sets as small as 300 translations, bleu confidence intervals can be computed as accurately as if they had been computed on a test set 100 times as large.', 'approximate randomization was subsequently proposed as an alternate to bootstrap resampling  #AUTHOR_TAG.', 'theoretically speaking, approximate randomization has an advantage over bootstrap resampling, in that it does not make the assumption that samples are representative of the populations from which they are drawn.', '']",0
"['##ped test set  #TAUTHOR_TAG, constructed by sampling with replacement from the original test set translations.', 'the bootstrap']","['bootstrapped test set  #TAUTHOR_TAG, constructed by sampling with replacement from the original test set translations.', 'the bootstrap']","['set  #TAUTHOR_TAG, constructed by sampling with replacement from the original test set translations.', 'the bootstrap distribution s boot of the test statistic is estimated by calculating the value of the pseudo - statistic the null hypothesis distribution s h 0 can be estimated from s boot by applying the shift method  #AUTHOR_TAG, which assumes']","['resampling provides a way of estimating the population distribution by sampling with replacement from a representative sample  #AUTHOR_TAG.', 'the test statistic is taken as the difference in scores of the two systems, s x − s y, which has an expected value of 0 under the null hypothesis that the two systems perform equally well.', 'a bootstrap pseudo - sample consists of the translations by the two systems ( x b, y b ) of a bootstrapped test set  #TAUTHOR_TAG, constructed by sampling with replacement from the original test set translations.', 'the bootstrap distribution s boot of the test statistic is estimated by calculating the value of the pseudo - statistic the null hypothesis distribution s h 0 can be estimated from s boot by applying the shift method  #AUTHOR_TAG, which assumes that s h 0 has the same shape but a different mean than s boot.', 'thus, s boot is transformed into s h 0 by subtracting the mean bootstrap statistic from every value in s boot.', 'once this shift - to - zero has taken place, the null hypothesis is rejected if the probability of observing a more extreme value than the actual statistic is lower than a predetermined p - value α, which is typically set to 0. 05.', 'in other words, the score difference is significant at level 1 − α.', 'figure 3 provides a one - sided implementation of bootstrap resampling, where h 0 is that the score of system x is less than or equal to the score of figure 5 includes a typical example of bootstrap resampling applied to bleu, for a pair of systems for which differences in scores are significant, while figure 6 shows the same for me - teor but for a pair of systems with no significant difference in scores']",0
"['also paired bootstrap resampling  #TAUTHOR_TAG, which is widely used in mt evaluation.', 'we']","['also paired bootstrap resampling  #TAUTHOR_TAG, which is widely used in mt evaluation.', 'we']","['also paired bootstrap resampling  #TAUTHOR_TAG, which is widely used in mt evaluation.', '']","['', 'we compare this method with approximate randomization and also paired bootstrap resampling  #TAUTHOR_TAG, which is widely used in mt evaluation.', 'we carry out evaluation over a range of mt systems, not only including pairs of systems that perform equally well, but also pairs of systems for which one system performs marginally better than the other.', '']",4
"['also paired bootstrap resampling  #TAUTHOR_TAG, which is widely used in mt evaluation.', 'we']","['also paired bootstrap resampling  #TAUTHOR_TAG, which is widely used in mt evaluation.', 'we']","['also paired bootstrap resampling  #TAUTHOR_TAG, which is widely used in mt evaluation.', '']","['', 'we compare this method with approximate randomization and also paired bootstrap resampling  #TAUTHOR_TAG, which is widely used in mt evaluation.', 'we carry out evaluation over a range of mt systems, not only including pairs of systems that perform equally well, but also pairs of systems for which one system performs marginally better than the other.', '']",1
"['##ped test set  #TAUTHOR_TAG, constructed by sampling with replacement from the original test set translations.', 'the bootstrap']","['bootstrapped test set  #TAUTHOR_TAG, constructed by sampling with replacement from the original test set translations.', 'the bootstrap']","['set  #TAUTHOR_TAG, constructed by sampling with replacement from the original test set translations.', 'the bootstrap distribution s boot of the test statistic is estimated by calculating the value of the pseudo - statistic the null hypothesis distribution s h 0 can be estimated from s boot by applying the shift method  #AUTHOR_TAG, which assumes']","['resampling provides a way of estimating the population distribution by sampling with replacement from a representative sample  #AUTHOR_TAG.', 'the test statistic is taken as the difference in scores of the two systems, s x − s y, which has an expected value of 0 under the null hypothesis that the two systems perform equally well.', 'a bootstrap pseudo - sample consists of the translations by the two systems ( x b, y b ) of a bootstrapped test set  #TAUTHOR_TAG, constructed by sampling with replacement from the original test set translations.', 'the bootstrap distribution s boot of the test statistic is estimated by calculating the value of the pseudo - statistic the null hypothesis distribution s h 0 can be estimated from s boot by applying the shift method  #AUTHOR_TAG, which assumes that s h 0 has the same shape but a different mean than s boot.', 'thus, s boot is transformed into s h 0 by subtracting the mean bootstrap statistic from every value in s boot.', 'once this shift - to - zero has taken place, the null hypothesis is rejected if the probability of observing a more extreme value than the actual statistic is lower than a predetermined p - value α, which is typically set to 0. 05.', 'in other words, the score difference is significant at level 1 − α.', 'figure 3 provides a one - sided implementation of bootstrap resampling, where h 0 is that the score of system x is less than or equal to the score of figure 5 includes a typical example of bootstrap resampling applied to bleu, for a pair of systems for which differences in scores are significant, while figure 6 shows the same for me - teor but for a pair of systems with no significant difference in scores']",5
"['bootstrap resampling  #TAUTHOR_TAG is shown in figure 4.', 'unlike']","['bootstrap resampling  #TAUTHOR_TAG is shown in figure 4.', 'unlike']","['bootstrap resampling  #TAUTHOR_TAG is shown in figure 4.', 'unlike']","['bootstrap resampling  #TAUTHOR_TAG is shown in figure 4.', 'unlike the other two randomized tests, this method makes no attempt to simulate the null hypothesis distribution.', 'instead, bootstrap samples are used to estimate confidence intervals of score differences, with confidence intervals not containing 0 implying a statistically significant difference.', 'we compare what takes place with the two other tests, by plotting differences in scores for bootstrapped samples']",7
['paired bootstrap resampling  #TAUTHOR_TAG and bootstrap resampling as shown in'],['paired bootstrap resampling  #TAUTHOR_TAG and bootstrap resampling as shown in'],"['the randomized methods of statistical significance testing.', 'we evaluate paired bootstrap resampling  #TAUTHOR_TAG and bootstrap resampling as shown in figure 3 and approximate randomization as shown in figure 2,']","['', 'only when a tie in adequacy scores occurs are fluency judgments used to break the tie.', 'in this case, p - values from significance tests applied to fluency scores of that system pair are used.', 'for example, in figure 7, adequacy scores of system b are not significantly greater than those of systems c, d and e, while fluency scores for system b are significantly greater than those of the three other systems.', 'the combined result for each pair of systems is therefore taken as the p - value from the corresponding fluency significance test.', 'we use the combined human evaluation pairwise significant tests as a gold standard against which to evaluate the randomized methods of statistical significance testing.', 'we evaluate paired bootstrap resampling  #TAUTHOR_TAG and bootstrap resampling as shown in figure 3 and approximate randomization as shown in figure 2, each in combination with four automatic mt metrics : bleu  #AUTHOR_TAG, nist ( nist, 2002 ), meteor  #AUTHOR_TAG and ter  #AUTHOR_TAG.', 'figure 8 shows the outcome of pairwise randomized significance tests']",7
"['- text tasks including simplification  #TAUTHOR_TAG, paraphrasing']","['to monolingual text - to - text tasks including simplification  #TAUTHOR_TAG, paraphrasing']","['to monolingual text - to - text tasks including simplification  #TAUTHOR_TAG, paraphrasing']","['from its application to machine translation, the encoder - decoder or sequence - to - sequence ( seq2seq ) paradigm has been successfully applied to monolingual text - to - text tasks including simplification  #TAUTHOR_TAG, paraphrasing  #AUTHOR_TAG, style transfer  #AUTHOR_TAG, sarcasm interpretation  #AUTHOR_TAG, automated lyric annotation  #AUTHOR_TAG and dialogue systems  #AUTHOR_TAG.', '']",0
"[' #TAUTHOR_TAG.', 'the training data includes 2']","[' #TAUTHOR_TAG.', 'the training data includes 2, 000 development and 359 test instances created by  #AUTHOR_TAG.', 'these are complex sentences paired with simplifications provided by amazon mechanical turk workers and provide a more reliable evaluation of the task']","['##2 automatically aligned complex and simple sentences from the ordinary and simple english wikipedia corpora, used extensively in previous work  #TAUTHOR_TAG.', 'the training data includes 2']","['our model is essentially task - agnostic, we demonstrate prior attention for the task of sentence simplification.', 'the goal of sentence simplification is to reduce the linguistic complexity of text, while still retaining its original information and meaning.', 'it has been suggested that sentence simplification can be defined by three major types of operations : splitting, deletion, and paraphrasing  #AUTHOR_TAG.', 'we hypothesize that these operations occur at varying frequencies in the training data.', 'we adopt our model in an attempt to capture these operations into attention matrices and the latent vector space, and thus control the form and degree of simplification through sampling from that space.', 'we train on the wikilarge collection used by  #AUTHOR_TAG.', 'wikilarge is a collection of 296, 402 automatically aligned complex and simple sentences from the ordinary and simple english wikipedia corpora, used extensively in previous work  #TAUTHOR_TAG.', 'the training data includes 2, 000 development and 359 test instances created by  #AUTHOR_TAG.', 'these are complex sentences paired with simplifications provided by amazon mechanical turk workers and provide a more reliable evaluation of the task']",0
"['scores.', 'tecture as  #AUTHOR_TAG and  #TAUTHOR_TAG : 2']","['bleu scores.', 'tecture as  #AUTHOR_TAG and  #TAUTHOR_TAG : 2']","['bleu scores.', 'tecture as  #AUTHOR_TAG and  #TAUTHOR_TAG : 2 layers of']","['extend the opennmt  #AUTHOR_TAG framework with functions for attention generation and release our code as a submodule.', 'we use a similar archi - table 2 : quantitative evaluation of existing baselines from previous work and seq2seq with prior attention from the cvae when choosing an optimal z sample for bleu scores.', 'tecture as  #AUTHOR_TAG and  #TAUTHOR_TAG : 2 layers of stacked unidirectional lstms with bi - linear global attention as proposed by  #AUTHOR_TAG, with hidden states of 512 dimensions.', 'the vocabulary is reduced to the 50, 000 most frequent tokens and embedded in a shared 500 - dimensional space.', 'we train using sgd with batches of 64 samples for 13 epochs after which the autoencoder is trained by translating sequences from training data.', 'both the encoder and decoder of the cvae comprise 2 fully connected layers of 128 nodes.', 'weights are optimized using adam  #AUTHOR_TAG.', 'we visualize and evaluate using a two - dimensional latent vector space.', 'source and target sequences are both padded or reduced to 50 tokens.', 'the integration of the cvae is analogous across the family of attention - based seq2seq models, i. e., our approach can be applied more generally with different models or training data']",5
"['on neural text simplification  #TAUTHOR_TAG.', 'in table 2, we compare our seq2seq model with attention and without prior attention.', 'a']","['on neural text simplification  #TAUTHOR_TAG.', 'in table 2, we compare our seq2seq model with attention and without prior attention.', 'a']","['on neural text simplification  #TAUTHOR_TAG.', 'in table 2, we compare our seq2seq model with attention and without prior attention.', 'a value']","['study the influence of sampling from different regions in the latent vector space, we visualize the resulting attention matrices and measure simplification quality using automated metrics.', 'figure 2a shows the two - dimensional latent space for a single source sentence encoding using 64 samples ranging from values −2 to 2.', 'next to the target - to - source length ratio, we apply automated measures commonly used to evaluate simplification systems  #AUTHOR_TAG : bleu, sari  #AUTHOR_TAG, fkgl 1  #AUTHOR_TAG.', 'automated evaluation metrics for matrices originating from samples from different regions of latent codes are shown in figure 2b.', 'inclusion of an attention mechanism was instrumental to match existing baselines.', 'our standard seq2seq model with attention, without prior attention, obtains a score of 89. 92 bleu points, which is close to scores obtained by similar models used in existing 1 fleish - kincaid grade level index.', 'work on neural text simplification  #TAUTHOR_TAG.', 'in table 2, we compare our seq2seq model with attention and without prior attention.', 'a value for bleu of 90. 14 is found for z = [ −2, 0 ] which was tuned on a development set.', '']",5
"['on neural text simplification  #TAUTHOR_TAG.', 'in table 2, we compare our seq2seq model with attention and without prior attention.', 'a']","['on neural text simplification  #TAUTHOR_TAG.', 'in table 2, we compare our seq2seq model with attention and without prior attention.', 'a']","['on neural text simplification  #TAUTHOR_TAG.', 'in table 2, we compare our seq2seq model with attention and without prior attention.', 'a value']","['study the influence of sampling from different regions in the latent vector space, we visualize the resulting attention matrices and measure simplification quality using automated metrics.', 'figure 2a shows the two - dimensional latent space for a single source sentence encoding using 64 samples ranging from values −2 to 2.', 'next to the target - to - source length ratio, we apply automated measures commonly used to evaluate simplification systems  #AUTHOR_TAG : bleu, sari  #AUTHOR_TAG, fkgl 1  #AUTHOR_TAG.', 'automated evaluation metrics for matrices originating from samples from different regions of latent codes are shown in figure 2b.', 'inclusion of an attention mechanism was instrumental to match existing baselines.', 'our standard seq2seq model with attention, without prior attention, obtains a score of 89. 92 bleu points, which is close to scores obtained by similar models used in existing 1 fleish - kincaid grade level index.', 'work on neural text simplification  #TAUTHOR_TAG.', 'in table 2, we compare our seq2seq model with attention and without prior attention.', 'a value for bleu of 90. 14 is found for z = [ −2, 0 ] which was tuned on a development set.', '']",3
"[' #TAUTHOR_TAG.', 'for the other language pairs, the input material was']","['as indomain following the "" invitation model ""  #TAUTHOR_TAG.', 'for the other language pairs, the input material was 30, 000 post - edited segments.', 'the main part of the training corpora ( approximately']","['as indomain following the "" invitation model ""  #TAUTHOR_TAG.', 'for the other language pairs, the input material was']","['iadaatpa 1 project coded as n • 2016 - eu - ia - 0132 that ended in february 2019 is made for building of customized, domain - specific engines for public administrations from eu member states.', 'the consortium of the project decided to use neural machine translation at the beginning of the project.', 'this represented a challenge for all involved, and the positive aspect is that all public administrations engaged in the iadaatpa project were able to try, test and use state - of - the - art neural technology with a high level of satisfaction.', 'one of the main challenges faced by all partners was data availability.', 'although all public administrations had some data available, it was clearly insufficient for high - level customization.', 'in some cases, we had merely a few hundred words or several tens of thousand words.', 'each domain ( field ) has its own unique word distribution and neural machine translation systems are known to suffer a decrease in performance when data is out - of - domain.', 'pangeanic is a language service provider ( lsp ) specialised in natural language processing and machine translation.', 'it provides solutions to cognitive companies, institutions, translation professionals, and corporations.', 'the problem faced by the iadaatpa project at pangeanic was twofold : data acquisition for translation from spanish to russian there was no available in - domain data.', 'therefore, 2 translators were contracted as part of the project to create 30, 000 segments of in - domain data, translating public administrations websites.', 'they also cleaned united nations material and post - edited general - domain data that was previously filtered as indomain following the "" invitation model ""  #TAUTHOR_TAG.', 'for the other language pairs, the input material was 30, 000 post - edited segments.', ""the main part of the training corpora ( approximately 75 % ) was part of pangeanic's own repository harvested through web crawling and also opensubtitles  #AUTHOR_TAG."", 'the rest of the corpus was automatically validated synthetic material using general data from leipzig  #AUTHOR_TAG engine customization the data was cleaned using the bicleaner tool ( sanchez -  #AUTHOR_TAG.', 'the data was lowercased and extra embeddings were added in order to keep the case information.', 'the tokenization used was the one provided by opennmt 3 and words were divided in subwords according to the bpe  #AUTHOR_TAG approach.', 'the models were trained with multi - domain data and we improved performance following a domainmixing approach  #AUTHOR_TAG.', 'the domain information was prepended with special tokens for each target sequence.', 'the domain prediction was based only on the']",0
"[' #TAUTHOR_TAG.', 'for the other language pairs, the input material was']","['as indomain following the "" invitation model ""  #TAUTHOR_TAG.', 'for the other language pairs, the input material was 30, 000']","['as indomain following the "" invitation model ""  #TAUTHOR_TAG.', 'for the other language pairs, the input material was']","['of the main challenges faced by all partners was data availability.', 'although all public administrations had some data available, it was clearly insufficient for high - level customization.', 'in some cases, we had merely a few hundred words or several tens of thousand words.', 'each domain ( field ) has its own unique word distribution and neural machine translation systems are known to suffer a decrease in performance when data is out - of - domain.', 'pangeanic is a language service provider ( lsp ) specialised in natural language processing and machine translation.', 'it provides solutions to cognitive companies, institutions, translation professionals, and corporations.', 'the problem faced by the iadaatpa project at pangeanic was twofold : data acquisition for translation from spanish to russian there was no available in - domain data.', 'therefore, 2 translators were contracted as part of the project to create 30, 000 segments of in - domain data, translating public administrations websites.', 'they also cleaned united nations material and post - edited general - domain data that was previously filtered as indomain following the "" invitation model ""  #TAUTHOR_TAG.', 'for the other language pairs, the input material was 30, 000 post - edited segments.', ""the main part of the training corpora ( approximately 75 % ) was part of pangeanic's own repository harvested through web crawling and also opensubtitles  #AUTHOR_TAG."", 'the rest of the corpus was automatically validated synthetic material using general data from leipzig  #AUTHOR_TAG.', 'engine customization the data was cleaned using the bicleaner tool ( sanchez -  #AUTHOR_TAG.', 'the data was lowercased and extra embeddings were added in order to keep the case information.', 'the tokenization used was the one provided by opennmt 3 and words were divided in subwords according to the bpe  #AUTHOR_TAG approach.', 'the models were trained with multi - domain data and we improved performance following a domainmixing approach  #AUTHOR_TAG.', 'the domain information was prepended with special tokens for each target sequence.', 'the domain prediction was based only on the source as the extra token was added at target - side and there was no need for apriori domain information.', 'this approach allowed the model to improve the quality for each domain']",0
"['', 'recently,  #TAUTHOR_TAG proposed a novel reconstruction - based approach']","['', 'recently,  #TAUTHOR_TAG proposed a novel reconstruction - based approach']","['', 'recently,  #TAUTHOR_TAG proposed a novel reconstruction - based approach']","['##s are frequently omitted in pro - drop languages, such as chinese, generally leading to significant challenges with respect to the production of complete translations.', 'recently,  #TAUTHOR_TAG proposed a novel reconstruction - based approach to alleviating dropped pronoun ( dp ) translation problems for neural machine translation models.', 'in this work, we improve the original model from two perspectives.', 'first, we employ a shared reconstructor to better exploit encoder and decoder representations.', 'second, we jointly learn to translate and predict dps in an end - to - end manner, to avoid the errors propagated from an external dp prediction model.', '']",0
"['dp translation  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is a pioneering']","['dp translation  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is a pioneering']","['dp translation  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is a pioneering work to model dp translation for neural machine trans - * zhaopeng tu is']","['##s are important in natural languages as they imply rich discourse information.', 'however, in pro - drop languages such as chinese and japanese, pronouns are frequently omitted when their referents can be pragmatically inferred from the context.', 'when translating sentences from a pro - drop language into a non - pro - drop language ( e. g. chinese - to - english ), translation models generally fail to translate invisible dropped pronouns ( dps ).', 'this phenomenon leads to various translation problems in terms of completeness, syntax and even semantics of translations.', 'a number of approaches have been investigated for dp translation  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is a pioneering work to model dp translation for neural machine trans - * zhaopeng tu is the corresponding author of the paper.', 'this work was conducted when longyue wang was studying and qun liu was working at the adapt centre in the school of computing at dublin city university.', 'lation ( nmt ) models.', 'they employ two separate reconstructors to respectively reconstruct encoder and decoder representations back to the dp - annotated source sentence.', 'the annotation of dp is provided by an external prediction model, which is trained on the parallel corpus using automatically learned alignment information  #AUTHOR_TAG.', '']",0
"['dp translation  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is a pioneering']","['dp translation  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is a pioneering']","['dp translation  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is a pioneering work to model dp translation for neural machine trans - * zhaopeng tu is']","['##s are important in natural languages as they imply rich discourse information.', 'however, in pro - drop languages such as chinese and japanese, pronouns are frequently omitted when their referents can be pragmatically inferred from the context.', 'when translating sentences from a pro - drop language into a non - pro - drop language ( e. g. chinese - to - english ), translation models generally fail to translate invisible dropped pronouns ( dps ).', 'this phenomenon leads to various translation problems in terms of completeness, syntax and even semantics of translations.', 'a number of approaches have been investigated for dp translation  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is a pioneering work to model dp translation for neural machine trans - * zhaopeng tu is the corresponding author of the paper.', 'this work was conducted when longyue wang was studying and qun liu was working at the adapt centre in the school of computing at dublin city university.', 'lation ( nmt ) models.', 'they employ two separate reconstructors to respectively reconstruct encoder and decoder representations back to the dp - annotated source sentence.', 'the annotation of dp is provided by an external prediction model, which is trained on the parallel corpus using automatically learned alignment information  #AUTHOR_TAG.', '']",0
"['shown in figure 1,  #TAUTHOR_TAG introduced two independent reconstruct']","['shown in figure 1,  #TAUTHOR_TAG introduced two independent reconstructors with their own parameters, which reconstruct the dpannotated source']","['shown in figure 1,  #TAUTHOR_TAG introduced two independent reconstructors with their own parameters, which reconstruct the dpannotated source sentence from the encoder']","['shown in figure 1,  #TAUTHOR_TAG introduced two independent reconstructors with their own parameters, which reconstruct the dpannotated source sentence from the encoder and decoder hidden states, respectively.', '']",0
"['shown in figure 1,  #TAUTHOR_TAG introduced two independent reconstruct']","['shown in figure 1,  #TAUTHOR_TAG introduced two independent reconstructors with their own parameters, which reconstruct the dpannotated source']","['shown in figure 1,  #TAUTHOR_TAG introduced two independent reconstructors with their own parameters, which reconstruct the dpannotated source sentence from the encoder']","['shown in figure 1,  #TAUTHOR_TAG introduced two independent reconstructors with their own parameters, which reconstruct the dpannotated source sentence from the encoder and decoder hidden states, respectively.', '']",0
"['', 'recently,  #TAUTHOR_TAG proposed a novel reconstruction - based approach']","['', 'recently,  #TAUTHOR_TAG proposed a novel reconstruction - based approach']","['', 'recently,  #TAUTHOR_TAG proposed a novel reconstruction - based approach']","['##s are frequently omitted in pro - drop languages, such as chinese, generally leading to significant challenges with respect to the production of complete translations.', 'recently,  #TAUTHOR_TAG proposed a novel reconstruction - based approach to alleviating dropped pronoun ( dp ) translation problems for neural machine translation models.', 'in this work, we improve the original model from two perspectives.', 'first, we employ a shared reconstructor to better exploit encoder and decoder representations.', 'second, we jointly learn to translate and predict dps in an end - to - end manner, to avoid the errors propagated from an external dp prediction model.', '']",5
"['work  #TAUTHOR_TAG,']","['work  #TAUTHOR_TAG,']","['by previous work  #TAUTHOR_TAG, we conducted experiments on their released chinese⇒english tv subtitle corpus.', '2 the training, validation,']","['compare our work with the results reported by previous work  #TAUTHOR_TAG, we conducted experiments on their released chinese⇒english tv subtitle corpus.', '']",5
"['work  #TAUTHOR_TAG,']","['work  #TAUTHOR_TAG,']","['by previous work  #TAUTHOR_TAG, we conducted experiments on their released chinese⇒english tv subtitle corpus.', '2 the training, validation,']","['compare our work with the results reported by previous work  #TAUTHOR_TAG, we conducted experiments on their released chinese⇒english tv subtitle corpus.', '']",5
"['best model reported in  #TAUTHOR_TAG,']","['best model reported in  #TAUTHOR_TAG,']","['( row 3 ) is the best model reported in  #TAUTHOR_TAG,']","['##s ( rows 1 - 4 ) : the three baselines ( rows 1, 2, and 4 ) differ regarding the training data used.', '"" separate - recs⇒ ( + dps ) "" ( row 3 ) is the best model reported in  #TAUTHOR_TAG, which we employed as another strong baseline.', 'the baseline trained on the dpp - annotated data ( "" baseline ( + dpps ) "", row 4 ) outperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating dps.', 'it suggests the necessity of jointly learning to translate and predict dps.', '']",5
"['', 'recently,  #TAUTHOR_TAG proposed a novel reconstruction - based approach']","['', 'recently,  #TAUTHOR_TAG proposed a novel reconstruction - based approach']","['', 'recently,  #TAUTHOR_TAG proposed a novel reconstruction - based approach']","['##s are frequently omitted in pro - drop languages, such as chinese, generally leading to significant challenges with respect to the production of complete translations.', 'recently,  #TAUTHOR_TAG proposed a novel reconstruction - based approach to alleviating dropped pronoun ( dp ) translation problems for neural machine translation models.', 'in this work, we improve the original model from two perspectives.', 'first, we employ a shared reconstructor to better exploit encoder and decoder representations.', 'second, we jointly learn to translate and predict dps in an end - to - end manner, to avoid the errors propagated from an external dp prediction model.', '']",6
"['dp translation  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is a pioneering']","['dp translation  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is a pioneering']","['dp translation  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is a pioneering work to model dp translation for neural machine trans - * zhaopeng tu is']","['##s are important in natural languages as they imply rich discourse information.', 'however, in pro - drop languages such as chinese and japanese, pronouns are frequently omitted when their referents can be pragmatically inferred from the context.', 'when translating sentences from a pro - drop language into a non - pro - drop language ( e. g. chinese - to - english ), translation models generally fail to translate invisible dropped pronouns ( dps ).', 'this phenomenon leads to various translation problems in terms of completeness, syntax and even semantics of translations.', 'a number of approaches have been investigated for dp translation  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is a pioneering work to model dp translation for neural machine trans - * zhaopeng tu is the corresponding author of the paper.', 'this work was conducted when longyue wang was studying and qun liu was working at the adapt centre in the school of computing at dublin city university.', 'lation ( nmt ) models.', 'they employ two separate reconstructors to respectively reconstruct encoder and decoder representations back to the dp - annotated source sentence.', 'the annotation of dp is provided by an external prediction model, which is trained on the parallel corpus using automatically learned alignment information  #AUTHOR_TAG.', '']",6
"['dp translation  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is a pioneering']","['dp translation  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is a pioneering']","['dp translation  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is a pioneering work to model dp translation for neural machine trans - * zhaopeng tu is']","['##s are important in natural languages as they imply rich discourse information.', 'however, in pro - drop languages such as chinese and japanese, pronouns are frequently omitted when their referents can be pragmatically inferred from the context.', 'when translating sentences from a pro - drop language into a non - pro - drop language ( e. g. chinese - to - english ), translation models generally fail to translate invisible dropped pronouns ( dps ).', 'this phenomenon leads to various translation problems in terms of completeness, syntax and even semantics of translations.', 'a number of approaches have been investigated for dp translation  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is a pioneering work to model dp translation for neural machine trans - * zhaopeng tu is the corresponding author of the paper.', 'this work was conducted when longyue wang was studying and qun liu was working at the adapt centre in the school of computing at dublin city university.', 'lation ( nmt ) models.', 'they employ two separate reconstructors to respectively reconstruct encoder and decoder representations back to the dp - annotated source sentence.', 'the annotation of dp is provided by an external prediction model, which is trained on the parallel corpus using automatically learned alignment information  #AUTHOR_TAG.', '']",4
"['', 'different from  #TAUTHOR_TAG, we']","['translation candidate from the generated n - best list at testing time.', 'different from  #TAUTHOR_TAG, we']","[', respectively.', 'as a reranking technique to select the best translation candidate from the generated n - best list at testing time.', 'different from  #TAUTHOR_TAG, we reconstruct dpp - annotated source sentence,']","['', 'following table 2 : evaluation of translation performance for chinese - english. "" baseline "" is trained and evaluated on the original data, while "" baseline ( + dps ) "" and "" baseline ( + dpps ) "" are trained on the data annotated with dps and dpps, respectively.', 'training and decoding ( beam size is 10 ) speeds are measured in words / second.', '"" † "" and "" ‡ "" indicate statistically significant difference ( p < 0. 01 ) from "" baseline ( + ddps ) "" and "" separate - recs⇒ ( + dps ) "", respectively.', 'as a reranking technique to select the best translation candidate from the generated n - best list at testing time.', 'different from  #TAUTHOR_TAG, we reconstruct dpp - annotated source sentence, which is predicted by an external model']",4
"['work  #TAUTHOR_TAG,']","['work  #TAUTHOR_TAG,']","['by previous work  #TAUTHOR_TAG, we conducted experiments on their released chinese⇒english tv subtitle corpus.', '2 the training, validation,']","['compare our work with the results reported by previous work  #TAUTHOR_TAG, we conducted experiments on their released chinese⇒english tv subtitle corpus.', '']",4
"['best model reported in  #TAUTHOR_TAG,']","['best model reported in  #TAUTHOR_TAG,']","['( row 3 ) is the best model reported in  #TAUTHOR_TAG,']","['##s ( rows 1 - 4 ) : the three baselines ( rows 1, 2, and 4 ) differ regarding the training data used.', '"" separate - recs⇒ ( + dps ) "" ( row 3 ) is the best model reported in  #TAUTHOR_TAG, which we employed as another strong baseline.', 'the baseline trained on the dpp - annotated data ( "" baseline ( + dpps ) "", row 4 ) outperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating dps.', 'it suggests the necessity of jointly learning to translate and predict dps.', '']",4
['model reported in  #TAUTHOR_TAG'],['model reported in  #TAUTHOR_TAG'],"['reported in  #TAUTHOR_TAG.', 'this is encouraging']",[' #TAUTHOR_TAG'],4
"['best model reported in  #TAUTHOR_TAG,']","['best model reported in  #TAUTHOR_TAG,']","['( row 3 ) is the best model reported in  #TAUTHOR_TAG,']","['##s ( rows 1 - 4 ) : the three baselines ( rows 1, 2, and 4 ) differ regarding the training data used.', '"" separate - recs⇒ ( + dps ) "" ( row 3 ) is the best model reported in  #TAUTHOR_TAG, which we employed as another strong baseline.', 'the baseline trained on the dpp - annotated data ( "" baseline ( + dpps ) "", row 4 ) outperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating dps.', 'it suggests the necessity of jointly learning to translate and predict dps.', '']",3
"['extended by adding character - level lstm  #TAUTHOR_TAG, gr']","['extended by adding character - level lstm  #TAUTHOR_TAG, gru  #AUTHOR_TAG,']","['has been extended by adding character - level lstm  #TAUTHOR_TAG, gr']","['proposed a seminal neural architecture for sequence labeling.', 'it captures word sequence information with a one - layer cnn based on pretrained word embeddings and handcrafted neural features, followed with a crf output layer.', 'dos  #AUTHOR_TAG extended this model by integrating character - level cnn features.', ' #AUTHOR_TAG built a deeper dilated cnn architecture to capture larger local features.', ' #AUTHOR_TAG was the first to exploit lstm for sequence labeling.', 'built a bilstm - crf structure, which has been extended by adding character - level lstm  #TAUTHOR_TAG, gru  #AUTHOR_TAG, and cnn  #AUTHOR_TAG features.', ' #AUTHOR_TAG a ) proposed a neural reranking model to improve ner models.', 'these models achieve state - of - the - art results in the literature.', ' #AUTHOR_TAG b ) compared several word - based lstm models for several sequence labeling tasks, reporting the score distributions over multiple runs rather than single value.', 'they investigated the influence of various hyperparameters and configurations.', 'our work is similar in comparing different neural architectures under unified settings, but differs in four main aspects : 1 ) their experiments are based on a bilstm with handcrafted word features, while our experiments are based on end - to - end neural models without human knowledge.', '2 ) their system gives relatively low performances on standard benchmarks 2, while ours can give comparable or better results with state - of - the - art models, rendering our observations more informative for practitioners.', '3 ) our findings are more consistent with most previous work on configurations such as usefulness of character information  #TAUTHOR_TAG and tag scheme  #AUTHOR_TAG.', 'in contrast, many results of  #AUTHOR_TAG b ) contradict existing reports.', '4 ) we conduct a wider range of comparison for word sequence representations, including all combinations of character cnn / lstm and word cnn / lstm structures, while  #AUTHOR_TAG b ) studied the word lstm models only']",6
"['extended by adding character - level lstm  #TAUTHOR_TAG, gr']","['extended by adding character - level lstm  #TAUTHOR_TAG, gru  #AUTHOR_TAG,']","['has been extended by adding character - level lstm  #TAUTHOR_TAG, gr']","['proposed a seminal neural architecture for sequence labeling.', 'it captures word sequence information with a one - layer cnn based on pretrained word embeddings and handcrafted neural features, followed with a crf output layer.', 'dos  #AUTHOR_TAG extended this model by integrating character - level cnn features.', ' #AUTHOR_TAG built a deeper dilated cnn architecture to capture larger local features.', ' #AUTHOR_TAG was the first to exploit lstm for sequence labeling.', 'built a bilstm - crf structure, which has been extended by adding character - level lstm  #TAUTHOR_TAG, gru  #AUTHOR_TAG, and cnn  #AUTHOR_TAG features.', ' #AUTHOR_TAG a ) proposed a neural reranking model to improve ner models.', 'these models achieve state - of - the - art results in the literature.', ' #AUTHOR_TAG b ) compared several word - based lstm models for several sequence labeling tasks, reporting the score distributions over multiple runs rather than single value.', 'they investigated the influence of various hyperparameters and configurations.', 'our work is similar in comparing different neural architectures under unified settings, but differs in four main aspects : 1 ) their experiments are based on a bilstm with handcrafted word features, while our experiments are based on end - to - end neural models without human knowledge.', '2 ) their system gives relatively low performances on standard benchmarks 2, while ours can give comparable or better results with state - of - the - art models, rendering our observations more informative for practitioners.', '3 ) our findings are more consistent with most previous work on configurations such as usefulness of character information  #TAUTHOR_TAG and tag scheme  #AUTHOR_TAG.', 'in contrast, many results of  #AUTHOR_TAG b ) contradict existing reports.', '4 ) we conduct a wider range of comparison for word sequence representations, including all combinations of character cnn / lstm and word cnn / lstm structures, while  #AUTHOR_TAG b ) studied the word lstm models only']",3
"['to character sequences in words, we can model word sequence information through lstm or cnn structures.', 'lstm has been widely used in sequence labeling  #TAUTHOR_TAG.', 'cnn can be much faster than lstm']","['to character sequences in words, we can model word sequence information through lstm or cnn structures.', 'lstm has been widely used in sequence labeling  #TAUTHOR_TAG.', 'cnn can be much faster than lstm']","['to character sequences in words, we can model word sequence information through lstm or cnn structures.', 'lstm has been widely used in sequence labeling  #TAUTHOR_TAG.', 'cnn can be much faster than lstm']","['to character sequences in words, we can model word sequence information through lstm or cnn structures.', 'lstm has been widely used in sequence labeling  #TAUTHOR_TAG.', 'cnn can be much faster than lstm due to the fact that convolution calculation can be parallel on the input sequence  #AUTHOR_TAG dos  #AUTHOR_TAG.', 'word cnn.', 'figure 3 ( a ) shows the multi - layer cnn on word sequence, where words are represented by embeddings.', 'if a character sequence representation layer is used, then word embeddings and character sequence representations are concatenated for word representations.', 'for each cnn layer, a window of size 3 slides along the sequence, extracting local features on the word inputs and a relu function  #AUTHOR_TAG is followed.', 'we follow  #AUTHOR_TAG by using 4 cnn layers.', 'batch normalization  #AUTHOR_TAG and dropout  #AUTHOR_TAG are applied following each cnn layer.', 'word lstm.', 'shown in figure 3 ( b ), word representations are fed into a forward lstm and backward lstm, respectively.', 'the forward lstm captures the word sequence information from left to right, while the backward lstm extracts information in a reversed direction.', 'the hidden states of the forward and backward lstms are concatenated at each word to give global information of the whole sequence']",3
['of  #TAUTHOR_TAG can be reproduced by'],['of  #TAUTHOR_TAG can be reproduced by'],"['', 'the results of  #TAUTHOR_TAG can be reproduced by']","['', 'the results of  #TAUTHOR_TAG can be reproduced by our clstm + wlstm + crf.', 'in most cases, our "" nochar "" based models underperform their corresponding prototypes  #AUTHOR_TAG, which utilize the hand - crafted features.', '']",3
['of  #TAUTHOR_TAG can be reproduced by'],['of  #TAUTHOR_TAG can be reproduced by'],"['', 'the results of  #TAUTHOR_TAG can be reproduced by']","['', 'the results of  #TAUTHOR_TAG can be reproduced by our clstm + wlstm + crf.', 'in most cases, our "" nochar "" based models underperform their corresponding prototypes  #AUTHOR_TAG, which utilize the hand - crafted features.', '']",3
['literature  #TAUTHOR_TAG'],['literature  #TAUTHOR_TAG'],"['significantly ( p < 0. 01 ), with a slower convergence process during training.', 'our observation is consistent with most literature  #TAUTHOR_TAG']","['addition to model structures, external factors such as pretrained embeddings, tag scheme, and optimizer can significantly influence system performance.', 'we investigate a set of external factors on the ner dataset with the two best models : clstm + wlstm + crf and ccnn + wlstm + crf.', 'pretrained embedding.', 'figure 4 ( a ) shows the f1 - scores of the two best models on the ner test set with two different pretrained embeddings, as well as the random initialization.', 'compared with the random initialization, models using pretrained embeddings give significant improvements ( p < 0. 01 ).', 'the glove 100 - dimension embeddings give higher f1 - scores than senna  #AUTHOR_TAG on both models, which is consistent with the observation of  #AUTHOR_TAG.', 'tag scheme.', 'we examine two different tag schemes : bio and bioes  #AUTHOR_TAG.', 'the results are shown in figure 4 ( b ).', 'in our experiments, models using bioes are significantly ( p < 0. 05 ) better than bio.', 'our observation is consistent with most literature  #AUTHOR_TAG.', ' #AUTHOR_TAG b ) report that the difference between the schemes is insignificant.', 'running environment.', ' #AUTHOR_TAG observe that neural sequence labeling models can give better results on gpu rather than cpu.', 'we conduct repeated experiments on both gpu and cpu environments.', 'the results are shown in figure 4 ( b ).', 'models run on cpu give a lower mean f1 - score than models run on gpu, while the difference is insignificant ( p > 0. 2 ).', 'optimizer.', 'we compare different optimizers including sgd, adagrad  #AUTHOR_TAG, adadelta  #AUTHOR_TAG rmsprop  #AUTHOR_TAG and adam  #AUTHOR_TAG.', 'the results are shown in figure 5 5.', 'in contrast to  #AUTHOR_TAG b ), who reported that sgd is the worst optimizer, our results show that sgd outperforms all other optimizers significantly ( p < 0. 01 ), with a slower convergence process during training.', 'our observation is consistent with most literature  #TAUTHOR_TAG']",3
"['sequence labeling tasks  #TAUTHOR_TAG, with lstm and cnn being']","['sequence labeling tasks  #TAUTHOR_TAG, with lstm and cnn being']","['sequence labeling tasks  #TAUTHOR_TAG, with lstm and cnn being']","['neural sequence labeling framework contains three layers, i. e., a character sequence representation layer, a word sequence representation layer and an inference layer, as shown in figure 1.', 'character information has been proven to be critical for sequence labeling tasks  #TAUTHOR_TAG, with lstm and cnn being used to model character sequence information ( "" char rep. "" ).', 'similarly, on the word level, lstm or cnn structures can be leveraged to capture long - term information or local features ( "" word rep. "" ), respectively.', 'subsequently, the inference layer assigns labels to each word using the hidden states of word sequence representations']",1
"['as prefix, suffix and capitalization can be represented with embeddings through a feature - based lookup table  #AUTHOR_TAG, or neural networks without human - defined features  #TAUTHOR_TAG.', 'in this work, we focus on neural character sequence representations without hand - engineered features.', 'character cnn.', 'using a cnn structure']","['as prefix, suffix and capitalization can be represented with embeddings through a feature - based lookup table  #AUTHOR_TAG, or neural networks without human - defined features  #TAUTHOR_TAG.', 'in this work, we focus on neural character sequence representations without hand - engineered features.', 'character cnn.', 'using a cnn structure']","['as prefix, suffix and capitalization can be represented with embeddings through a feature - based lookup table  #AUTHOR_TAG, or neural networks without human - defined features  #TAUTHOR_TAG.', 'in this work, we focus on neural character sequence representations without hand - engineered features.', 'character cnn.', 'using a cnn structure']","['features such as prefix, suffix and capitalization can be represented with embeddings through a feature - based lookup table  #AUTHOR_TAG, or neural networks without human - defined features  #TAUTHOR_TAG.', 'in this work, we focus on neural character sequence representations without hand - engineered features.', 'character cnn.', 'using a cnn structure to encode character sequences was firstly proposed by  #AUTHOR_TAG, and followed by many subsequent investigations ( dos  #AUTHOR_TAG.', 'in our experiments, we take the same structure as  #AUTHOR_TAG, using one layer cnn structure with max - pooling to capture character - level representations.', 'figure 2 ( a ) shows the cnn structure on representing word "" mexico "".', 'character lstm.', '']",5
"['pos category for all chinese unknown words  #TAUTHOR_TAG.', 'this']","['pos category for all chinese unknown words  #TAUTHOR_TAG.', 'this']","['compute the probability of a word having a particular pos category for all chinese unknown words  #TAUTHOR_TAG.', 'this approach']","['words constitute a major source of difficulty for chinese part - of - speech ( pos ) tagging, yet relatively little work has been done on pos guessing of chinese unknown words.', 'the few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular pos category for all chinese unknown words  #TAUTHOR_TAG.', '']",0
"['pos category for all chinese unknown words  #TAUTHOR_TAG.', 'this']","['pos category for all chinese unknown words  #TAUTHOR_TAG.', 'this']","['compute the probability of a word having a particular pos category for all chinese unknown words  #TAUTHOR_TAG.', 'this approach']","['words constitute a major source of difficulty for chinese part - of - speech ( pos ) tagging, yet relatively little work has been done on pos guessing of chinese unknown words.', 'the few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular pos category for all chinese unknown words  #TAUTHOR_TAG.', '']",0
"['morphemes of many unknown words is available in the training lexicon.', 'second,  #TAUTHOR_TAG argued that assigning pos to chinese unknown words on']","['morphemes of many unknown words is available in the training lexicon.', 'second,  #TAUTHOR_TAG argued that assigning pos to chinese unknown words on']","['morphemes of many unknown words is available in the training lexicon.', 'second,  #TAUTHOR_TAG argued that assigning pos to chinese unknown words on the basis of the internal structure of those words will "" result in']","['', ' #AUTHOR_TAG claimed that it does not work because the syntactic and semantic information for each character or morpheme is unavailable.', 'this claim does not fully hold, as the pos information about the component words or morphemes of many unknown words is available in the training lexicon.', 'second,  #TAUTHOR_TAG argued that assigning pos to chinese unknown words on the basis of the internal structure of those words will "" result in massive overgeneration "" ( p. 48 ).', 'we will show that overgeneration can be controlled by additional constraints']",0
"['pos category for all chinese unknown words  #TAUTHOR_TAG.', 'this']","['pos category for all chinese unknown words  #TAUTHOR_TAG.', 'this']","['compute the probability of a word having a particular pos category for all chinese unknown words  #TAUTHOR_TAG.', 'this approach']","['words constitute a major source of difficulty for chinese part - of - speech ( pos ) tagging, yet relatively little work has been done on pos guessing of chinese unknown words.', 'the few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular pos category for all chinese unknown words  #TAUTHOR_TAG.', '']",1
"['morphemes of many unknown words is available in the training lexicon.', 'second,  #TAUTHOR_TAG argued that assigning pos to chinese unknown words on']","['morphemes of many unknown words is available in the training lexicon.', 'second,  #TAUTHOR_TAG argued that assigning pos to chinese unknown words on']","['morphemes of many unknown words is available in the training lexicon.', 'second,  #TAUTHOR_TAG argued that assigning pos to chinese unknown words on the basis of the internal structure of those words will "" result in']","['', ' #AUTHOR_TAG claimed that it does not work because the syntactic and semantic information for each character or morpheme is unavailable.', 'this claim does not fully hold, as the pos information about the component words or morphemes of many unknown words is available in the training lexicon.', 'second,  #TAUTHOR_TAG argued that assigning pos to chinese unknown words on the basis of the internal structure of those words will "" result in massive overgeneration "" ( p. 48 ).', 'we will show that overgeneration can be controlled by additional constraints']",4
"['trigram model, and the statistical model developed by  #TAUTHOR_TAG.', 'combination of the three models will be']","['trigram model, and the statistical model developed by  #TAUTHOR_TAG.', 'combination of the three models will be']","['at better results for this task.', 'the models we will consider are a rule - based model, the trigram model, and the statistical model developed by  #TAUTHOR_TAG.', 'combination of the three models will be based on the evaluation of their individual performances on the training data']","['propose a hybrid model that combines the strengths of different models to arrive at better results for this task.', 'the models we will consider are a rule - based model, the trigram model, and the statistical model developed by  #TAUTHOR_TAG.', 'combination of the three models will be based on the evaluation of their individual performances on the training data']",5
['by  #TAUTHOR_TAG'],"['by  #TAUTHOR_TAG.', '']",['by  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
"['out by  #TAUTHOR_TAG, is']","['out by  #TAUTHOR_TAG, is']","['out by  #TAUTHOR_TAG, is']",[' #TAUTHOR_TAG'],0
['by  #TAUTHOR_TAG'],"['by  #TAUTHOR_TAG.', '']",['by  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],5
"['##ser  #TAUTHOR_TAG.', 'following  #TAUTHOR_TAG we evaluate']","['zpar and add it to the error comparison between maltparser and mstparser  #TAUTHOR_TAG.', 'following  #TAUTHOR_TAG we evaluate']","['##ser  #TAUTHOR_TAG.', 'following  #TAUTHOR_TAG we evaluate the parsers on the conll - x shared task data  #AUTHOR_TAG, which include']","['this section we study the effect of global learning and beam - search on the error distributions of transition - based dependency parsing.', 'we characterize the errors of zpar and add it to the error comparison between maltparser and mstparser  #TAUTHOR_TAG.', 'following  #TAUTHOR_TAG we evaluate the parsers on the conll - x shared task data  #AUTHOR_TAG, which include training and test sentences for 13 different languages.', 'for each parser, we conjoin the outputs for all 13 languages in the same way as  #TAUTHOR_TAG, and calculate error distributions over the aggregated output.', 'accuracies are measured using the labeled attached score ( las ) evaluation metric, which is defined as the percentage of words ( excluding punctuation ) that are assigned both the correct head word and the correct arc label.', 'to handle non - projectivity, pseudo - projective parsing  #AUTHOR_TAG is applied to zpar and maltparser, transforming non - projective trees into pseudo - projective trees in the training data, and post - processing pseudo - projective outputs by the parser to transform them into non - projective trees.', 'mstparser produces non - projective trees from projective trees by scorebased rearrangements of arcs']",5
"['##ser  #TAUTHOR_TAG.', 'following  #TAUTHOR_TAG we evaluate']","['zpar and add it to the error comparison between maltparser and mstparser  #TAUTHOR_TAG.', 'following  #TAUTHOR_TAG we evaluate']","['##ser  #TAUTHOR_TAG.', 'following  #TAUTHOR_TAG we evaluate the parsers on the conll - x shared task data  #AUTHOR_TAG, which include']","['this section we study the effect of global learning and beam - search on the error distributions of transition - based dependency parsing.', 'we characterize the errors of zpar and add it to the error comparison between maltparser and mstparser  #TAUTHOR_TAG.', 'following  #TAUTHOR_TAG we evaluate the parsers on the conll - x shared task data  #AUTHOR_TAG, which include training and test sentences for 13 different languages.', 'for each parser, we conjoin the outputs for all 13 languages in the same way as  #TAUTHOR_TAG, and calculate error distributions over the aggregated output.', 'accuracies are measured using the labeled attached score ( las ) evaluation metric, which is defined as the percentage of words ( excluding punctuation ) that are assigned both the correct head word and the correct arc label.', 'to handle non - projectivity, pseudo - projective parsing  #AUTHOR_TAG is applied to zpar and maltparser, transforming non - projective trees into pseudo - projective trees in the training data, and post - processing pseudo - projective outputs by the parser to transform them into non - projective trees.', 'mstparser produces non - projective trees from projective trees by scorebased rearrangements of arcs']",5
"['##ser  #TAUTHOR_TAG.', 'following  #TAUTHOR_TAG we evaluate']","['zpar and add it to the error comparison between maltparser and mstparser  #TAUTHOR_TAG.', 'following  #TAUTHOR_TAG we evaluate']","['##ser  #TAUTHOR_TAG.', 'following  #TAUTHOR_TAG we evaluate the parsers on the conll - x shared task data  #AUTHOR_TAG, which include']","['this section we study the effect of global learning and beam - search on the error distributions of transition - based dependency parsing.', 'we characterize the errors of zpar and add it to the error comparison between maltparser and mstparser  #TAUTHOR_TAG.', 'following  #TAUTHOR_TAG we evaluate the parsers on the conll - x shared task data  #AUTHOR_TAG, which include training and test sentences for 13 different languages.', 'for each parser, we conjoin the outputs for all 13 languages in the same way as  #TAUTHOR_TAG, and calculate error distributions over the aggregated output.', 'accuracies are measured using the labeled attached score ( las ) evaluation metric, which is defined as the percentage of words ( excluding punctuation ) that are assigned both the correct head word and the correct arc label.', 'to handle non - projectivity, pseudo - projective parsing  #AUTHOR_TAG is applied to zpar and maltparser, transforming non - projective trees into pseudo - projective trees in the training data, and post - processing pseudo - projective outputs by the parser to transform them into non - projective trees.', 'mstparser produces non - projective trees from projective trees by scorebased rearrangements of arcs']",5
"['out by  #TAUTHOR_TAG, is']","['out by  #TAUTHOR_TAG, is']","['out by  #TAUTHOR_TAG, is']",[' #TAUTHOR_TAG'],3
"['of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG']","['of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG']","['dependency parsing have now been applied to a variety of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG.', 'for english, the interest in']","['', 'deterministic methods for dependency parsing have now been applied to a variety of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG.', 'for english, the interest in dependency parsing has been weaker than for other languages.', 'to some extent, this can probably be explained by the strong tradition of constituent analysis in anglo - american linguistics, but this trend has been reinforced by the fact that the major treebank of american english, the penn treebank  #AUTHOR_TAG, is annotated primarily with constituent analysis.', 'on the other hand, the best available parsers trained on the penn treebank, those of  #AUTHOR_TAG and  #AUTHOR_TAG, use statistical models for disambiguation that make crucial use of dependency relations.', '']",0
"['of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG']","['of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG']","['dependency parsing have now been applied to a variety of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG.', 'for english, the interest in']","['', 'deterministic methods for dependency parsing have now been applied to a variety of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG.', 'for english, the interest in dependency parsing has been weaker than for other languages.', 'to some extent, this can probably be explained by the strong tradition of constituent analysis in anglo - american linguistics, but this trend has been reinforced by the fact that the major treebank of american english, the penn treebank  #AUTHOR_TAG, is annotated primarily with constituent analysis.', 'on the other hand, the best available parsers trained on the penn treebank, those of  #AUTHOR_TAG and  #AUTHOR_TAG, use statistical models for disambiguation that make crucial use of dependency relations.', '']",0
"['of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG']","['of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG']","['dependency parsing have now been applied to a variety of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG.', 'for english, the interest in']","['', 'deterministic methods for dependency parsing have now been applied to a variety of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG.', 'for english, the interest in dependency parsing has been weaker than for other languages.', 'to some extent, this can probably be explained by the strong tradition of constituent analysis in anglo - american linguistics, but this trend has been reinforced by the fact that the major treebank of american english, the penn treebank  #AUTHOR_TAG, is annotated primarily with constituent analysis.', 'on the other hand, the best available parsers trained on the penn treebank, those of  #AUTHOR_TAG and  #AUTHOR_TAG, use statistical models for disambiguation that make crucial use of dependency relations.', '']",4
"['of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG']","['of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG']","['dependency parsing have now been applied to a variety of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG.', 'for english, the interest in']","['', 'deterministic methods for dependency parsing have now been applied to a variety of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG.', 'for english, the interest in dependency parsing has been weaker than for other languages.', 'to some extent, this can probably be explained by the strong tradition of constituent analysis in anglo - american linguistics, but this trend has been reinforced by the fact that the major treebank of american english, the penn treebank  #AUTHOR_TAG, is annotated primarily with constituent analysis.', 'on the other hand, the best available parsers trained on the penn treebank, those of  #AUTHOR_TAG and  #AUTHOR_TAG, use statistical models for disambiguation that make crucial use of dependency relations.', '']",4
"['of the previous results on labeling accuracy is', 'strictly comparable to ours, it nevertheless seems fair to conclude that the  #TAUTHOR_TAG labeling accuracy of the present parser is close to the']","['of the previous results on labeling accuracy is', 'strictly comparable to ours, it nevertheless seems fair to conclude that the  #TAUTHOR_TAG labeling accuracy of the present parser is close to the']","['( dep labels included', '), which can be compared with 97. 0 % for a similar ( but larger )', 'set of labels in  #AUTHOR_TAG. 6 although none of the previous results on labeling accuracy is', 'strictly comparable to ours, it nevertheless seems fair to conclude that the  #TAUTHOR_TAG labeling accuracy of the present parser is close to the state of the art, even if its capacity to derive correct structures is', 'not']","['', 'table 3. here we see that for really short sentences ( up to 10 words ) root accuracy is indeed higher than dependency accuracy', ', but while dependency accuracy degrades gracefully with sentence length, the root accuracy drops more drastically ( which also very clearly affects', 'the complete match score ). this may be taken to suggest that some kind of preprocessing in the form of clausing may help to improve overall accuracy. turning finally to the assessment of labeled dependency accuracy, we are not aware of any strictly comparable results for the', 'given data set, but  #AUTHOR_TAG reports a labeled accuracy of 72. 6 % for the assignment of grammatical relations using a cascade of memory - based processors. this can be compared with a labeled attachment score of 84. 4 % for model 2 with our b set, which is of about the same size as the set used by buchholz, although the labels are not the same. in another study,  #AUTHOR_TAG report an f - measure of 98. 9 % for the assignment of penn treebank grammatical', 'role labels ( our g set ) to phrases that were correctly parsed by the parser described in  #AUTHOR_TAG. if null labels ( corresponding to our dep labels ) are excluded, the f - score drops to 95.', '7 %. the corresponding f - measures for our best', 'parser ( model 2, bg ) are 99. 0 % and 94. 7 %. for the larger b set, our best parser achieves an f - measure of 96. 9 % ( dep labels included', '), which can be compared with 97. 0 % for a similar ( but larger )', 'set of labels in  #AUTHOR_TAG. 6 although none of the previous results on labeling accuracy is', 'strictly comparable to ours, it nevertheless seems fair to conclude that the  #TAUTHOR_TAG labeling accuracy of the present parser is close to the state of the art, even if its capacity to derive correct structures is', 'not']",4
"['of the previous results on labeling accuracy is', 'strictly comparable to ours, it nevertheless seems fair to conclude that the  #TAUTHOR_TAG labeling accuracy of the present parser is close to the']","['of the previous results on labeling accuracy is', 'strictly comparable to ours, it nevertheless seems fair to conclude that the  #TAUTHOR_TAG labeling accuracy of the present parser is close to the']","['( dep labels included', '), which can be compared with 97. 0 % for a similar ( but larger )', 'set of labels in  #AUTHOR_TAG. 6 although none of the previous results on labeling accuracy is', 'strictly comparable to ours, it nevertheless seems fair to conclude that the  #TAUTHOR_TAG labeling accuracy of the present parser is close to the state of the art, even if its capacity to derive correct structures is', 'not']","['', 'table 3. here we see that for really short sentences ( up to 10 words ) root accuracy is indeed higher than dependency accuracy', ', but while dependency accuracy degrades gracefully with sentence length, the root accuracy drops more drastically ( which also very clearly affects', 'the complete match score ). this may be taken to suggest that some kind of preprocessing in the form of clausing may help to improve overall accuracy. turning finally to the assessment of labeled dependency accuracy, we are not aware of any strictly comparable results for the', 'given data set, but  #AUTHOR_TAG reports a labeled accuracy of 72. 6 % for the assignment of grammatical relations using a cascade of memory - based processors. this can be compared with a labeled attachment score of 84. 4 % for model 2 with our b set, which is of about the same size as the set used by buchholz, although the labels are not the same. in another study,  #AUTHOR_TAG report an f - measure of 98. 9 % for the assignment of penn treebank grammatical', 'role labels ( our g set ) to phrases that were correctly parsed by the parser described in  #AUTHOR_TAG. if null labels ( corresponding to our dep labels ) are excluded, the f - score drops to 95.', '7 %. the corresponding f - measures for our best', 'parser ( model 2, bg ) are 99. 0 % and 94. 7 %. for the larger b set, our best parser achieves an f - measure of 96. 9 % ( dep labels included', '), which can be compared with 97. 0 % for a similar ( but larger )', 'set of labels in  #AUTHOR_TAG. 6 although none of the previous results on labeling accuracy is', 'strictly comparable to ours, it nevertheless seems fair to conclude that the  #TAUTHOR_TAG labeling accuracy of the present parser is close to the state of the art, even if its capacity to derive correct structures is', 'not']",4
"['of the previous results on labeling accuracy is', 'strictly comparable to ours, it nevertheless seems fair to conclude that the  #TAUTHOR_TAG labeling accuracy of the present parser is close to the']","['of the previous results on labeling accuracy is', 'strictly comparable to ours, it nevertheless seems fair to conclude that the  #TAUTHOR_TAG labeling accuracy of the present parser is close to the']","['( dep labels included', '), which can be compared with 97. 0 % for a similar ( but larger )', 'set of labels in  #AUTHOR_TAG. 6 although none of the previous results on labeling accuracy is', 'strictly comparable to ours, it nevertheless seems fair to conclude that the  #TAUTHOR_TAG labeling accuracy of the present parser is close to the state of the art, even if its capacity to derive correct structures is', 'not']","['', 'table 3. here we see that for really short sentences ( up to 10 words ) root accuracy is indeed higher than dependency accuracy', ', but while dependency accuracy degrades gracefully with sentence length, the root accuracy drops more drastically ( which also very clearly affects', 'the complete match score ). this may be taken to suggest that some kind of preprocessing in the form of clausing may help to improve overall accuracy. turning finally to the assessment of labeled dependency accuracy, we are not aware of any strictly comparable results for the', 'given data set, but  #AUTHOR_TAG reports a labeled accuracy of 72. 6 % for the assignment of grammatical relations using a cascade of memory - based processors. this can be compared with a labeled attachment score of 84. 4 % for model 2 with our b set, which is of about the same size as the set used by buchholz, although the labels are not the same. in another study,  #AUTHOR_TAG report an f - measure of 98. 9 % for the assignment of penn treebank grammatical', 'role labels ( our g set ) to phrases that were correctly parsed by the parser described in  #AUTHOR_TAG. if null labels ( corresponding to our dep labels ) are excluded, the f - score drops to 95.', '7 %. the corresponding f - measures for our best', 'parser ( model 2, bg ) are 99. 0 % and 94. 7 %. for the larger b set, our best parser achieves an f - measure of 96. 9 % ( dep labels included', '), which can be compared with 97. 0 % for a similar ( but larger )', 'set of labels in  #AUTHOR_TAG. 6 although none of the previous results on labeling accuracy is', 'strictly comparable to ours, it nevertheless seems fair to conclude that the  #TAUTHOR_TAG labeling accuracy of the present parser is close to the state of the art, even if its capacity to derive correct structures is', 'not']",4
"['of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG']","['of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG']","['dependency parsing have now been applied to a variety of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG.', 'for english, the interest in']","['', 'deterministic methods for dependency parsing have now been applied to a variety of languages, including japanese  #AUTHOR_TAG, english  #TAUTHOR_TAG, turkish  #AUTHOR_TAG, and swedish  #AUTHOR_TAG.', 'for english, the interest in dependency parsing has been weaker than for other languages.', 'to some extent, this can probably be explained by the strong tradition of constituent analysis in anglo - american linguistics, but this trend has been reinforced by the fact that the major treebank of american english, the penn treebank  #AUTHOR_TAG, is annotated primarily with constituent analysis.', 'on the other hand, the best available parsers trained on the penn treebank, those of  #AUTHOR_TAG and  #AUTHOR_TAG, use statistical models for disambiguation that make crucial use of dependency relations.', '']",6
"['we hypothesized, the reference model achieves much better precision than the context model from  #TAUTHOR_TAG resources,']","['we hypothesized, the reference model achieves much better precision than the context model from  #TAUTHOR_TAG resources,']","['we hypothesized, the reference model achieves much better precision than the context model from  #TAUTHOR_TAG resources, yielding a lower f1.', 'yet, its higher precision pays off for the bootstrapping step ( section 4. 2 ).', 'finally, when the two models are combined a small precision improvement is observed']","['we hypothesized, the reference model achieves much better precision than the context model from  #TAUTHOR_TAG resources, yielding a lower f1.', 'yet, its higher precision pays off for the bootstrapping step ( section 4. 2 ).', 'finally, when the two models are combined a small precision improvement is observed']",4
['of both languages is utilized using the architecture by  #TAUTHOR_TAG and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data'],['of both languages is utilized using the architecture by  #TAUTHOR_TAG and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data'],"[') based neural machine translation ( nmt ) system, a semi - supervised nmt system where monolingual data of both languages is utilized using the architecture by  #TAUTHOR_TAG and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data']","['paper describes our submission to shared task on similar language translation in fourth conference on machine translation ( wmt 2019 ).', 'we submitted three systems for hindi → nepali direction in which we have examined the performance of a recursive neural network ( rnn ) based neural machine translation ( nmt ) system, a semi - supervised nmt system where monolingual data of both languages is utilized using the architecture by  #TAUTHOR_TAG and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data']",5
['proposed in  #TAUTHOR_TAG where encoder is shared'],['proposed in  #TAUTHOR_TAG where encoder is shared'],['proposed in  #TAUTHOR_TAG where encoder is shared'],"['section describes the specification of the systems submitted in detail.', 'we have submitted sys - tems for hindi - nepali language pair in hindi → nepali direction.', 'hindi and nepali both are indoaryan languages and are very similar to each other.', 'they share a significant portion of the vocabulary and similar word orders.', 'the three submitted systems are :', '• a pure rnn based nmt system', '• semi - supervised rnn based nmt system', '• utilization of copied data in rnn based nmt first system is pure rnn based nmt system.', 'to train this we have utilized only parallel corpora.', 'second system is trained using a semi - supervised nmt system where monolingual data from both languages is utilized.', 'we have utilized architecture proposed in  #TAUTHOR_TAG where encoder is shared and decoders are separate for each language and model is trained by alternating between denoising and back - translation.', 'this architecture can also be utilized for completely unsupervised setting.', 'third system is also a pure rnn based nmt system where additional parallel data ( synthetic data ) is created by copying source side sentences to target side and target side sentences to source side, but we do this only for the available parallel sentences, no additional monolingual data is utilized.', 'in this way the amount of available data becomes three times of the original data.', 'all the data is combined together, shuffled and then provided to the nmt system, there is no identification provided to distinguish between parallel data and copy data.', 'to train all three systems we have utilized the implementation of  #TAUTHOR_TAG']",5
['proposed in  #TAUTHOR_TAG where encoder is shared'],['proposed in  #TAUTHOR_TAG where encoder is shared'],['proposed in  #TAUTHOR_TAG where encoder is shared'],"['section describes the specification of the systems submitted in detail.', 'we have submitted sys - tems for hindi - nepali language pair in hindi → nepali direction.', 'hindi and nepali both are indoaryan languages and are very similar to each other.', 'they share a significant portion of the vocabulary and similar word orders.', 'the three submitted systems are :', '• a pure rnn based nmt system', '• semi - supervised rnn based nmt system', '• utilization of copied data in rnn based nmt first system is pure rnn based nmt system.', 'to train this we have utilized only parallel corpora.', 'second system is trained using a semi - supervised nmt system where monolingual data from both languages is utilized.', 'we have utilized architecture proposed in  #TAUTHOR_TAG where encoder is shared and decoders are separate for each language and model is trained by alternating between denoising and back - translation.', 'this architecture can also be utilized for completely unsupervised setting.', 'third system is also a pure rnn based nmt system where additional parallel data ( synthetic data ) is created by copying source side sentences to target side and target side sentences to source side, but we do this only for the available parallel sentences, no additional monolingual data is utilized.', 'in this way the amount of available data becomes three times of the original data.', 'all the data is combined together, shuffled and then provided to the nmt system, there is no identification provided to distinguish between parallel data and copy data.', 'to train all three systems we have utilized the implementation of  #TAUTHOR_TAG']",5
"['a ), utilizing other similar language pairs through pivoting  #AUTHOR_TAG or transfer learning  #AUTHOR_TAG, complete unsupervised architectures  #TAUTHOR_TAG  #AUTHOR_TAG and many others have been proposed']","['a ), utilizing other similar language pairs through pivoting  #AUTHOR_TAG or transfer learning  #AUTHOR_TAG, complete unsupervised architectures  #TAUTHOR_TAG  #AUTHOR_TAG and many others have been proposed']","['back - translation  #AUTHOR_TAG a ), utilizing other similar language pairs through pivoting  #AUTHOR_TAG or transfer learning  #AUTHOR_TAG, complete unsupervised architectures  #TAUTHOR_TAG  #AUTHOR_TAG and many others have been proposed']","['architectures have been proposed for neural machine translation.', 'most famous one is rnn based encoder - decoder proposed in ( cho et al. ), where encoder and decoder are both recursive neural networks, encoder can be bi - directional.', 'after this attention based sequence to sequence models where attention is utilized to improve performance in nmt are proposed in  #AUTHOR_TAG,  #AUTHOR_TAG.', 'attention basically instructs the system about which words to focus more, while generating a particular target word.', 'transformer based encoder - decoder architecture for nmt is proposed in  #AUTHOR_TAG, which is completely based on self - attention and positional encoding.', 'this does not follow recurrent architecture.', 'positional encoding provides the system with information of order of words.', 'nmt needs lots of parallel data to train a system.', 'this task basically focuses on how to improve performance for languages which are similar but resource scarce.', 'there are many language pairs for which parallel data does not exist or exist in a very small amount.', 'in past, to improve the performance of nmt systems various techniques like back - translation  #AUTHOR_TAG a ), utilizing other similar language pairs through pivoting  #AUTHOR_TAG or transfer learning  #AUTHOR_TAG, complete unsupervised architectures  #TAUTHOR_TAG  #AUTHOR_TAG and many others have been proposed']",0
['proposed in  #TAUTHOR_TAG follows an'],['proposed in  #TAUTHOR_TAG follows an'],"['unsupervised nmt, where only monolingual data is utilized.', 'the unsupervised nmt approach proposed in  #TAUTHOR_TAG follows an architecture where encoder is shared']","['has been good amount of work done on how we can utilize monolingual data to improve performance of an nmt system.', 'back - translation was introduced by  #AUTHOR_TAG b ), to utilize monolingual data of target language.', 'this requires a translation system in opposite direction.', 'in  #AUTHOR_TAG b ), a method where empty sentences are provided in the input for target side monolingual data is also evaluated, backtranslation performs better than this.', 'in iterative back - translation, systems in both directions improve each other  #AUTHOR_TAG, it is done in an incremental fashion.', 'to generate backtranslated data, current system in opposite direction is utilized.', 'in  #AUTHOR_TAG, target side monolingual data is copied to generate source synthetic translations and the system is trained by combining this synthetic data with parallel data.', 'in  #AUTHOR_TAG, source side monolingual data is utilized to iteratively generate synthetic sentences from the same model.', 'in  #AUTHOR_TAG, there is a separate layer for target side language model in training, decoder utilize both source dependent and source independent representations to generate a particular target word.', 'in  #AUTHOR_TAG, it is claimed that quality of back - translated sentences is important.', 'recently many systems have been proposed for unsupervised nmt, where only monolingual data is utilized.', 'the unsupervised nmt approach proposed in  #TAUTHOR_TAG follows an architecture where encoder is shared and decoder is separate for each language.', 'encoder tries to map sentences from both languages in the same space, which is supported by cross - lingual word embeddings.', 'they fix cross - lingual word embeddings in the encoder while training, which helps in generating cross - lingual sentence representations in the same space.', 'the system with one shared encoder and two separate decoders with no parallel data is trained by iterating between denoising and back - translation.', 'denoising tries to generate the correct sentence from noisy sentences, in that way the decoder is learning how to generate sentences in that particular language.', 'these noisy sentences are created by shuffling words within a window.', 'if the system is only trained with denoising then it may turn out to be a denoising auto - encoder.', 'so they have also introduced back - translation in the training process to introduce translation task.', 'training is done by alternating between denoising and back - translation for mini - batches if parallel data is not available.', 'in a semi - supervised setting if some amount of parallel data is available, training alternates between denoising, back - translation and parallel sentences.', 'in  #AUTHOR_TAG, encoder and decoder both are shared between the languages.', 'training is done by alternating between denoising and back - translation.', 'initialization is performed using a system trained on wordword translated sentences which is']",0
"['classes encountered in the wild.', 'compared to meta - learning baselines and recent approaches which use language supervision as a more fundamental bottleneck in a model  #TAUTHOR_TAG, we find this simple auxiliary training objective results in']","['classes encountered in the wild.', 'compared to meta - learning baselines and recent approaches which use language supervision as a more fundamental bottleneck in a model  #TAUTHOR_TAG, we find this simple auxiliary training objective results in']","['##en classes encountered in the wild.', 'compared to meta - learning baselines and recent approaches which use language supervision as a more fundamental bottleneck in a model  #TAUTHOR_TAG, we find this simple auxiliary training objective results in learned representations']","['are powerful and data - efficient learners partially due to the ability to learn from language [ 6, 30 ] : for instance, we can learn about robins not by seeing thousands of examples, but by being told that a robin is a bird with a red belly and brown feathers.', 'this language not only helps us learn about robins, but shapes the way we view the world, constraining the hypotheses we form for other concepts [ 12 ] : given a new bird like seagulls, even without language we know to attend to salient features including belly and feather color.', 'in this paper, we propose to use language as a guide for representation learning, building few - shot classification models that learn visual representations while jointly predicting task - specific language during training.', 'crucially, our models can operate without language at test time : a more practical setting, since it is often unrealistic to assume that linguistic supervision is available for unseen classes encountered in the wild.', 'compared to meta - learning baselines and recent approaches which use language supervision as a more fundamental bottleneck in a model  #TAUTHOR_TAG, we find this simple auxiliary training objective results in learned representations that generalize better to new concepts']",4
"['retrieval, but do not directly figure  #TAUTHOR_TAG which explicitly use language as a bottleneck']","['retrieval, but do not directly figure  #TAUTHOR_TAG which explicitly use language as a bottleneck']","['image retrieval, but do not directly figure  #TAUTHOR_TAG which explicitly use language as a bottleneck']","['has been shown to assist visual classification in various settings, including traditional visual classification with no transfer [ 16 ] and with language available at test time in the form of class labels or descriptions for zero - [ 10, 11, 27 ] or few - shot [ 24, 33 ] learning.', 'unlike this work, we study a setting where we have no language at test time and test tasks are unseen, so language from training can no longer be used as additional class information [ cf. 16 ] or weak supervision for labeling additional in - domain data [ cf. 15 ].', 'our work can thus be seen as an instance of the learning using privileged information ( lupi ) [ 31 ] framework, where richer supervision augments a model during training only.', 'in this framework, learning with attributes and other domain - specific rationales has been tackled extensively [ 8, 9, 29 ], but language remains relatively unexplored.', '[ 13 ] use meteor scores between captions as a similarity metric for specializing embeddings for image retrieval, but do not directly figure  #TAUTHOR_TAG which explicitly use language as a bottleneck for classification.', 'ground language explanations.', '[ 28 ] explore a supervision setting similar to ours, except in highly structured text and symbolic domains where descriptions can be easily converted to executable forms via semantic parsing.', 'another line of work studies models which generate natural language explanations of decisions for interpretability for both textual ( e. g. natural language inference ; [ 3 ] ) and visual [ 17, 18 ] tasks, but here we examine whether this act of predicting language can actually improve downstream task performance ; similar ideas have been explored in text [ 22 ] and reinforcement learning [ 2, 14 ] domains.', 'our work is most similar to  #TAUTHOR_TAG, which we describe and compare to later']",4
"['1 shapeworld f θ. like  #TAUTHOR_TAG, f θ starts with features']","['code is publicly available at https : / / github. com / jayelm / lsl.', 'a. 1 shapeworld f θ. like  #TAUTHOR_TAG, f θ starts with features']","['1 shapeworld f θ. like  #TAUTHOR_TAG, f θ starts with features']","['code is publicly available at https : / / github. com / jayelm / lsl.', 'a. 1 shapeworld f θ. like  #TAUTHOR_TAG, f θ starts with features extracted from the last convolutional layer of a fixed imagenetpretrained vgg - 19 network [ 25 ].', '']",4
"['1 shapeworld f θ. like  #TAUTHOR_TAG, f θ starts with features']","['code is publicly available at https : / / github. com / jayelm / lsl.', 'a. 1 shapeworld f θ. like  #TAUTHOR_TAG, f θ starts with features']","['1 shapeworld f θ. like  #TAUTHOR_TAG, f θ starts with features']","['code is publicly available at https : / / github. com / jayelm / lsl.', 'a. 1 shapeworld f θ. like  #TAUTHOR_TAG, f θ starts with features extracted from the last convolutional layer of a fixed imagenetpretrained vgg - 19 network [ 25 ].', '']",4
"['1 shapeworld f θ. like  #TAUTHOR_TAG, f θ starts with features']","['code is publicly available at https : / / github. com / jayelm / lsl.', 'a. 1 shapeworld f θ. like  #TAUTHOR_TAG, f θ starts with features']","['1 shapeworld f θ. like  #TAUTHOR_TAG, f θ starts with features']","['code is publicly available at https : / / github. com / jayelm / lsl.', 'a. 1 shapeworld f θ. like  #TAUTHOR_TAG, f θ starts with features extracted from the last convolutional layer of a fixed imagenetpretrained vgg - 19 network [ 25 ].', '']",4
"['20 ] dataset devised by  #TAUTHOR_TAG 000 validation, and 4000 test tasks ( figure 2 ).', '3']","['we describe our two tasks and models.', 'for full training details and code, see appendix a.', 'shapeworld.', 'first, we use the shapeworld [ 20 ] dataset devised by  #TAUTHOR_TAG 000 validation, and 4000 test tasks ( figure 2 ).', '3']","['we describe our two tasks and models.', 'for full training details and code, see appendix a.', 'shapeworld.', 'first, we use the shapeworld [ 20 ] dataset devised by  #TAUTHOR_TAG 000 validation, and 4000 test tasks ( figure 2 ).', '3 each task contains a single support set of k = 4 images representing a visual concept with an associated ( artificial ) english language description, generated with a minimal recursion semantics representation of the concept [ 7 ].', 'each concept is a spatial relation between two objects, optionally qualified by color and / or shape ; 2 - 3 distractor shapes are also present in']","['we describe our two tasks and models.', 'for full training details and code, see appendix a.', 'shapeworld.', 'first, we use the shapeworld [ 20 ] dataset devised by  #TAUTHOR_TAG 000 validation, and 4000 test tasks ( figure 2 ).', '3 each task contains a single support set of k = 4 images representing a visual concept with an associated ( artificial ) english language description, generated with a minimal recursion semantics representation of the concept [ 7 ].', 'each concept is a spatial relation between two objects, optionally qualified by color and / or shape ; 2 - 3 distractor shapes are also present in each image.', 'the task is to predict whether a single query image x belongs to the concept.', 'model details are identical to  #TAUTHOR_TAG for easy comparison.', 'f θ is the final convolutional layer of a fixed imagenet - pretrained vgg - 16 [ 25 ] fed through two fully - connected layers :', 'since this is a binary classification task with only 1 ( positive ) support class s and prototype c, we define the similarity function s ( a, b ) = σ ( a · b ) and the prediction p ( y = 1 | x ) = s ( f θ ( x ), c ).', 'g φ is a gated recurrent unit ( gru ) rnn [ 5 ] with hidden size h = 512, trained with teacher forcing.', 'using a grid search on the validation set, we set λ nl = 20.', 'birds. to see if lsl can scale to more realistic scenarios, we use the caltech - ucsd birds dataset [ 32 ], which contains 200 bird species, each with 40 - 60 images, split into 100 train, 50 validation,', 'the bird has a white underbelly, black feathers in the wings, a large wingspan, and a white beak']",5
"['20 ] dataset devised by  #TAUTHOR_TAG 000 validation, and 4000 test tasks ( figure 2 ).', '3']","['we describe our two tasks and models.', 'for full training details and code, see appendix a.', 'shapeworld.', 'first, we use the shapeworld [ 20 ] dataset devised by  #TAUTHOR_TAG 000 validation, and 4000 test tasks ( figure 2 ).', '3']","['we describe our two tasks and models.', 'for full training details and code, see appendix a.', 'shapeworld.', 'first, we use the shapeworld [ 20 ] dataset devised by  #TAUTHOR_TAG 000 validation, and 4000 test tasks ( figure 2 ).', '3 each task contains a single support set of k = 4 images representing a visual concept with an associated ( artificial ) english language description, generated with a minimal recursion semantics representation of the concept [ 7 ].', 'each concept is a spatial relation between two objects, optionally qualified by color and / or shape ; 2 - 3 distractor shapes are also present in']","['we describe our two tasks and models.', 'for full training details and code, see appendix a.', 'shapeworld.', 'first, we use the shapeworld [ 20 ] dataset devised by  #TAUTHOR_TAG 000 validation, and 4000 test tasks ( figure 2 ).', '3 each task contains a single support set of k = 4 images representing a visual concept with an associated ( artificial ) english language description, generated with a minimal recursion semantics representation of the concept [ 7 ].', 'each concept is a spatial relation between two objects, optionally qualified by color and / or shape ; 2 - 3 distractor shapes are also present in each image.', 'the task is to predict whether a single query image x belongs to the concept.', 'model details are identical to  #TAUTHOR_TAG for easy comparison.', 'f θ is the final convolutional layer of a fixed imagenet - pretrained vgg - 16 [ 25 ] fed through two fully - connected layers :', 'since this is a binary classification task with only 1 ( positive ) support class s and prototype c, we define the similarity function s ( a, b ) = σ ( a · b ) and the prediction p ( y = 1 | x ) = s ( f θ ( x ), c ).', 'g φ is a gated recurrent unit ( gru ) rnn [ 5 ] with hidden size h = 512, trained with teacher forcing.', 'using a grid search on the validation set, we set λ nl = 20.', 'birds. to see if lsl can scale to more realistic scenarios, we use the caltech - ucsd birds dataset [ 32 ], which contains 200 bird species, each with 40 - 60 images, split into 100 train, 50 validation,', 'the bird has a white underbelly, black feathers in the wings, a large wingspan, and a white beak']",5
"['1 shapeworld f θ. like  #TAUTHOR_TAG, f θ starts with features']","['code is publicly available at https : / / github. com / jayelm / lsl.', 'a. 1 shapeworld f θ. like  #TAUTHOR_TAG, f θ starts with features']","['1 shapeworld f θ. like  #TAUTHOR_TAG, f θ starts with features']","['code is publicly available at https : / / github. com / jayelm / lsl.', 'a. 1 shapeworld f θ. like  #TAUTHOR_TAG, f θ starts with features extracted from the last convolutional layer of a fixed imagenetpretrained vgg - 19 network [ 25 ].', '']",5
"['##m french words.', 'we follow  #TAUTHOR_TAG to restrict that sentences are no longer than 50 words.', 'the concatenation of news - test']","['french words.', 'we follow  #TAUTHOR_TAG to restrict that sentences are no longer than 50 words.', 'the concatenation of news - test - 2012 and news - test - 2013 is used as']","['##m french words.', 'we follow  #TAUTHOR_TAG to restrict that sentences are no longer than 50 words.', 'the concatenation of news - test']","['', 'for chinese - english, the training corpus from ldc consists of 2. 56m sentence pairs with 67. 53m chinese words and 74. 81m english words.', 'we used the nist 2006 dataset as the validation set for hyperparameter optimization and model selection and the nist 2002 nist, 2003 nist, 2004 nist, 2005 nist, and 2008 datasets as the test sets.', 'in the nist chinese - english datasets, each chinese sentence has four corresponding english translations.', 'to build english - chinese evaluation datasets, we select the first english sentence in the four references as the source sentence and the chinese sentence as the single reference translation.', 'for english - french, the training corpus from wmt 2014 consists of 12. 07m sentence pairs with 303. 88m english words and 348. 24m french words.', 'we follow  #TAUTHOR_TAG to restrict that sentences are no longer than 50 words.', 'the concatenation of news - test - 2012 and news - test - 2013 is used as the validation set and news - test - 2014 as the test set.', 'the french - english evaluation sets can be easily obtained by reversing the english - french datasets.', 'we compared our approach with two state - of - the - art smt and nmt systems : [  #AUTHOR_TAG ].', 'groundhog is an attention - based neural machine translation system  #TAUTHOR_TAG.', 'we introduce agreement - based joint training for bidirectional attention - based nmt.', 'nist06 is the validation set and nist02 - 05, 08 are test sets.', 'the bleu scores are case - insensitive. "" * "" : significantly better than moses ( p < 0. 05 ) ; "" * * "" : significantly better than moses ( p < 0. 01 ) ; "" + "" : significantly better than groundhog with independent training ( p < 0. 05 ) ; "" + + "" : significantly better than groundhog with independent training ( p < 0. 01 ).', 'table 2 : results on the chinese - english word alignment task.', 'the evaluation metric is alignment error rate. "" * * "" : significantly better than groundhog with independent training ( p < 0. 01 ).', '1. moses [  #AUTHOR_TAG ] : a phrase - based smt system ; 2. groundhog  #TAUTHOR_TAG : an attention - based nmt system.', 'for moses, we used the parallel corpus to train the phrase - based translation model and the targetside part of the parallel corpus to train a 4 - gram language model using the srilm toolkit [  #AUTHOR_TAG ].', 'for groundhog']",6
"['##hog  #TAUTHOR_TAG, we find that modeling the structural divergence of natural languages is so challenging that unidirectional models']","['by groundhog  #TAUTHOR_TAG, we find that modeling the structural divergence of natural languages is so challenging that unidirectional models']","['##hog  #TAUTHOR_TAG, we find that modeling the structural divergence of natural languages is so challenging that unidirectional models']","['analyzing the alignment matrices generated by groundhog  #TAUTHOR_TAG, we find that modeling the structural divergence of natural languages is so challenging that unidirectional models can only capture part of alignment regularities.', 'this finding inspires us to improve attention - based nmt by combining two unidirectional models.', 'in this work, we only apply agreement - based joint learning to groundhog.', 'as our approach does not assume specific network architectures, it is possible to apply it to the models proposed by luong et al. [ 2015 ]']",1
"['alone  #TAUTHOR_TAG and in work in synchronous tag semantics, constructions']","['be modeled with tl - mctag alone  #TAUTHOR_TAG and in work in synchronous tag semantics, constructions']","['alone  #TAUTHOR_TAG and in work in synchronous tag semantics, constructions']","['- adjoining grammar ( tag ) has long been popular for natural language applications because of its ability to naturally capture syntactic relationships while also remaining efficient to process.', 'more recent applications of tag to the domain of semantics as well as new attention to syntactic phenomena such as scrambling have given rise to increased interested in multicomponent tag formalisms ( mc - tag ), which extend the flexibility, and in some cases generative capacity of the formalism but also have substantial costs in terms of efficient processing.', 'much work in tag semantics makes use of tree - local mctag ( tl - mctag ) to model phenomena such as quantifier scoping, wh - question formation, and many other constructions.', 'certain applications, however, appear to require even more flexibility than is provided by tl - mctag.', 'scrambling is one well - known example  #AUTHOR_TAG.', 'in addition, in the semantics domain, the use of a new tag operation, flexible composition, is used to perform certain semantic operations that seemingly cannot be modeled with tl - mctag alone  #TAUTHOR_TAG and in work in synchronous tag semantics, constructions such as nested quantifiers require a set - local mctag ( sl - mctag ) analysis  #AUTHOR_TAG.', 'in this paper we suggest a shift in focus from constraining locality and complexity through restrictions that all trees in a tree set must adjoin within a single tree or tree set to constraining locality and complexity through restrictions on the derivational distance between trees in the same tree set in a valid derivation.', 'we examine three formalisms, two of them introduced in this work for the first time, that use derivational distance to constrain locality and demonstrate by construction of parsers their relationship to tl - mctag in both expressivity and complexity.', 'in section 2 we give a very brief introduction to tag.', 'in section 3 we elaborate further the distinction between these two types of locality restrictions using tag derivation trees.', 'section 4 briefly addresses the simultaneity requirement present in mctag formalisms but not in vector - tag formalisms and argues for dropping the requirement.', 'in sections 5 and 6 we introduce two novel formalisms, restricted non - simultaneous mc - tag and restricted vector - tag, respectively, and define cky - style parsers for them.', 'in section 7 we recall the delayed tl - mctag formalism introduced by  #TAUTHOR_TAG and define a cky - style parser for it as well.', 'in section 8 we explore the complexity of all three parsers and the relationship between the formalisms.', 'in section 9 we discuss the linguistic applications of these formalisms and show that they']",0
"['##ed.', 'there are a few applications, including flexible composition and scrambling in free - word order languages that benefit from tag - based grammars that drop the simultaneity requirement  #TAUTHOR_TAG.', 'from a complexity']","['is strictly obeyed.', 'there are a few applications, including flexible composition and scrambling in free - word order languages that benefit from tag - based grammars that drop the simultaneity requirement  #TAUTHOR_TAG.', 'from a complexity perspective, however, checking the simultaneity requirement is expensive  #AUTHOR_TAG.', 'as a result, it can be advantageous to select a base formalism that does not require simultaneity even if the grammars implemented with it do not make use of that additional freedom']","['##ed.', 'there are a few applications, including flexible composition and scrambling in free - word order languages that benefit from tag - based grammars that drop the simultaneity requirement  #TAUTHOR_TAG.', 'from a complexity perspective, however, checking the simultaneity requirement is expensive  #AUTHOR_TAG.', 'as a result, it can be advantageous to select a base formalism that does not require simultane']","['addition to lexical locality constraints the definition of mctag requires that all trees from a set adjoin simultaneously.', 'in terms of well - formed derivation trees, this amounts to disallowing derivations in which a tree from a given set is the ancestor of a tree from the same tree set.', 'for most linguistic applications of tag, this requirement seems natural and is strictly obeyed.', 'there are a few applications, including flexible composition and scrambling in free - word order languages that benefit from tag - based grammars that drop the simultaneity requirement  #TAUTHOR_TAG.', 'from a complexity perspective, however, checking the simultaneity requirement is expensive  #AUTHOR_TAG.', 'as a result, it can be advantageous to select a base formalism that does not require simultaneity even if the grammars implemented with it do not make use of that additional freedom']",0
[' #TAUTHOR_TAG introduce the delayed tl - mctag formalism which'],"['delayed tl - mctag  #TAUTHOR_TAG introduce the delayed tl - mctag formalism which makes use of a derivational distance restriction in a somewhat different way. rather', 'than restricting the absolute distance between the trees of']",['delayed tl - mctag  #TAUTHOR_TAG introduce the delayed tl - mctag formalism which makes use of a derivational distance restriction in a somewhat different way'],"['', 'accurately. we note that in order to allow a non - total ordering of the trees in a vector we would simply have to record all trees in a tree vector in the histories as', 'is done in the delayed tl - mctag parser. 7 delayed tl - mctag  #TAUTHOR_TAG introduce the delayed tl - mctag formalism which makes use of a derivational distance restriction in a somewhat different way. rather', '']",0
"['alone  #TAUTHOR_TAG and in work in synchronous tag semantics, constructions']","['be modeled with tl - mctag alone  #TAUTHOR_TAG and in work in synchronous tag semantics, constructions']","['alone  #TAUTHOR_TAG and in work in synchronous tag semantics, constructions']","['- adjoining grammar ( tag ) has long been popular for natural language applications because of its ability to naturally capture syntactic relationships while also remaining efficient to process.', 'more recent applications of tag to the domain of semantics as well as new attention to syntactic phenomena such as scrambling have given rise to increased interested in multicomponent tag formalisms ( mc - tag ), which extend the flexibility, and in some cases generative capacity of the formalism but also have substantial costs in terms of efficient processing.', 'much work in tag semantics makes use of tree - local mctag ( tl - mctag ) to model phenomena such as quantifier scoping, wh - question formation, and many other constructions.', 'certain applications, however, appear to require even more flexibility than is provided by tl - mctag.', 'scrambling is one well - known example  #AUTHOR_TAG.', 'in addition, in the semantics domain, the use of a new tag operation, flexible composition, is used to perform certain semantic operations that seemingly cannot be modeled with tl - mctag alone  #TAUTHOR_TAG and in work in synchronous tag semantics, constructions such as nested quantifiers require a set - local mctag ( sl - mctag ) analysis  #AUTHOR_TAG.', 'in this paper we suggest a shift in focus from constraining locality and complexity through restrictions that all trees in a tree set must adjoin within a single tree or tree set to constraining locality and complexity through restrictions on the derivational distance between trees in the same tree set in a valid derivation.', 'we examine three formalisms, two of them introduced in this work for the first time, that use derivational distance to constrain locality and demonstrate by construction of parsers their relationship to tl - mctag in both expressivity and complexity.', 'in section 2 we give a very brief introduction to tag.', 'in section 3 we elaborate further the distinction between these two types of locality restrictions using tag derivation trees.', 'section 4 briefly addresses the simultaneity requirement present in mctag formalisms but not in vector - tag formalisms and argues for dropping the requirement.', 'in sections 5 and 6 we introduce two novel formalisms, restricted non - simultaneous mc - tag and restricted vector - tag, respectively, and define cky - style parsers for them.', 'in section 7 we recall the delayed tl - mctag formalism introduced by  #TAUTHOR_TAG and define a cky - style parser for it as well.', 'in section 8 we explore the complexity of all three parsers and the relationship between the formalisms.', 'in section 9 we discuss the linguistic applications of these formalisms and show that they']",5
[' #TAUTHOR_TAG introduce the delayed tl - mctag formalism which'],"['delayed tl - mctag  #TAUTHOR_TAG introduce the delayed tl - mctag formalism which makes use of a derivational distance restriction in a somewhat different way. rather', 'than restricting the absolute distance between the trees of']",['delayed tl - mctag  #TAUTHOR_TAG introduce the delayed tl - mctag formalism which makes use of a derivational distance restriction in a somewhat different way'],"['', 'accurately. we note that in order to allow a non - total ordering of the trees in a vector we would simply have to record all trees in a tree vector in the histories as', 'is done in the delayed tl - mctag parser. 7 delayed tl - mctag  #TAUTHOR_TAG introduce the delayed tl - mctag formalism which makes use of a derivational distance restriction in a somewhat different way. rather', '']",5
[' #TAUTHOR_TAG introduce the delayed tl - mctag formalism which'],"['delayed tl - mctag  #TAUTHOR_TAG introduce the delayed tl - mctag formalism which makes use of a derivational distance restriction in a somewhat different way. rather', 'than restricting the absolute distance between the trees of']",['delayed tl - mctag  #TAUTHOR_TAG introduce the delayed tl - mctag formalism which makes use of a derivational distance restriction in a somewhat different way'],"['', 'accurately. we note that in order to allow a non - total ordering of the trees in a vector we would simply have to record all trees in a tree vector in the histories as', 'is done in the delayed tl - mctag parser. 7 delayed tl - mctag  #TAUTHOR_TAG introduce the delayed tl - mctag formalism which makes use of a derivational distance restriction in a somewhat different way. rather', '']",4
"['describe semantics and pragmatics, as well as syntax and behavior. my own slow progress  #TAUTHOR_TAG shows that there']","['describe semantics and pragmatics, as well as syntax and behavior. my own slow progress  #TAUTHOR_TAG shows that there']","['is that these models must describe semantics and pragmatics, as well as syntax and behavior. my own slow progress  #TAUTHOR_TAG shows that there']","['', 'response is more likely to resolve a pending task ambiguity ( de  #AUTHOR_TAG.', 'i expect many further exciting developments in our understanding of meaning and interpretation as we enrich the social intelligence of nl', '##g. modeling efforts will remain crucial to the exploration of these new capabilities. when we build and assemble models of actions and interpretations, we get systems', ""that can plan their own behavior simply by exploiting what they know about communication. these systems give new evidence about the information and problem - solving that's involved. the"", ""challenge is that these models must describe semantics and pragmatics, as well as syntax and behavior. my own slow progress  #TAUTHOR_TAG shows that there's still lots of hard work needed to develop suitable techniques"", '. i keep going because of the methodological payoffs i see on the horizon. modeling lets us take social intelligence seriously as a general implementation principle, and thus to aim for', ""systems whose multimodal behavior matches the flexibility and coordination that distinguishes our own embodied meanings. more generally, modeling replaces programming with data fitting, and a good model of action and interpretation in particular would let an agent's own"", 'experience in conversational interaction determine the repertoire of behaviors and meanings it uses to make itself understood']",1
"['describe semantics and pragmatics, as well as syntax and behavior. my own slow progress  #TAUTHOR_TAG shows that there']","['describe semantics and pragmatics, as well as syntax and behavior. my own slow progress  #TAUTHOR_TAG shows that there']","['is that these models must describe semantics and pragmatics, as well as syntax and behavior. my own slow progress  #TAUTHOR_TAG shows that there']","['', 'response is more likely to resolve a pending task ambiguity ( de  #AUTHOR_TAG.', 'i expect many further exciting developments in our understanding of meaning and interpretation as we enrich the social intelligence of nl', '##g. modeling efforts will remain crucial to the exploration of these new capabilities. when we build and assemble models of actions and interpretations, we get systems', ""that can plan their own behavior simply by exploiting what they know about communication. these systems give new evidence about the information and problem - solving that's involved. the"", ""challenge is that these models must describe semantics and pragmatics, as well as syntax and behavior. my own slow progress  #TAUTHOR_TAG shows that there's still lots of hard work needed to develop suitable techniques"", '. i keep going because of the methodological payoffs i see on the horizon. modeling lets us take social intelligence seriously as a general implementation principle, and thus to aim for', ""systems whose multimodal behavior matches the flexibility and coordination that distinguishes our own embodied meanings. more generally, modeling replaces programming with data fitting, and a good model of action and interpretation in particular would let an agent's own"", 'experience in conversational interaction determine the repertoire of behaviors and meanings it uses to make itself understood']",2
"['', 'however, several recent studies show that training word embeddings in large corpora could lead to encoding societal biases present in these human - produced data  #TAUTHOR_TAG']","['on many nlp tasks.', 'however, several recent studies show that training word embeddings in large corpora could lead to encoding societal biases present in these human - produced data  #TAUTHOR_TAG']","['mc  #AUTHOR_TAG have led to huge performance improvement on many nlp tasks.', 'however, several recent studies show that training word embeddings in large corpora could lead to encoding societal biases present in these human - produced data  #TAUTHOR_TAG.', 'in this work, we extend these analyses to']","['representations of words in the form of word embeddings  #AUTHOR_TAG and contextualized word embeddings  #AUTHOR_TAG mc  #AUTHOR_TAG have led to huge performance improvement on many nlp tasks.', 'however, several recent studies show that training word embeddings in large corpora could lead to encoding societal biases present in these human - produced data  #TAUTHOR_TAG.', 'in this work, we extend these analyses to the elmo contextualized word embeddings.', 'our work provides a new intrinsic analysis of how elmo represents gender in biased ways.', 'first, the corpus used for training elmo has a significant gender skew : male entities are nearly three times more common than female entities, which leads to gender bias in the downloadable pre - trained contextualized embeddings.', 'then, we apply principal component analysis ( pca ) to show that after training on such biased corpora, there exists a lowdimensional subspace that captures much of the gender information in the contextualized embeddings.', 'finally, we evaluate how faithfully elmo preserves gender information in sentences by measuring how predictable gender is from elmo representations of occupation words that co - occur with gender revealing pronouns.', 'our results show that elmo embeddings perform unequally on male and female pronouns : male entities can be predicted from occupation words 14 % more accurately than female entities.', 'in addition, we examine how gender bias in elmo propagates to the downstream applications.', ""specifically, we evaluate a state - of - the - art coreference resolution system ) that makes use of elmo's contextual embeddings on winobias  #AUTHOR_TAG a ), a coreference diagnostic dataset that evaluates whether systems behave differently on decisions involving male and female entities of stereotyped or anti - stereotyped occupations."", 'we find that in the most challenging setting, the elmo - based system has a disparity in accuracy between pro - and anti - stereotypical predictions, which is nearly 30 % higher than a similar system based on glove  #AUTHOR_TAG.', '']",0
"['representations,  #TAUTHOR_TAG and  #AUTHOR_TAG show that word embeddings encode societal biases']","['representations,  #TAUTHOR_TAG and  #AUTHOR_TAG show that word embeddings encode societal biases']","['word representations,  #TAUTHOR_TAG and  #AUTHOR_TAG show that word embeddings encode societal biases']","['bias has been shown to affect several realworld applications relying on automatic language analysis, including online news  #AUTHOR_TAG, advertisements  #AUTHOR_TAG, abusive language detection  #AUTHOR_TAG, machine translation ( font and costa - jussa, 2019 ;  #AUTHOR_TAG, and web search  #AUTHOR_TAG.', 'in many cases, a model not only replicates bias in the training data but also amplifies it  #AUTHOR_TAG.', 'for word representations,  #TAUTHOR_TAG and  #AUTHOR_TAG show that word embeddings encode societal biases about gender roles and occupations, e. g. engineers are stereotypically men, and nurses are stereotypically women.', 'as a consequence, downstream applications that use these pretrained word embeddings also reflect this bias.', 'for example,  #AUTHOR_TAG a ) and  #AUTHOR_TAG show that coreference resolution systems relying on word embeddings encode such occupational stereotypes.', 'in concurrent work,  #AUTHOR_TAG measure gender bias in sentence embeddings, but their evaluation is on the aggregation of word representations.', 'in contrast, we analyze bias in contextualized word representations and its effect on a downstream task.', 'to mitigate bias from word embeddings,  #TAUTHOR_TAG propose a post - processing method to project out the bias subspace from the pre - trained embeddings.', 'their method is shown to reduce the gender information from the embeddings of gender - neutral words, and, remarkably, maintains the same level of performance on different downstream nlp tasks.', ' #AUTHOR_TAG b ) further propose a training mechanism to separate gender information from other factors.', ' #AUTHOR_TAG argue that entirely removing bias is difficult, if not impossible, and the gender bias information can be often recovered.', 'this paper investigates a natural follow - up question : what are effective bias mitigation techniques for contextualized embeddings']",0
"['representations,  #TAUTHOR_TAG and  #AUTHOR_TAG show that word embeddings encode societal biases']","['representations,  #TAUTHOR_TAG and  #AUTHOR_TAG show that word embeddings encode societal biases']","['word representations,  #TAUTHOR_TAG and  #AUTHOR_TAG show that word embeddings encode societal biases']","['bias has been shown to affect several realworld applications relying on automatic language analysis, including online news  #AUTHOR_TAG, advertisements  #AUTHOR_TAG, abusive language detection  #AUTHOR_TAG, machine translation ( font and costa - jussa, 2019 ;  #AUTHOR_TAG, and web search  #AUTHOR_TAG.', 'in many cases, a model not only replicates bias in the training data but also amplifies it  #AUTHOR_TAG.', 'for word representations,  #TAUTHOR_TAG and  #AUTHOR_TAG show that word embeddings encode societal biases about gender roles and occupations, e. g. engineers are stereotypically men, and nurses are stereotypically women.', 'as a consequence, downstream applications that use these pretrained word embeddings also reflect this bias.', 'for example,  #AUTHOR_TAG a ) and  #AUTHOR_TAG show that coreference resolution systems relying on word embeddings encode such occupational stereotypes.', 'in concurrent work,  #AUTHOR_TAG measure gender bias in sentence embeddings, but their evaluation is on the aggregation of word representations.', 'in contrast, we analyze bias in contextualized word representations and its effect on a downstream task.', 'to mitigate bias from word embeddings,  #TAUTHOR_TAG propose a post - processing method to project out the bias subspace from the pre - trained embeddings.', 'their method is shown to reduce the gender information from the embeddings of gender - neutral words, and, remarkably, maintains the same level of performance on different downstream nlp tasks.', ' #AUTHOR_TAG b ) further propose a training mechanism to separate gender information from other factors.', ' #AUTHOR_TAG argue that entirely removing bias is difficult, if not impossible, and the gender bias information can be often recovered.', 'this paper investigates a natural follow - up question : what are effective bias mitigation techniques for contextualized embeddings']",0
"['', 'however, several recent studies show that training word embeddings in large corpora could lead to encoding societal biases present in these human - produced data  #TAUTHOR_TAG']","['on many nlp tasks.', 'however, several recent studies show that training word embeddings in large corpora could lead to encoding societal biases present in these human - produced data  #TAUTHOR_TAG']","['mc  #AUTHOR_TAG have led to huge performance improvement on many nlp tasks.', 'however, several recent studies show that training word embeddings in large corpora could lead to encoding societal biases present in these human - produced data  #TAUTHOR_TAG.', 'in this work, we extend these analyses to']","['representations of words in the form of word embeddings  #AUTHOR_TAG and contextualized word embeddings  #AUTHOR_TAG mc  #AUTHOR_TAG have led to huge performance improvement on many nlp tasks.', 'however, several recent studies show that training word embeddings in large corpora could lead to encoding societal biases present in these human - produced data  #TAUTHOR_TAG.', 'in this work, we extend these analyses to the elmo contextualized word embeddings.', 'our work provides a new intrinsic analysis of how elmo represents gender in biased ways.', 'first, the corpus used for training elmo has a significant gender skew : male entities are nearly three times more common than female entities, which leads to gender bias in the downloadable pre - trained contextualized embeddings.', 'then, we apply principal component analysis ( pca ) to show that after training on such biased corpora, there exists a lowdimensional subspace that captures much of the gender information in the contextualized embeddings.', 'finally, we evaluate how faithfully elmo preserves gender information in sentences by measuring how predictable gender is from elmo representations of occupation words that co - occur with gender revealing pronouns.', 'our results show that elmo embeddings perform unequally on male and female pronouns : male entities can be predicted from occupation words 14 % more accurately than female entities.', 'in addition, we examine how gender bias in elmo propagates to the downstream applications.', ""specifically, we evaluate a state - of - the - art coreference resolution system ) that makes use of elmo's contextual embeddings on winobias  #AUTHOR_TAG a ), a coreference diagnostic dataset that evaluates whether systems behave differently on decisions involving male and female entities of stereotyped or anti - stereotyped occupations."", 'we find that in the most challenging setting, the elmo - based system has a disparity in accuracy between pro - and anti - stereotypical predictions, which is nearly 30 % higher than a similar system based on glove  #AUTHOR_TAG.', '']",1
"['representations,  #TAUTHOR_TAG and  #AUTHOR_TAG show that word embeddings encode societal biases']","['representations,  #TAUTHOR_TAG and  #AUTHOR_TAG show that word embeddings encode societal biases']","['word representations,  #TAUTHOR_TAG and  #AUTHOR_TAG show that word embeddings encode societal biases']","['bias has been shown to affect several realworld applications relying on automatic language analysis, including online news  #AUTHOR_TAG, advertisements  #AUTHOR_TAG, abusive language detection  #AUTHOR_TAG, machine translation ( font and costa - jussa, 2019 ;  #AUTHOR_TAG, and web search  #AUTHOR_TAG.', 'in many cases, a model not only replicates bias in the training data but also amplifies it  #AUTHOR_TAG.', 'for word representations,  #TAUTHOR_TAG and  #AUTHOR_TAG show that word embeddings encode societal biases about gender roles and occupations, e. g. engineers are stereotypically men, and nurses are stereotypically women.', 'as a consequence, downstream applications that use these pretrained word embeddings also reflect this bias.', 'for example,  #AUTHOR_TAG a ) and  #AUTHOR_TAG show that coreference resolution systems relying on word embeddings encode such occupational stereotypes.', 'in concurrent work,  #AUTHOR_TAG measure gender bias in sentence embeddings, but their evaluation is on the aggregation of word representations.', 'in contrast, we analyze bias in contextualized word representations and its effect on a downstream task.', 'to mitigate bias from word embeddings,  #TAUTHOR_TAG propose a post - processing method to project out the bias subspace from the pre - trained embeddings.', 'their method is shown to reduce the gender information from the embeddings of gender - neutral words, and, remarkably, maintains the same level of performance on different downstream nlp tasks.', ' #AUTHOR_TAG b ) further propose a training mechanism to separate gender information from other factors.', ' #AUTHOR_TAG argue that entirely removing bias is difficult, if not impossible, and the gender bias information can be often recovered.', 'this paper investigates a natural follow - up question : what are effective bias mitigation techniques for contextualized embeddings']",1
"['which only has one  #TAUTHOR_TAG.', 'the two principal components in el']","['glove which only has one  #TAUTHOR_TAG.', 'the two principal components in elmo seem to represent the gender from the contextual information ( contextual gender ) as well as the gender embedded in']","['which only has one  #TAUTHOR_TAG.', 'the two principal components in el']","[', we analyze the gender subspace in elmo.', 'we first sample 400 sentences with at least one gendered word ( e. g., he or she from the ontonotes 5. 0 dataset  #AUTHOR_TAG and generate the corresponding gender - swapped variants ( changing he to she and vice - versa ).', 'we then calculate the difference of elmo embeddings between occupation words in corresponding sentences and conduct principal component analysis for all pairs of sentences.', 'figure 1 shows there are two principal components for gender in elmo, in contrast to glove which only has one  #TAUTHOR_TAG.', 'the two principal components in elmo seem to represent the gender from the contextual information ( contextual gender ) as well as the gender embedded in the word itself ( occupational gender ).', 'to visualize the gender subspace, we pick a few sentence pairs from winobias  #AUTHOR_TAG a ).', 'each sentence in the corpus contains one gendered pronoun and two occupation words, such as "" the developer corrected the secretary because she made a mistake "" and also the same sentence with the opposite pronoun ( he ).', 'in figure 1 on the right, we project the elmo embeddings of occupation words that are co - referent with the pronoun ( e. g. secretary in the above example ) for when the pronoun is male ( blue dots ) and female ( orange dots ) on the two principal components from the pca analysis.', 'qualitatively, we can see the first component separates male and female contexts while the second component groups male related words such as lawyer and developer and female related words such as cashier and nurse']",4
['word embeddings from  #TAUTHOR_TAG'],['word embeddings from  #TAUTHOR_TAG'],"['word embeddings from  #TAUTHOR_TAG.', 'we evaluate the performance of both aspects of this approach']","['', ' #AUTHOR_TAG a ) propose a method to reduce gender bias in coreference resolution by augmenting the training corpus for this task.', 'data augmentation is performed by replacing gender revealing entities in the ontonotes dataset with words indicating the opposite gender and then training on the union of the original data and this swapped data.', 'in addition, they find it useful to also mitigate bias in supporting resources and therefore replace standard glove embeddings with bias mitigated word embeddings from  #TAUTHOR_TAG.', 'we evaluate the performance of both aspects of this approach']",5
"['in subsets of the  #TAUTHOR_TAG datasets, proving that']","['in subsets of the  #TAUTHOR_TAG datasets, proving that']","['in subsets of the  #TAUTHOR_TAG datasets, proving that it is in fact competitive.', '']","['', 'when the classification is binary, root9 achieves the following results against the baseline : hypernyms - co - hyponyms 95. 7 % vs. 69. 8 %, hypernyms - random 91. 8 % vs. 64. 1 % and co - hyponyms - random 97. 8 % vs. 79. 4 %.', 'in order to compare the performance with the state - of - the - art, we have also evaluated root9 in subsets of the  #TAUTHOR_TAG datasets, proving that it is in fact competitive.', 'finally, we investigated whether the system learns the semantic relation or it simply learns the prototypical hypernyms, as claimed by  #AUTHOR_TAG.', 'the second possibility seems to be the most likely, even though root9 can be trained on negative examples ( i. e., switched hypernyms ) to drastically reduce this bias']",5
"['in the  #TAUTHOR_TAG datasets. unfortunately,']","['in the  #TAUTHOR_TAG datasets. unfortunately, root9 was', 'not able to cover']","['- of - the - art, we have also evaluated it in the  #TAUTHOR_TAG datasets. unfortunately,']","['', 'and 97. 8 % for co - hyponyms and randoms. in order to compare root9 with the state - of - the - art, we have also evaluated it in the  #TAUTHOR_TAG datasets. unfortunately, root9 was', 'not able to cover the full datasets, as', 'several words in their pairs were missing from our distributional semantic model ( dsm ) because of their low frequency. nevertheless, the authors kindly provided the results of their models on our subsets, so that the comparison can be considered reliable. also in 1 the 9, 600 pairs are available at https : / / github. com / esantus / root9 relation to the state', 'of the art, root9 is proved to be competitive, being slightly outperformed in all the datasets only by the svmcat model  #TAUTHOR_TAG, which is a support vector machine ( svm ) classifier run on the', 'concatenation of the distributional vectors of the words in the pairs. finally, we carried out an extra test to verify whether the system was actually learning the semantic relation between two word pairs', ', or simply identifying prototypical hypernyms ( levy et al. 2015 ). the test consisted in providing to the trained model switched hypernyms ( e. g. from "" dog hyper animal "" to "" dog random fruit "" ), and verify how they were classified. our results show that most of the switched hypernyms were in fact misclass', '##ified as hypernyms ( especially when the words in the switched hypernyms were the same used to train the model on the', 'real hypernyms ), and that the only way to overcome such problem is to explicitly provide the model with bad examples ( i. e., switched hypernyms tagged', 'as randoms ) during the training']",5
"[', and - in particularagainst the best performant models in  #TAUTHOR_TAG ;']","['of the art, and - in particularagainst the best performant models in  #TAUTHOR_TAG ;']","['of the art, and - in particularagainst the best performant models in  #TAUTHOR_TAG ;']","['have performed three tasks : i ) an ablation test to evaluate the contribution of the features on our dataset ( henceforth, root9 dataset ; see section 4. 2 ) ; ii ) an evaluation against the state of the art, and - in particularagainst the best performant models in  #TAUTHOR_TAG ; iii ) an evaluation on switched pairs to verify whether the actual semantic relations or the prototypical hypernyms  #AUTHOR_TAG were learnt.', 'for what concerns the ablation test, we performed it on a tree - classes classification task ( hypernyms, co - hyponyms and randoms ), removing each feature at a time and measuring the loss / gain ( f1 score is used for the evaluation on a 10 - fold cross validation ).', 'thanks to this task, we have found that four of our features were in fact redundant, and we have therefore removed them from the final model, turning root13 into root9.', 'this is discussed in section 5.', 'once the best model has been identified, we have performed three binary classification tasks, involving only two classes per time.', 'f1 score on a 10 - fold cross validation was chosen as accuracy measure.', 'the second task, which is described in section 6, consisted in binary classification tasks on the four datasets proposed by  #TAUTHOR_TAG.', 'these datasets are described below, in section 4. 3.', 'the task allowed us to compare root9 against the state of the art models reported in  #TAUTHOR_TAG.', 'the last task is described in section 7.', 'it was performed on an extended root9 dataset, including also 3, 200 randomly switched hypernyms to verify whether they were classified as hypernyms or as randoms']",5
"[', and - in particularagainst the best performant models in  #TAUTHOR_TAG ;']","['of the art, and - in particularagainst the best performant models in  #TAUTHOR_TAG ;']","['of the art, and - in particularagainst the best performant models in  #TAUTHOR_TAG ;']","['have performed three tasks : i ) an ablation test to evaluate the contribution of the features on our dataset ( henceforth, root9 dataset ; see section 4. 2 ) ; ii ) an evaluation against the state of the art, and - in particularagainst the best performant models in  #TAUTHOR_TAG ; iii ) an evaluation on switched pairs to verify whether the actual semantic relations or the prototypical hypernyms  #AUTHOR_TAG were learnt.', 'for what concerns the ablation test, we performed it on a tree - classes classification task ( hypernyms, co - hyponyms and randoms ), removing each feature at a time and measuring the loss / gain ( f1 score is used for the evaluation on a 10 - fold cross validation ).', 'thanks to this task, we have found that four of our features were in fact redundant, and we have therefore removed them from the final model, turning root13 into root9.', 'this is discussed in section 5.', 'once the best model has been identified, we have performed three binary classification tasks, involving only two classes per time.', 'f1 score on a 10 - fold cross validation was chosen as accuracy measure.', 'the second task, which is described in section 6, consisted in binary classification tasks on the four datasets proposed by  #TAUTHOR_TAG.', 'these datasets are described below, in section 4. 3.', 'the task allowed us to compare root9 against the state of the art models reported in  #TAUTHOR_TAG.', 'the last task is described in section 7.', 'it was performed on an extended root9 dataset, including also 3, 200 randomly switched hypernyms to verify whether they were classified as hypernyms or as randoms']",5
"[', and - in particularagainst the best performant models in  #TAUTHOR_TAG ;']","['of the art, and - in particularagainst the best performant models in  #TAUTHOR_TAG ;']","['of the art, and - in particularagainst the best performant models in  #TAUTHOR_TAG ;']","['have performed three tasks : i ) an ablation test to evaluate the contribution of the features on our dataset ( henceforth, root9 dataset ; see section 4. 2 ) ; ii ) an evaluation against the state of the art, and - in particularagainst the best performant models in  #TAUTHOR_TAG ; iii ) an evaluation on switched pairs to verify whether the actual semantic relations or the prototypical hypernyms  #AUTHOR_TAG were learnt.', 'for what concerns the ablation test, we performed it on a tree - classes classification task ( hypernyms, co - hyponyms and randoms ), removing each feature at a time and measuring the loss / gain ( f1 score is used for the evaluation on a 10 - fold cross validation ).', 'thanks to this task, we have found that four of our features were in fact redundant, and we have therefore removed them from the final model, turning root13 into root9.', 'this is discussed in section 5.', 'once the best model has been identified, we have performed three binary classification tasks, involving only two classes per time.', 'f1 score on a 10 - fold cross validation was chosen as accuracy measure.', 'the second task, which is described in section 6, consisted in binary classification tasks on the four datasets proposed by  #TAUTHOR_TAG.', 'these datasets are described below, in section 4. 3.', 'the task allowed us to compare root9 against the state of the art models reported in  #TAUTHOR_TAG.', 'the last task is described in section 7.', 'it was performed on an extended root9 dataset, including also 3, 200 randomly switched hypernyms to verify whether they were classified as hypernyms or as randoms']",5
"[' #TAUTHOR_TAG.', '2 these are four']","[' #TAUTHOR_TAG.', '2 these are four datasets, containing respectively : i ) hypernyms']","[' #TAUTHOR_TAG.', '2 these are four datasets, containing']","['order to compare root9 to the state - of - the - art, we have evaluated it with the datasets created by  #TAUTHOR_TAG.', '2 these are four datasets, containing respectively : i ) hypernyms versus other relations ( extracted from wordnet ; henceforth wn hyper ) ; ii ) co - hyponyms versus other relations ( extracted from wordnet ; henceforth wn co - hyp ) ; iii ) hypernyms versus other relations ( extracted from bless ; henceforth bless hyper ) ; iv ) co - hyponyms versus other relations ( extracted from bless ; henceforth bless co - hyp ).', 'the wn dataset  #TAUTHOR_TAG - meaning both wn hyper and wn co - hyp - in particular, was built after noticing that supervised systems tended to perform well also on random vectors.', 'this happens because they are able to learn ontological information and re - use it whenever the words re - appear in other pairs.', 'for this reason, the authors have constructed a dataset where words occurred at most twice ( once on the left and once on the right of the relation ).', 'in this dataset, ontological information cannot be learnt and re - used, and indeed the random vectors cannot perform well.', 'unfortunately our dsm did not cover the whole datasets, because of the chosen frequency threshold ( in table 1, we report the size of our subsets in comparison to the original datasets ).', '']",5
['by  #TAUTHOR_TAG namely that random vectors'],['by  #TAUTHOR_TAG namely that random vectors'],['by  #TAUTHOR_TAG namely that random vectors'],"['our internal tests, we have implemented two baselines, which can be used as reference for evaluating the performance of root9 : cosine and random13.', 'the first baseline simply uses the vector cosine ( cosine ) with a random forest classifier in the default settings ( i. e. 100 trees, 1 seed, and maxdepth and numfeatures initialized to 0 ).', 'this baseline is supposed to perform particularly well in discriminating similar words ( i. e. hypernyms and co - hyponyms ) from randoms.', 'in fact, this measure has been extensively used to identify word similarity in vector spaces  #AUTHOR_TAG because it verifies the normalized correlation between the vectors of 1 and 2 :', 'where is the i - th dimension in the vector x. the second baseline ( random13 ) relies on a default random forest classifier, but uses thirteen randomly initialized features, with values between 0 and 1.', 'while the vector cosine achieves a reasonable accuracy, which is anyway far below the results obtained by our model, the random baseline performs much worst.', 'the discrepancy with what found by  #TAUTHOR_TAG namely that random vectors perform particularly well when words are re - used in the dataset - may depend on the small number of features, which does not allow the system to identify discriminative random dimensions.', 'in the second task ( see section 6 ), we have used as baselines the most competitive models reported in  #TAUTHOR_TAG, namely the svm classifiers trained on the ppmi vector of the second word ( svmsingle ), or on the concatenated ( svmcat ), summed ( svmadd ), multiplied ( svmmult ) and subtracted ( svmdiff ) ppmi vectors of the words in the pair.', 'such vectors contain as features all major grammatical dependency relations involving open class parts of speech.', 'also, the performance of three main unsupervised methods is reported as a reference : cosine ( see above in this section ), balapinc  #AUTHOR_TAG and invcl  #AUTHOR_TAG.', 'a threshold p empirically found in a training set was used in these methods for the decision, table 2.', 'ablation test, f1 scores on a 10 - fold cross validation and loss / gain values.', 'scores are in percent']",5
"['the other classifiers in this dataset.', 'however, it is worth noticing here that such difference disappears with the wn datasets proposed by  #TAUTHOR_TAG.', 'see section 6, and - in']","['the other classifiers in this dataset.', 'however, it is worth noticing here that such difference disappears with the wn datasets proposed by  #TAUTHOR_TAG.', 'see section 6, and - in']","['the other classifiers in this dataset.', 'however, it is worth noticing here that such difference disappears with the wn datasets proposed by  #TAUTHOR_TAG.', 'see section 6, and - in']","['', 'this system outperforms all the baselines ( cosine, random13 ) and root13.', 'for the sake of completeness, in table 2 we also report the performance of root9 using logistic regression  #AUTHOR_TAG and smo  #AUTHOR_TAG classifiers.', 'as it can be seen, the random forest version largely outperforms the other classifiers in this dataset.', 'however, it is worth noticing here that such difference disappears with the wn datasets proposed by  #TAUTHOR_TAG.', 'see section 6, and - in particular - table 3.', 'f1 scores on a 10 - fold cross validation for binary classification tasks.', 'scores are in percent.', 'table 3 describes the results of root9 and the baseline in the binary classification tasks.', 'these results confirm the analysis suggested above']",5
"['systems reported by  #TAUTHOR_TAG.', 'the scores']","['systems reported by  #TAUTHOR_TAG.', 'the scores']","['s performance compared to the best systems reported by  #TAUTHOR_TAG.', 'the scores']","[""table 4, we show root9's performance compared to the best systems reported by  #TAUTHOR_TAG."", ""the scores are all calculated on subsets of  #TAUTHOR_TAG's datasets, as reported in section 4. 3."", 'considering all the datasets, root9 is the second best performing system, after svmcat  #TAUTHOR_TAG, which uses the svm classifier on the concatenation of ppmi vectors, containing as features all major grammatical dependency relations involving open class parts of speech.', 'the svm classifier on the sum ( svmadd ) and the multiplication ( svmmult ) of the same ppmi vectors performs better in identifying co - hyponyms, but worst in identifying hypernyms.', 'the svm on the difference ( svmdiff ) and on the second ppmi vector ( svmsingle ) is instead particularly good at identifying hypernyms, while it performs bad at identifying co - hyponyms.', 'among the unsupervised methods, we report the results for the cosine and the methods of  #AUTHOR_TAG ; invcl ) and  #AUTHOR_TAG ; balapinc table 4.', 'f1 scores, in percent, on a 10 - fold cross validation ( state of the art models are evaluated on a 5 - fold cross validation ).', '{ bold = best results vs. root9 ; italics = other classifiers }']",5
"['systems reported by  #TAUTHOR_TAG.', 'the scores']","['systems reported by  #TAUTHOR_TAG.', 'the scores']","['s performance compared to the best systems reported by  #TAUTHOR_TAG.', 'the scores']","[""table 4, we show root9's performance compared to the best systems reported by  #TAUTHOR_TAG."", ""the scores are all calculated on subsets of  #TAUTHOR_TAG's datasets, as reported in section 4. 3."", 'considering all the datasets, root9 is the second best performing system, after svmcat  #TAUTHOR_TAG, which uses the svm classifier on the concatenation of ppmi vectors, containing as features all major grammatical dependency relations involving open class parts of speech.', 'the svm classifier on the sum ( svmadd ) and the multiplication ( svmmult ) of the same ppmi vectors performs better in identifying co - hyponyms, but worst in identifying hypernyms.', 'the svm on the difference ( svmdiff ) and on the second ppmi vector ( svmsingle ) is instead particularly good at identifying hypernyms, while it performs bad at identifying co - hyponyms.', 'among the unsupervised methods, we report the results for the cosine and the methods of  #AUTHOR_TAG ; invcl ) and  #AUTHOR_TAG ; balapinc table 4.', 'f1 scores, in percent, on a 10 - fold cross validation ( state of the art models are evaluated on a 5 - fold cross validation ).', '{ bold = best results vs. root9 ; italics = other classifiers }']",5
"['models presented in  #TAUTHOR_TAG in future experiment, we plan to increase']","['contribution assessed.', 'the impressive results in our dataset, developed by randomly extracting 9, 600 pairs from evalution, lenci / benotto  #AUTHOR_TAG and bless  #AUTHOR_TAG, were further tested against the state - of - the - art models presented in  #TAUTHOR_TAG in future experiment, we plan to increase']","['contribution assessed.', 'the impressive results in our dataset, developed by randomly extracting 9, 600 pairs from evalution, lenci / benotto  #AUTHOR_TAG and bless  #AUTHOR_TAG, were further tested against the state - of - the - art models presented in  #TAUTHOR_TAG in future experiment, we plan to increase the number of features, investigating new distributional properties that may help in the classification without incurring in memorization effects']","['this paper, we have described root9, a classifier for hypernyms, co - hyponyms and random words that is derived from an optimization of root13  #AUTHOR_TAG b ).', 'the classifier, based on the random forest algorithm, uses only nine unsupervised corpus - based features, which have been described, and their contribution assessed.', 'the impressive results in our dataset, developed by randomly extracting 9, 600 pairs from evalution, lenci / benotto  #AUTHOR_TAG and bless  #AUTHOR_TAG, were further tested against the state - of - the - art models presented in  #TAUTHOR_TAG in future experiment, we plan to increase the number of features, investigating new distributional properties that may help in the classification without incurring in memorization effects such as those described by  #AUTHOR_TAG']",5
"['in the  #TAUTHOR_TAG datasets. unfortunately,']","['in the  #TAUTHOR_TAG datasets. unfortunately, root9 was', 'not able to cover']","['- of - the - art, we have also evaluated it in the  #TAUTHOR_TAG datasets. unfortunately,']","['', 'and 97. 8 % for co - hyponyms and randoms. in order to compare root9 with the state - of - the - art, we have also evaluated it in the  #TAUTHOR_TAG datasets. unfortunately, root9 was', 'not able to cover the full datasets, as', 'several words in their pairs were missing from our distributional semantic model ( dsm ) because of their low frequency. nevertheless, the authors kindly provided the results of their models on our subsets, so that the comparison can be considered reliable. also in 1 the 9, 600 pairs are available at https : / / github. com / esantus / root9 relation to the state', 'of the art, root9 is proved to be competitive, being slightly outperformed in all the datasets only by the svmcat model  #TAUTHOR_TAG, which is a support vector machine ( svm ) classifier run on the', 'concatenation of the distributional vectors of the words in the pairs. finally, we carried out an extra test to verify whether the system was actually learning the semantic relation between two word pairs', ', or simply identifying prototypical hypernyms ( levy et al. 2015 ). the test consisted in providing to the trained model switched hypernyms ( e. g. from "" dog hyper animal "" to "" dog random fruit "" ), and verify how they were classified. our results show that most of the switched hypernyms were in fact misclass', '##ified as hypernyms ( especially when the words in the switched hypernyms were the same used to train the model on the', 'real hypernyms ), and that the only way to overcome such problem is to explicitly provide the model with bad examples ( i. e., switched hypernyms tagged', 'as randoms ) during the training']",0
"['in the  #TAUTHOR_TAG datasets. unfortunately,']","['in the  #TAUTHOR_TAG datasets. unfortunately, root9 was', 'not able to cover']","['- of - the - art, we have also evaluated it in the  #TAUTHOR_TAG datasets. unfortunately,']","['', 'and 97. 8 % for co - hyponyms and randoms. in order to compare root9 with the state - of - the - art, we have also evaluated it in the  #TAUTHOR_TAG datasets. unfortunately, root9 was', 'not able to cover the full datasets, as', 'several words in their pairs were missing from our distributional semantic model ( dsm ) because of their low frequency. nevertheless, the authors kindly provided the results of their models on our subsets, so that the comparison can be considered reliable. also in 1 the 9, 600 pairs are available at https : / / github. com / esantus / root9 relation to the state', 'of the art, root9 is proved to be competitive, being slightly outperformed in all the datasets only by the svmcat model  #TAUTHOR_TAG, which is a support vector machine ( svm ) classifier run on the', 'concatenation of the distributional vectors of the words in the pairs. finally, we carried out an extra test to verify whether the system was actually learning the semantic relation between two word pairs', ', or simply identifying prototypical hypernyms ( levy et al. 2015 ). the test consisted in providing to the trained model switched hypernyms ( e. g. from "" dog hyper animal "" to "" dog random fruit "" ), and verify how they were classified. our results show that most of the switched hypernyms were in fact misclass', '##ified as hypernyms ( especially when the words in the switched hypernyms were the same used to train the model on the', 'real hypernyms ), and that the only way to overcome such problem is to explicitly provide the model with bad examples ( i. e., switched hypernyms tagged', 'as randoms ) during the training']",0
"['in the  #TAUTHOR_TAG datasets. unfortunately,']","['in the  #TAUTHOR_TAG datasets. unfortunately, root9 was', 'not able to cover']","['- of - the - art, we have also evaluated it in the  #TAUTHOR_TAG datasets. unfortunately,']","['', 'and 97. 8 % for co - hyponyms and randoms. in order to compare root9 with the state - of - the - art, we have also evaluated it in the  #TAUTHOR_TAG datasets. unfortunately, root9 was', 'not able to cover the full datasets, as', 'several words in their pairs were missing from our distributional semantic model ( dsm ) because of their low frequency. nevertheless, the authors kindly provided the results of their models on our subsets, so that the comparison can be considered reliable. also in 1 the 9, 600 pairs are available at https : / / github. com / esantus / root9 relation to the state', 'of the art, root9 is proved to be competitive, being slightly outperformed in all the datasets only by the svmcat model  #TAUTHOR_TAG, which is a support vector machine ( svm ) classifier run on the', 'concatenation of the distributional vectors of the words in the pairs. finally, we carried out an extra test to verify whether the system was actually learning the semantic relation between two word pairs', ', or simply identifying prototypical hypernyms ( levy et al. 2015 ). the test consisted in providing to the trained model switched hypernyms ( e. g. from "" dog hyper animal "" to "" dog random fruit "" ), and verify how they were classified. our results show that most of the switched hypernyms were in fact misclass', '##ified as hypernyms ( especially when the words in the switched hypernyms were the same used to train the model on the', 'real hypernyms ), and that the only way to overcome such problem is to explicitly provide the model with bad examples ( i. e., switched hypernyms tagged', 'as randoms ) during the training']",0
"['in the  #TAUTHOR_TAG datasets. unfortunately,']","['in the  #TAUTHOR_TAG datasets. unfortunately, root9 was', 'not able to cover']","['- of - the - art, we have also evaluated it in the  #TAUTHOR_TAG datasets. unfortunately,']","['', 'and 97. 8 % for co - hyponyms and randoms. in order to compare root9 with the state - of - the - art, we have also evaluated it in the  #TAUTHOR_TAG datasets. unfortunately, root9 was', 'not able to cover the full datasets, as', 'several words in their pairs were missing from our distributional semantic model ( dsm ) because of their low frequency. nevertheless, the authors kindly provided the results of their models on our subsets, so that the comparison can be considered reliable. also in 1 the 9, 600 pairs are available at https : / / github. com / esantus / root9 relation to the state', 'of the art, root9 is proved to be competitive, being slightly outperformed in all the datasets only by the svmcat model  #TAUTHOR_TAG, which is a support vector machine ( svm ) classifier run on the', 'concatenation of the distributional vectors of the words in the pairs. finally, we carried out an extra test to verify whether the system was actually learning the semantic relation between two word pairs', ', or simply identifying prototypical hypernyms ( levy et al. 2015 ). the test consisted in providing to the trained model switched hypernyms ( e. g. from "" dog hyper animal "" to "" dog random fruit "" ), and verify how they were classified. our results show that most of the switched hypernyms were in fact misclass', '##ified as hypernyms ( especially when the words in the switched hypernyms were the same used to train the model on the', 'real hypernyms ), and that the only way to overcome such problem is to explicitly provide the model with bad examples ( i. e., switched hypernyms tagged', 'as randoms ) during the training']",0
"['division ) of the vectors.', "" #AUTHOR_TAG used the vectors'difference, while  #TAUTHOR_TAG""]","['division ) of the vectors.', "" #AUTHOR_TAG used the vectors'difference, while  #TAUTHOR_TAG""]","['division ) of the vectors.', "" #AUTHOR_TAG used the vectors'difference, while  #TAUTHOR_TAG""]","['', ' #AUTHOR_TAG adapted this measure to check not only to which extent the features of the narrower term are included in the features of the broader, but also how the features of the broader are not included in the features of the narrower.', ' #AUTHOR_TAG combined average precision ( ap ) with the balancing approach of  #AUTHOR_TAG, outperforming the above mentioned methods.', ' #AUTHOR_TAG measured the kullback - leibler ( kl ) divergence between the probability distribution over context words for a term, and the background probability distribution, based on the idea that the smaller such kl was, the less informative the word was ( and therefore more likely to be a hypernym ).', ' #AUTHOR_TAG considered the top features in a context vector as topics and used a topic coherence ( tc ) measure.', ' #AUTHOR_TAG a ) formulated the distributional informativeness hypothesis ( dinh ), according to which the generality of a term can be inferred from the informativeness of its most typical linguistic contexts.', ""in their evaluation, the authors have shown that hypernyms'most typical contexts are in fact less informative than hyponyms'ones."", 'among the supervised methods,  #AUTHOR_TAG proposed to use an svm classifier on the concatenation ( after having tried also subtraction and division ) of the vectors.', "" #AUTHOR_TAG used the vectors'difference, while  #TAUTHOR_TAG""]",0
"[' #TAUTHOR_TAG.', '2 these are four']","[' #TAUTHOR_TAG.', '2 these are four datasets, containing respectively : i ) hypernyms']","[' #TAUTHOR_TAG.', '2 these are four datasets, containing']","['order to compare root9 to the state - of - the - art, we have evaluated it with the datasets created by  #TAUTHOR_TAG.', '2 these are four datasets, containing respectively : i ) hypernyms versus other relations ( extracted from wordnet ; henceforth wn hyper ) ; ii ) co - hyponyms versus other relations ( extracted from wordnet ; henceforth wn co - hyp ) ; iii ) hypernyms versus other relations ( extracted from bless ; henceforth bless hyper ) ; iv ) co - hyponyms versus other relations ( extracted from bless ; henceforth bless co - hyp ).', 'the wn dataset  #TAUTHOR_TAG - meaning both wn hyper and wn co - hyp - in particular, was built after noticing that supervised systems tended to perform well also on random vectors.', 'this happens because they are able to learn ontological information and re - use it whenever the words re - appear in other pairs.', 'for this reason, the authors have constructed a dataset where words occurred at most twice ( once on the left and once on the right of the relation ).', 'in this dataset, ontological information cannot be learnt and re - used, and indeed the random vectors cannot perform well.', 'unfortunately our dsm did not cover the whole datasets, because of the chosen frequency threshold ( in table 1, we report the size of our subsets in comparison to the original datasets ).', '']",0
['by  #TAUTHOR_TAG namely that random vectors'],['by  #TAUTHOR_TAG namely that random vectors'],['by  #TAUTHOR_TAG namely that random vectors'],"['our internal tests, we have implemented two baselines, which can be used as reference for evaluating the performance of root9 : cosine and random13.', 'the first baseline simply uses the vector cosine ( cosine ) with a random forest classifier in the default settings ( i. e. 100 trees, 1 seed, and maxdepth and numfeatures initialized to 0 ).', 'this baseline is supposed to perform particularly well in discriminating similar words ( i. e. hypernyms and co - hyponyms ) from randoms.', 'in fact, this measure has been extensively used to identify word similarity in vector spaces  #AUTHOR_TAG because it verifies the normalized correlation between the vectors of 1 and 2 :', 'where is the i - th dimension in the vector x. the second baseline ( random13 ) relies on a default random forest classifier, but uses thirteen randomly initialized features, with values between 0 and 1.', 'while the vector cosine achieves a reasonable accuracy, which is anyway far below the results obtained by our model, the random baseline performs much worst.', 'the discrepancy with what found by  #TAUTHOR_TAG namely that random vectors perform particularly well when words are re - used in the dataset - may depend on the small number of features, which does not allow the system to identify discriminative random dimensions.', 'in the second task ( see section 6 ), we have used as baselines the most competitive models reported in  #TAUTHOR_TAG, namely the svm classifiers trained on the ppmi vector of the second word ( svmsingle ), or on the concatenated ( svmcat ), summed ( svmadd ), multiplied ( svmmult ) and subtracted ( svmdiff ) ppmi vectors of the words in the pair.', 'such vectors contain as features all major grammatical dependency relations involving open class parts of speech.', 'also, the performance of three main unsupervised methods is reported as a reference : cosine ( see above in this section ), balapinc  #AUTHOR_TAG and invcl  #AUTHOR_TAG.', 'a threshold p empirically found in a training set was used in these methods for the decision, table 2.', 'ablation test, f1 scores on a 10 - fold cross validation and loss / gain values.', 'scores are in percent']",0
"['systems reported by  #TAUTHOR_TAG.', 'the scores']","['systems reported by  #TAUTHOR_TAG.', 'the scores']","['s performance compared to the best systems reported by  #TAUTHOR_TAG.', 'the scores']","[""table 4, we show root9's performance compared to the best systems reported by  #TAUTHOR_TAG."", ""the scores are all calculated on subsets of  #TAUTHOR_TAG's datasets, as reported in section 4. 3."", 'considering all the datasets, root9 is the second best performing system, after svmcat  #TAUTHOR_TAG, which uses the svm classifier on the concatenation of ppmi vectors, containing as features all major grammatical dependency relations involving open class parts of speech.', 'the svm classifier on the sum ( svmadd ) and the multiplication ( svmmult ) of the same ppmi vectors performs better in identifying co - hyponyms, but worst in identifying hypernyms.', 'the svm on the difference ( svmdiff ) and on the second ppmi vector ( svmsingle ) is instead particularly good at identifying hypernyms, while it performs bad at identifying co - hyponyms.', 'among the unsupervised methods, we report the results for the cosine and the methods of  #AUTHOR_TAG ; invcl ) and  #AUTHOR_TAG ; balapinc table 4.', 'f1 scores, in percent, on a 10 - fold cross validation ( state of the art models are evaluated on a 5 - fold cross validation ).', '{ bold = best results vs. root9 ; italics = other classifiers }']",0
"['in the  #TAUTHOR_TAG datasets. unfortunately,']","['in the  #TAUTHOR_TAG datasets. unfortunately, root9 was', 'not able to cover']","['- of - the - art, we have also evaluated it in the  #TAUTHOR_TAG datasets. unfortunately,']","['', 'and 97. 8 % for co - hyponyms and randoms. in order to compare root9 with the state - of - the - art, we have also evaluated it in the  #TAUTHOR_TAG datasets. unfortunately, root9 was', 'not able to cover the full datasets, as', 'several words in their pairs were missing from our distributional semantic model ( dsm ) because of their low frequency. nevertheless, the authors kindly provided the results of their models on our subsets, so that the comparison can be considered reliable. also in 1 the 9, 600 pairs are available at https : / / github. com / esantus / root9 relation to the state', 'of the art, root9 is proved to be competitive, being slightly outperformed in all the datasets only by the svmcat model  #TAUTHOR_TAG, which is a support vector machine ( svm ) classifier run on the', 'concatenation of the distributional vectors of the words in the pairs. finally, we carried out an extra test to verify whether the system was actually learning the semantic relation between two word pairs', ', or simply identifying prototypical hypernyms ( levy et al. 2015 ). the test consisted in providing to the trained model switched hypernyms ( e. g. from "" dog hyper animal "" to "" dog random fruit "" ), and verify how they were classified. our results show that most of the switched hypernyms were in fact misclass', '##ified as hypernyms ( especially when the words in the switched hypernyms were the same used to train the model on the', 'real hypernyms ), and that the only way to overcome such problem is to explicitly provide the model with bad examples ( i. e., switched hypernyms tagged', 'as randoms ) during the training']",6
"['in the  #TAUTHOR_TAG datasets. unfortunately,']","['in the  #TAUTHOR_TAG datasets. unfortunately, root9 was', 'not able to cover']","['- of - the - art, we have also evaluated it in the  #TAUTHOR_TAG datasets. unfortunately,']","['', 'and 97. 8 % for co - hyponyms and randoms. in order to compare root9 with the state - of - the - art, we have also evaluated it in the  #TAUTHOR_TAG datasets. unfortunately, root9 was', 'not able to cover the full datasets, as', 'several words in their pairs were missing from our distributional semantic model ( dsm ) because of their low frequency. nevertheless, the authors kindly provided the results of their models on our subsets, so that the comparison can be considered reliable. also in 1 the 9, 600 pairs are available at https : / / github. com / esantus / root9 relation to the state', 'of the art, root9 is proved to be competitive, being slightly outperformed in all the datasets only by the svmcat model  #TAUTHOR_TAG, which is a support vector machine ( svm ) classifier run on the', 'concatenation of the distributional vectors of the words in the pairs. finally, we carried out an extra test to verify whether the system was actually learning the semantic relation between two word pairs', ', or simply identifying prototypical hypernyms ( levy et al. 2015 ). the test consisted in providing to the trained model switched hypernyms ( e. g. from "" dog hyper animal "" to "" dog random fruit "" ), and verify how they were classified. our results show that most of the switched hypernyms were in fact misclass', '##ified as hypernyms ( especially when the words in the switched hypernyms were the same used to train the model on the', 'real hypernyms ), and that the only way to overcome such problem is to explicitly provide the model with bad examples ( i. e., switched hypernyms tagged', 'as randoms ) during the training']",4
"['systems reported by  #TAUTHOR_TAG.', 'the scores']","['systems reported by  #TAUTHOR_TAG.', 'the scores']","['s performance compared to the best systems reported by  #TAUTHOR_TAG.', 'the scores']","[""table 4, we show root9's performance compared to the best systems reported by  #TAUTHOR_TAG."", ""the scores are all calculated on subsets of  #TAUTHOR_TAG's datasets, as reported in section 4. 3."", 'considering all the datasets, root9 is the second best performing system, after svmcat  #TAUTHOR_TAG, which uses the svm classifier on the concatenation of ppmi vectors, containing as features all major grammatical dependency relations involving open class parts of speech.', 'the svm classifier on the sum ( svmadd ) and the multiplication ( svmmult ) of the same ppmi vectors performs better in identifying co - hyponyms, but worst in identifying hypernyms.', 'the svm on the difference ( svmdiff ) and on the second ppmi vector ( svmsingle ) is instead particularly good at identifying hypernyms, while it performs bad at identifying co - hyponyms.', 'among the unsupervised methods, we report the results for the cosine and the methods of  #AUTHOR_TAG ; invcl ) and  #AUTHOR_TAG ; balapinc table 4.', 'f1 scores, in percent, on a 10 - fold cross validation ( state of the art models are evaluated on a 5 - fold cross validation ).', '{ bold = best results vs. root9 ; italics = other classifiers }']",4
"['models presented in  #TAUTHOR_TAG in future experiment, we plan to increase']","['contribution assessed.', 'the impressive results in our dataset, developed by randomly extracting 9, 600 pairs from evalution, lenci / benotto  #AUTHOR_TAG and bless  #AUTHOR_TAG, were further tested against the state - of - the - art models presented in  #TAUTHOR_TAG in future experiment, we plan to increase']","['contribution assessed.', 'the impressive results in our dataset, developed by randomly extracting 9, 600 pairs from evalution, lenci / benotto  #AUTHOR_TAG and bless  #AUTHOR_TAG, were further tested against the state - of - the - art models presented in  #TAUTHOR_TAG in future experiment, we plan to increase the number of features, investigating new distributional properties that may help in the classification without incurring in memorization effects']","['this paper, we have described root9, a classifier for hypernyms, co - hyponyms and random words that is derived from an optimization of root13  #AUTHOR_TAG b ).', 'the classifier, based on the random forest algorithm, uses only nine unsupervised corpus - based features, which have been described, and their contribution assessed.', 'the impressive results in our dataset, developed by randomly extracting 9, 600 pairs from evalution, lenci / benotto  #AUTHOR_TAG and bless  #AUTHOR_TAG, were further tested against the state - of - the - art models presented in  #TAUTHOR_TAG in future experiment, we plan to increase the number of features, investigating new distributional properties that may help in the classification without incurring in memorization effects such as those described by  #AUTHOR_TAG']",2
['individual time step of 50ms.  #TAUTHOR_TAG are trained to predict a vector of probability scores for speech activity in'],['individual time step of 50ms.  #TAUTHOR_TAG are trained to predict a vector of probability scores for speech activity in'],['at each individual time step of 50ms.  #TAUTHOR_TAG are trained to predict a vector of probability scores for speech activity in'],"['s speech activity at each individual time step of 50ms.  #TAUTHOR_TAG are trained to predict a vector of probability scores for speech activity in each individual frame within a set future window. rather than designing classifiers', 'to make specific decisions,  #TAUTHOR_TAG are trained on.  #TAUTHOR_TAG can therefore be applied to a wide variety of turn - taking prediction tasks and have been shown to outperform traditional classifiers when applied to hold / shift predictions. a downside', 'to the approach in  #TAUTHOR_TAG is that, since a single lstm is being used,', 'all input features must be processed at the same rate. when considering linguistic features, the the relevant information for turn - taking prediction happens at a coarse temporal granularity in comparison with acoustic features,', 'where much of the relevant information occurs at the sub - word prosodic level. when using a single lstm, this requires either averaging the acoustic features to represent them at a word - level temporal resolution, or upsampling the linguistic features to represent them at the acoustic temporal resolution. both of these options have their drawbacks. in the', 'case of averaging the acoustic features, we lose the finer - grained prosodic inflections', '']",0
['main objective of continuous turn - taking prediction as proposed in  #TAUTHOR_TAG is to predict the future speech activity annotations of'],['main objective of continuous turn - taking prediction as proposed in  #TAUTHOR_TAG is to predict the future speech activity annotations of'],['main objective of continuous turn - taking prediction as proposed in  #TAUTHOR_TAG is to predict the future speech activity annotations of'],"['main objective of continuous turn - taking prediction as proposed in  #TAUTHOR_TAG is to predict the future speech activity annotations of one of the speakers in a dyadic conversation using input speech features from both speakers ( x t ).', 'at each timestep t of frame - size 50ms, speech features are extracted and input into an lstm that is used to predict the future speech activity ( y t ) of one of the speakers.', 'the future speech activity is a 3 second window comprising of 60 frames of the binary annotations for frames t + 1 to t + 60.', ""the output layer of the network uses an element - wise sigmoid activation function to predict a probability score for the target speaker's speech activity at each future frame."", 'to represent linguistic features in this model the word - rate features are upsampled to the 50ms acoustic feature rate.', 'they use one - hot encodings, where the feature is "" switched on "" for a single frame, 100ms after the word is finished ( to simulate real - world asr conditions ).', 'to model more fine - grained prosodic inflections the 50ms acoustic feature rate could be increased.', 'however, doing this leads to a sparser linguistic feature representation since all features input into an lstm must be processed at the same rate.', 'this makes it more difficult for the network to model longer - term dependencies that exist in the linguistic modality']",0
"[' #TAUTHOR_TAG with "" acous 10ms']","[' #TAUTHOR_TAG with "" acous 10ms "" are not possible when using']","[' #TAUTHOR_TAG with "" acous 10ms "" are not possible when']","['our experiments, the networks were trained to minimize binary cross entropy ( bce ) loss which was shown to produce good results in [ 11 ].', 'we test the impact of using three different network configurations with multiple combinations of modalities at different temporal resolutions.', 'the three network configurations are : "" no subnets "", which corresponds to an early fusion approach in which the modalities are fed directly into a single lstm ; "" one subnet "", which corresponds to the use of only one sub - network lstm ; and "" two subnets "", which corresponds to the use of separate lstm subnetworks for the individual modalities.', 'we note that combinations such as  #TAUTHOR_TAG with "" acous 10ms "" are not possible when using the "" no subnets "" and "" one subnet "" configurations since the features are being input into the same lstm and cannot operate at different temporal resolutions.', 'grid searches for three hyperparameters ( hidden node size, dropout, and l2 regularization ) were performed for each network configuration.', 'in order to limit the influence of parameter count changes between the different network configurations, the hidden node count in a given network was limited to a sum of 150.', 'once the hyperparameters for a network are chosen, we train the network five times and report the mean values of the different evaluation metrics in tables 1 and 2.', 'the best performing modality combination for a given network configuration is shown in bold and the best overall performance is shown in italics.', 'in our discussion below we use two - tailed t - tests to report on the difference between the means of metrics']",0
['individual time step of 50ms.  #TAUTHOR_TAG are trained to predict a vector of probability scores for speech activity in'],['individual time step of 50ms.  #TAUTHOR_TAG are trained to predict a vector of probability scores for speech activity in'],['at each individual time step of 50ms.  #TAUTHOR_TAG are trained to predict a vector of probability scores for speech activity in'],"['s speech activity at each individual time step of 50ms.  #TAUTHOR_TAG are trained to predict a vector of probability scores for speech activity in each individual frame within a set future window. rather than designing classifiers', 'to make specific decisions,  #TAUTHOR_TAG are trained on.  #TAUTHOR_TAG can therefore be applied to a wide variety of turn - taking prediction tasks and have been shown to outperform traditional classifiers when applied to hold / shift predictions. a downside', 'to the approach in  #TAUTHOR_TAG is that, since a single lstm is being used,', 'all input features must be processed at the same rate. when considering linguistic features, the the relevant information for turn - taking prediction happens at a coarse temporal granularity in comparison with acoustic features,', 'where much of the relevant information occurs at the sub - word prosodic level. when using a single lstm, this requires either averaging the acoustic features to represent them at a word - level temporal resolution, or upsampling the linguistic features to represent them at the acoustic temporal resolution. both of these options have their drawbacks. in the', 'case of averaging the acoustic features, we lose the finer - grained prosodic inflections', '']",6
"['the processing of linguistic features are tested in our experiments.', 'in our discussion and results below,  #TAUTHOR_TAG.', '"" ling 10ms "" refers to using word features that are sampled at a faster rate of']","['the processing of linguistic features are tested in our experiments.', 'in our discussion and results below,  #TAUTHOR_TAG.', '"" ling 10ms "" refers to using word features that are sampled at a faster rate of 10ms.', '"" ling asynch "" refers to using an irregular update rate, where the lstm only processes the linguistic features when a new word is available']","['the processing of linguistic features are tested in our experiments.', 'in our discussion and results below,  #TAUTHOR_TAG.', '"" ling 10ms "" refers to using word features that are sampled at a faster rate of']","['', 'linguistic features.', 'for linguistic features we use the word annotations supplied with the corpus.', 'the words were represented as an enumerated vocabulary where the raw word features were transformed into a linear embedding of size 64 that is jointly trained with the rest of the network.', 'in an effort to simulate the conditions of a real - time system, the linguistic features were not provided to the system until 100ms after the end of the word.', 'three different temporal rates for the processing of linguistic features are tested in our experiments.', 'in our discussion and results below,  #TAUTHOR_TAG.', '"" ling 10ms "" refers to using word features that are sampled at a faster rate of 10ms.', '"" ling asynch "" refers to using an irregular update rate, where the lstm only processes the linguistic features when a new word is available']",5
['dataset by  #TAUTHOR_TAG of 0. 76'],['dataset by  #TAUTHOR_TAG of 0. 762. looking at the results'],['baselines reported on the same dataset by  #TAUTHOR_TAG of 0. 76'],"['', 'comparing our results with previously published baselines reported on the same dataset by  #TAUTHOR_TAG of 0. 762. looking at the results from the fusion of visual and acoustic modalities shown in in table 2, we were able to achieve our best bce loss using our multiscale approach to fuse acoustic features at a 10ms timescale and visual features at a 58hz timescale.', 'comparing this result ( 9 ) with our best "" no subnets "" result ( 2 ) gives a statistically significant improvement ( p = 0. 035 ).', 'we note that using early fusion with gaze features ( 5 ) does not add any value when compared to acoustic features on their own ( 1 ).', 'the results also indicate that the faster 58hz gaze features perform better than the averaged 50ms visual features when used in conjunction with the acoustic features.', 'this suggests that we loose relevant information by averaging the gaze features within a timestep']",5
['dataset by  #TAUTHOR_TAG of 0. 76'],['dataset by  #TAUTHOR_TAG of 0. 762. looking at the results'],['baselines reported on the same dataset by  #TAUTHOR_TAG of 0. 76'],"['', 'comparing our results with previously published baselines reported on the same dataset by  #TAUTHOR_TAG of 0. 762. looking at the results from the fusion of visual and acoustic modalities shown in in table 2, we were able to achieve our best bce loss using our multiscale approach to fuse acoustic features at a 10ms timescale and visual features at a 58hz timescale.', 'comparing this result ( 9 ) with our best "" no subnets "" result ( 2 ) gives a statistically significant improvement ( p = 0. 035 ).', 'we note that using early fusion with gaze features ( 5 ) does not add any value when compared to acoustic features on their own ( 1 ).', 'the results also indicate that the faster 58hz gaze features perform better than the averaged 50ms visual features when used in conjunction with the acoustic features.', 'this suggests that we loose relevant information by averaging the gaze features within a timestep']",4
"['in a siamese architecture  #TAUTHOR_TAG.', 'this is']","['in a siamese architecture  #TAUTHOR_TAG.', 'this is']","['in a siamese architecture  #TAUTHOR_TAG.', 'this is also used in']","['many tasks in natural language processing, it is necessary to match or compare two distributed representations.', 'these representations may refer to whole sentences, word contexts or any other construct.', 'for concreteness, let u and v be two representations we want to match.', 'in order to facilitate the matching, it is often beneficial to explicitly create new features like element - wise absolute difference ( | u−v | ) and element - wise product ( u · v ) that augment u and v. the combined feature vector is then processed by further layers in the task specific neural network.', 'for example,  #AUTHOR_TAG use these heuristics to improve semantic representations.', 'most notably, for the natural language inference task, augmenting the hypothesis ( u ) and premise ( v ) representations with | u−v | and u · v considerably improves performance in a siamese architecture  #TAUTHOR_TAG.', 'this is also used in the more sophisticated models of  #AUTHOR_TAG, where u and v represent word contexts.', 'several of these approaches are explored in the compare - and - aggregate framework by  #AUTHOR_TAG.', 'in this paper we focus on polynomial features like u · v for the natural language inference task, where it is trying to capture similarity between u and v. it is also a monomial of degree 2.', 'we investigate two aspects of such terms - the use of scaling and the use of higher degree polynomials.', 'the motivation for the former is the following.', 'the values taken by individual elements of u and u · v will in general have slightly different statistical distributions.', 'for example, if elements of both u and v are approximately zero mean gaussians with variance σ 2, the variance of the elements of u · v will be approximately σ 4.', 'as such, subsequent layers in the neural network use weights that are initialized assuming identically distributed inputs  #AUTHOR_TAG, which is clearly not the case when σ = 1.', 'an appropriate scaling coefficient attached to u · v that can bring its variance close to that of u is one possible way of addressing this anomaly.', 'the motivation for the latter is to incorporate more complex multiplicative interaction between u and v through degree 3 and 4 polynomials.', 'our findings are two - fold.', 'through numerical experiments using the stanford natural language inference ( snli )  #AUTHOR_TAG dataset, we show that in the absence of scaling, using higher degree polynomial features instead of u · v improves the performance of baseline models.', 'in the presence of scaling, this difference all but vanishes and in fact the scaled u · v achieves the best performance.', '']",0
"['##t model  #AUTHOR_TAG.', 'the standard matching feature of  #TAUTHOR_TAG 0.', '( 1 )', 'to incorporate polynomial multiplicative features between']","['infersent model  #AUTHOR_TAG.', 'the standard matching feature of  #TAUTHOR_TAG 0.', '( 1 )', 'to incorporate polynomial multiplicative features between']","['the infersent model  #AUTHOR_TAG.', 'the standard matching feature of  #TAUTHOR_TAG 0.', '( 1 )', 'to incorporate polynomial multiplicative features between']","['', 'the standard matching feature of  #TAUTHOR_TAG 0.', '( 1 )', 'to incorporate polynomial multiplicative features between u and v of degree 3 and 4, we define the following define the following matching feature vectors.', 'and', '']",0
"['in a siamese architecture  #TAUTHOR_TAG.', 'this is']","['in a siamese architecture  #TAUTHOR_TAG.', 'this is']","['in a siamese architecture  #TAUTHOR_TAG.', 'this is also used in']","['many tasks in natural language processing, it is necessary to match or compare two distributed representations.', 'these representations may refer to whole sentences, word contexts or any other construct.', 'for concreteness, let u and v be two representations we want to match.', 'in order to facilitate the matching, it is often beneficial to explicitly create new features like element - wise absolute difference ( | u−v | ) and element - wise product ( u · v ) that augment u and v. the combined feature vector is then processed by further layers in the task specific neural network.', 'for example,  #AUTHOR_TAG use these heuristics to improve semantic representations.', 'most notably, for the natural language inference task, augmenting the hypothesis ( u ) and premise ( v ) representations with | u−v | and u · v considerably improves performance in a siamese architecture  #TAUTHOR_TAG.', 'this is also used in the more sophisticated models of  #AUTHOR_TAG, where u and v represent word contexts.', 'several of these approaches are explored in the compare - and - aggregate framework by  #AUTHOR_TAG.', 'in this paper we focus on polynomial features like u · v for the natural language inference task, where it is trying to capture similarity between u and v. it is also a monomial of degree 2.', 'we investigate two aspects of such terms - the use of scaling and the use of higher degree polynomials.', 'the motivation for the former is the following.', 'the values taken by individual elements of u and u · v will in general have slightly different statistical distributions.', 'for example, if elements of both u and v are approximately zero mean gaussians with variance σ 2, the variance of the elements of u · v will be approximately σ 4.', 'as such, subsequent layers in the neural network use weights that are initialized assuming identically distributed inputs  #AUTHOR_TAG, which is clearly not the case when σ = 1.', 'an appropriate scaling coefficient attached to u · v that can bring its variance close to that of u is one possible way of addressing this anomaly.', 'the motivation for the latter is to incorporate more complex multiplicative interaction between u and v through degree 3 and 4 polynomials.', 'our findings are two - fold.', 'through numerical experiments using the stanford natural language inference ( snli )  #AUTHOR_TAG dataset, we show that in the absence of scaling, using higher degree polynomial features instead of u · v improves the performance of baseline models.', 'in the presence of scaling, this difference all but vanishes and in fact the scaled u · v achieves the best performance.', '']",5
"['##t model  #AUTHOR_TAG.', 'the standard matching feature of  #TAUTHOR_TAG 0.', '( 1 )', 'to incorporate polynomial multiplicative features between']","['infersent model  #AUTHOR_TAG.', 'the standard matching feature of  #TAUTHOR_TAG 0.', '( 1 )', 'to incorporate polynomial multiplicative features between']","['the infersent model  #AUTHOR_TAG.', 'the standard matching feature of  #TAUTHOR_TAG 0.', '( 1 )', 'to incorporate polynomial multiplicative features between']","['', 'the standard matching feature of  #TAUTHOR_TAG 0.', '( 1 )', 'to incorporate polynomial multiplicative features between u and v of degree 3 and 4, we define the following define the following matching feature vectors.', 'and', '']",5
"['##t model  #AUTHOR_TAG.', 'the standard matching feature of  #TAUTHOR_TAG 0.', '( 1 )', 'to incorporate polynomial multiplicative features between']","['infersent model  #AUTHOR_TAG.', 'the standard matching feature of  #TAUTHOR_TAG 0.', '( 1 )', 'to incorporate polynomial multiplicative features between']","['the infersent model  #AUTHOR_TAG.', 'the standard matching feature of  #TAUTHOR_TAG 0.', '( 1 )', 'to incorporate polynomial multiplicative features between']","['', 'the standard matching feature of  #TAUTHOR_TAG 0.', '( 1 )', 'to incorporate polynomial multiplicative features between u and v of degree 3 and 4, we define the following define the following matching feature vectors.', 'and', '']",5
"['by  #TAUTHOR_TAG, on end -']","['by  #TAUTHOR_TAG, on end - toend memory networks ( n2ns ), which exhibit remarkable reasoning capabilities, e. g. for reasoning and goal - oriented dialogue tasks.', 'typically, such']","['by  #TAUTHOR_TAG, on end -']","['neural network models have demonstrated strong performance on a number of challenging tasks, such as image classification  #AUTHOR_TAG, speech recognition  #AUTHOR_TAG, and various natural language processing tasks  #AUTHOR_TAG.', 'recently, the augmentation of neural networks with external memory components has been shown to be a powerful means of capturing context of different types  #AUTHOR_TAG.', 'of particular interest to this work is the work by  #TAUTHOR_TAG, on end - toend memory networks ( n2ns ), which exhibit remarkable reasoning capabilities, e. g. for reasoning and goal - oriented dialogue tasks.', '']",0
"['memory networks : building on top of memory networks,  #TAUTHOR_TAG ing the memory position supervision and']","['memory networks : building on top of memory networks,  #TAUTHOR_TAG ing the memory position supervision and']","['end memory networks : building on top of memory networks,  #TAUTHOR_TAG ing the memory position supervision and making the model trainable in an end - to - end fashion, through the advent of supporting memories and a memory access controller.', 'representations']","['- to - end memory networks : building on top of memory networks,  #TAUTHOR_TAG ing the memory position supervision and making the model trainable in an end - to - end fashion, through the advent of supporting memories and a memory access controller.', '']",0
"['memory networks : building on top of memory networks,  #TAUTHOR_TAG ing the memory position supervision and']","['memory networks : building on top of memory networks,  #TAUTHOR_TAG ing the memory position supervision and']","['end memory networks : building on top of memory networks,  #TAUTHOR_TAG ing the memory position supervision and making the model trainable in an end - to - end fashion, through the advent of supporting memories and a memory access controller.', 'representations']","['- to - end memory networks : building on top of memory networks,  #TAUTHOR_TAG ing the memory position supervision and making the model trainable in an end - to - end fashion, through the advent of supporting memories and a memory access controller.', '']",0
"[' #TAUTHOR_TAG, two types of weight tying were explored for n2n, namely adjacent ( ""']","[' #TAUTHOR_TAG, two types of weight tying were explored for n2n, namely adjacent ( "" adj "" ) and layer - wise ( "" lw "" ).', 'with lw, the']","[' #TAUTHOR_TAG, two types of weight tying were explored for n2n, namely adjacent ( ""']","[' #TAUTHOR_TAG, two types of weight tying were explored for n2n, namely adjacent ( "" adj "" ) and layer - wise ( "" lw "" ).', 'with lw, the input and output embedding matrices are shared across different hops ( i. e., a 1 = a 2 =... = a k and c 1 = c 2 =... = c k ), resembling rnns.', 'with adj, on the other hand, not only is the output embedding for a given layer shared with the corresponding input embedding ( i. e., a k + 1 = c k ), the answer prediction matrix w and question embedding matrix b are also constrained such that w > = c k and b = a 1.', 'while both adj and lw work well, achieving comparable overall performance in terms of mean error over the 20 babi tasks, their performance on a subset of the tasks ( i. e., tasks 3, 16, 17 and 19, as shown in table 1 ) is inconsistent, with one performing very well, and the other performing poorly.', 'based on this observation, we propose a unified weight tying mechanism exploiting the benefits of both adj and lw, and capable of dynamically determining the best weight tying approach for a given task.', '']",0
"[' #TAUTHOR_TAG, two types of weight tying were explored for n2n, namely adjacent ( ""']","[' #TAUTHOR_TAG, two types of weight tying were explored for n2n, namely adjacent ( "" adj "" ) and layer - wise ( "" lw "" ).', 'with lw, the']","[' #TAUTHOR_TAG, two types of weight tying were explored for n2n, namely adjacent ( ""']","[' #TAUTHOR_TAG, two types of weight tying were explored for n2n, namely adjacent ( "" adj "" ) and layer - wise ( "" lw "" ).', 'with lw, the input and output embedding matrices are shared across different hops ( i. e., a 1 = a 2 =... = a k and c 1 = c 2 =... = c k ), resembling rnns.', 'with adj, on the other hand, not only is the output embedding for a given layer shared with the corresponding input embedding ( i. e., a k + 1 = c k ), the answer prediction matrix w and question embedding matrix b are also constrained such that w > = c k and b = a 1.', 'while both adj and lw work well, achieving comparable overall performance in terms of mean error over the 20 babi tasks, their performance on a subset of the tasks ( i. e., tasks 3, 16, 17 and 19, as shown in table 1 ) is inconsistent, with one performing very well, and the other performing poorly.', 'based on this observation, we propose a unified weight tying mechanism exploiting the benefits of both adj and lw, and capable of dynamically determining the best weight tying approach for a given task.', '']",0
"['by  #TAUTHOR_TAG, on end -']","['by  #TAUTHOR_TAG, on end - toend memory networks ( n2ns ), which exhibit remarkable reasoning capabilities, e. g. for reasoning and goal - oriented dialogue tasks.', 'typically, such']","['by  #TAUTHOR_TAG, on end -']","['neural network models have demonstrated strong performance on a number of challenging tasks, such as image classification  #AUTHOR_TAG, speech recognition  #AUTHOR_TAG, and various natural language processing tasks  #AUTHOR_TAG.', 'recently, the augmentation of neural networks with external memory components has been shown to be a powerful means of capturing context of different types  #AUTHOR_TAG.', 'of particular interest to this work is the work by  #TAUTHOR_TAG, on end - toend memory networks ( n2ns ), which exhibit remarkable reasoning capabilities, e. g. for reasoning and goal - oriented dialogue tasks.', '']",1
"['moreover, following  #TAUTHOR_TAG,']","['moreover, following  #TAUTHOR_TAG, we add a linear mapping h 2 r d', '##⇥']","['. moreover, following  #TAUTHOR_TAG, we add a linear mapping h 2 r d', '##⇥d to the update connection between memory hops, but in our case, down - weight it by 1', 'z, resulting in : regularisation : in order to prevent the input and output embedding matrices a k and c', 'k from being dominated by the unconstrained embedding matrices, it', 'is necessary to restrain the magnitude of the values ina 1 andc k']","['', 'context sentences in the memory encoded in h t. note that the gating vector z can be replaced by a gating scalar z, but we choose to use', 'a vector for more fine - grained control as in lstms  #AUTHOR_TAG and grus. to simplify the model, we constrain b and w > to share the same parameters as a 1 and c k. moreover, following  #TAUTHOR_TAG, we add a linear mapping h 2 r d', '##⇥d to the update connection between memory hops, but in our case, down - weight it by 1', 'z, resulting in : regularisation : in order to prevent the input and output embedding matrices a k and c', 'k from being dominated by the unconstrained embedding matrices, it', 'is necessary to restrain the magnitude of the values ina 1 andc k. therefore, in addition to the cross entropy loss over n training', 'instances : where y andy are the true and predicted answer, we enforce a regularisation penalty and formulate the new objective function as : model implementation : we implement un2n with tensorflow  #AUTHOR_TAG and the code is available at https', ': / / github. com / liufly / umemn2n']",5
"['', '3', 'training details : following  #TAUTHOR_TAG, we hold out']","['version.', '3', 'training details : following  #TAUTHOR_TAG, we hold out 10 % of the babi training set to form a development set.', 'position encoding and temporal encoding ( with']","['', '3', 'training details : following  #TAUTHOR_TAG, we hold out']","['', '3', 'training details : following  #TAUTHOR_TAG, we hold out 10 % of the babi training set to form a development set.', 'position encoding and temporal encoding ( with 10 % random noise ) are also incorporated into the model.', 'training is performed over 100 epochs with a batch size of 32 using the adam optimiser  #AUTHOR_TAG with a learning rate of 0. 005.', 'following  #TAUTHOR_TAG, linear start is employed in all our experiments for the first 20 epochs.', 'all weight parameters are initialised based on a gaussian distribution with zero mean and = 0. 1.', 'gradients with an ` 2 norm of 40 are divided by a scalar to have norm 40.', 'also following  #TAUTHOR_TAG, we use only the most recent 50 sentences as the memory and set the number of memory hops to 3, the embedding size to 20, and']",5
"['', '3', 'training details : following  #TAUTHOR_TAG, we hold out']","['version.', '3', 'training details : following  #TAUTHOR_TAG, we hold out 10 % of the babi training set to form a development set.', 'position encoding and temporal encoding ( with']","['', '3', 'training details : following  #TAUTHOR_TAG, we hold out']","['', '3', 'training details : following  #TAUTHOR_TAG, we hold out 10 % of the babi training set to form a development set.', 'position encoding and temporal encoding ( with 10 % random noise ) are also incorporated into the model.', 'training is performed over 100 epochs with a batch size of 32 using the adam optimiser  #AUTHOR_TAG with a learning rate of 0. 005.', 'following  #TAUTHOR_TAG, linear start is employed in all our experiments for the first 20 epochs.', 'all weight parameters are initialised based on a gaussian distribution with zero mean and = 0. 1.', 'gradients with an ` 2 norm of 40 are divided by a scalar to have norm 40.', 'also following  #TAUTHOR_TAG, we use only the most recent 50 sentences as the memory and set the number of memory hops to 3, the embedding size to 20, and']",5
"['', '3', 'training details : following  #TAUTHOR_TAG, we hold out']","['version.', '3', 'training details : following  #TAUTHOR_TAG, we hold out 10 % of the babi training set to form a development set.', 'position encoding and temporal encoding ( with']","['', '3', 'training details : following  #TAUTHOR_TAG, we hold out']","['', '3', 'training details : following  #TAUTHOR_TAG, we hold out 10 % of the babi training set to form a development set.', 'position encoding and temporal encoding ( with 10 % random noise ) are also incorporated into the model.', 'training is performed over 100 epochs with a batch size of 32 using the adam optimiser  #AUTHOR_TAG with a learning rate of 0. 005.', 'following  #TAUTHOR_TAG, linear start is employed in all our experiments for the first 20 epochs.', 'all weight parameters are initialised based on a gaussian distribution with zero mean and = 0. 1.', 'gradients with an ` 2 norm of 40 are divided by a scalar to have norm 40.', 'also following  #TAUTHOR_TAG, we use only the most recent 50 sentences as the memory and set the number of memory hops to 3, the embedding size to 20, and']",5
"['with other published results over babi  #TAUTHOR_TAG, we repeat training 30 times for']","['with other published results over babi  #TAUTHOR_TAG, we repeat training 30 times for']","['with other published results over babi  #TAUTHOR_TAG, we repeat training 30 times for each task, and select the model which performs best on the development set']","['with other published results over babi  #TAUTHOR_TAG, we repeat training 30 times for each task, and select the model which performs best on the development set']",5
['with adj and lw  #TAUTHOR_TAG'],['n2n with adj and lw  #TAUTHOR_TAG'],['with adj and lw  #TAUTHOR_TAG'],"['results on the 20 babi qa tasks are presented in table 3.', 'we benchmark against other memory network based models : ( 1 ) n2n with adj and lw  #TAUTHOR_TAG ; ( 2 ) dmn  #AUTHOR_TAG and its improved version dmn +  #AUTHOR_TAG ; and ( 3 ) gn2n  #AUTHOR_TAG.', 'major improvements on the difficult tasks.', '']",5
"['to how', 'sensitive memory - based models are to parameter initialisation, following  #TAUTHOR_TAG and  #AUTHOR_TAG, we repeat each training 10 times using the']","['. 1. as a large variance can be observed due to how', 'sensitive memory - based models are to parameter initialisation, following  #TAUTHOR_TAG and  #AUTHOR_TAG, we repeat each training 10 times using the table', '4 : per - response']","['the match features. in terms of the training procedure, experiments are carried out with the same configuration as described in section 4. 1. as a large variance can be observed due to how', 'sensitive memory - based models are to parameter initialisation, following  #TAUTHOR_TAG and  #AUTHOR_TAG, we repeat each training 10 times using the table', '4 : per - response accuracy on the dialog babi tasks. n2', '##n :. gn2n :  #AUTHOR_TAG. + match suggests the use of the match features in section 5. 1. bold indicates the best result in']","['from the second dialog state tracking challenge  #AUTHOR_TAG with real human - bot conversations. training details : following the works of ( bordes and and  #AUTHOR_TAG, we frame the task in the same fashion : at the t - th time step, the preceding sequence of utterances, c u 1, c r 1, c u 2, c r 2,..., c u t 1, c r t 1 ( alternating between the user request, denoted c u i and the system response, denoted', 'c r i ), is stored in the memory as m i', 'and c i. taking the memory as contextual evidence, the goal of the model is to offers an answer c r t ( the bot utterance at time t ) to the question c u t ( the user utterance at time t ). it is important to notice', 'that the answers in this dataset may no longer be a single but can be comprised of multiple ones.', 'following, we replace the final prediction step in equation ( 4 ) with : where w 0 2 r d', '##⇥ | v | is the weight parameter matrix for the model to learn, u = o k + u k ( k is the total number of hops ), y i is the i th response in the', 'candidate set c such that y i 2 c, | c | the size of the candidate set, and (', '· ) a function which maps the input text into a bag of dimension | v |. additionally, we also append several key features to, following and  #AUTHOR_TAG. first, we mark the identity of the speaker of a given utterance ( either user or bot ). second, we extend by 7 additional features, one for each of the 7 properties associated with a restaurant. each of these 7 features indicates whether there are any exact matches between words in the candidate and those in the question or memory. we', 'refer to these 7 features as the match features. in terms of the training procedure, experiments are carried out with the same configuration as described in section 4. 1. as a large variance can be observed due to how', 'sensitive memory - based models are to parameter initialisation, following  #TAUTHOR_TAG and  #AUTHOR_TAG, we repeat each training 10 times using the table', '4 : per - response accuracy on the dialog babi tasks. n2', '##n :. gn2n :  #AUTHOR_TAG. + match suggests the use of the match features in section 5. 1. bold indicates the best result in each group ( with or without the match features ) for a given task. same hyper - parameters and choose the best system based on validation performance']",5
"['memory network - based models : 4 ( 1 ) n2n  #TAUTHOR_TAG.', 'while']","['memory network - based models : 4 ( 1 ) n2n  #TAUTHOR_TAG.', 'while']","['of baselines, we benchmark against other memory network - based models : 4 ( 1 ) n2n  #TAUTHOR_TAG.', 'while']","['results on the dialog babi tasks are shown in table 4.', 'in terms of baselines, we benchmark against other memory network - based models : 4 ( 1 ) n2n  #TAUTHOR_TAG.', 'while the results of gn2n is achieved with adj, the type of weight tying for n2n is not reported in.', '']",5
"['in  #TAUTHOR_TAG, which is extended by an unigram orientation model.', 'the units of translation are blocks, pairs of phrases without internal structure.', '']","['in  #TAUTHOR_TAG, which is extended by an unigram orientation model.', 'the units of translation are blocks, pairs of phrases without internal structure.', 'fig. 1 shows an']","['in  #TAUTHOR_TAG, which is extended by an unigram orientation model.', 'the units of translation are blocks, pairs of phrases without internal structure.', '']","['recent years, phrase - based systems for statistical machine translation  #AUTHOR_TAG have delivered state - of - the - art performance on standard translation tasks.', 'in this paper, we present a phrase - based unigram system similar to the one in  #TAUTHOR_TAG, which is extended by an unigram orientation model.', 'the units of translation are blocks, pairs of phrases without internal structure.', 'fig. 1 shows an example block translation using five arabic - english blocks', '. the unigram orientation model is trained from word - aligned training data.', 'during decoding, we view translation as a block segmentation process, where the input sentence is segmented from left to right and the target sentence is generated from bottom to top, one block at a time.', 'a monotone block sequence is generated except for the possibility to swap a pair of neighbor blocks.', 'the novel orientation model is used to assist the block swapping : as shown in section 3, block swapping where only a trigram language model is used to compute probabilities between neighbor blocks fails to improve translation performance.', ' #AUTHOR_TAG present re - ordering models that make use of a straight / inverted orientation model that is related to our work.', 'here, we investigate in detail the effect of restricting the word re - ordering to neighbor block swapping only.', 'in this paper, we assume a block generation process that generates block sequences from bottom to top, one block at a time.', 'the score of a successor block depends on its predecessor block', 'where is a block and ).', 'the neutral orientation is not modeled explicitly in this paper, rather it is handled as a default case as explained below.', 'in fig. 1, the orientation sequence is.', 'these counts are defined via an enumeration process and are used to define the orientation model e 9 ¡']",3
"['monotone model described in  #TAUTHOR_TAG.', 'here, we select blocks from']","['unigram monotone model described in  #TAUTHOR_TAG.', 'here, we select blocks from word - aligned training']","['adjacent block predecessors.', 'our baseline model is the unigram monotone model described in  #TAUTHOR_TAG.', 'here, we select blocks from']","['basic idea of the orientation model can be illustrated as follows : in the example translation in fig with right orientation, i. e. it is always involved in swapping.', 'this intuition is formalized using unigram counts with orientation.', 'the orientation model is related to the distortion model in  #AUTHOR_TAG, but we do not compute a block alignment during training.', 'we rather enumerate all relevant blocks in some order.', 'enumeration does not allow us to capture position dependent distortion probabilities, but we can compute statistics about adjacent block predecessors.', 'our baseline model is the unigram monotone model described in  #TAUTHOR_TAG.', 'here, we select blocks from word - aligned training data and unigram block occurrence counts 0 ¡ [UNK] a re computed : all blocks for a training sentence pair are enumerated in some order and we count how often a given block occurs in the parallel training data ¡ of the predecessor is ignored.', 'the are chosen to be optimal on the devtest set ( the optimal parameter setting is shown in table.', '1 ).', 'only two parameters have to be optimized due to the constraint that the have to sum to ` p', 'where', 'are not optimized separately, rather we define :', '. straightforward normalization over all successor blocks in eq. 2 and in eq. 3 is not feasible : there are tens of millions of possible successor blocks.', 'in future work, normalization over a restricted successor set, e. g. for a given source input sentence, all blocks that match this sentence might be useful for both training and decoding.', 'the segmentation model in eq. 1 naturally prefers translations that make use of a smaller number of blocks which leads to a smaller number of factors in eq. 1.', ""using fewer'bigger'blocks to carry out the translation generally seems to improve translation performance."", 'since normalization does not influence the number of blocks used to carry out the translation, it might be less important for our segmentation model.', 'we use a dp - based beam search procedure similar to the one presented in  #TAUTHOR_TAG.', 'we']",3
"['monotone model described in  #TAUTHOR_TAG.', 'here, we select blocks from']","['unigram monotone model described in  #TAUTHOR_TAG.', 'here, we select blocks from word - aligned training']","['adjacent block predecessors.', 'our baseline model is the unigram monotone model described in  #TAUTHOR_TAG.', 'here, we select blocks from']","['basic idea of the orientation model can be illustrated as follows : in the example translation in fig with right orientation, i. e. it is always involved in swapping.', 'this intuition is formalized using unigram counts with orientation.', 'the orientation model is related to the distortion model in  #AUTHOR_TAG, but we do not compute a block alignment during training.', 'we rather enumerate all relevant blocks in some order.', 'enumeration does not allow us to capture position dependent distortion probabilities, but we can compute statistics about adjacent block predecessors.', 'our baseline model is the unigram monotone model described in  #TAUTHOR_TAG.', 'here, we select blocks from word - aligned training data and unigram block occurrence counts 0 ¡ [UNK] a re computed : all blocks for a training sentence pair are enumerated in some order and we count how often a given block occurs in the parallel training data ¡ of the predecessor is ignored.', 'the are chosen to be optimal on the devtest set ( the optimal parameter setting is shown in table.', '1 ).', 'only two parameters have to be optimized due to the constraint that the have to sum to ` p', 'where', 'are not optimized separately, rather we define :', '. straightforward normalization over all successor blocks in eq. 2 and in eq. 3 is not feasible : there are tens of millions of possible successor blocks.', 'in future work, normalization over a restricted successor set, e. g. for a given source input sentence, all blocks that match this sentence might be useful for both training and decoding.', 'the segmentation model in eq. 1 naturally prefers translations that make use of a smaller number of blocks which leads to a smaller number of factors in eq. 1.', ""using fewer'bigger'blocks to carry out the translation generally seems to improve translation performance."", 'since normalization does not influence the number of blocks used to carry out the translation, it might be less important for our segmentation model.', 'we use a dp - based beam search procedure similar to the one presented in  #TAUTHOR_TAG.', 'we']",5
"['model presented in  #TAUTHOR_TAG.', 'for the ) model, the sentence is translated mostly monotonously, and only']","['model presented in  #TAUTHOR_TAG.', 'for the ) model, the sentence is translated mostly monotonously, and only']","['translation system is tested on an arabic - to - english translation task.', 'the training data comes from the un news sources :.', 'this is the model presented in  #TAUTHOR_TAG.', 'for the ) model, the sentence is translated mostly monotonously, and only neighbor blocks are allowed to be swapped (']","['translation system is tested on an arabic - to - english translation task.', 'the training data comes from the un news sources :.', 'this is the model presented in  #TAUTHOR_TAG.', 'for the ) model, the sentence is translated mostly monotonously, and only neighbor blocks are allowed to be swapped ( at most ` block is skipped ).', 'the table 1 : three bleu results are presented for both devtest set and blind test set.', 'two scaling parameters are set on the devtest set and copied for use on the blind test set.', 'the second column shows the model name, the third column presents the optimal weighting as obtained from the devtest set by carrying out an exhaustive grid search.', 'the fourth column shows bleu results together with confidence intervals ( here, the word casing is ignored ).', 'the block swapping model table 2 presents devtest set example blocks that have actually been swapped.', 'the training data is unsegmented, as can be seen from the first two blocks.', 'the block in the first line has been seen times more often with left than with right orientation.', 'blocks for which the [UNK] © 9 © 9', 'is bigger than u p s t q are likely candidates for swapping in our arabic - english experiments.', 'the [UNK] is not currently used in the orientation model.', 'the orientation model mostly effects blocks where the arabic and english words are verbs or nouns.', 'as shown in fig. 1, the orientation model uses the orientation probability a ¡ for the noun block, and only the default model for the adjective block ¡.', 'although the noun block might occur by itself without adjective, the swapping is not controlled by the occurrence of the adjective block ¢ ¡ ( which does not have adjacent predecessors ).', 'we rather model the fact that a noun block is typically preceded by some block ¥ ©', '. this situation seems typical for the block swapping that occurs on the evaluation test set']",5
"['given a single cue word  #TAUTHOR_TAG 15 ].', 'the representations']","['given a single cue word  #TAUTHOR_TAG 15 ].', 'the representations']","['given a single cue word  #TAUTHOR_TAG 15 ].', 'the representations']","['', 'it is well known that word representations can be learned from the distributional patterns in corpora.', ""originally, such representations were constructed by counting word co - occurrences, so that the features in one word's representation corresponded to other words [ 11, 17 ]."", 'neural language models, an alternative means to learn word representations, use language data to optimise ( latent ) features with respect to a language modelling objective.', 'the objective can be to predict either the next word given the initial words of a sentence [ 4, 14, 8 ], or simply a nearby word given a single cue word  #TAUTHOR_TAG 15 ].', 'the representations learned by neural models ( sometimes called embeddings ) generally outperform those acquired by co - occurrence counting models when applied to nlp tasks [ 3 ].', 'despite these clear results, it is not well understood how the architecture of neural models affects the information encoded in their embeddings.', 'here, we explore this question by considering the embeddings learned by architectures with a very different objective function to monolingual language models : neural machine translation models.', 'we show that translation - based embeddings outperform monolingual embeddings on two types of task : those that require knowledge of conceptual similarity ( rather than simply association or relatedness ), and those that require knowledge of syntactic role.', 'we discuss what the findings indicate about the information content of different embeddings, and suggest how this content might emerge as a consequence of the translation objective.', 'both neural language models and translation models learn real - valued embeddings ( of specified dimension ) for words in some pre - specified vocabulary, v, covering many or all words in their training corpus.', ""at each training step, a'score'for the current training example ( or batch ) is computed based on the embeddings in their current state."", ""this score is compared to the model's objective function, and the error is backpropagated to update both the model weights ( affecting how the score is computed from the embeddings ) and the embedding features."", 'at the end of this process, the embeddings should encode information that enables the model to optimally satisfy its objective']",0
"[' #TAUTHOR_TAG 15 ].', 'given a single']","[' #TAUTHOR_TAG 15 ].', 'given a single']","['all sentences in the training corpus.', 'more recent work has shown that high quality word embeddings can be learned via models with no nonlinear hidden layer  #TAUTHOR_TAG 15 ].', 'given a single word in the corpus, these models simply predict']","['the original neural language model [ 4 ] and subsequent variants [ 8 ], each training example consists of n subsequent words, of which the model is trained to predict the n - th word given the first n − 1 words.', ""the model first represents the input as an ordered sequence of embeddings, which it transforms into a single fixed length'hidden'representation by, e. g., concatenation and non - linear projection."", 'based on this representation, a probability distribution is computed over the vocabulary, from which the model can sample a guess at the next word.', 'the model weights and embeddings are updated to maximise the probability of correct guesses for all sentences in the training corpus.', 'more recent work has shown that high quality word embeddings can be learned via models with no nonlinear hidden layer  #TAUTHOR_TAG 15 ].', 'given a single word in the corpus, these models simply predict which other words will occur nearby.', 'for each word w in v, a list of training cases ( w, c ) : c ∈ v is extracted from the training corpus.', ""for instance, in the skipgram approach  #TAUTHOR_TAG, for each'cue word'w the'context words'c are sampled from windows either side of tokens of w in the corpus ( with c more likely to be sampled if it occurs closer to w )."", '1 for each w in v, the model initialises both a cue - embedding, representing the w when it occurs as a cue - word, and a context - embedding, used when w occurs as a context - word.', 'for a cue word w, the model can use the corresponding cueembedding and all context - embeddings to compute a probability distribution over v that reflects the probability of a word occurring in the context of w. when a training example ( w, c ) is observed, the model updates both the cue - word embedding of w and the context - word embeddings in order to increase the conditional probability of c']",0
"[' #TAUTHOR_TAG 15 ].', 'given a single']","[' #TAUTHOR_TAG 15 ].', 'given a single']","['all sentences in the training corpus.', 'more recent work has shown that high quality word embeddings can be learned via models with no nonlinear hidden layer  #TAUTHOR_TAG 15 ].', 'given a single word in the corpus, these models simply predict']","['the original neural language model [ 4 ] and subsequent variants [ 8 ], each training example consists of n subsequent words, of which the model is trained to predict the n - th word given the first n − 1 words.', ""the model first represents the input as an ordered sequence of embeddings, which it transforms into a single fixed length'hidden'representation by, e. g., concatenation and non - linear projection."", 'based on this representation, a probability distribution is computed over the vocabulary, from which the model can sample a guess at the next word.', 'the model weights and embeddings are updated to maximise the probability of correct guesses for all sentences in the training corpus.', 'more recent work has shown that high quality word embeddings can be learned via models with no nonlinear hidden layer  #TAUTHOR_TAG 15 ].', 'given a single word in the corpus, these models simply predict which other words will occur nearby.', 'for each word w in v, a list of training cases ( w, c ) : c ∈ v is extracted from the training corpus.', ""for instance, in the skipgram approach  #TAUTHOR_TAG, for each'cue word'w the'context words'c are sampled from windows either side of tokens of w in the corpus ( with c more likely to be sampled if it occurs closer to w )."", '1 for each w in v, the model initialises both a cue - embedding, representing the w when it occurs as a cue - word, and a context - embedding, used when w occurs as a context - word.', 'for a cue word w, the model can use the corresponding cueembedding and all context - embeddings to compute a probability distribution over v that reflects the probability of a word occurring in the context of w. when a training example ( w, c ) is observed, the model updates both the cue - word embedding of w and the context - word embeddings in order to increase the conditional probability of c']",0
"['analogy questions are an alternative way of evaluating word representations  #TAUTHOR_TAG 15 ].', 'in this task, models must identify the']","['analogy questions are an alternative way of evaluating word representations  #TAUTHOR_TAG 15 ].', 'in this task, models must identify the']","['analogy questions are an alternative way of evaluating word representations  #TAUTHOR_TAG 15 ].', 'in this task, models must identify']","['analogy questions are an alternative way of evaluating word representations  #TAUTHOR_TAG 15 ].', ""in this task, models must identify the correct answer ( girl ) when presented with questions such as'man is to boy as woman is to... '."", 'for skipgram - style embeddings, it has been shown that if m, b and w are the embeddings for man, boy and woman respectively, the correct answer is often the nearest neighbour in the vocabulary ( by cosine distance ) to the vector v = w + b − m  #TAUTHOR_TAG.', 'monolingual skipgram / glove models are better at semantic analogies ( father, man ; mother, woman )', 'we evaluated the embeddings on this task using the same vector - algebra method as  #TAUTHOR_TAG.', 'as before we excluded questions containing a word outside the intersection of all model vocabularies, and restricted all answer searches to this reduced vocabulary, leaving 11, 166 analogies.', ""of these, 7219 are classed as'syntactic ', in that they exemplify mappings between parts - of - speech or syntactic roles ( fast, fastest ; heavy - heaviest ), and 3947 are classed as'semantic'( ottawa, canada ; parisfrance ), deriving from wider world knowledge."", 'as shown in fig. 2, the translation - based embeddings seem to yield poor answers to semantic analogy questions, but are very effective for syntactic analogies, outperforming the monolingual embeddings, even those trained on much more data']",0
"['analogy questions are an alternative way of evaluating word representations  #TAUTHOR_TAG 15 ].', 'in this task, models must identify the']","['analogy questions are an alternative way of evaluating word representations  #TAUTHOR_TAG 15 ].', 'in this task, models must identify the']","['analogy questions are an alternative way of evaluating word representations  #TAUTHOR_TAG 15 ].', 'in this task, models must identify']","['analogy questions are an alternative way of evaluating word representations  #TAUTHOR_TAG 15 ].', ""in this task, models must identify the correct answer ( girl ) when presented with questions such as'man is to boy as woman is to... '."", 'for skipgram - style embeddings, it has been shown that if m, b and w are the embeddings for man, boy and woman respectively, the correct answer is often the nearest neighbour in the vocabulary ( by cosine distance ) to the vector v = w + b − m  #TAUTHOR_TAG.', 'monolingual skipgram / glove models are better at semantic analogies ( father, man ; mother, woman )', 'we evaluated the embeddings on this task using the same vector - algebra method as  #TAUTHOR_TAG.', 'as before we excluded questions containing a word outside the intersection of all model vocabularies, and restricted all answer searches to this reduced vocabulary, leaving 11, 166 analogies.', ""of these, 7219 are classed as'syntactic ', in that they exemplify mappings between parts - of - speech or syntactic roles ( fast, fastest ; heavy - heaviest ), and 3947 are classed as'semantic'( ottawa, canada ; parisfrance ), deriving from wider world knowledge."", 'as shown in fig. 2, the translation - based embeddings seem to yield poor answers to semantic analogy questions, but are very effective for syntactic analogies, outperforming the monolingual embeddings, even those trained on much more data']",0
['monolingual skipgram model  #TAUTHOR_TAG and'],['monolingual skipgram model  #TAUTHOR_TAG and'],['a monolingual skipgram model  #TAUTHOR_TAG and'],"['learn translation - based embeddings, we trained both the rnn encoder - decoder [ rnnenc, 7 ] and the rnn search architectures [ 2 ] on a 300m word corpus of english - french sentence pairs.', 'we conducted all experiments with the resulting ( english ) source embeddings from these models.', 'for comparison, we trained a monolingual skipgram model  #TAUTHOR_TAG and its glove variant [ 15 ] for the same number of epochs on the english half of the bilingual corpus.', 'we also extracted embeddings from a full - sentence language model [ cw, 8 ] trained for several months on a larger 1bn word corpus.', 'as in previous studies [ 1, 5, 3 ], we evaluate embeddings by calculating pairwise ( cosine ) distances and correlating these distances with ( gold - standard ) human judgements.', 'table 1 shows the correlations of different model embeddings with three such gold - standard resources, wordsim - 353 [ 1 ], men [ 5 ] and simlex - 999 [ 10 ].', 'interestingly, translation embeddings perform best on simlex - 999, while the two sets of monolingual embeddings perform better on modelling the men and wordsim - 353.', 'to interpret these results, it should be noted that simlex - 999 evaluation quantifies conceptual similarity ( dog - wolf ), whereas men and wordsim - 353 ( despite its name ) quantify more general relatedness ( dog - collar ) [ 10 ].', 'the results seem to indicate that translation - based embeddings better capture similarity, while monolingual embeddings better capture relatedness.', 'table 1 : translation - based embeddings outperform alternatives on similarity - focused evaluations.', 'to test this hypothesis further, we ran two more evaluations focused specifically on similarity.', 'the toefl synonym test contains 80 cue words, each with four possible answers, of which one is a correct synonym [ 11 ].', ""we computed the proportion of questions answered correctly by each model, where a model's answer was the nearest ( cosine ) neighbour to the cue word in its vocabulary."", '3 in addition, we tested how well different embeddings enabled a supervised classifier to distinguish between synonyms and antonyms.', 'for 500 hand - labelled pairs we presented a gaussian svm with the concatenation of the two word embeddings.', 'we evaluated accuracy using 8 - fold cross - validation.', 'as shown in table 1, translation - based embeddings outperform all monolingual embeddings on these two additional similarity - focused tasks.', 'qualitative analysis of nearest neighbours ( bottom rows ) also supports the conclusion that proximity in the translation embedding space corresponds to similarity while proximity in the monol']",5
"['analogy questions are an alternative way of evaluating word representations  #TAUTHOR_TAG 15 ].', 'in this task, models must identify the']","['analogy questions are an alternative way of evaluating word representations  #TAUTHOR_TAG 15 ].', 'in this task, models must identify the']","['analogy questions are an alternative way of evaluating word representations  #TAUTHOR_TAG 15 ].', 'in this task, models must identify']","['analogy questions are an alternative way of evaluating word representations  #TAUTHOR_TAG 15 ].', ""in this task, models must identify the correct answer ( girl ) when presented with questions such as'man is to boy as woman is to... '."", 'for skipgram - style embeddings, it has been shown that if m, b and w are the embeddings for man, boy and woman respectively, the correct answer is often the nearest neighbour in the vocabulary ( by cosine distance ) to the vector v = w + b − m  #TAUTHOR_TAG.', 'monolingual skipgram / glove models are better at semantic analogies ( father, man ; mother, woman )', 'we evaluated the embeddings on this task using the same vector - algebra method as  #TAUTHOR_TAG.', 'as before we excluded questions containing a word outside the intersection of all model vocabularies, and restricted all answer searches to this reduced vocabulary, leaving 11, 166 analogies.', ""of these, 7219 are classed as'syntactic ', in that they exemplify mappings between parts - of - speech or syntactic roles ( fast, fastest ; heavy - heaviest ), and 3947 are classed as'semantic'( ottawa, canada ; parisfrance ), deriving from wider world knowledge."", 'as shown in fig. 2, the translation - based embeddings seem to yield poor answers to semantic analogy questions, but are very effective for syntactic analogies, outperforming the monolingual embeddings, even those trained on much more data']",5
"['different word embeddings.', ' #TAUTHOR_TAG show that using word embeddings for simple analog']","['different word embeddings.', ' #TAUTHOR_TAG show that using word embeddings for simple analogies surfaces many gender stereotypes.', '']","['different word embeddings.', ' #TAUTHOR_TAG show that using word embeddings for simple analogies surfaces many gender stereo']","['embeddings have become an important component in many nlp models and are widely used for a vast range of downstream tasks.', 'however, these word representations have been proven to reflect social biases ( e. g. race and gender ) that naturally occur in the data used to train them.', 'in this paper we focus on gender bias.', 'gender bias was demonstrated to be consistent and pervasive across different word embeddings.', ' #TAUTHOR_TAG show that using word embeddings for simple analogies surfaces many gender stereotypes.', 'for example, the word embedding they use ( word2vec embedding trained on the google news dataset 1  #AUTHOR_TAG ) an - 1 https : / / code. google. com / archive / p / word2vec / swer the analogy "" man is to computer programmer as woman is to x "" with "" x = homemaker "".', 'further demonstrate association between female / male names and groups of words stereotypically assigned to females / males ( e. g. arts vs. science ).', 'in addition, they demonstrate that word embeddings reflect actual gender gaps in reality by showing the correlation between the gender association of occupation words and labor - force participation data.', 'recently, some work has been done to reduce the gender bias in word embeddings, both as a post - processing step  #TAUTHOR_TAG and as part of the training procedure  #AUTHOR_TAG.', 'both works substantially reduce the bias with respect to the same definition : the projection on the gender direction ( i. e. − → he − −→ she ), introduced in the former.', 'they also show that performance on word similarity tasks is not hurt.', 'we argue that current debiasing methods, which lean on the above definition for gender bias and directly target it, are mostly hiding the bias rather than removing it.', 'we show that even when drastically reducing the gender bias according to this definition, it is still reflected in the geometry of the representation of "" gender - neutral "" words, and a lot of the bias information can be recovered']",0
"['different word embeddings.', ' #TAUTHOR_TAG show that using word embeddings for simple analog']","['different word embeddings.', ' #TAUTHOR_TAG show that using word embeddings for simple analogies surfaces many gender stereotypes.', '']","['different word embeddings.', ' #TAUTHOR_TAG show that using word embeddings for simple analogies surfaces many gender stereo']","['embeddings have become an important component in many nlp models and are widely used for a vast range of downstream tasks.', 'however, these word representations have been proven to reflect social biases ( e. g. race and gender ) that naturally occur in the data used to train them.', 'in this paper we focus on gender bias.', 'gender bias was demonstrated to be consistent and pervasive across different word embeddings.', ' #TAUTHOR_TAG show that using word embeddings for simple analogies surfaces many gender stereotypes.', 'for example, the word embedding they use ( word2vec embedding trained on the google news dataset 1  #AUTHOR_TAG ) an - 1 https : / / code. google. com / archive / p / word2vec / swer the analogy "" man is to computer programmer as woman is to x "" with "" x = homemaker "".', 'further demonstrate association between female / male names and groups of words stereotypically assigned to females / males ( e. g. arts vs. science ).', 'in addition, they demonstrate that word embeddings reflect actual gender gaps in reality by showing the correlation between the gender association of occupation words and labor - force participation data.', 'recently, some work has been done to reduce the gender bias in word embeddings, both as a post - processing step  #TAUTHOR_TAG and as part of the training procedure  #AUTHOR_TAG.', 'both works substantially reduce the bias with respect to the same definition : the projection on the gender direction ( i. e. − → he − −→ she ), introduced in the former.', 'they also show that performance on word similarity tasks is not hurt.', 'we argue that current debiasing methods, which lean on the above definition for gender bias and directly target it, are mostly hiding the bias rather than removing it.', 'we show that even when drastically reducing the gender bias according to this definition, it is still reflected in the geometry of the representation of "" gender - neutral "" words, and a lot of the bias information can be recovered']",0
"['follows we refer to words and their vectors interchangeably.', ' #TAUTHOR_TAG define the gender bias of a word w by']","['follows we refer to words and their vectors interchangeably.', ' #TAUTHOR_TAG define the gender bias of a word w by']","['follows we refer to words and their vectors interchangeably.', ' #TAUTHOR_TAG define the gender bias of a word w by']","['what follows we refer to words and their vectors interchangeably.', ' #TAUTHOR_TAG define the gender bias of a word w by its projection on the "" gender direction "" : − → w · ( − → he − −→ she ), assuming all vectors are normalized.', ""the larger a word's projection is on − → he − −→ she, the more biased it is."", 'they also quantify the bias in word embeddings using this definition and show it aligns well with social stereotypes']",0
"['is good gender debiasing', ': according to  #TAUTHOR_TAG, there is']","['is good gender debiasing', ': according to  #TAUTHOR_TAG, there is']","['that we show is insufficient. these works implicitly define what is good gender debiasing', ': according to  #TAUTHOR_TAG, there is no gender bias if each nonexplicitly']","['words, and encouraging words that belong to different groups to differ in their last coordinate. in addition, they encourage', 'the representation of neutral - gender words ( excluding the last coordinate ) to be orthogonal to', 'the gender direction. 4 this work did a step forward by trying to 2 another work in this spirit is that of  #AUTHOR_TAG, which uses an adversarial network to debias word embeddings. there, the authors rely on the same', 'definition of gender bias that considers the projection on the gender direction. we expect similar results for', 'this method as well, however, we did not verify that. 3 the gender direction is chosen to be the top principal component ( pc ) of ten gender pair difference vectors. 4 the gender direction is estimated during training by', 'averaging the differences between female words and their male remove the bias during training rather than in postprocessing, which we believe to be the right approach. unfortunately, it relies on the same definition that we show is insufficient. these works implicitly define what is good gender debiasing', ': according to  #TAUTHOR_TAG, there is no gender bias if each nonexplicitly gendered word in the vocabulary', 'is in equal distance to both elements of all explicitly gendered pairs. in other words, if one cannot determine the gender association of a word by looking at its projection on any', 'gendered pair.  #AUTHOR_TAG the definition is similar, but restricted to projections on the gender', '- direction. remaining bias after using debiasing methods both works provide very compelling results as evidence of reducing the bias without hurting the performance of the embeddings for standard tasks. however', ', both methods and their results rely on the specific bias definition. we claim', 'that the bias is much more profound and systematic, and that simply reducing the projection of words on a gender direction is insufficient : it merely hides', 'the bias, which is still reflected in similarities between "" gender - neutral "" words ( i. e., words such as ""', 'math "" or "" delicate "" are in principle genderneutral, but in practice have strong stereotypical gender associations, which reflect on, and are reflected by,', 'neighbouring words ). our key observation is that, almost by definition, most word pairs maintain their previous similarity', ', despite their change in relation to the gender direction. the implication of this is that most', 'words that had a specific bias before are still grouped together, and apart from changes with respect to specific gendered', ""words, the word embeddings'spatial geometry stays largely the same. 5 in what follows, we provide a series of experiments that demonstrate the remaining bias in the debiased embeddings""]",0
"['is good gender debiasing', ': according to  #TAUTHOR_TAG, there is']","['is good gender debiasing', ': according to  #TAUTHOR_TAG, there is']","['that we show is insufficient. these works implicitly define what is good gender debiasing', ': according to  #TAUTHOR_TAG, there is no gender bias if each nonexplicitly']","['words, and encouraging words that belong to different groups to differ in their last coordinate. in addition, they encourage', 'the representation of neutral - gender words ( excluding the last coordinate ) to be orthogonal to', 'the gender direction. 4 this work did a step forward by trying to 2 another work in this spirit is that of  #AUTHOR_TAG, which uses an adversarial network to debias word embeddings. there, the authors rely on the same', 'definition of gender bias that considers the projection on the gender direction. we expect similar results for', 'this method as well, however, we did not verify that. 3 the gender direction is chosen to be the top principal component ( pc ) of ten gender pair difference vectors. 4 the gender direction is estimated during training by', 'averaging the differences between female words and their male remove the bias during training rather than in postprocessing, which we believe to be the right approach. unfortunately, it relies on the same definition that we show is insufficient. these works implicitly define what is good gender debiasing', ': according to  #TAUTHOR_TAG, there is no gender bias if each nonexplicitly gendered word in the vocabulary', 'is in equal distance to both elements of all explicitly gendered pairs. in other words, if one cannot determine the gender association of a word by looking at its projection on any', 'gendered pair.  #AUTHOR_TAG the definition is similar, but restricted to projections on the gender', '- direction. remaining bias after using debiasing methods both works provide very compelling results as evidence of reducing the bias without hurting the performance of the embeddings for standard tasks. however', ', both methods and their results rely on the specific bias definition. we claim', 'that the bias is much more profound and systematic, and that simply reducing the projection of words on a gender direction is insufficient : it merely hides', 'the bias, which is still reflected in similarities between "" gender - neutral "" words ( i. e., words such as ""', 'math "" or "" delicate "" are in principle genderneutral, but in practice have strong stereotypical gender associations, which reflect on, and are reflected by,', 'neighbouring words ). our key observation is that, almost by definition, most word pairs maintain their previous similarity', ', despite their change in relation to the gender direction. the implication of this is that most', 'words that had a specific bias before are still grouped together, and apart from changes with respect to specific gendered', ""words, the word embeddings'spatial geometry stays largely the same. 5 in what follows, we provide a series of experiments that demonstrate the remaining bias in the debiased embeddings""]",0
"['is good gender debiasing', ': according to  #TAUTHOR_TAG, there is']","['is good gender debiasing', ': according to  #TAUTHOR_TAG, there is']","['that we show is insufficient. these works implicitly define what is good gender debiasing', ': according to  #TAUTHOR_TAG, there is no gender bias if each nonexplicitly']","['words, and encouraging words that belong to different groups to differ in their last coordinate. in addition, they encourage', 'the representation of neutral - gender words ( excluding the last coordinate ) to be orthogonal to', 'the gender direction. 4 this work did a step forward by trying to 2 another work in this spirit is that of  #AUTHOR_TAG, which uses an adversarial network to debias word embeddings. there, the authors rely on the same', 'definition of gender bias that considers the projection on the gender direction. we expect similar results for', 'this method as well, however, we did not verify that. 3 the gender direction is chosen to be the top principal component ( pc ) of ten gender pair difference vectors. 4 the gender direction is estimated during training by', 'averaging the differences between female words and their male remove the bias during training rather than in postprocessing, which we believe to be the right approach. unfortunately, it relies on the same definition that we show is insufficient. these works implicitly define what is good gender debiasing', ': according to  #TAUTHOR_TAG, there is no gender bias if each nonexplicitly gendered word in the vocabulary', 'is in equal distance to both elements of all explicitly gendered pairs. in other words, if one cannot determine the gender association of a word by looking at its projection on any', 'gendered pair.  #AUTHOR_TAG the definition is similar, but restricted to projections on the gender', '- direction. remaining bias after using debiasing methods both works provide very compelling results as evidence of reducing the bias without hurting the performance of the embeddings for standard tasks. however', ', both methods and their results rely on the specific bias definition. we claim', 'that the bias is much more profound and systematic, and that simply reducing the projection of words on a gender direction is insufficient : it merely hides', 'the bias, which is still reflected in similarities between "" gender - neutral "" words ( i. e., words such as ""', 'math "" or "" delicate "" are in principle genderneutral, but in practice have strong stereotypical gender associations, which reflect on, and are reflected by,', 'neighbouring words ). our key observation is that, almost by definition, most word pairs maintain their previous similarity', ', despite their change in relation to the gender direction. the implication of this is that most', 'words that had a specific bias before are still grouped together, and apart from changes with respect to specific gendered', ""words, the word embeddings'spatial geometry stays largely the same. 5 in what follows, we provide a series of experiments that demonstrate the remaining bias in the debiased embeddings""]",0
[' #TAUTHOR_TAG and gn'],['refer to the word embeddings of the previous works as hard - debiased  #TAUTHOR_TAG and gn - glove ( gender - neutral'],['refer to the word embeddings of the previous works as hard - debiased  #TAUTHOR_TAG and gn'],"['refer to the word embeddings of the previous works as hard - debiased  #TAUTHOR_TAG and gn - glove ( gender - neutral glove ) counterparts in a predefined set.', ' #AUTHOR_TAG.', 'for each debiased word embedding we quantify the hidden bias with respect to the biased version.', 'for hard - debiased we compare to the embeddings before applying the debiasing procedure.', 'for gn - glove we compare to embedding trained with standard glove on the same corpus.', '6 unless otherwise specified, we follow  #TAUTHOR_TAG and use a reduced version of the vocabulary for both word embeddings : we take the most frequent 50, 000 words and phrases and remove words with upper - case letters, digits, or punctuation, and words longer than 20 characters.', 'in addition, to avoid quantifying the bias of words that are inherently gendered ( e. g. mother, father, queen ), we remove from each vocabulary the respective set of gendered words as pre - defined in each work.', '7 this yeilds a vocabulary of 26, 189 words for hard - debiased and of 47, 698 words for gn - glove.', 'as explained in section 2 and according to the definition in previous works, we compute the bias of a word by taking its projection on the gender direction :', '− → he − −→ she.', 'in order to quantify the association between sets of words, we follow and use their word embedding association test ( weat ) : consider two sets of target words ( e. g., male and female professions ) and two sets of attribute words ( e. g., male and female names ).', 'a permutation test estimates the probability that a random permutation of the target words would produce equal or greater similarities to the attribute sets']",3
,,,,3
,,,,3
[' #TAUTHOR_TAG and gn'],['refer to the word embeddings of the previous works as hard - debiased  #TAUTHOR_TAG and gn - glove ( gender - neutral'],['refer to the word embeddings of the previous works as hard - debiased  #TAUTHOR_TAG and gn'],"['refer to the word embeddings of the previous works as hard - debiased  #TAUTHOR_TAG and gn - glove ( gender - neutral glove ) counterparts in a predefined set.', ' #AUTHOR_TAG.', 'for each debiased word embedding we quantify the hidden bias with respect to the biased version.', 'for hard - debiased we compare to the embeddings before applying the debiasing procedure.', 'for gn - glove we compare to embedding trained with standard glove on the same corpus.', '6 unless otherwise specified, we follow  #TAUTHOR_TAG and use a reduced version of the vocabulary for both word embeddings : we take the most frequent 50, 000 words and phrases and remove words with upper - case letters, digits, or punctuation, and words longer than 20 characters.', 'in addition, to avoid quantifying the bias of words that are inherently gendered ( e. g. mother, father, queen ), we remove from each vocabulary the respective set of gendered words as pre - defined in each work.', '7 this yeilds a vocabulary of 26, 189 words for hard - debiased and of 47, 698 words for gn - glove.', 'as explained in section 2 and according to the definition in previous works, we compute the bias of a word by taking its projection on the gender direction :', '− → he − −→ she.', 'in order to quantify the association between sets of words, we follow and use their word embedding association test ( weat ) : consider two sets of target words ( e. g., male and female professions ) and two sets of attribute words ( e. g., male and female names ).', 'a permutation test estimates the probability that a random permutation of the target words would produce equal or greater similarities to the attribute sets']",4
[' #TAUTHOR_TAG and gn'],['refer to the word embeddings of the previous works as hard - debiased  #TAUTHOR_TAG and gn - glove ( gender - neutral'],['refer to the word embeddings of the previous works as hard - debiased  #TAUTHOR_TAG and gn'],"['refer to the word embeddings of the previous works as hard - debiased  #TAUTHOR_TAG and gn - glove ( gender - neutral glove ) counterparts in a predefined set.', ' #AUTHOR_TAG.', 'for each debiased word embedding we quantify the hidden bias with respect to the biased version.', 'for hard - debiased we compare to the embeddings before applying the debiasing procedure.', 'for gn - glove we compare to embedding trained with standard glove on the same corpus.', '6 unless otherwise specified, we follow  #TAUTHOR_TAG and use a reduced version of the vocabulary for both word embeddings : we take the most frequent 50, 000 words and phrases and remove words with upper - case letters, digits, or punctuation, and words longer than 20 characters.', 'in addition, to avoid quantifying the bias of words that are inherently gendered ( e. g. mother, father, queen ), we remove from each vocabulary the respective set of gendered words as pre - defined in each work.', '7 this yeilds a vocabulary of 26, 189 words for hard - debiased and of 47, 698 words for gn - glove.', 'as explained in section 2 and according to the definition in previous works, we compute the bias of a word by taking its projection on the gender direction :', '− → he − −→ she.', 'in order to quantify the association between sets of words, we follow and use their word embedding association test ( weat ) : consider two sets of target words ( e. g., male and female professions ) and two sets of attribute words ( e. g., male and female names ).', 'a permutation test estimates the probability that a random permutation of the target words would produce equal or greater similarities to the attribute sets']",6
,,,,5
,,,,5
['resolution  #TAUTHOR_TAG use syntactic prep'],['on bridging anaphora resolution  #TAUTHOR_TAG use syntactic preposition patterns'],['on bridging anaphora resolution  #TAUTHOR_TAG use syntactic preposition patterns'],"['work on bridging anaphora resolution  #TAUTHOR_TAG use syntactic preposition patterns to calculate word relatedness.', ""however, such patterns only consider nps'head nouns and hence do not fully capture the semantics of nps."", ' #AUTHOR_TAG created word embeddings ( embeddings pp ) to capture associative similarity ( i. e., relatedness ) between nouns by exploring the syntactic structure of noun phrases. but embeddings pp only contains word representations for nouns.', 'in this paper, we create new word vectors by combining embeddings pp with glove.', 'this new word embeddings ( embeddings bridging ) are a more general lexical knowledge resource for bridging and allow us to represent the meaning of an np beyond its head easily.', 'we therefore develop a deterministic approach for bridging anaphora resolution, which represents the semantics of an np based on its head noun and modifications.', 'we show that this simple approach achieves the competitive results compared to the best system in  #TAUTHOR_TAG which explores markov logic networks to model the problem.', ""additionally, we further improve the results for bridging anaphora resolution reported in  #AUTHOR_TAG by combining our simple deterministic approach with  #TAUTHOR_TAG's best system mln ii""]",0
['on bridging  #TAUTHOR_TAG focus on bridging'],['on bridging  #TAUTHOR_TAG focus on bridging'],['on bridging  #TAUTHOR_TAG focus on bridging'],"['. most previous empirical research on bridging  #TAUTHOR_TAG focus on bridging anaphora resolution, a subt', '##ask of bridging resolution that aims to choose the antecedents for bridging anaphors. for this substask, most previous work  #TAUTHOR_TAG calculate semantic relatedness between an anaphor and its antecedent based on word co - occurrence counts using certain syntactic patterns. however, such patterns only consider', 'head noun knowledge and hence are not sufficient for bridging relations which require the semantics of modification. in example 1, in order to find the antecedent ( dialysis products ) for the bridging anaphor "" distribution arrangements "", we', 'have to understand the semantics of the modification "" distribution "". over the past few years, word embeddings gained a lot popularity in the nlp community. state - of - the - art word vectors such as', 'word2vec skip - gram and glove  #AUTHOR_TAG have been shown to perform well across a variety', 'of nlp tasks, including textual entailment ( rocktaschel et al., 2016 ), reading comprehension  #AUTHOR_TAG and coreference resolution  #AUTHOR_TAG.  #AUTHOR_TAG found that', 'these vanilla word embeddings capture both "" genuine "" similarity and relatedness, and hence they are not suitable for bridging anaphora resolution which requires lexical association knowledge instead', 'of semantic similarity information between synonyms or hypernyms.  #AUTHOR_TAG created word embeddings for bridging ( embeddings pp ) by exploring', 'the syntactic structure of noun phrases ( nps ) to derive contexts for nouns in the glove model. however, embeddings pp only contains the word representations for nouns. in this paper, we', 'improve embeddings pp by combining it with glove. the resulting word embeddings ( embeddings bridging )', 'are a more general lexical knowledge resource for bridging anaphora resolution. compared to embeddings pp, the coverage of lexicon in embeddings bridging is much larger. also the word representations for nouns without the suffix "" pp "" are more accurate because they are trained on many more instances in the vanilla glove.', ""based on this general vector space, we develop a deterministic algorithm to select antecedents for bridging anaphors. our approach combines the semantics of an np's head with the semantics of its modifications by vector average using embeddings bridging. we"", 'show that this simple, efficient method achieves the competitive results on isnotes for the task of bridging anaphora resolution compared to the best system in  #TAUTHOR_TAG which explores markov', 'logic networks to model the problem. the main contributions of our', 'work are : ( 1 ) a general word representation resource 2 for bridging ;', 'and ( 2 ) a simple yet competitive deterministic approach for bridging anaphora resolution which models the meaning of an', 'np based on its head noun and modifications']",0
['on bridging  #TAUTHOR_TAG focus on bridging'],['on bridging  #TAUTHOR_TAG focus on bridging'],['on bridging  #TAUTHOR_TAG focus on bridging'],"['. most previous empirical research on bridging  #TAUTHOR_TAG focus on bridging anaphora resolution, a subt', '##ask of bridging resolution that aims to choose the antecedents for bridging anaphors. for this substask, most previous work  #TAUTHOR_TAG calculate semantic relatedness between an anaphor and its antecedent based on word co - occurrence counts using certain syntactic patterns. however, such patterns only consider', 'head noun knowledge and hence are not sufficient for bridging relations which require the semantics of modification. in example 1, in order to find the antecedent ( dialysis products ) for the bridging anaphor "" distribution arrangements "", we', 'have to understand the semantics of the modification "" distribution "". over the past few years, word embeddings gained a lot popularity in the nlp community. state - of - the - art word vectors such as', 'word2vec skip - gram and glove  #AUTHOR_TAG have been shown to perform well across a variety', 'of nlp tasks, including textual entailment ( rocktaschel et al., 2016 ), reading comprehension  #AUTHOR_TAG and coreference resolution  #AUTHOR_TAG.  #AUTHOR_TAG found that', 'these vanilla word embeddings capture both "" genuine "" similarity and relatedness, and hence they are not suitable for bridging anaphora resolution which requires lexical association knowledge instead', 'of semantic similarity information between synonyms or hypernyms.  #AUTHOR_TAG created word embeddings for bridging ( embeddings pp ) by exploring', 'the syntactic structure of noun phrases ( nps ) to derive contexts for nouns in the glove model. however, embeddings pp only contains the word representations for nouns. in this paper, we', 'improve embeddings pp by combining it with glove. the resulting word embeddings ( embeddings bridging )', 'are a more general lexical knowledge resource for bridging anaphora resolution. compared to embeddings pp, the coverage of lexicon in embeddings bridging is much larger. also the word representations for nouns without the suffix "" pp "" are more accurate because they are trained on many more instances in the vanilla glove.', ""based on this general vector space, we develop a deterministic algorithm to select antecedents for bridging anaphors. our approach combines the semantics of an np's head with the semantics of its modifications by vector average using embeddings bridging. we"", 'show that this simple, efficient method achieves the competitive results on isnotes for the task of bridging anaphora resolution compared to the best system in  #TAUTHOR_TAG which explores markov', 'logic networks to model the problem. the main contributions of our', 'work are : ( 1 ) a general word representation resource 2 for bridging ;', 'and ( 2 ) a simple yet competitive deterministic approach for bridging anaphora resolution which models the meaning of an', 'np based on its head noun and modifications']",0
[' #TAUTHOR_TAG. we also'],"[' #TAUTHOR_TAG. we also improve', '']","['advanced machine learning - based approach  #TAUTHOR_TAG. we also improve', '']","['', 'bridging anaphors are not limited to definite nps as in previous work  #AUTHOR_TAG ( poesio et al.,, 2004  #AUTHOR_TAG. also', 'in isnotes, the semantic relations between anaphor and antecedent are not restricted to meronymic relations. we therefore choose isnotes to evaluate', 'our algorithm for bridging anaphora resolution. our approach is deterministic and simple, but achieves the competitive results compared to the advanced machine learning - based approach  #TAUTHOR_TAG. we also improve', '']",0
[' #TAUTHOR_TAG. we also'],"[' #TAUTHOR_TAG. we also improve', '']","['advanced machine learning - based approach  #TAUTHOR_TAG. we also improve', '']","['', 'bridging anaphors are not limited to definite nps as in previous work  #AUTHOR_TAG ( poesio et al.,, 2004  #AUTHOR_TAG. also', 'in isnotes, the semantic relations between anaphor and antecedent are not restricted to meronymic relations. we therefore choose isnotes to evaluate', 'our algorithm for bridging anaphora resolution. our approach is deterministic and simple, but achieves the competitive results compared to the advanced machine learning - based approach  #TAUTHOR_TAG. we also improve', '']",0
"["" #TAUTHOR_TAG's experimental setup, we""]","["" #TAUTHOR_TAG's experimental setup, we""]","["" #TAUTHOR_TAG's experimental setup, we resolve bridging""]","["" #TAUTHOR_TAG's experimental setup, we resolve bridging anaphors to entity antecedents."", '']",0
"["" #TAUTHOR_TAG's experimental setup, we""]","["" #TAUTHOR_TAG's experimental setup, we""]","["" #TAUTHOR_TAG's experimental setup, we resolve bridging""]","["" #TAUTHOR_TAG's experimental setup, we resolve bridging anaphors to entity antecedents."", '']",0
"['ii system  #TAUTHOR_TAG.', 'similarly, we add a constraint on top of mln ii using']","['head ) based on embeddings pp into the mln ii system  #TAUTHOR_TAG.', 'similarly, we add a constraint on top of mln ii using']","['noun modifiers ( appearing before the head ) based on embeddings pp into the mln ii system  #TAUTHOR_TAG.', 'similarly, we add a constraint on top of mln ii using']","['bridging anaphora resolution,  #AUTHOR_TAG integrates a much simpler deterministic approach by combining an np head with its noun modifiers ( appearing before the head ) based on embeddings pp into the mln ii system  #TAUTHOR_TAG.', 'similarly, we add a constraint on top of mln ii using our deterministic approach ( np head + modifiers ) based on embeddings bridging.', 'table 8 lists the results of different systems 8 for bridging anaphora resolution in isnotes.', 'it shows that combining our deterministic approach ( np head + modifiers ) with mln ii slightly improves the result compared to  #AUTHOR_TAG.', 'although combining np head + modifiers with mln ii achieves significant improvement over np 8 we also reimplement the algorithms from schulte im  #AUTHOR_TAG and  #AUTHOR_TAG as baselines ( table 8 ).', 'schulte im  #AUTHOR_TAG resolved bridging anaphors to the closest antecedent candidate in a high - dimensional space.', 'we use the 2, 000 most frequent words ( adjectives, common nouns, proper nouns, and lexical verbs ) from gigaword as the context words.', ' #AUTHOR_TAG applied a pairwise model combining lexical semantic features and salience features to perform mereological bridging resolution in the gnome corpus.', '']",0
['resolution  #TAUTHOR_TAG use syntactic prep'],['on bridging anaphora resolution  #TAUTHOR_TAG use syntactic preposition patterns'],['on bridging anaphora resolution  #TAUTHOR_TAG use syntactic preposition patterns'],"['work on bridging anaphora resolution  #TAUTHOR_TAG use syntactic preposition patterns to calculate word relatedness.', ""however, such patterns only consider nps'head nouns and hence do not fully capture the semantics of nps."", ' #AUTHOR_TAG created word embeddings ( embeddings pp ) to capture associative similarity ( i. e., relatedness ) between nouns by exploring the syntactic structure of noun phrases. but embeddings pp only contains word representations for nouns.', 'in this paper, we create new word vectors by combining embeddings pp with glove.', 'this new word embeddings ( embeddings bridging ) are a more general lexical knowledge resource for bridging and allow us to represent the meaning of an np beyond its head easily.', 'we therefore develop a deterministic approach for bridging anaphora resolution, which represents the semantics of an np based on its head noun and modifications.', 'we show that this simple approach achieves the competitive results compared to the best system in  #TAUTHOR_TAG which explores markov logic networks to model the problem.', ""additionally, we further improve the results for bridging anaphora resolution reported in  #AUTHOR_TAG by combining our simple deterministic approach with  #TAUTHOR_TAG's best system mln ii""]",1
['resolution  #TAUTHOR_TAG use syntactic prep'],['on bridging anaphora resolution  #TAUTHOR_TAG use syntactic preposition patterns'],['on bridging anaphora resolution  #TAUTHOR_TAG use syntactic preposition patterns'],"['work on bridging anaphora resolution  #TAUTHOR_TAG use syntactic preposition patterns to calculate word relatedness.', ""however, such patterns only consider nps'head nouns and hence do not fully capture the semantics of nps."", ' #AUTHOR_TAG created word embeddings ( embeddings pp ) to capture associative similarity ( i. e., relatedness ) between nouns by exploring the syntactic structure of noun phrases. but embeddings pp only contains word representations for nouns.', 'in this paper, we create new word vectors by combining embeddings pp with glove.', 'this new word embeddings ( embeddings bridging ) are a more general lexical knowledge resource for bridging and allow us to represent the meaning of an np beyond its head easily.', 'we therefore develop a deterministic approach for bridging anaphora resolution, which represents the semantics of an np based on its head noun and modifications.', 'we show that this simple approach achieves the competitive results compared to the best system in  #TAUTHOR_TAG which explores markov logic networks to model the problem.', ""additionally, we further improve the results for bridging anaphora resolution reported in  #AUTHOR_TAG by combining our simple deterministic approach with  #TAUTHOR_TAG's best system mln ii""]",4
['on bridging  #TAUTHOR_TAG focus on bridging'],['on bridging  #TAUTHOR_TAG focus on bridging'],['on bridging  #TAUTHOR_TAG focus on bridging'],"['. most previous empirical research on bridging  #TAUTHOR_TAG focus on bridging anaphora resolution, a subt', '##ask of bridging resolution that aims to choose the antecedents for bridging anaphors. for this substask, most previous work  #TAUTHOR_TAG calculate semantic relatedness between an anaphor and its antecedent based on word co - occurrence counts using certain syntactic patterns. however, such patterns only consider', 'head noun knowledge and hence are not sufficient for bridging relations which require the semantics of modification. in example 1, in order to find the antecedent ( dialysis products ) for the bridging anaphor "" distribution arrangements "", we', 'have to understand the semantics of the modification "" distribution "". over the past few years, word embeddings gained a lot popularity in the nlp community. state - of - the - art word vectors such as', 'word2vec skip - gram and glove  #AUTHOR_TAG have been shown to perform well across a variety', 'of nlp tasks, including textual entailment ( rocktaschel et al., 2016 ), reading comprehension  #AUTHOR_TAG and coreference resolution  #AUTHOR_TAG.  #AUTHOR_TAG found that', 'these vanilla word embeddings capture both "" genuine "" similarity and relatedness, and hence they are not suitable for bridging anaphora resolution which requires lexical association knowledge instead', 'of semantic similarity information between synonyms or hypernyms.  #AUTHOR_TAG created word embeddings for bridging ( embeddings pp ) by exploring', 'the syntactic structure of noun phrases ( nps ) to derive contexts for nouns in the glove model. however, embeddings pp only contains the word representations for nouns. in this paper, we', 'improve embeddings pp by combining it with glove. the resulting word embeddings ( embeddings bridging )', 'are a more general lexical knowledge resource for bridging anaphora resolution. compared to embeddings pp, the coverage of lexicon in embeddings bridging is much larger. also the word representations for nouns without the suffix "" pp "" are more accurate because they are trained on many more instances in the vanilla glove.', ""based on this general vector space, we develop a deterministic algorithm to select antecedents for bridging anaphors. our approach combines the semantics of an np's head with the semantics of its modifications by vector average using embeddings bridging. we"", 'show that this simple, efficient method achieves the competitive results on isnotes for the task of bridging anaphora resolution compared to the best system in  #TAUTHOR_TAG which explores markov', 'logic networks to model the problem. the main contributions of our', 'work are : ( 1 ) a general word representation resource 2 for bridging ;', 'and ( 2 ) a simple yet competitive deterministic approach for bridging anaphora resolution which models the meaning of an', 'np based on its head noun and modifications']",4
[' #TAUTHOR_TAG. we also'],"[' #TAUTHOR_TAG. we also improve', '']","['advanced machine learning - based approach  #TAUTHOR_TAG. we also improve', '']","['', 'bridging anaphors are not limited to definite nps as in previous work  #AUTHOR_TAG ( poesio et al.,, 2004  #AUTHOR_TAG. also', 'in isnotes, the semantic relations between anaphor and antecedent are not restricted to meronymic relations. we therefore choose isnotes to evaluate', 'our algorithm for bridging anaphora resolution. our approach is deterministic and simple, but achieves the competitive results compared to the advanced machine learning - based approach  #TAUTHOR_TAG. we also improve', '']",4
"["" #TAUTHOR_TAG's experimental setup, we""]","["" #TAUTHOR_TAG's experimental setup, we""]","["" #TAUTHOR_TAG's experimental setup, we resolve bridging""]","["" #TAUTHOR_TAG's experimental setup, we resolve bridging anaphors to entity antecedents."", '']",4
['in  #TAUTHOR_TAG which is'],['in  #TAUTHOR_TAG which is'],['in  #TAUTHOR_TAG which is heavily'],"['improve the word representation resource embeddings pp  #AUTHOR_TAG by combining it with glove.', 'the resulting word embeddings ( embeddings bridging ) are a more general word representation resource for bridging.', 'based on embeddings bridging, we propose a deterministic approach for choosing antecedents for bridging anaphors.', 'we show that this simple and efficient method achieves the competitive result on bridging anaphora resolution compared to the advanced machine learning - based approach in  #TAUTHOR_TAG which is heavily dependent on a lot of carefully designed complex features.', 'we also demonstrate that using embeddings bridging yields better results than using embeddings pp for bridging anaphora resolution.', 'for the task of bridging anaphora resolution,  #TAUTHOR_TAG pointed out that considering only head noun knowledge is not enough and future work needs to explore wider context to resolve context - specific bridging relations.', 'in this work we explore the context within nps - that is, we combine the semantics of certain modifications and the head by vector average using embeddings bridging.', 'but in some cases, knowledge about nps themselves is not enough for resolving bridging.', 'for instance, in example 3, knowing that any loosening has the ability to "" rekindle inflation "" from the context of the second sentence can help us to find its antecedent "" the high rates "" ( which is used to against inflation ).', '( 3 ) chancellor of the exchequer nigel lawson views the high rates as his chief weapon against inflation, which was ignited by tax cuts and loose credit policies in 1986 and 1987.', 'officials fear that any loosening this year could rekindle inflation or further weaken the pound against other major currencies.', 'in the future, we will study how to integrate context outside of nps for the task of choosing antencedents for bridging anaphors.', 'also we hope that our word representation resource will facilitate other related research problems such as semantic role labeling']",4
['resolution  #TAUTHOR_TAG use syntactic prep'],['on bridging anaphora resolution  #TAUTHOR_TAG use syntactic preposition patterns'],['on bridging anaphora resolution  #TAUTHOR_TAG use syntactic preposition patterns'],"['work on bridging anaphora resolution  #TAUTHOR_TAG use syntactic preposition patterns to calculate word relatedness.', ""however, such patterns only consider nps'head nouns and hence do not fully capture the semantics of nps."", ' #AUTHOR_TAG created word embeddings ( embeddings pp ) to capture associative similarity ( i. e., relatedness ) between nouns by exploring the syntactic structure of noun phrases. but embeddings pp only contains word representations for nouns.', 'in this paper, we create new word vectors by combining embeddings pp with glove.', 'this new word embeddings ( embeddings bridging ) are a more general lexical knowledge resource for bridging and allow us to represent the meaning of an np beyond its head easily.', 'we therefore develop a deterministic approach for bridging anaphora resolution, which represents the semantics of an np based on its head noun and modifications.', 'we show that this simple approach achieves the competitive results compared to the best system in  #TAUTHOR_TAG which explores markov logic networks to model the problem.', ""additionally, we further improve the results for bridging anaphora resolution reported in  #AUTHOR_TAG by combining our simple deterministic approach with  #TAUTHOR_TAG's best system mln ii""]",6
[' #TAUTHOR_TAG. we also'],"[' #TAUTHOR_TAG. we also improve', '']","['advanced machine learning - based approach  #TAUTHOR_TAG. we also improve', '']","['', 'bridging anaphors are not limited to definite nps as in previous work  #AUTHOR_TAG ( poesio et al.,, 2004  #AUTHOR_TAG. also', 'in isnotes, the semantic relations between anaphor and antecedent are not restricted to meronymic relations. we therefore choose isnotes to evaluate', 'our algorithm for bridging anaphora resolution. our approach is deterministic and simple, but achieves the competitive results compared to the advanced machine learning - based approach  #TAUTHOR_TAG. we also improve', '']",6
"["" #TAUTHOR_TAG's experimental setup, we""]","["" #TAUTHOR_TAG's experimental setup, we""]","["" #TAUTHOR_TAG's experimental setup, we resolve bridging""]","["" #TAUTHOR_TAG's experimental setup, we resolve bridging anaphors to entity antecedents."", '']",6
"["" #TAUTHOR_TAG's experimental setup, we""]","["" #TAUTHOR_TAG's experimental setup, we""]","["" #TAUTHOR_TAG's experimental setup, we resolve bridging""]","["" #TAUTHOR_TAG's experimental setup, we resolve bridging anaphors to entity antecedents."", '']",5
"["" #TAUTHOR_TAG's experimental setup, we""]","["" #TAUTHOR_TAG's experimental setup, we""]","["" #TAUTHOR_TAG's experimental setup, we resolve bridging""]","["" #TAUTHOR_TAG's experimental setup, we resolve bridging anaphors to entity antecedents."", '']",5
"['resolution from  #TAUTHOR_TAG.', 'pairwise model']","['bridging anaphora resolution from  #TAUTHOR_TAG.', 'pairwise model']","['bridging anaphora resolution from  #TAUTHOR_TAG.', 'pairwise model iii is a pairwise mentionentity model based on various semantic, syntactic and lexical features.', 'mln model ii is a joint inference framework based on markov logic networks  #AUTHOR_TAG.', 'it models that semantically or syntactic']","['carried out experiments using the deterministic algorithm described in section 4 together with different word embeddings.', 'again we do not add the suffix "" pp "" to the bridging anaphors for glove gigawiki14 and glove giga.', 'table 6 lists the best results of the two models for bridging anaphora resolution from  #TAUTHOR_TAG.', 'pairwise model iii is a pairwise mentionentity model based on various semantic, syntactic and lexical features.', 'mln model ii is a joint inference framework based on markov logic networks  #AUTHOR_TAG.', '']",5
"['resolution from  #TAUTHOR_TAG.', 'pairwise model']","['bridging anaphora resolution from  #TAUTHOR_TAG.', 'pairwise model']","['bridging anaphora resolution from  #TAUTHOR_TAG.', 'pairwise model iii is a pairwise mentionentity model based on various semantic, syntactic and lexical features.', 'mln model ii is a joint inference framework based on markov logic networks  #AUTHOR_TAG.', 'it models that semantically or syntactic']","['carried out experiments using the deterministic algorithm described in section 4 together with different word embeddings.', 'again we do not add the suffix "" pp "" to the bridging anaphors for glove gigawiki14 and glove giga.', 'table 6 lists the best results of the two models for bridging anaphora resolution from  #TAUTHOR_TAG.', 'pairwise model iii is a pairwise mentionentity model based on various semantic, syntactic and lexical features.', 'mln model ii is a joint inference framework based on markov logic networks  #AUTHOR_TAG.', '']",5
"['ii system  #TAUTHOR_TAG.', 'similarly, we add a constraint on top of mln ii using']","['head ) based on embeddings pp into the mln ii system  #TAUTHOR_TAG.', 'similarly, we add a constraint on top of mln ii using']","['noun modifiers ( appearing before the head ) based on embeddings pp into the mln ii system  #TAUTHOR_TAG.', 'similarly, we add a constraint on top of mln ii using']","['bridging anaphora resolution,  #AUTHOR_TAG integrates a much simpler deterministic approach by combining an np head with its noun modifiers ( appearing before the head ) based on embeddings pp into the mln ii system  #TAUTHOR_TAG.', 'similarly, we add a constraint on top of mln ii using our deterministic approach ( np head + modifiers ) based on embeddings bridging.', 'table 8 lists the results of different systems 8 for bridging anaphora resolution in isnotes.', 'it shows that combining our deterministic approach ( np head + modifiers ) with mln ii slightly improves the result compared to  #AUTHOR_TAG.', 'although combining np head + modifiers with mln ii achieves significant improvement over np 8 we also reimplement the algorithms from schulte im  #AUTHOR_TAG and  #AUTHOR_TAG as baselines ( table 8 ).', 'schulte im  #AUTHOR_TAG resolved bridging anaphors to the closest antecedent candidate in a high - dimensional space.', 'we use the 2, 000 most frequent words ( adjectives, common nouns, proper nouns, and lexical verbs ) from gigaword as the context words.', ' #AUTHOR_TAG applied a pairwise model combining lexical semantic features and salience features to perform mereological bridging resolution in the gnome corpus.', '']",5
"['resolution from  #TAUTHOR_TAG.', 'pairwise model']","['bridging anaphora resolution from  #TAUTHOR_TAG.', 'pairwise model']","['bridging anaphora resolution from  #TAUTHOR_TAG.', 'pairwise model iii is a pairwise mentionentity model based on various semantic, syntactic and lexical features.', 'mln model ii is a joint inference framework based on markov logic networks  #AUTHOR_TAG.', 'it models that semantically or syntactic']","['carried out experiments using the deterministic algorithm described in section 4 together with different word embeddings.', 'again we do not add the suffix "" pp "" to the bridging anaphors for glove gigawiki14 and glove giga.', 'table 6 lists the best results of the two models for bridging anaphora resolution from  #TAUTHOR_TAG.', 'pairwise model iii is a pairwise mentionentity model based on various semantic, syntactic and lexical features.', 'mln model ii is a joint inference framework based on markov logic networks  #AUTHOR_TAG.', '']",3
['in  #TAUTHOR_TAG which is'],['in  #TAUTHOR_TAG which is'],['in  #TAUTHOR_TAG which is heavily'],"['improve the word representation resource embeddings pp  #AUTHOR_TAG by combining it with glove.', 'the resulting word embeddings ( embeddings bridging ) are a more general word representation resource for bridging.', 'based on embeddings bridging, we propose a deterministic approach for choosing antecedents for bridging anaphors.', 'we show that this simple and efficient method achieves the competitive result on bridging anaphora resolution compared to the advanced machine learning - based approach in  #TAUTHOR_TAG which is heavily dependent on a lot of carefully designed complex features.', 'we also demonstrate that using embeddings bridging yields better results than using embeddings pp for bridging anaphora resolution.', 'for the task of bridging anaphora resolution,  #TAUTHOR_TAG pointed out that considering only head noun knowledge is not enough and future work needs to explore wider context to resolve context - specific bridging relations.', 'in this work we explore the context within nps - that is, we combine the semantics of certain modifications and the head by vector average using embeddings bridging.', 'but in some cases, knowledge about nps themselves is not enough for resolving bridging.', 'for instance, in example 3, knowing that any loosening has the ability to "" rekindle inflation "" from the context of the second sentence can help us to find its antecedent "" the high rates "" ( which is used to against inflation ).', '( 3 ) chancellor of the exchequer nigel lawson views the high rates as his chief weapon against inflation, which was ignited by tax cuts and loose credit policies in 1986 and 1987.', 'officials fear that any loosening this year could rekindle inflation or further weaken the pound against other major currencies.', 'in the future, we will study how to integrate context outside of nps for the task of choosing antencedents for bridging anaphors.', 'also we hope that our word representation resource will facilitate other related research problems such as semantic role labeling']",2
['email threads  #TAUTHOR_TAG online forums ('],"['email threads  #TAUTHOR_TAG online forums ( danescu - niculescu  #AUTHOR_TAG danescuniculescu -  #AUTHOR_TAG, chats  #AUTHOR_TAG, and off - line']",['participants of organizational email threads  #TAUTHOR_TAG online forums ('],"['analyzing the social context in which language is used has gathered great interest within the nlp community recently.', 'one of the areas that has generated substantial research is the study of how social power relations between people affect and / or are revealed in their interactions with one another.', 'researchers have proposed systems to detect social power relations between participants of organizational email threads  #TAUTHOR_TAG online forums ( danescu - niculescu  #AUTHOR_TAG danescuniculescu -  #AUTHOR_TAG, chats  #AUTHOR_TAG, and off - line interactions such as presidential debates  #AUTHOR_TAG.', 'automatically identifying power and influence from interactions can have many practical applications ranging from law enforcement and intelligence to online marketing.', 'a significant number of these studies are performed in the domain of organizational email where there is a well defined notion of power ( organizational hierarchy ).', ' #AUTHOR_TAG and  #AUTHOR_TAG predict hierarchical power relations between people in the enron email corpus using lexical features extracted from all the messages exchanged between them.', 'however, their approaches primarily apply to situations where large collections of messages exchanged between pairs of people are available.', 'in, we introduced the problem of detecting whether a participant of an email thread has power over someone else in the thread and established the importance of dialog structure in that task.', 'however, in that work we did not detect over whom that person has power.', 'in this paper, we introduce a new problem formulation.', 'we predict the hierarchical power relation between pairs of participants in an email interaction thread based solely on features extracted from that thread.', 'as a second major contribution, we introduce a new set of features to capture aspects of participant behavior such as responsiveness, and we show that these features are significantly correlated with the direction of power.', 'we present a fully automatic system for this task obtaining an accuracy of 73. 0 %, an improvement of 6. 9 % over 68. 3 % by a system using only lexical features.', 'this best - performing system uses our new feature set']",0
[' #TAUTHOR_TAG  #AUTHOR_TAG built systems to predict hierarchical power'],[' #TAUTHOR_TAG  #AUTHOR_TAG built systems to predict hierarchical power'],"[' #TAUTHOR_TAG  #AUTHOR_TAG built systems to predict hierarchical power relations between people in the enron email corpus using lexical features from all the messages exchanged between them.', 'one limitation of this approach is that it relies solely on lexical cues and hence works best when large collections of messages exchanged between the pairs of people are available.', 'for example,  #TAUTHOR_TAG excluded sender']","['nlp - based approaches such as  #TAUTHOR_TAG  #AUTHOR_TAG built systems to predict hierarchical power relations between people in the enron email corpus using lexical features from all the messages exchanged between them.', 'one limitation of this approach is that it relies solely on lexical cues and hence works best when large collections of messages exchanged between the pairs of people are available.', 'for example,  #TAUTHOR_TAG excluded sender - recipient pairs who exchanged fewer than 500 words from their evaluation set, since they found smaller text samples are harder to classify.', 'by taking the message out of the context of the interaction in which it was exchanged, they fail to utilize cues from the structure of interactions, which complements the lexical cues in detecting power relations, as we showed in.', 'we modeled the problem of detecting power relationships differently in : we predicted whether a participant in an email thread has a certain type of power or not.', '']",0
"['shown to be valuable in predicting power relations  #TAUTHOR_TAG.', 'we use another feature set lex to capture word ngrams, pos ( part of speech ) ngrams and mixed ngrams.', 'a mixed ngram  #AUTHOR_TAG is a special']","['shown to be valuable in predicting power relations  #TAUTHOR_TAG.', 'we use another feature set lex to capture word ngrams, pos ( part of speech ) ngrams and mixed ngrams.', 'a mixed ngram  #AUTHOR_TAG is a special']","['indicator feature.', 'lexical features have already been shown to be valuable in predicting power relations  #TAUTHOR_TAG.', 'we use another feature set lex to capture word ngrams, pos ( part of speech ) ngrams and mixed ngrams.', 'a mixed ngram  #AUTHOR_TAG is a special case of word ngram where words belonging to open classes are replaced with their pos tags.', 'we found the best setting to be using both unigrams and bigrams for']","['', 'since we use a quadratic kernel, we expect the svm to pick up the interaction between each feature and its indicator feature.', 'lexical features have already been shown to be valuable in predicting power relations  #TAUTHOR_TAG.', 'we use another feature set lex to capture word ngrams, pos ( part of speech ) ngrams and mixed ngrams.', 'a mixed ngram  #AUTHOR_TAG is a special case of word ngram where words belonging to open classes are replaced with their pos tags.', 'we found the best setting to be using both unigrams and bigrams for all three types of ngrams, by tuning in our dev set.', 'we then performed experiments using all subsets of { lex, thr new, thr pr, dia pr }.', 'table 3 presents the results obtained using various feature subsets.', 'we use a majority class baseline assigning hp ( p 1, p 2 ) to be always superior, which obtains 52. 5 % accuracy.', 'we also use a stronger baseline using word unigrams and bigrams as features, which obtained an accuracy of 68. 6 %.', 'the performance of the system using each structural feature class on its own is very low.', '']",0
[' #TAUTHOR_TAG  #AUTHOR_TAG built systems to predict hierarchical power'],[' #TAUTHOR_TAG  #AUTHOR_TAG built systems to predict hierarchical power'],"[' #TAUTHOR_TAG  #AUTHOR_TAG built systems to predict hierarchical power relations between people in the enron email corpus using lexical features from all the messages exchanged between them.', 'one limitation of this approach is that it relies solely on lexical cues and hence works best when large collections of messages exchanged between the pairs of people are available.', 'for example,  #TAUTHOR_TAG excluded sender']","['nlp - based approaches such as  #TAUTHOR_TAG  #AUTHOR_TAG built systems to predict hierarchical power relations between people in the enron email corpus using lexical features from all the messages exchanged between them.', 'one limitation of this approach is that it relies solely on lexical cues and hence works best when large collections of messages exchanged between the pairs of people are available.', 'for example,  #TAUTHOR_TAG excluded sender - recipient pairs who exchanged fewer than 500 words from their evaluation set, since they found smaller text samples are harder to classify.', 'by taking the message out of the context of the interaction in which it was exchanged, they fail to utilize cues from the structure of interactions, which complements the lexical cues in detecting power relations, as we showed in.', 'we modeled the problem of detecting power relationships differently in : we predicted whether a participant in an email thread has a certain type of power or not.', '']",4
"['hp ( p 1, p 2 ).', 'this problem formulation is similar to the ones in  #TAUTHOR_TAG']","['hp ( p 1, p 2 ).', 'this problem formulation is similar to the ones in  #TAUTHOR_TAG']","['hp ( p 1, p 2 ).', 'this problem formulation is similar to the ones in  #TAUTHOR_TAG']","['', 'table 1 shows the total number of pairs in ipp t and ripp t from all the threads in our corpus and across train, dev and test sets.', 'given a thread t and a pair of participants ( p 1, p 2 ) ∈ ripp t, we want to automatically detect hp ( p 1, p 2 ).', 'this problem formulation is similar to the ones in  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'however, the difference is that for us an instance is a pair of participants in a single thread of interaction ( which may or may not include other people ), whereas for them an instance constitutes all messages exchanged between a pair of people in the entire corpus.', 'our formulation also differs from in that we detect power relations between pairs of participants, instead of just whether a participant had power over anyone in the thread']",3
"['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic']","['', 'use word embeddings to compute similarities between words and embed these similarity scores into the kernel functions, e. g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic tree kernel ( sstk ), cf. § 2. 1 ). we explore various methods to apply word embeddings to generate the', '']",0
"['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic']","['', 'use word embeddings to compute similarities between words and embed these similarity scores into the kernel functions, e. g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic tree kernel ( sstk ), cf. § 2. 1 ). we explore various methods to apply word embeddings to generate the', '']",0
"['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic']","['', 'use word embeddings to compute similarities between words and embed these similarity scores into the kernel functions, e. g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic tree kernel ( sstk ), cf. § 2. 1 ). we explore various methods to apply word embeddings to generate the', '']",0
"['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic']","['', 'use word embeddings to compute similarities between words and embed these similarity scores into the kernel functions, e. g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic tree kernel ( sstk ), cf. § 2. 1 ). we explore various methods to apply word embeddings to generate the', '']",0
"['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic']","['', 'use word embeddings to compute similarities between words and embed these similarity scores into the kernel functions, e. g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic tree kernel ( sstk ), cf. § 2. 1 ). we explore various methods to apply word embeddings to generate the', '']",0
"['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic']","['', 'use word embeddings to compute similarities between words and embed these similarity scores into the kernel functions, e. g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic tree kernel ( sstk ), cf. § 2. 1 ). we explore various methods to apply word embeddings to generate the', '']",0
"['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic']","['', 'use word embeddings to compute similarities between words and embed these similarity scores into the kernel functions, e. g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic tree kernel ( sstk ), cf. § 2. 1 ). we explore various methods to apply word embeddings to generate the', '']",0
"['method  #TAUTHOR_TAG, a relation mention ( the two']","['the tree kernel - based method  #TAUTHOR_TAG, a relation mention ( the two']","['the tree kernel - based method  #TAUTHOR_TAG, a relation mention ( the two entity mentions and the sentence containing']","['the tree kernel - based method  #TAUTHOR_TAG, a relation mention ( the two entity mentions and the sentence containing them ) is represented by the path - enclosed tree ( pet ), the smallest constituency - based subtree including the two target entity mentions  #AUTHOR_TAG.', 'the syntactic tree kernel ( stk ) is then defined to compute the similarity between two pet trees ( where target entities are marked ) by counting the common sub - trees, without enumerating the whole fragment space  #AUTHOR_TAG.', 'stk is then applied in the support vector machines ( svms ) for re.', '']",0
"['method  #TAUTHOR_TAG, a relation mention ( the two']","['the tree kernel - based method  #TAUTHOR_TAG, a relation mention ( the two']","['the tree kernel - based method  #TAUTHOR_TAG, a relation mention ( the two entity mentions and the sentence containing']","['the tree kernel - based method  #TAUTHOR_TAG, a relation mention ( the two entity mentions and the sentence containing them ) is represented by the path - enclosed tree ( pet ), the smallest constituency - based subtree including the two target entity mentions  #AUTHOR_TAG.', 'the syntactic tree kernel ( stk ) is then defined to compute the similarity between two pet trees ( where target entities are marked ) by counting the common sub - trees, without enumerating the whole fragment space  #AUTHOR_TAG.', 'stk is then applied in the support vector machines ( svms ) for re.', '']",0
"['proposed methods.', 'in particular, one limitation of the syntactic semantic tree kernel presented in  #TAUTHOR_TAG ( § 2. 1 ) is that semantics is']","['proposed methods.', 'in particular, one limitation of the syntactic semantic tree kernel presented in  #TAUTHOR_TAG ( § 2. 1 ) is that semantics is']","['proposed methods.', 'in particular, one limitation of the syntactic semantic tree kernel presented in  #TAUTHOR_TAG ( § 2. 1 ) is that semantics is highly']","['this section, we first give the intuition that guides us in designing the proposed methods.', 'in particular, one limitation of the syntactic semantic tree kernel presented in  #TAUTHOR_TAG ( § 2. 1 ) is that semantics is highly tied to syntax ( the pet trees ) in the kernel computation, limiting the generalization capacity of semantics to the extent of syntactic matches.', '']",0
"['', 'however, as shown by  #TAUTHOR_TAG, instance weighting is not very useful for']","['', 'however, as shown by  #TAUTHOR_TAG, instance weighting is not very useful for']","['is instance weighting  #AUTHOR_TAG b ).', 'however, as shown by  #TAUTHOR_TAG, instance weighting is not very useful for']","['embeddings are only applied to re recently.', ' #AUTHOR_TAG b ) use word embeddings as input for matrix - vector recursive neural networks in relation classification while  #AUTHOR_TAG, and  #AUTHOR_TAG employ word embeddings in the framework of convolutional neural networks for relation classification and extraction, respectively.', ' #AUTHOR_TAG utilize word embeddings to reduce noise of training data in distant supervision.', ' #AUTHOR_TAG present a string kernel for bio - relation extraction with word embeddings, and  #AUTHOR_TAG ; study the factor - based compositional embedding models.', 'however, none of this work examines word embeddings for tree kernels as well as domain adaptation as we do.', 'regarding da, in the unsupervised da setting,  #AUTHOR_TAG attempt to learn multidimensional feature representations while  #AUTHOR_TAG introduce structural correspondence learning.', 'daume ( 2007 ) proposes an easy adaptation framework ( ea ) while  #AUTHOR_TAG present a log - bilinear language adaptation technique in the supervised da setting.', 'unfortunately, all of this work assumes some prior ( in the form of either labeled or unlabeled data ) on the target domains for the sequential labeling tasks, in contrast to our single - system unsupervised da setting for relation extraction.', 'an alternative method that is also popular to da is instance weighting  #AUTHOR_TAG b ).', 'however, as shown by  #TAUTHOR_TAG, instance weighting is not very useful for da of re']",0
"['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic']","['', 'use word embeddings to compute similarities between words and embed these similarity scores into the kernel functions, e. g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic tree kernel ( sstk ), cf. § 2. 1 ). we explore various methods to apply word embeddings to generate the', '']",5
"['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic']","['', 'use word embeddings to compute similarities between words and embed these similarity scores into the kernel functions, e. g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic tree kernel ( sstk ), cf. § 2. 1 ). we explore various methods to apply word embeddings to generate the', '']",5
"['', 'in order to ensure the two methods  #TAUTHOR_TAG']","['', 'in order to ensure the two methods  #TAUTHOR_TAG']","['', 'in order to ensure the two methods  #TAUTHOR_TAG']","['', 'in order to ensure the two methods  #TAUTHOR_TAG are compared compatibly on the same resources, we make sure the two systems have access to the same amount of information.', 'thus, we follow  #TAUTHOR_TAG and use the pet trees ( beside word clusters and word embeddings ) as the only resource the two methods can exploit.', '']",5
"['', 'in order to ensure the two methods  #TAUTHOR_TAG']","['', 'in order to ensure the two methods  #TAUTHOR_TAG']","['', 'in order to ensure the two methods  #TAUTHOR_TAG']","['', 'in order to ensure the two methods  #TAUTHOR_TAG are compared compatibly on the same resources, we make sure the two systems have access to the same amount of information.', 'thus, we follow  #TAUTHOR_TAG and use the pet trees ( beside word clusters and word embeddings ) as the only resource the two methods can exploit.', '']",5
"['by  #TAUTHOR_TAG on the ukwac corpus  #AUTHOR_TAG with 2 billion words, and the c & w word embeddings']","['by  #TAUTHOR_TAG on the ukwac corpus  #AUTHOR_TAG with 2 billion words, and the c & w word embeddings']","['by  #TAUTHOR_TAG on the ukwac corpus  #AUTHOR_TAG with 2 billion words, and the c & w word embeddings']","['use the word clusters trained by  #TAUTHOR_TAG on the ukwac corpus  #AUTHOR_TAG with 2 billion words, and the c & w word embeddings from turian el al. ( 2010 ) 2 with 50 dimensions following  #AUTHOR_TAG.', 'in order to make the comparisons compatible, we introduce word embeddings into the tree kernel by extending the package provided by  #TAUTHOR_TAG, which uses the charniak parser to obtain the constituent trees, the svm - light - tk for the sstk kernel in svm, the directional relation classes, etc.', 'we utilize the default vector kernel in the svm - light - tk package ( d = 3 ).', 'for the feature - based method, we apply the maxent classifier in the mallet 3 package with the l2 regularizer on the hierarchical architecture for relation extraction as in  #AUTHOR_TAG.', 'following prior work, we evaluate the systems on the ace 2005 dataset which involves 6 domains : broadcast news ( bn ), newswire ( nw ), broadcast conversation ( bc ), telephone conversation ( cts ), weblogs ( wl ) and usenet ( un ).', 'the union of bn and nw ( news ) is used as the source domain while bc, cts and wl play the role of the target domains.', 'we take half of bc as the only target development set, and use the remaining data and domains for testing.', 'the dataset partition is exactly the same as in  #TAUTHOR_TAG.', 'as described in their paper, the target domains quite differ from the source domain in the relation distributions and vocabulary']",5
"['by  #TAUTHOR_TAG on the ukwac corpus  #AUTHOR_TAG with 2 billion words, and the c & w word embeddings']","['by  #TAUTHOR_TAG on the ukwac corpus  #AUTHOR_TAG with 2 billion words, and the c & w word embeddings']","['by  #TAUTHOR_TAG on the ukwac corpus  #AUTHOR_TAG with 2 billion words, and the c & w word embeddings']","['use the word clusters trained by  #TAUTHOR_TAG on the ukwac corpus  #AUTHOR_TAG with 2 billion words, and the c & w word embeddings from turian el al. ( 2010 ) 2 with 50 dimensions following  #AUTHOR_TAG.', 'in order to make the comparisons compatible, we introduce word embeddings into the tree kernel by extending the package provided by  #TAUTHOR_TAG, which uses the charniak parser to obtain the constituent trees, the svm - light - tk for the sstk kernel in svm, the directional relation classes, etc.', 'we utilize the default vector kernel in the svm - light - tk package ( d = 3 ).', 'for the feature - based method, we apply the maxent classifier in the mallet 3 package with the l2 regularizer on the hierarchical architecture for relation extraction as in  #AUTHOR_TAG.', 'following prior work, we evaluate the systems on the ace 2005 dataset which involves 6 domains : broadcast news ( bn ), newswire ( nw ), broadcast conversation ( bc ), telephone conversation ( cts ), weblogs ( wl ) and usenet ( un ).', 'the union of bn and nw ( news ) is used as the source domain while bc, cts and wl play the role of the target domains.', 'we take half of bc as the only target development set, and use the remaining data and domains for testing.', 'the dataset partition is exactly the same as in  #TAUTHOR_TAG.', 'as described in their paper, the target domains quite differ from the source domain in the relation distributions and vocabulary']",5
['tree kernel - based method in  #TAUTHOR_TAG vs'],['tree kernel - based method in  #TAUTHOR_TAG vs'],['section aims to compare the tree kernel - based method in  #TAUTHOR_TAG vs'],"['section aims to compare the tree kernel - based method in  #TAUTHOR_TAG vs feature - based in  #AUTHOR_TAG.', 'all the comparisons between the tree kernel - based method and the feature - based method in this table are significant with p < 0. 05.', 'procedure, the same model of directional relation classes, the same pet trees for tree kernels and feature extraction, the same word clusters and the same word embeddings ).', 'we first evaluate the feature - based system with different combinations of embeddings ( i. e, head, phrase and tree ) on the bc development set.', 'based on the evaluation results, we then discuss the effect of the semantic representations on the feature - based system and the tree kernel - based system, and then compare the performance of the two methods when they are augmented with their best corresponding embedding combinations.', '']",5
['of  #TAUTHOR_TAG ('],['of  #TAUTHOR_TAG ( up'],['the best system of  #TAUTHOR_TAG ('],[' #TAUTHOR_TAG'],5
"['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic']","['', 'use word embeddings to compute similarities between words and embed these similarity scores into the kernel functions, e. g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic tree kernel ( sstk ), cf. § 2. 1 ). we explore various methods to apply word embeddings to generate the', '']",1
"['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG']","['g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic']","['', 'use word embeddings to compute similarities between words and embed these similarity scores into the kernel functions, e. g., by resembling the method of  #TAUTHOR_TAG that', 'exploited lsa ( in the semantic syntactic tree kernel ( sstk ), cf. § 2. 1 ). we explore various methods to apply word embeddings to generate the', '']",1
"['proposed methods.', 'in particular, one limitation of the syntactic semantic tree kernel presented in  #TAUTHOR_TAG ( § 2. 1 ) is that semantics is']","['proposed methods.', 'in particular, one limitation of the syntactic semantic tree kernel presented in  #TAUTHOR_TAG ( § 2. 1 ) is that semantics is']","['proposed methods.', 'in particular, one limitation of the syntactic semantic tree kernel presented in  #TAUTHOR_TAG ( § 2. 1 ) is that semantics is highly']","['this section, we first give the intuition that guides us in designing the proposed methods.', 'in particular, one limitation of the syntactic semantic tree kernel presented in  #TAUTHOR_TAG ( § 2. 1 ) is that semantics is highly tied to syntax ( the pet trees ) in the kernel computation, limiting the generalization capacity of semantics to the extent of syntactic matches.', '']",1
"['of  #TAUTHOR_TAG. in particularly, the sim method simply replaces the similarity scores between', 'word pairs']","['of  #TAUTHOR_TAG. in particularly, the sim method simply replaces the similarity scores between', 'word pairs']","['of  #TAUTHOR_TAG. in particularly, the sim method simply replaces the similarity scores between', 'word pairs']","['', ' #AUTHOR_TAG b ;  #AUTHOR_TAG on representing phrase semantics. tree : this is motivated by the training of recursive neural networks  #AUTHOR_TAG a )', 'for semantic compositionality and attempts to aggregate the context words embeddings syntactically. in particular, we compute an embedding for every', 'node in the pet tree in a bottom - up manner. the embeddings of the leaves are the embeddings of the words associated with them while the embeddings of', 'the internal nodes are the means of the embeddings of their children nodes. we use the embeddings of the root of the pet tree to represent the relation mention', 'in this case. both phrase and tree have d dimensions. it is also interesting to examine combinations of these', 'three representations ( cf., table 1 ). sim : finally, for completeness, we experiment with a more obvious way to introduce word embeddings into tree kernels,', 'resembling more closely the approach of  #TAUTHOR_TAG. in particularly, the sim method simply replaces the similarity scores between', 'word pairs obtained from lsa by the', 'cosine similarities between the word embeddings to be used in the sstk kernel']",3
"['by  #TAUTHOR_TAG on the ukwac corpus  #AUTHOR_TAG with 2 billion words, and the c & w word embeddings']","['by  #TAUTHOR_TAG on the ukwac corpus  #AUTHOR_TAG with 2 billion words, and the c & w word embeddings']","['by  #TAUTHOR_TAG on the ukwac corpus  #AUTHOR_TAG with 2 billion words, and the c & w word embeddings']","['use the word clusters trained by  #TAUTHOR_TAG on the ukwac corpus  #AUTHOR_TAG with 2 billion words, and the c & w word embeddings from turian el al. ( 2010 ) 2 with 50 dimensions following  #AUTHOR_TAG.', 'in order to make the comparisons compatible, we introduce word embeddings into the tree kernel by extending the package provided by  #TAUTHOR_TAG, which uses the charniak parser to obtain the constituent trees, the svm - light - tk for the sstk kernel in svm, the directional relation classes, etc.', 'we utilize the default vector kernel in the svm - light - tk package ( d = 3 ).', 'for the feature - based method, we apply the maxent classifier in the mallet 3 package with the l2 regularizer on the hierarchical architecture for relation extraction as in  #AUTHOR_TAG.', 'following prior work, we evaluate the systems on the ace 2005 dataset which involves 6 domains : broadcast news ( bn ), newswire ( nw ), broadcast conversation ( bc ), telephone conversation ( cts ), weblogs ( wl ) and usenet ( un ).', 'the union of bn and nw ( news ) is used as the source domain while bc, cts and wl play the role of the target domains.', 'we take half of bc as the only target development set, and use the remaining data and domains for testing.', 'the dataset partition is exactly the same as in  #TAUTHOR_TAG.', 'as described in their paper, the target domains quite differ from the source domain in the relation distributions and vocabulary']",6
"['pet trees, word clusters and lsa in  #TAUTHOR_TAG as the baselines and augment']","['pet trees, word clusters and lsa in  #TAUTHOR_TAG as the baselines and augment']","['of re in the tree kernelbased method.', 'in particular, we take the systems using the pet trees, word clusters and lsa in  #TAUTHOR_TAG as the baselines and augment']","['this section, we examine the semantic representation for da of re in the tree kernelbased method.', 'in particular, we take the systems using the pet trees, word clusters and lsa in  #TAUTHOR_TAG as the baselines and augment them with the embeddings wed = head + phrase.', '']",6
['best model in  #TAUTHOR_TAG ( row 11 in'],['best model in  #TAUTHOR_TAG ( row 11 in'],['the tree - kernel based method we focus on the comparison of the best model in  #TAUTHOR_TAG ( row 11 in'],"['', 'word embeddings for the tree - kernel based method we focus on the comparison of the best model in  #TAUTHOR_TAG ( row 11 in table 2 ) ( called p ) with the same model but augmented with the embedding wed ( row 12 in tabel 2 ) ( called p + wed ).', 'one of the most interesting insights is that the embedding wed helps to semantically generalize the phrases connecting the two target entity mentions beyond the syntactic constraints.', 'for instance, model p fails to discover the relation between "" chuck hagel "" and "" vietnam "" in the sentence ( of the target domain bc ) : "" sergeant chuck hagel was seriously wounded twice in vietnam. "" ( i. e, it returns the none relation as the prediction ) as the substructure associated with "" seriously wounded twice "" does not appear with any relation in the source domain.', 'model p + wed, on the other hand, correctly predicts the phys ( located ) relation between the two entities as the phrase embedding of "" chuck hagel was seriously wounded twice in vietnam. "" ( phrase x1 ) is very close to the embedding of the source domain phrase : "" stewart faces up to 30 years in prison "" ( phrase x2 ) ( annotated with the phys relation between "" stewart "" and "" prison "" ).', '']",6
"['pet trees, word clusters and lsa in  #TAUTHOR_TAG as the baselines and augment']","['pet trees, word clusters and lsa in  #TAUTHOR_TAG as the baselines and augment']","['of re in the tree kernelbased method.', 'in particular, we take the systems using the pet trees, word clusters and lsa in  #TAUTHOR_TAG as the baselines and augment']","['this section, we examine the semantic representation for da of re in the tree kernelbased method.', 'in particular, we take the systems using the pet trees, word clusters and lsa in  #TAUTHOR_TAG as the baselines and augment them with the embeddings wed = head + phrase.', '']",4
"['pet trees, word clusters and lsa in  #TAUTHOR_TAG as the baselines and augment']","['pet trees, word clusters and lsa in  #TAUTHOR_TAG as the baselines and augment']","['of re in the tree kernelbased method.', 'in particular, we take the systems using the pet trees, word clusters and lsa in  #TAUTHOR_TAG as the baselines and augment']","['this section, we examine the semantic representation for da of re in the tree kernelbased method.', 'in particular, we take the systems using the pet trees, word clusters and lsa in  #TAUTHOR_TAG as the baselines and augment them with the embeddings wed = head + phrase.', '']",4
['of  #TAUTHOR_TAG ('],['of  #TAUTHOR_TAG ( up'],['the best system of  #TAUTHOR_TAG ('],[' #TAUTHOR_TAG'],4
['syntactic trees  #TAUTHOR_TAG have brought the hope that the'],['syntactic trees  #TAUTHOR_TAG have brought the hope that the'],['syntactic trees  #TAUTHOR_TAG have brought the hope that the same approach'],"['successes in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees  #TAUTHOR_TAG have brought the hope that the same approach could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence.', '']",0
['syntactic trees  #TAUTHOR_TAG have brought the hope that the'],['syntactic trees  #TAUTHOR_TAG have brought the hope that the'],['syntactic trees  #TAUTHOR_TAG have brought the hope that the same approach'],"['successes in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees  #TAUTHOR_TAG have brought the hope that the same approach could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence.', '']",5
"['simple synchrony network ( ssn ) parsers  #TAUTHOR_TAG,']","['simple synchrony network ( ssn ) parsers  #TAUTHOR_TAG,']","['- art history - based statistical parsers, the simple synchrony network ( ssn ) parsers  #TAUTHOR_TAG,']","['this section we describe the augmentations to our base parsing models necessary to tackle the joint learning of parse tree and semantic role labels.', 'propbank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the penn treebank  #AUTHOR_TAG.', 'verbal predicates in the penn treebank ( ptb ) receive a label rel and their arguments are annotated with abstract semantic role labels a0 - a5 or aa for those complements of the predicative verb that are considered arguments while those complements of the verb labelled with a semantic functional label in the original ptb receive the composite semantic role label am - x, where x stands for labels such as loc, tmp or adv, for locative, temporal and adverbial modifiers respectively.', 'propbank uses two levels of granularity in its annotation, at least conceptually.', 'arguments receiving labels a0 - a5 or aa do not express consistent semantic roles and are specific to a verb, while arguments receiving an am - x label are supposed to be adjuncts, and the roles they express are consistent across all verbs.', 'to achieve the complex task of assigning semantic role labels while parsing, we use a family of state - of - the - art history - based statistical parsers, the simple synchrony network ( ssn ) parsers  #TAUTHOR_TAG, which use a form of left - corner parse strategy to map parse trees to sequences of derivation steps.', 'these parsers do not impose any a priori independence assumptions, but instead smooth their parameters by means of the novel ssn neural network architecture.', 'this architecture is capable of inducing a finite history representation of an unbounded sequence of derivation steps, which we denote', 'is computed from a set f of handcrafted features of the derivation move d i−1, and from a finite set d of recent history representations h ( d 1,..., d j ), where j < i − 1.', 'because the history representation computed for the move i − 1 is included in the inputs to the computation of the representation for the next move i, virtually any information about the derivation history could flow from history representation to history representation and be used to estimate the probability of a derivation move.', 'in our experiments, the set d of earlier history representations is modified to yield a model that is sensitive to regularities in structurally defined sequences of nodes bearing semantic role labels, within and across constituents.', 'for more information on this technique to capture structural domains, see  #AUTHOR_TAG where the technique was applied to function parsing.', 'given the hidden history representation h ( d 1, · · ·, d i−1 )']",5
"['##ging  #TAUTHOR_TAG separately.', 'modeling them jointly has the potential to']","['supertagging  #TAUTHOR_TAG separately.', 'modeling them jointly has the potential to']","['##ging  #TAUTHOR_TAG separately.', 'modeling them jointly has the potential to']","['##mms to supertagging for the categories defined in ccgbank for english.', 'fully supervised maximum entropy markov models have been used for cascaded prediction of pos tags followed by supertags  #AUTHOR_TAG.', 'here, we learn supertaggers given only a pos tag dictionary and supertag dictionary or a small amount of material labeled with both types of information.', 'previous work has used bayesian hmms to learn taggers for both pos tagging and supertagging  #TAUTHOR_TAG separately.', 'modeling them jointly has the potential to produce more robust and accurate supertaggers trained with less supervision and thereby potentially help in the creation of useful models for new languages and domains.', 'our results show that joint inference improves supervised supertag prediction ( compared to hmms ), especially when labeled training data is scarce.', 'secondly, when training data is limited, the generative fhmms and a maximum entropy markov model ( a discriminative model like c & c ) can bootstrap each other, in a single round co - training setup, to complement each other.', 'finally, fhmms trained on tag dictionaries also outperform standard hmms, thereby providing a stronger basis for learning accurate supertaggers with less supervision']",0
['bitag hmm of  #TAUTHOR_TAG'],['bitag hmm of  #TAUTHOR_TAG'],['bitag hmm of  #TAUTHOR_TAG'],"['the tag dictionary of that word, when the lexicon is constructed. this is in fact a form of supervision, which we use here as an oracle to explore the effect of', 'reducing lexical ambiguity. results of this experiment for α = 1. 0, on ambiguous ccg categories, are tabulated in table', '5 ( a ). the results for α = 0. 05 is shown in table 6 ( a ). we also report the', 'ccg accuracy values inclusive of unambiguous types in table 5 ( b ) for α =', '1. 0 and table 6 ( b ) respectively. the performance of the hmm model ( 31 % ) in table 5 ( a ) without any frequency cut -', 'off on the ccg categories, is comparable to the bitag hmm of  #TAUTHOR_TAG that uses variational bayes em ( 33 % ). our complexity based initialization is not directly comparable to the results in  #TAUTHOR_TAG because the values there are based on a', 'weighted combination of complexity based initialization and modified transition priors based on the ccg formalism. however, it is encouraging to see that when there is no cut - off based filtering of the categories,', 'fhmmb ( 47. 98 % ) greatly outperforms the hmm - em model of  #TAUTHOR_TAG. it is however, quite short of the 56. 1 % accuracy achieved by the model of  #TAUTHOR_TAG that uses', 'grammar informed initialization ( combination of category based initialization along with category transition rules ). without any frequency cut - off on ccg categories, fhmmb achieves over 17 % improvement in the', 'prediction accuracy of ambiguous ccg categories, in comparison with the hmm. the hmm performs much better when there is a high level of frequency based filtering of the categories. however,', 'recall that frequency based filtering of categories is a strong form of supervision that we use here only as', 'an oracle and which one could not expect to have in real world tag dictionaries. the pos accuracies in these experiments were 83. 5 - 85 %, 84. 5 - 86. 2', '% and 78. 3 - 78. 4 % for models fhmmb, fhmma and hmm respectively ( without any frequency cut - off ). in the weakly supervised setting, the choice of the transition prior α of 0. 05 lead to severe degradation', 'in the prediction accuracy of ccg tags. unlike pos tagging, where a symmetric transition prior of α = 0. 05 captured the sparsity of the tag', 'transition distribution, in supertagging the transition priors are asymmetric. we expect that ccg transition rules  #TAUTHOR_TAG when encoded as category specific transition priors, will lead to better performance', 'with the fhmms']",0
"['this experiment, we use the training and test sets used by  #TAUTHOR_TAG from ccgbank.', 'we vary the amount of training material by using 100, 1000, 10, 000 and']","['this experiment, we use the training and test sets used by  #TAUTHOR_TAG from ccgbank.', 'we vary the amount of training material by using 100, 1000, 10, 000 and']","['this experiment, we use the training and test sets used by  #TAUTHOR_TAG from ccgbank.', 'we vary the amount of training material by using 100, 1000, 10, 000 and all 38015 training set sentences.', 'we also vary the transition prior α choosing α = 1']","['this experiment, we use the training and test sets used by  #TAUTHOR_TAG from ccgbank.', 'we vary the amount of training material by using 100, 1000, 10, 000 and all 38015 training set sentences.', 'we also vary the transition prior α choosing α = 1. 0 and α = 0. 05 on the ccg tags.', 'the emission prior β was held constant at 1. 0.', 'the results of these experiments for α = 0. 05 are tabulated in table 3 ( a ).', 'for comparison, we also show the results of the c & c supertagger of  #AUTHOR_TAG in table 3 ( b ).', 'the parameter α, which determines the sparsity of the transition matrix, has been reported to have a greater influence on the performance of the tagger in  #AUTHOR_TAG in weakly supervised pos tagging.', 'we also observed this in supervised supertagging, in the models hmm and fhmmb.', 'the hmm model and fhmmb showed a slight dip in their performance for α = 1. 0 while fhmma did slightly better.', 'what stands out in these results is the performance of the fhmm models with minimal amount of training data ( for 100 sentences, fhmmb is quite close to the discriminatively trained c & c supertagger ).', '']",5
"['on grammar informed initialization  #TAUTHOR_TAG.', 'we consider the prior probability of occurrence of categories based']","['on grammar informed initialization  #TAUTHOR_TAG.', 'we consider the prior probability of occurrence of categories based']","['on grammar informed initialization  #TAUTHOR_TAG.', 'we consider the prior probability of occurrence of categories based']","['annotation is costly, we are interested in automatic annotation of unlabeled sentences with minimal supervision.', 'in the weakly supervised learning setting, we are provided with a lexicon that lists possible pos tags and supertags for many, though not all, words.', 'we draw the initial sample of ccg tag sequences corresponding to the observation sequence, using probabilities based on grammar informed initialization  #TAUTHOR_TAG.', 'we consider the prior probability of occurrence of categories based on their complexity : given a lexicon l, the probability of a category c i is inversely proportional to its complexity :', 'where complexity ( c i ) is defined as the number of sub - categories contained in category c i.', '']",5
"['this experiment, we use the training and test sets used by  #TAUTHOR_TAG from ccgbank.', 'we vary the amount of training material by using 100, 1000, 10, 000 and']","['this experiment, we use the training and test sets used by  #TAUTHOR_TAG from ccgbank.', 'we vary the amount of training material by using 100, 1000, 10, 000 and']","['this experiment, we use the training and test sets used by  #TAUTHOR_TAG from ccgbank.', 'we vary the amount of training material by using 100, 1000, 10, 000 and all 38015 training set sentences.', 'we also vary the transition prior α choosing α = 1']","['this experiment, we use the training and test sets used by  #TAUTHOR_TAG from ccgbank.', 'we vary the amount of training material by using 100, 1000, 10, 000 and all 38015 training set sentences.', 'we also vary the transition prior α choosing α = 1. 0 and α = 0. 05 on the ccg tags.', 'the emission prior β was held constant at 1. 0.', 'the results of these experiments for α = 0. 05 are tabulated in table 3 ( a ).', 'for comparison, we also show the results of the c & c supertagger of  #AUTHOR_TAG in table 3 ( b ).', 'the parameter α, which determines the sparsity of the transition matrix, has been reported to have a greater influence on the performance of the tagger in  #AUTHOR_TAG in weakly supervised pos tagging.', 'we also observed this in supervised supertagging, in the models hmm and fhmmb.', 'the hmm model and fhmmb showed a slight dip in their performance for α = 1. 0 while fhmma did slightly better.', 'what stands out in these results is the performance of the fhmm models with minimal amount of training data ( for 100 sentences, fhmmb is quite close to the discriminatively trained c & c supertagger ).', '']",3
['bitag hmm of  #TAUTHOR_TAG'],['bitag hmm of  #TAUTHOR_TAG'],['bitag hmm of  #TAUTHOR_TAG'],"['the tag dictionary of that word, when the lexicon is constructed. this is in fact a form of supervision, which we use here as an oracle to explore the effect of', 'reducing lexical ambiguity. results of this experiment for α = 1. 0, on ambiguous ccg categories, are tabulated in table', '5 ( a ). the results for α = 0. 05 is shown in table 6 ( a ). we also report the', 'ccg accuracy values inclusive of unambiguous types in table 5 ( b ) for α =', '1. 0 and table 6 ( b ) respectively. the performance of the hmm model ( 31 % ) in table 5 ( a ) without any frequency cut -', 'off on the ccg categories, is comparable to the bitag hmm of  #TAUTHOR_TAG that uses variational bayes em ( 33 % ). our complexity based initialization is not directly comparable to the results in  #TAUTHOR_TAG because the values there are based on a', 'weighted combination of complexity based initialization and modified transition priors based on the ccg formalism. however, it is encouraging to see that when there is no cut - off based filtering of the categories,', 'fhmmb ( 47. 98 % ) greatly outperforms the hmm - em model of  #TAUTHOR_TAG. it is however, quite short of the 56. 1 % accuracy achieved by the model of  #TAUTHOR_TAG that uses', 'grammar informed initialization ( combination of category based initialization along with category transition rules ). without any frequency cut - off on ccg categories, fhmmb achieves over 17 % improvement in the', 'prediction accuracy of ambiguous ccg categories, in comparison with the hmm. the hmm performs much better when there is a high level of frequency based filtering of the categories. however,', 'recall that frequency based filtering of categories is a strong form of supervision that we use here only as', 'an oracle and which one could not expect to have in real world tag dictionaries. the pos accuracies in these experiments were 83. 5 - 85 %, 84. 5 - 86. 2', '% and 78. 3 - 78. 4 % for models fhmmb, fhmma and hmm respectively ( without any frequency cut - off ). in the weakly supervised setting, the choice of the transition prior α of 0. 05 lead to severe degradation', 'in the prediction accuracy of ccg tags. unlike pos tagging, where a symmetric transition prior of α = 0. 05 captured the sparsity of the tag', 'transition distribution, in supertagging the transition priors are asymmetric. we expect that ccg transition rules  #TAUTHOR_TAG when encoded as category specific transition priors, will lead to better performance', 'with the fhmms']",3
"['from tag dictionaries alone and incorporating more informative prior distributions such as those in  #TAUTHOR_TAG.', 'this may']","['from tag dictionaries alone and incorporating more informative prior distributions such as those in  #TAUTHOR_TAG.', 'this may']","['from tag dictionaries alone and incorporating more informative prior distributions such as those in  #TAUTHOR_TAG.', 'this may make']","['demonstrated that joint inference in supertagging, boosts the prediction accuracy of both pos and ccg tags by a considerable margin.', 'the improvement is more significant when training data is scarce.', 'the results from the single round co - training experiments were encouraging.', 'the generative fhmm model is able to rival a discriminative model like the c & c supertagger, when more labeled sentences are made available by a bootstrapped supertagger.', 'to the best of our knowledge, this is the first work on joint inference in the bayesian framework for supertagging.', 'there is plenty of scope for further improvements.', 'overall, the discriminative c & c supertagger outperforms the fhmms in all supervised settings.', 'despite this, the fhmms are suited for estimating models with less supervision, such as from tag dictionaries alone and incorporating more informative prior distributions such as those in  #TAUTHOR_TAG.', 'this may make them more appropriate for developing ccgbanks for other languages and domains.', 'furthermore, bayesian inference is modular and extensible, so our models could be supplemented by finding optimal values of the hyperparameters α ( for pos tags ) and β']",3
"['this experiment, we use the training and test sets used by  #TAUTHOR_TAG from ccgbank.', 'we vary the amount of training material by using 100, 1000, 10, 000 and']","['this experiment, we use the training and test sets used by  #TAUTHOR_TAG from ccgbank.', 'we vary the amount of training material by using 100, 1000, 10, 000 and']","['this experiment, we use the training and test sets used by  #TAUTHOR_TAG from ccgbank.', 'we vary the amount of training material by using 100, 1000, 10, 000 and all 38015 training set sentences.', 'we also vary the transition prior α choosing α = 1']","['this experiment, we use the training and test sets used by  #TAUTHOR_TAG from ccgbank.', 'we vary the amount of training material by using 100, 1000, 10, 000 and all 38015 training set sentences.', 'we also vary the transition prior α choosing α = 1. 0 and α = 0. 05 on the ccg tags.', 'the emission prior β was held constant at 1. 0.', 'the results of these experiments for α = 0. 05 are tabulated in table 3 ( a ).', 'for comparison, we also show the results of the c & c supertagger of  #AUTHOR_TAG in table 3 ( b ).', 'the parameter α, which determines the sparsity of the transition matrix, has been reported to have a greater influence on the performance of the tagger in  #AUTHOR_TAG in weakly supervised pos tagging.', 'we also observed this in supervised supertagging, in the models hmm and fhmmb.', 'the hmm model and fhmmb showed a slight dip in their performance for α = 1. 0 while fhmma did slightly better.', 'what stands out in these results is the performance of the fhmm models with minimal amount of training data ( for 100 sentences, fhmmb is quite close to the discriminatively trained c & c supertagger ).', '']",4
['bitag hmm of  #TAUTHOR_TAG'],['bitag hmm of  #TAUTHOR_TAG'],['bitag hmm of  #TAUTHOR_TAG'],"['the tag dictionary of that word, when the lexicon is constructed. this is in fact a form of supervision, which we use here as an oracle to explore the effect of', 'reducing lexical ambiguity. results of this experiment for α = 1. 0, on ambiguous ccg categories, are tabulated in table', '5 ( a ). the results for α = 0. 05 is shown in table 6 ( a ). we also report the', 'ccg accuracy values inclusive of unambiguous types in table 5 ( b ) for α =', '1. 0 and table 6 ( b ) respectively. the performance of the hmm model ( 31 % ) in table 5 ( a ) without any frequency cut -', 'off on the ccg categories, is comparable to the bitag hmm of  #TAUTHOR_TAG that uses variational bayes em ( 33 % ). our complexity based initialization is not directly comparable to the results in  #TAUTHOR_TAG because the values there are based on a', 'weighted combination of complexity based initialization and modified transition priors based on the ccg formalism. however, it is encouraging to see that when there is no cut - off based filtering of the categories,', 'fhmmb ( 47. 98 % ) greatly outperforms the hmm - em model of  #TAUTHOR_TAG. it is however, quite short of the 56. 1 % accuracy achieved by the model of  #TAUTHOR_TAG that uses', 'grammar informed initialization ( combination of category based initialization along with category transition rules ). without any frequency cut - off on ccg categories, fhmmb achieves over 17 % improvement in the', 'prediction accuracy of ambiguous ccg categories, in comparison with the hmm. the hmm performs much better when there is a high level of frequency based filtering of the categories. however,', 'recall that frequency based filtering of categories is a strong form of supervision that we use here only as', 'an oracle and which one could not expect to have in real world tag dictionaries. the pos accuracies in these experiments were 83. 5 - 85 %, 84. 5 - 86. 2', '% and 78. 3 - 78. 4 % for models fhmmb, fhmma and hmm respectively ( without any frequency cut - off ). in the weakly supervised setting, the choice of the transition prior α of 0. 05 lead to severe degradation', 'in the prediction accuracy of ccg tags. unlike pos tagging, where a symmetric transition prior of α = 0. 05 captured the sparsity of the tag', 'transition distribution, in supertagging the transition priors are asymmetric. we expect that ccg transition rules  #TAUTHOR_TAG when encoded as category specific transition priors, will lead to better performance', 'with the fhmms']",4
['bitag hmm of  #TAUTHOR_TAG'],['bitag hmm of  #TAUTHOR_TAG'],['bitag hmm of  #TAUTHOR_TAG'],"['the tag dictionary of that word, when the lexicon is constructed. this is in fact a form of supervision, which we use here as an oracle to explore the effect of', 'reducing lexical ambiguity. results of this experiment for α = 1. 0, on ambiguous ccg categories, are tabulated in table', '5 ( a ). the results for α = 0. 05 is shown in table 6 ( a ). we also report the', 'ccg accuracy values inclusive of unambiguous types in table 5 ( b ) for α =', '1. 0 and table 6 ( b ) respectively. the performance of the hmm model ( 31 % ) in table 5 ( a ) without any frequency cut -', 'off on the ccg categories, is comparable to the bitag hmm of  #TAUTHOR_TAG that uses variational bayes em ( 33 % ). our complexity based initialization is not directly comparable to the results in  #TAUTHOR_TAG because the values there are based on a', 'weighted combination of complexity based initialization and modified transition priors based on the ccg formalism. however, it is encouraging to see that when there is no cut - off based filtering of the categories,', 'fhmmb ( 47. 98 % ) greatly outperforms the hmm - em model of  #TAUTHOR_TAG. it is however, quite short of the 56. 1 % accuracy achieved by the model of  #TAUTHOR_TAG that uses', 'grammar informed initialization ( combination of category based initialization along with category transition rules ). without any frequency cut - off on ccg categories, fhmmb achieves over 17 % improvement in the', 'prediction accuracy of ambiguous ccg categories, in comparison with the hmm. the hmm performs much better when there is a high level of frequency based filtering of the categories. however,', 'recall that frequency based filtering of categories is a strong form of supervision that we use here only as', 'an oracle and which one could not expect to have in real world tag dictionaries. the pos accuracies in these experiments were 83. 5 - 85 %, 84. 5 - 86. 2', '% and 78. 3 - 78. 4 % for models fhmmb, fhmma and hmm respectively ( without any frequency cut - off ). in the weakly supervised setting, the choice of the transition prior α of 0. 05 lead to severe degradation', 'in the prediction accuracy of ccg tags. unlike pos tagging, where a symmetric transition prior of α = 0. 05 captured the sparsity of the tag', 'transition distribution, in supertagging the transition priors are asymmetric. we expect that ccg transition rules  #TAUTHOR_TAG when encoded as category specific transition priors, will lead to better performance', 'with the fhmms']",4
['bitag hmm of  #TAUTHOR_TAG'],['bitag hmm of  #TAUTHOR_TAG'],['bitag hmm of  #TAUTHOR_TAG'],"['the tag dictionary of that word, when the lexicon is constructed. this is in fact a form of supervision, which we use here as an oracle to explore the effect of', 'reducing lexical ambiguity. results of this experiment for α = 1. 0, on ambiguous ccg categories, are tabulated in table', '5 ( a ). the results for α = 0. 05 is shown in table 6 ( a ). we also report the', 'ccg accuracy values inclusive of unambiguous types in table 5 ( b ) for α =', '1. 0 and table 6 ( b ) respectively. the performance of the hmm model ( 31 % ) in table 5 ( a ) without any frequency cut -', 'off on the ccg categories, is comparable to the bitag hmm of  #TAUTHOR_TAG that uses variational bayes em ( 33 % ). our complexity based initialization is not directly comparable to the results in  #TAUTHOR_TAG because the values there are based on a', 'weighted combination of complexity based initialization and modified transition priors based on the ccg formalism. however, it is encouraging to see that when there is no cut - off based filtering of the categories,', 'fhmmb ( 47. 98 % ) greatly outperforms the hmm - em model of  #TAUTHOR_TAG. it is however, quite short of the 56. 1 % accuracy achieved by the model of  #TAUTHOR_TAG that uses', 'grammar informed initialization ( combination of category based initialization along with category transition rules ). without any frequency cut - off on ccg categories, fhmmb achieves over 17 % improvement in the', 'prediction accuracy of ambiguous ccg categories, in comparison with the hmm. the hmm performs much better when there is a high level of frequency based filtering of the categories. however,', 'recall that frequency based filtering of categories is a strong form of supervision that we use here only as', 'an oracle and which one could not expect to have in real world tag dictionaries. the pos accuracies in these experiments were 83. 5 - 85 %, 84. 5 - 86. 2', '% and 78. 3 - 78. 4 % for models fhmmb, fhmma and hmm respectively ( without any frequency cut - off ). in the weakly supervised setting, the choice of the transition prior α of 0. 05 lead to severe degradation', 'in the prediction accuracy of ccg tags. unlike pos tagging, where a symmetric transition prior of α = 0. 05 captured the sparsity of the tag', 'transition distribution, in supertagging the transition priors are asymmetric. we expect that ccg transition rules  #TAUTHOR_TAG when encoded as category specific transition priors, will lead to better performance', 'with the fhmms']",2
"['of  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG.', ' #AUTHOR_TAG uses fhmms']","['of  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG.', ' #AUTHOR_TAG uses fhmms']","['paper follows the work of  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG.', ' #AUTHOR_TAG uses fhmms']","['paper follows the work of  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG.', ' #AUTHOR_TAG uses fhmms for jointly labeling the pos and np chunk tags for the conll2000 dataset  #AUTHOR_TAG.', 'his is a fully supervised model for a simpler task.', 'we address the harder problem of supertagging in this paper and especially in the weakly supervised setting, with fhmms.', ' #AUTHOR_TAG uses a bayesian tritag hmm ( bhmm ) for pos tagging and considers three different scenarios : ( 1 ) a weakly supervised setting with fixed hyperparameters α and β, ( 2 ) hyper parameter inference ( learning the optimal values for α and β ) and ( 3 ) hyper parameter inference with varying corpus size and dictionary knowledge.', 'our bitag hmm achieved results close to what was reported by her bhmm on a random 24000 word subset of the wsj.', 'in all our experiments, we have kept the test set separate from the training set from which the dictionary was built ; this distinction is not made by goldwater.', 'again, our work focuses on the harder problem of supertagging.', 'mc  #AUTHOR_TAG have also used a factorial model for performing joint labeling of the pos and chunk tags but by using dynamic conditional random fields ( dcrf ).', 'the advantage of using an fhmm over dcrf is the the ability to use less supervision in training the model.', 'even in the supervised training scenario, fhmm has the advantage of lower training time when compared to discriminative training models like dcrf']",6
"['bilingual embeddings instead of a seed dictionary.', ' #TAUTHOR_TAG use a very small, automatically - generated seed lexicon of identical numerals as']","['bilingual embeddings instead of a seed dictionary.', ' #TAUTHOR_TAG use a very small, automatically - generated seed lexicon of identical numerals as']","['bilingual embeddings instead of a seed dictionary.', ' #TAUTHOR_TAG use a very small, automatically - generated seed lexicon of identical numerals as the initialization in an iterative self - learning framework']","['the past few years, new methods for bilingual lexicon induction have been proposed that are applicable to low - resource language pairs, for which very little sentence - aligned parallel data is available.', 'parallel data can be very expensive to create, so methods that require less of it or that can utilize more readily available data are desirable.', ""one prevalent strategy involves creating multilingual word embeddings, where each language's vocabulary is embedded in the same latent space ( vulic and  #AUTHOR_TAG a ;  #AUTHOR_TAG ; however, many of these methods still require a strong cross - lingual signal in the form of a large seed dictionary."", 'more recent work has focused on reducing that constraint.', 'vulic and  #AUTHOR_TAG and  #AUTHOR_TAG use document - aligned data to learn bilingual embeddings instead of a seed dictionary.', ' #TAUTHOR_TAG use a very small, automatically - generated seed lexicon of identical numerals as the initialization in an iterative self - learning framework to learn a linear mapping between monolingual embedding spaces ;  #AUTHOR_TAG use an adversarial training method to learn a similar mapping.', ' #AUTHOR_TAG a ) use a series of techniques to align monolingual embedding spaces in a completely unsupervised way ; their method is used by  #AUTHOR_TAG b ) as the initialization for a completely unsupervised machine translation system.', 'these recent advances in unsupervised bilingual lexicon induction show promise for use in low - resource contexts.', 'however, none of them make use of linguistic features of the languages themselves ( with the arguable exception of syntactic / semantic information encoded in the word embeddings ).', 'this is in contrast to work that predates many of these embedding - based methods that leveraged linguistic features such as edit distance and orthographic similarity :  #AUTHOR_TAG and berg -  #AUTHOR_TAG investigate using linguistic features for word alignment, and  #AUTHOR_TAG use linguistic features for unsupervised bilingual lexicon induction.', 'these features can help identify words with common ancestry ( such as the english - italian pair agile - agile ) and borrowed words ( macaronimaccheroni ).', 'the addition of linguistic features led to increased performance in these earlier models, especially for related languages, yet these features have not been applied to more modern methods.', 'in this work, we extend the modern embeddingbased approach of  #TAUTHOR_TAG with orthographic information in order to leverage similarities between related languages for increased accuracy in bilingual lexicon induction']",1
"['bilingual embeddings instead of a seed dictionary.', ' #TAUTHOR_TAG use a very small, automatically - generated seed lexicon of identical numerals as']","['bilingual embeddings instead of a seed dictionary.', ' #TAUTHOR_TAG use a very small, automatically - generated seed lexicon of identical numerals as']","['bilingual embeddings instead of a seed dictionary.', ' #TAUTHOR_TAG use a very small, automatically - generated seed lexicon of identical numerals as the initialization in an iterative self - learning framework']","['the past few years, new methods for bilingual lexicon induction have been proposed that are applicable to low - resource language pairs, for which very little sentence - aligned parallel data is available.', 'parallel data can be very expensive to create, so methods that require less of it or that can utilize more readily available data are desirable.', ""one prevalent strategy involves creating multilingual word embeddings, where each language's vocabulary is embedded in the same latent space ( vulic and  #AUTHOR_TAG a ;  #AUTHOR_TAG ; however, many of these methods still require a strong cross - lingual signal in the form of a large seed dictionary."", 'more recent work has focused on reducing that constraint.', 'vulic and  #AUTHOR_TAG and  #AUTHOR_TAG use document - aligned data to learn bilingual embeddings instead of a seed dictionary.', ' #TAUTHOR_TAG use a very small, automatically - generated seed lexicon of identical numerals as the initialization in an iterative self - learning framework to learn a linear mapping between monolingual embedding spaces ;  #AUTHOR_TAG use an adversarial training method to learn a similar mapping.', ' #AUTHOR_TAG a ) use a series of techniques to align monolingual embedding spaces in a completely unsupervised way ; their method is used by  #AUTHOR_TAG b ) as the initialization for a completely unsupervised machine translation system.', 'these recent advances in unsupervised bilingual lexicon induction show promise for use in low - resource contexts.', 'however, none of them make use of linguistic features of the languages themselves ( with the arguable exception of syntactic / semantic information encoded in the word embeddings ).', 'this is in contrast to work that predates many of these embedding - based methods that leveraged linguistic features such as edit distance and orthographic similarity :  #AUTHOR_TAG and berg -  #AUTHOR_TAG investigate using linguistic features for word alignment, and  #AUTHOR_TAG use linguistic features for unsupervised bilingual lexicon induction.', 'these features can help identify words with common ancestry ( such as the english - italian pair agile - agile ) and borrowed words ( macaronimaccheroni ).', 'the addition of linguistic features led to increased performance in these earlier models, especially for related languages, yet these features have not been applied to more modern methods.', 'in this work, we extend the modern embeddingbased approach of  #TAUTHOR_TAG with orthographic information in order to leverage similarities between related languages for increased accuracy in bilingual lexicon induction']",6
"['work of  #TAUTHOR_TAG.', 'following their']","['work of  #TAUTHOR_TAG.', 'following their work, let x ∈']","['work is directly based on the work of  #TAUTHOR_TAG.', 'following their']","['work is directly based on the work of  #TAUTHOR_TAG.', 'following their work, let x ∈ r | vs | ×d and z ∈ r | vt | ×d be the word embedding matrices of two distinct languages, referred to respectively as the source and target, such that each row corresponds to the d - dimensional embedding of a single word.', 'we refer to the ith row of one of these matrices as x i * or z i *.', 'the vocabularies for each language are v s and v t, respectively.', '']",6
"['dictionary induction phase of the self - learning framework of  #TAUTHOR_TAG, which uses']","['dictionary induction phase of the self - learning framework of  #TAUTHOR_TAG, which uses']","[""method modifies the similarity score for each word pair during the dictionary induction phase of the self - learning framework of  #TAUTHOR_TAG, which uses the dot product of two words'embeddings to quantify similarity."", 'we modify this similarity score by adding a measure of orthographic similarity, which is a function of the normalized string edit distance of the two words.', '']","[""method modifies the similarity score for each word pair during the dictionary induction phase of the self - learning framework of  #TAUTHOR_TAG, which uses the dot product of two words'embeddings to quantify similarity."", 'we modify this similarity score by adding a measure of orthographic similarity, which is a function of the normalized string edit distance of the two words.', 'the normalized edit distance is defined as the levenshtein distance ( l ( ·, · ) )  #AUTHOR_TAG divided by the length of the longer word.', 'the levenshtein distance represents the minimum number of insertions, deletions, and substitutions required to transform one word into the other.', 'the normalized edit distance function is denoted as nl ( ·, · ).', 'we define the orthographic similarity of two words w 1 and w 2 as log ( 2. 0−nl ( w 1, w 2 ) ).', 'these similarity scores are used to form an orthographic similarity matrix s, where each entry corresponds to a source - target word pair.', '']",6
"['use the datasets used by  #TAUTHOR_TAG, consisting of three language pairs : englishitalian,']","['use the datasets used by  #TAUTHOR_TAG, consisting of three language pairs : englishitalian, english - german, and english - finnish.', '']","['use the datasets used by  #TAUTHOR_TAG, consisting of three language pairs : englishitalian, english - german, and']","['use the datasets used by  #TAUTHOR_TAG, consisting of three language pairs : englishitalian, english - german, and english - finnish.', 'the english - italian dataset was introduced in  #AUTHOR_TAG ; the other datasets were created by  #AUTHOR_TAG.', 'each dataset includes monolingual word embeddings ( trained with word2vec  #AUTHOR_TAG b ) ) for both languages and a bilingual dictionary, separated into a training and test set.', 'we do not use the training set as the input dictionary to the system, instead using an automatically - generated dictionary consisting only of numeral identity translations ( such as 2 - 2, 3 - 3, et cetera ) as in  #TAUTHOR_TAG.', '1 however, because the methods presented in this work feature tunable hyperparameters, we use a portion of the training set as devel - table 1 : comparison of methods on test data.', 'scaling constants c e and c s were selected based on performance on development data over all three language pairs.', 'the last two rows report the results of using both methods together']",3
"['system of  #TAUTHOR_TAG, using scaling']","['system of  #TAUTHOR_TAG, using scaling']","['1 compares our methods against the system of  #TAUTHOR_TAG, using scaling factors selected based on development data results.', 'because approximately']","['our experiments with orthographic extension of word embeddings, each embedding was extended by the size of the union of the alphabets of both languages.', 'the size of this union was 199 for english - italian, 200 for english - german, and 287 for english - finnish.', 'these numbers are perhaps unintuitively high.', 'however, the corpora include many other characters, including diacritical markings and various symbols ( %, [,!, etc. ) that are an indication that tokenization of the data could be improved.', 'we did not filter these characters in this work.', 'for our experiments with orthographic similarity adjustment, the heuristic identified approximately 2 million word pairs for each language pair out of a possible 40 billion, resulting in significant computation savings. and c s = 1 as our hyperparameters.', 'the local optima were not identical for all three languages, but we felt that these values struck the best compromise among them.', 'table 1 compares our methods against the system of  #TAUTHOR_TAG, using scaling factors selected based on development data results.', 'because approximately 20 % of source - target pairs in the dictionary were identical, we also extended all systems to guess the identity translation if the source word appeared in the target vocabulary.', '']",4
"['##ing  #TAUTHOR_TAG, discussing the techniques of']","['video captioning  #TAUTHOR_TAG, discussing the techniques of leveraging hierarchies in']","['video captioning  #TAUTHOR_TAG, discussing the techniques of']","['', 'drl methods could easily combine embedding based representation learning with reasoning, and optimize for a variety of non - differentiable rewards.', 'however, a key challenge for applying deep reinforcement learning techniques to real - world sized nlp problems is the model design issue.', 'this tutorial draws connections from theories of deep reinforcement learning to practical applications in nlp.', 'in particular, we start with the gentle introduction to the fundamentals of reinforcement learning  #AUTHOR_TAG.', 'we further discuss we further discuss several critical issues in drl solutions for nlp tasks, including ( 1 ) the efficient and practical design of the action space, state space, and reward functions ; ( 2 ) the trade - off between exploration and exploitation ; and ( 3 ) the goal of incorporating linguistic structures in drl.', 'to address the model design issue, we discuss several recent solutions  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'we then focus on a new case study of hierarchical deep reinforcement learning for video captioning  #TAUTHOR_TAG, discussing the techniques of leveraging hierarchies in drl for nlp generation problems.', 'this tutorial aims at introducing deep reinforcement learning methods to researchers in the nlp community.', 'we do not assume any particular prior knowledge in reinforcement learning.', 'the intended length of the tutorial is 3 hours, including a coffee break.', 'representation learning, reasoning ( learning to search ), and scalability are three closely']",5
"['##ing  #TAUTHOR_TAG, discussing the techniques of leveraging hierarchies in']","['video captioning  #TAUTHOR_TAG, discussing the techniques of leveraging hierarchies in']","['video captioning  #TAUTHOR_TAG, discussing the techniques of leveraging hierarchies in']","['', 'we outline the applications of deep reinforcement learning in nlp, including dialog  #AUTHOR_TAG, semi - supervised text classification  #AUTHOR_TAG, coreference  #AUTHOR_TAG, knowledge graph reasoning  #AUTHOR_TAG, text games  #AUTHOR_TAG a ), social media  #AUTHOR_TAG b ;  #AUTHOR_TAG, information extraction  #AUTHOR_TAG, language and vision  #AUTHOR_TAG a, b, c ;  #AUTHOR_TAG, etc.', 'we further discuss several critical issues in drl solutions for nlp tasks, including ( 1 ) the efficient and practical design of the action space, state space, and reward functions ; ( 2 ) the trade - off between exploration and exploitation ; and ( 3 ) the goal of incorporating linguistic structures in drl.', 'to address the model design issue, we discuss several recent solutions  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'we then focus on a new case study of hierarchical deep reinforcement learning for video captioning  #TAUTHOR_TAG, discussing the techniques of leveraging hierarchies in drl for nlp generation problems.', 'this tutorial aims at introducing deep reinforcement learning methods to researchers in the nlp community.', 'we do not assume any particular prior knowledge in reinforcement learning.', 'the intended length of the tutorial is 3 hours, including a coffee break']",5
"['', 'we will show case a recent study  #TAUTHOR_TAG that leverages hierarchical deep reinforcement']","['', 'we will show case a recent study  #TAUTHOR_TAG that leverages hierarchical deep reinforcement']","['', 'we will show case a recent study  #TAUTHOR_TAG that leverages hierarchical deep reinforcement learning for language and vision, and extend']","['', 'more specifically, we will discuss three important issues, including problem formulation / model design, exploration vs. exploitation, and the integration of linguistic structures in drl.', 'we will show case a recent study  #TAUTHOR_TAG that leverages hierarchical deep reinforcement learning for language and vision, and extend the discussion.', 'practical advice including programming advice will be provided as a part of the demonstration']",5
"[' #TAUTHOR_TAG 24 ] was proposed, which uses']","[' #TAUTHOR_TAG 24 ] was proposed, which uses']","['- attention  #TAUTHOR_TAG 24 ] was proposed, which uses the whole sequence']","['works proposed partially - or purely - convolutional ctc models [ 8 ] [ 9 ] [ 10 ] [ 11 ] and convolution - heavy encoder - decoder models [ 16 ] for asr.', 'however, convolutional models must be significantly deeper to retrieve the same temporal receptive field [ 23 ].', 'recently, the mechanism of self - attention  #TAUTHOR_TAG 24 ] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time.', 'its use in both encoder - decoder and feedforward contexts has led to faster training and state - of - the - art results in translation ( via the transformer  #TAUTHOR_TAG, sentiment analysis [ 25 ], and other tasks.', '']",0
"[' #TAUTHOR_TAG 24 ] was proposed, which uses']","[' #TAUTHOR_TAG 24 ] was proposed, which uses']","['- attention  #TAUTHOR_TAG 24 ] was proposed, which uses the whole sequence']","['works proposed partially - or purely - convolutional ctc models [ 8 ] [ 9 ] [ 10 ] [ 11 ] and convolution - heavy encoder - decoder models [ 16 ] for asr.', 'however, convolutional models must be significantly deeper to retrieve the same temporal receptive field [ 23 ].', 'recently, the mechanism of self - attention  #TAUTHOR_TAG 24 ] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time.', 'its use in both encoder - decoder and feedforward contexts has led to faster training and state - of - the - art results in translation ( via the transformer  #TAUTHOR_TAG, sentiment analysis [ 25 ], and other tasks.', '']",0
"['- product, self - attention  #TAUTHOR_TAG.', 'for']","['performs multi - head, scaled dot - product, self - attention  #TAUTHOR_TAG.', 'for']","['- product, self - attention  #TAUTHOR_TAG.', 'for']","['', 'the first sublayer performs multi - head, scaled dot - product, self - attention  #TAUTHOR_TAG.', '']",0
"['- product, self - attention  #TAUTHOR_TAG.', 'for']","['performs multi - head, scaled dot - product, self - attention  #TAUTHOR_TAG.', 'for']","['- product, self - attention  #TAUTHOR_TAG.', 'for']","['', 'the first sublayer performs multi - head, scaled dot - product, self - attention  #TAUTHOR_TAG.', '']",0
"['- attention is inherently content - based  #TAUTHOR_TAG, and so one often encodes position into the post - embedding vectors.', 'we use standard trigonometric embeddings, where for 0 ≤ i ≤ demb / 2, we define', 'for position t. we consider three approaches : content - only']","['- attention is inherently content - based  #TAUTHOR_TAG, and so one often encodes position into the post - embedding vectors.', 'we use standard trigonometric embeddings, where for 0 ≤ i ≤ demb / 2, we define', 'for position t. we consider three approaches : content - only [ 21 ], which forgoes position encodings ; additive [ 19 ],']","['- attention is inherently content - based  #TAUTHOR_TAG, and so one often encodes position into the post - embedding vectors.', 'we use standard trigonometric embeddings, where for 0 ≤ i ≤ demb / 2, we define', 'for position t. we consider three approaches : content - only [ 21 ], which forg']","['- attention is inherently content - based  #TAUTHOR_TAG, and so one often encodes position into the post - embedding vectors.', 'we use standard trigonometric embeddings, where for 0 ≤ i ≤ demb / 2, we define', 'for position t. we consider three approaches : content - only [ 21 ], which forgoes position encodings ; additive [ 19 ], which takes demb = dh and adds the encoding to the embedding ; and concatenative, where one takes demb = 40 and concatenates it to the embedding.', 'the latter was found necessary for self - attentional las [ 27 ], as additive encodings did not give convergence.', 'however, the monotonicity of ctc is a further positional inductive bias, which may enable the success of content - only and additive encodings']",0
"['in the transformer encoder  #TAUTHOR_TAG, previous explorations of self - attention in asr']","['in the transformer encoder  #TAUTHOR_TAG, previous explorations of self - attention in asr [ 19, 27 ], and defined in section']","['in the transformer encoder  #TAUTHOR_TAG, previous explorations of self - attention in asr [ 19, 27 ], and defined in']","['now replace recurrent and convolutional layers for ctc with self - attention [ 24 ].', 'our proposed framework ( figure 1a ) is built around self - attention layers, as used in the transformer encoder  #TAUTHOR_TAG, previous explorations of self - attention in asr [ 19, 27 ], and defined in section 2. 3.', 'the other stages are downsampling, which reduces input length t via methods like those in section 2. 4 ; embedding, which learns a dh - dim. embedding that also describes token position ( section 2. 5 ) ; and projection, where each final representation is mapped framewise to logits over the intermediate alphabet l.', ""the first implements self - attention, where the success of attention in ctc and encoder - decoder models [ 14, 31 ] is parallelized by using each position's representation to attend to all others, giving a contextualized representation for that position."", 'hence, the full receptive field is immediately available at the cost of o ( t 2 ) inner products ( table 1 ), enabling richer representations in fewer layers']",5
"['layer type, based on  #TAUTHOR_TAG.', 't is input length,']","['layer type, based on  #TAUTHOR_TAG.', 't is input length,']","['1 : operation complexity of each layer type, based on  #TAUTHOR_TAG.', 't is input length, d is']","['path length table 1 : operation complexity of each layer type, based on  #TAUTHOR_TAG.', 't is input length, d is no. of hidden units, and k is filter / context width.', 'we also see inspiration from convolutional blocks : residual connections, layer normalization, and tied dense layers with relu for representation learning.', 'in particular, multi - head attention is akin to having a number of infinitely - wide filters whose weights adapt to the content ( allowing fewer "" filters "" to suffice ).', 'one can also assign interpretations ; for example, [ 27 ] argue their las self - attention heads are differentiated phoneme detectors.', 'further inductive biases like filter widths and causality could be expressed through time - restricted self - attention [ 26 ] and directed self - attention [ 25 ], respectively']",5
"['in the transformer encoder  #TAUTHOR_TAG, previous explorations of self - attention in asr']","['in the transformer encoder  #TAUTHOR_TAG, previous explorations of self - attention in asr [ 19, 27 ], and defined in section']","['in the transformer encoder  #TAUTHOR_TAG, previous explorations of self - attention in asr [ 19, 27 ], and defined in']","['now replace recurrent and convolutional layers for ctc with self - attention [ 24 ].', 'our proposed framework ( figure 1a ) is built around self - attention layers, as used in the transformer encoder  #TAUTHOR_TAG, previous explorations of self - attention in asr [ 19, 27 ], and defined in section 2. 3.', 'the other stages are downsampling, which reduces input length t via methods like those in section 2. 4 ; embedding, which learns a dh - dim. embedding that also describes token position ( section 2. 5 ) ; and projection, where each final representation is mapped framewise to logits over the intermediate alphabet l.', ""the first implements self - attention, where the success of attention in ctc and encoder - decoder models [ 14, 31 ] is parallelized by using each position's representation to attend to all others, giving a contextualized representation for that position."", 'hence, the full receptive field is immediately available at the cost of o ( t 2 ) inner products ( table 1 ), enabling richer representations in fewer layers']",3
"['layer type, based on  #TAUTHOR_TAG.', 't is input length,']","['layer type, based on  #TAUTHOR_TAG.', 't is input length,']","['1 : operation complexity of each layer type, based on  #TAUTHOR_TAG.', 't is input length, d is']","['path length table 1 : operation complexity of each layer type, based on  #TAUTHOR_TAG.', 't is input length, d is no. of hidden units, and k is filter / context width.', 'we also see inspiration from convolutional blocks : residual connections, layer normalization, and tied dense layers with relu for representation learning.', 'in particular, multi - head attention is akin to having a number of infinitely - wide filters whose weights adapt to the content ( allowing fewer "" filters "" to suffice ).', 'one can also assign interpretations ; for example, [ 27 ] argue their las self - attention heads are differentiated phoneme detectors.', 'further inductive biases like filter widths and causality could be expressed through time - restricted self - attention [ 26 ] and directed self - attention [ 25 ], respectively']",3
"['use the standard linear warmup period before inverse square decay associated with these architectures [ 19,  #TAUTHOR_TAG.', '']","['use the standard linear warmup period before inverse square decay associated with these architectures [ 19,  #TAUTHOR_TAG.', '']","['use the standard linear warmup period before inverse square decay associated with these architectures [ 19,  #TAUTHOR_TAG.', '']","['take ( nlayers, dh, nheads, dff ) = ( 10, 512, 8, 2048 ), giving ∼30m parameters.', 'this is on par with models on wsj ( 10 - 30m ) [ 4, 5, 9 ] and an order of magnitude below models on librispeech ( 100 - 250m ) [ 8, 23 ].', 'we use mxnet [ 37 ] for modeling and kaldi / eesen [ 7, 38 ] for data preparation and decoding.', ""our self - attention code is based on gluonnlp's implementation."", 'at train time, utterances are sorted by length : we exclude those longer than 1800 frames ( 1 % of each training set ).', 'we take a window of 25ms, a hop of 10ms, and concatenate cepstral mean - variance normalized features with temporal first - and second - order differences.', '1 we downsample by a factor of k = 3 ( this also gave an ideal t / k ≈ dh for our data ; see table 1 ).', 'we perform nesterov - accelerated gradient descent on batches of 20 utterances.', 'as self - attention architectures can be unstable in early training, we clip gradients to a global norm of 1 and use the standard linear warmup period before inverse square decay associated with these architectures [ 19,  #TAUTHOR_TAG.', 'let n denote the global step number of the batch ( across epochs ) ; the learning rate is given by', '1 rescaling so that these differences also have var.', '≈ 1 helped wsj training.', '[ 17 ] 11. 5 - 9. 0 - enc - dec ( 4 - 1 ) [ 17 ] 12. 0 - 8. 2 - enc - dec + ctc ( 4 - 1 ) [ 17 ] 11. 3 - 7. 4 - enc - dec ( 4 - 1 ) [ 39 ] - - 6. 4 9. 3 ctc / asg ( gated cnn ) [ 40 ] 6. 9 9. 5 4. 9 6. 6 enc - dec ( 2, 1, 3 - 1 table 3 : ctc phoneme models with wfst decoding on wsj.', 'where we take λ = 400 and nwarmup as a hyperparameter.', 'however, such a decay led to early stagnation in validation accuracy, so we later divide the learning rate by 10 and run at the decayed rate for 20 epochs.', 'we do this twice, then take the epoch with the best validation score.', 'xavier initialization gave validation accuracies of zero for the first few epochs, suggesting room for improvement.', 'like']",4
"['use the standard linear warmup period before inverse square decay associated with these architectures [ 19,  #TAUTHOR_TAG.', '']","['use the standard linear warmup period before inverse square decay associated with these architectures [ 19,  #TAUTHOR_TAG.', '']","['use the standard linear warmup period before inverse square decay associated with these architectures [ 19,  #TAUTHOR_TAG.', '']","['take ( nlayers, dh, nheads, dff ) = ( 10, 512, 8, 2048 ), giving ∼30m parameters.', 'this is on par with models on wsj ( 10 - 30m ) [ 4, 5, 9 ] and an order of magnitude below models on librispeech ( 100 - 250m ) [ 8, 23 ].', 'we use mxnet [ 37 ] for modeling and kaldi / eesen [ 7, 38 ] for data preparation and decoding.', ""our self - attention code is based on gluonnlp's implementation."", 'at train time, utterances are sorted by length : we exclude those longer than 1800 frames ( 1 % of each training set ).', 'we take a window of 25ms, a hop of 10ms, and concatenate cepstral mean - variance normalized features with temporal first - and second - order differences.', '1 we downsample by a factor of k = 3 ( this also gave an ideal t / k ≈ dh for our data ; see table 1 ).', 'we perform nesterov - accelerated gradient descent on batches of 20 utterances.', 'as self - attention architectures can be unstable in early training, we clip gradients to a global norm of 1 and use the standard linear warmup period before inverse square decay associated with these architectures [ 19,  #TAUTHOR_TAG.', 'let n denote the global step number of the batch ( across epochs ) ; the learning rate is given by', '1 rescaling so that these differences also have var.', '≈ 1 helped wsj training.', '[ 17 ] 11. 5 - 9. 0 - enc - dec ( 4 - 1 ) [ 17 ] 12. 0 - 8. 2 - enc - dec + ctc ( 4 - 1 ) [ 17 ] 11. 3 - 7. 4 - enc - dec ( 4 - 1 ) [ 39 ] - - 6. 4 9. 3 ctc / asg ( gated cnn ) [ 40 ] 6. 9 9. 5 4. 9 6. 6 enc - dec ( 2, 1, 3 - 1 table 3 : ctc phoneme models with wfst decoding on wsj.', 'where we take λ = 400 and nwarmup as a hyperparameter.', 'however, such a decay led to early stagnation in validation accuracy, so we later divide the learning rate by 10 and run at the decayed rate for 20 epochs.', 'we do this twice, then take the epoch with the best validation score.', 'xavier initialization gave validation accuracies of zero for the first few epochs, suggesting room for improvement.', 'like']",6
"['of semantic relatedness tasks  #TAUTHOR_TAG.', 'graphs are another way of representing relations between linguistic entities, and']","['of semantic relatedness tasks  #TAUTHOR_TAG.', 'graphs are another way of representing relations between linguistic entities, and']","['space models, representing word meanings as points in high - dimensional space, have been used in a variety of semantic relatedness tasks  #TAUTHOR_TAG.', 'graphs are another way of representing relations between linguistic entities, and they have been used to capture semantic relatedness by using both corpus - based evidence and the graph structure']","['space models, representing word meanings as points in high - dimensional space, have been used in a variety of semantic relatedness tasks  #TAUTHOR_TAG.', 'graphs are another way of representing relations between linguistic entities, and they have been used to capture semantic relatedness by using both corpus - based evidence and the graph structure of wordnet and wikipedia  #AUTHOR_TAG.', 'we study the relationship between vector space models and graph random walk models by embedding vector space models in graphs.', 'the flexibility offered by graph random walk models allows us to compare the vector space - based similarity measures to extended notions of relatedness and similarity.', 'in particular, a random walk model can be viewed as smoothing direct similarity between two vectors using second - order and even higher - order vectors.', 'this view leads to the second focal point of this paper : we investigate whether random walk models can simulate the smoothing effects obtained by methods like singular value decomposition ( svd ).', 'to answer this question, we compute models on reduced ( downsampled ) versions of our dataset and evaluate the robustness of random walk models, a classic vector - based model, and svd - based models against data sparseness']",0
"['of semantic relatedness tasks  #TAUTHOR_TAG.', 'graphs are another way of representing relations between linguistic entities, and']","['of semantic relatedness tasks  #TAUTHOR_TAG.', 'graphs are another way of representing relations between linguistic entities, and']","['space models, representing word meanings as points in high - dimensional space, have been used in a variety of semantic relatedness tasks  #TAUTHOR_TAG.', 'graphs are another way of representing relations between linguistic entities, and they have been used to capture semantic relatedness by using both corpus - based evidence and the graph structure']","['space models, representing word meanings as points in high - dimensional space, have been used in a variety of semantic relatedness tasks  #TAUTHOR_TAG.', 'graphs are another way of representing relations between linguistic entities, and they have been used to capture semantic relatedness by using both corpus - based evidence and the graph structure of wordnet and wikipedia  #AUTHOR_TAG.', 'we study the relationship between vector space models and graph random walk models by embedding vector space models in graphs.', 'the flexibility offered by graph random walk models allows us to compare the vector space - based similarity measures to extended notions of relatedness and similarity.', 'in particular, a random walk model can be viewed as smoothing direct similarity between two vectors using second - order and even higher - order vectors.', 'this view leads to the second focal point of this paper : we investigate whether random walk models can simulate the smoothing effects obtained by methods like singular value decomposition ( svd ).', 'to answer this question, we compute models on reduced ( downsampled ) versions of our dataset and evaluate the robustness of random walk models, a classic vector - based model, and svd - based models against data sparseness']",5
"['', 'with minipar. 2 following  #TAUTHOR_TAG,']","['tuples from the 2 - billion word ukwac corpus, 1 dependency - parsed', 'with minipar. 2 following  #TAUTHOR_TAG,']","['', 'with minipar. 2 following  #TAUTHOR_TAG']","['pair of walks, one starting at a node determined by q 1 and the other starting at a node governed by q 2, ending at the same node. discussion. direct and indirect relatedness measures together with variation in walk length give us a', 'simple, powerful and flexible way to capture different kinds of similarity ( with traditional vectorbased approach as a special case ). longer walks', 'or flexible walks will capture higher order effects that may help coping with data sparseness, similar to the use of second - order vectors.', 'dimensionality reduction techniques like singular value decomposition ( svd ) also capture these higher - order effects, and it has been argued that that makes them more resistant against sparseness ( schutze, 1997 ). to our knowledge,', 'no systematic comparison of svd and classical vector - based methods has been', 'done on different corpus sizes. in our experiments, we will compare the performance of', 'svd and flexible - walk smoothing at different corpus sizes and for a variety of tasks. implementation : we extract tuples from the 2 - billion word ukwac corpus, 1 dependency - parsed', 'with minipar. 2 following  #TAUTHOR_TAG, we only consider co - occurrences where two target words are connected by certain dependency paths, namely : the top 30 most frequent preposition -', '']",5
"['subordinate pairs ( supersub ) and phrasal associates ( phrasacc ).', 'following previous simulations of this data - set  #TAUTHOR_TAG, we measure the']","['requires capturing different forms of semantic relatedness between prime - target pairs : synonyms ( synonym ), coordinates ( coord ), antonyms ( antonym ), free association pairs ( conass ), superand subordinate pairs ( supersub ) and phrasal associates ( phrasacc ).', 'following previous simulations of this data - set  #TAUTHOR_TAG, we measure the']","[', coordinates ( coord ), antonyms ( antonym ), free association pairs ( conass ), superand subordinate pairs ( supersub ) and phrasal associates ( phrasacc ).', 'following previous simulations of this data - set  #TAUTHOR_TAG, we measure the similarity of each related target - prime pair, and we compare']","['', 'the full graph are in table 1, line 1.', 'the svd model clearly outperforms the pure - vector based approach and the graph - based approaches.', 'its performance is above that of previous models trained on the same corpus  #AUTHOR_TAG.', 'the best model that we report is based on web search engine results  #AUTHOR_TAG.', 'among the graph - based random walk models, flexible walk with parameter 0. 5 and fixed 1 - step walk with indirect relatedness measures using dot product similarity achieve the highest performance.', 'concept categorization :  #AUTHOR_TAG proposed a set of 402 nouns to be categorized into 21 classes of both concrete ( animals, fruit... ) and abstract ( feelings, times... ) concepts.', 'our results on this clustering task are given in table 1 ( line 2 ).', 'the difference between svd and pure - vector models is negligible and they both obtain the best performance in terms of both cluster entropy ( not shown in the table ) and purity.', ""both models'performances are comparable with the previously reported studies, and above that of random walks."", 'semantic priming : the next dataset comes from  #AUTHOR_TAG and it is of interest since it requires capturing different forms of semantic relatedness between prime - target pairs : synonyms ( synonym ), coordinates ( coord ), antonyms ( antonym ), free association pairs ( conass ), superand subordinate pairs ( supersub ) and phrasal associates ( phrasacc ).', 'following previous simulations of this data - set  #TAUTHOR_TAG, we measure the similarity of each related target - prime pair, and we compare it to the average similarity of the target to all the other primes instantiating the same relation, treating the latter quantity as our surrogate of an unrelated target - prime pair.', 'we report results in terms of differences between unrelated and related pairs, normalized to t - scores, marking significance according to twotailed paired t - tests for the relevant degrees of freedom.', 'even though the svd']",5
,,,,3
,,,,3
"['the usmt method of  #TAUTHOR_TAG and  #AUTHOR_TAG to gec.', 'the unmt method  #AUTHOR_TAG was ineffective under the gec setting in our preliminary experiments.', 'ge']","['the usmt method of  #TAUTHOR_TAG and  #AUTHOR_TAG to gec.', 'the unmt method  #AUTHOR_TAG was ineffective under the gec setting in our preliminary experiments.', 'gec with nmt / smt several studies that introduce sequence - to - sequence models in gec heavily rely on large']","[' #AUTHOR_TAG b ).', 'in this study, we apply the usmt method of  #TAUTHOR_TAG and  #AUTHOR_TAG to gec.', 'the unmt method  #AUTHOR_TAG was ineffective under the gec setting in our preliminary experiments.', 'ge']","['', ""although we observed some usage error examples of'watch'in the synthetic source data, our model was not able to replace'watch'to'see'based on the context."", 'both nmt  #AUTHOR_TAG and smt  #AUTHOR_TAG b ).', 'in this study, we apply the usmt method of  #TAUTHOR_TAG and  #AUTHOR_TAG to gec.', 'the unmt method  #AUTHOR_TAG was ineffective under the gec setting in our preliminary experiments.', 'gec with nmt / smt several studies that introduce sequence - to - sequence models in gec heavily rely on large amounts of training data.', ' #AUTHOR_TAG, who presented state - of - the - art results in gec, proposed a supervised nmt method trained on corpora of a total 5. 4 m sentence pairs.', 'on the other hand, we mainly use the monolingual corpus and use small learner data as the tuning data.', 'despite the success of nmt, many studies on gec traditionally use smt junczys -  #AUTHOR_TAG.', 'these studies apply an offthe - shelf smt toolkit, moses, to ge']",3
,,,,5
"['the usmt method of  #TAUTHOR_TAG and  #AUTHOR_TAG to gec.', 'the unmt method  #AUTHOR_TAG was ineffective under the gec setting in our preliminary experiments.', 'ge']","['the usmt method of  #TAUTHOR_TAG and  #AUTHOR_TAG to gec.', 'the unmt method  #AUTHOR_TAG was ineffective under the gec setting in our preliminary experiments.', 'gec with nmt / smt several studies that introduce sequence - to - sequence models in gec heavily rely on large']","[' #AUTHOR_TAG b ).', 'in this study, we apply the usmt method of  #TAUTHOR_TAG and  #AUTHOR_TAG to gec.', 'the unmt method  #AUTHOR_TAG was ineffective under the gec setting in our preliminary experiments.', 'ge']","['', ""although we observed some usage error examples of'watch'in the synthetic source data, our model was not able to replace'watch'to'see'based on the context."", 'both nmt  #AUTHOR_TAG and smt  #AUTHOR_TAG b ).', 'in this study, we apply the usmt method of  #TAUTHOR_TAG and  #AUTHOR_TAG to gec.', 'the unmt method  #AUTHOR_TAG was ineffective under the gec setting in our preliminary experiments.', 'gec with nmt / smt several studies that introduce sequence - to - sequence models in gec heavily rely on large amounts of training data.', ' #AUTHOR_TAG, who presented state - of - the - art results in gec, proposed a supervised nmt method trained on corpora of a total 5. 4 m sentence pairs.', 'on the other hand, we mainly use the monolingual corpus and use small learner data as the tuning data.', 'despite the success of nmt, many studies on gec traditionally use smt junczys -  #AUTHOR_TAG.', 'these studies apply an offthe - shelf smt toolkit, moses, to ge']",5
,,,,4
,,,,4
,,,,6
,,,,6
,,,,0
,,,,0
"['2 decreases by 1, 303.', ' #TAUTHOR_TAG and  #AUTHOR_TAG reported that the bleu score  #AUTHOR_TAG of unsupervised mt with backward - refinement']","['2 decreases by 1, 303.', ' #TAUTHOR_TAG and  #AUTHOR_TAG reported that the bleu score  #AUTHOR_TAG of unsupervised mt with backward - refinement']","['2 decreases by 1, 303.', ' #TAUTHOR_TAG and  #AUTHOR_TAG reported that the bleu score  #AUTHOR_TAG of unsupervised mt with backward - refinement improves with increasing iterations.', 'in gec, increasing the iterations of usmt backward']","['', 'as for usmt backward, the number of corrections from iter 1 to iter 2 decreases by 1, 303.', ' #TAUTHOR_TAG and  #AUTHOR_TAG reported that the bleu score  #AUTHOR_TAG of unsupervised mt with backward - refinement improves with increasing iterations.', 'in gec, increasing the iterations of usmt backward improves the gec accuracy by predicting less corrections.', 'the gleu score for usmt forward is considered higher than that for usmt backward because the language model makes up for the synthetic target data.', 'to compare the fluency, the outputs of each best iter on jfleg were evaluated with the perplexity based on the common crawl language model 10.', 'the perplexity of usmt forward in iter 1 is 179. 23 and that of usmt backward in iter 1 is 187. 49 ; hence, the perplexity suggests usmt forward produces more likely outputs than usmt backward under the language model of com - 10 http : / / data. statmt. org / romang / gec - emnlp16 / cclm. tgz table 5 : gec results with dev data.', 'the bold scores represent the best score without the spell checker.', 'mon crawl text.', 'effect of the source language we also examine how source languages of machine translation affect performance.', 'table 3 shows the result in changing the source - side data on conll - 14.', 'the outputs using finnish data is the best score among various languages ; the more similar to english the source - side data is, the lower the f 0. 5 score of the output.', 'table 4 shows']",0
"['language modelling  #TAUTHOR_TAG,']","['language modelling  #TAUTHOR_TAG,']","['language modelling  #TAUTHOR_TAG,']","[', bidirectional long short - term memory networks ( bi - lstm )  #AUTHOR_TAG have been used for language modelling  #TAUTHOR_TAG, transition - based dependency parsing  #AUTHOR_TAG, fine - grained sentiment analysis  #AUTHOR_TAG, syntactic chunking  #AUTHOR_TAG, and semantic role labeling  #AUTHOR_TAG.', 'lstms are recurrent neural networks ( rnns ) in which layers are designed to prevent vanishing gradients.', 'bidirectional lstms make a backward and forward pass through the sequence before passing on to the next layer.', 'for further details, see  #AUTHOR_TAG.', 'we consider using bi - lstms for pos tagging.', 'previous work on using deep learning - based methods for pos tagging has focused either on a single language  #TAUTHOR_TAG.', 'instead we evaluate our models across 22 languages.', 'in addition, we compare performance with representations at different levels of granularity ( words, characters, and bytes ).', 'these levels of representation were previously introduced in different efforts ( chrupała, 2013 ;  #TAUTHOR_TAG, but a comparative evaluation was missing.', 'moreover, deep networks are often said to require large volumes of training data.', 'we investigate to what extent bi - lstms are more sensitive to the amount of training data and label noise than standard pos taggers.', 'finally, we introduce a novel model, a bi - lstm trained with auxiliary loss.', 'the model jointly predicts the pos and the log frequency of the next word.', 'the intuition behind this model is that the auxiliary loss, being predictive of word frequency, helps to differentiate the representations of rare and common words.', 'we indeed observe performance gains on rare and out - of - vocabulary words.', 'these performance gains transfer into general improvements for morphologically rich languages.', 'contributions in this paper, we a ) evaluate the effectiveness of different representations in bilstms, b ) compare these models across a large set of languages and under varying conditions ( data size, label noise ) and c ) propose a novel bi - lstm model with auxiliary loss ( logfreq )']",0
"['language modelling  #TAUTHOR_TAG,']","['language modelling  #TAUTHOR_TAG,']","['language modelling  #TAUTHOR_TAG,']","[', bidirectional long short - term memory networks ( bi - lstm )  #AUTHOR_TAG have been used for language modelling  #TAUTHOR_TAG, transition - based dependency parsing  #AUTHOR_TAG, fine - grained sentiment analysis  #AUTHOR_TAG, syntactic chunking  #AUTHOR_TAG, and semantic role labeling  #AUTHOR_TAG.', 'lstms are recurrent neural networks ( rnns ) in which layers are designed to prevent vanishing gradients.', 'bidirectional lstms make a backward and forward pass through the sequence before passing on to the next layer.', 'for further details, see  #AUTHOR_TAG.', 'we consider using bi - lstms for pos tagging.', 'previous work on using deep learning - based methods for pos tagging has focused either on a single language  #TAUTHOR_TAG.', 'instead we evaluate our models across 22 languages.', 'in addition, we compare performance with representations at different levels of granularity ( words, characters, and bytes ).', 'these levels of representation were previously introduced in different efforts ( chrupała, 2013 ;  #TAUTHOR_TAG, but a comparative evaluation was missing.', 'moreover, deep networks are often said to require large volumes of training data.', 'we investigate to what extent bi - lstms are more sensitive to the amount of training data and label noise than standard pos taggers.', 'finally, we introduce a novel model, a bi - lstm trained with auxiliary loss.', 'the model jointly predicts the pos and the log frequency of the next word.', 'the intuition behind this model is that the auxiliary loss, being predictive of word frequency, helps to differentiate the representations of rare and common words.', 'we indeed observe performance gains on rare and out - of - vocabulary words.', 'these performance gains transfer into general improvements for morphologically rich languages.', 'contributions in this paper, we a ) evaluate the effectiveness of different representations in bilstms, b ) compare these models across a large set of languages and under varying conditions ( data size, label noise ) and c ) propose a novel bi - lstm model with auxiliary loss ( logfreq )']",0
"['language modelling  #TAUTHOR_TAG,']","['language modelling  #TAUTHOR_TAG,']","['language modelling  #TAUTHOR_TAG,']","[', bidirectional long short - term memory networks ( bi - lstm )  #AUTHOR_TAG have been used for language modelling  #TAUTHOR_TAG, transition - based dependency parsing  #AUTHOR_TAG, fine - grained sentiment analysis  #AUTHOR_TAG, syntactic chunking  #AUTHOR_TAG, and semantic role labeling  #AUTHOR_TAG.', 'lstms are recurrent neural networks ( rnns ) in which layers are designed to prevent vanishing gradients.', 'bidirectional lstms make a backward and forward pass through the sequence before passing on to the next layer.', 'for further details, see  #AUTHOR_TAG.', 'we consider using bi - lstms for pos tagging.', 'previous work on using deep learning - based methods for pos tagging has focused either on a single language  #TAUTHOR_TAG.', 'instead we evaluate our models across 22 languages.', 'in addition, we compare performance with representations at different levels of granularity ( words, characters, and bytes ).', 'these levels of representation were previously introduced in different efforts ( chrupała, 2013 ;  #TAUTHOR_TAG, but a comparative evaluation was missing.', 'moreover, deep networks are often said to require large volumes of training data.', 'we investigate to what extent bi - lstms are more sensitive to the amount of training data and label noise than standard pos taggers.', 'finally, we introduce a novel model, a bi - lstm trained with auxiliary loss.', 'the model jointly predicts the pos and the log frequency of the next word.', 'the intuition behind this model is that the auxiliary loss, being predictive of word frequency, helps to differentiate the representations of rare and common words.', 'we indeed observe performance gains on rare and out - of - vocabulary words.', 'these performance gains transfer into general improvements for morphologically rich languages.', 'contributions in this paper, we a ) evaluate the effectiveness of different representations in bilstms, b ) compare these models across a large set of languages and under varying conditions ( data size, label noise ) and c ) propose a novel bi - lstm model with auxiliary loss ( logfreq )']",0
"['language modelling  #TAUTHOR_TAG,']","['language modelling  #TAUTHOR_TAG,']","['language modelling  #TAUTHOR_TAG,']","[', bidirectional long short - term memory networks ( bi - lstm )  #AUTHOR_TAG have been used for language modelling  #TAUTHOR_TAG, transition - based dependency parsing  #AUTHOR_TAG, fine - grained sentiment analysis  #AUTHOR_TAG, syntactic chunking  #AUTHOR_TAG, and semantic role labeling  #AUTHOR_TAG.', 'lstms are recurrent neural networks ( rnns ) in which layers are designed to prevent vanishing gradients.', 'bidirectional lstms make a backward and forward pass through the sequence before passing on to the next layer.', 'for further details, see  #AUTHOR_TAG.', 'we consider using bi - lstms for pos tagging.', 'previous work on using deep learning - based methods for pos tagging has focused either on a single language  #TAUTHOR_TAG.', 'instead we evaluate our models across 22 languages.', 'in addition, we compare performance with representations at different levels of granularity ( words, characters, and bytes ).', 'these levels of representation were previously introduced in different efforts ( chrupała, 2013 ;  #TAUTHOR_TAG, but a comparative evaluation was missing.', 'moreover, deep networks are often said to require large volumes of training data.', 'we investigate to what extent bi - lstms are more sensitive to the amount of training data and label noise than standard pos taggers.', 'finally, we introduce a novel model, a bi - lstm trained with auxiliary loss.', 'the model jointly predicts the pos and the log frequency of the next word.', 'the intuition behind this model is that the auxiliary loss, being predictive of word frequency, helps to differentiate the representations of rare and common words.', 'we indeed observe performance gains on rare and out - of - vocabulary words.', 'these performance gains transfer into general improvements for morphologically rich languages.', 'contributions in this paper, we a ) evaluate the effectiveness of different representations in bilstms, b ) compare these models across a large set of languages and under varying conditions ( data size, label noise ) and c ) propose a novel bi - lstm model with auxiliary loss ( logfreq )']",1
"['we incorporate subtoken information using an hierarchical bi - lstm architecture  #TAUTHOR_TAG.', 'we']","['we incorporate subtoken information using an hierarchical bi - lstm architecture  #TAUTHOR_TAG.', 'we']","['we incorporate subtoken information using an hierarchical bi - lstm architecture  #TAUTHOR_TAG.', 'we compute subtokenlevel ( either characters c or unicode byte b ) embeddings of words using a sequence bi - lstm at']","['', 'thus, the state vector v i in this bi - rnn encodes information at position i and its entire sequential context.', 'another view of the context bi - rnn is of taking a sequence x 1 : n and returning the corresponding sequence of state vectors v 1 : n.', 'lstms  #AUTHOR_TAG are a variant of rnns that replace the cells of rnns with lstm cells that were designed to prevent vanishing gradients.', 'bidirectional lstms are the bi - rnn counterpart based on lstms.', 'our basic bi - lstm tagging model is a context bi - lstm taking as input word embeddings w. we incorporate subtoken information using an hierarchical bi - lstm architecture  #TAUTHOR_TAG.', 'we compute subtokenlevel ( either characters c or unicode byte b ) embeddings of words using a sequence bi - lstm at the lower level.', 'this representation is then concatenated with the ( learned ) word embeddings vector w which forms the input to the context bi - lstm at the next layer.', 'this model, illustrated in figure 1 ( lower part in']",5
['of  #TAUTHOR_TAG that'],['of  #TAUTHOR_TAG that'],"['in predicting pos for oov tokens ( cf.', 'table 2 oov acc columns ), especially for languages like arabic, farsi, hebrew, finnish.', 'we examined simple rnns and confirm the finding of  #TAUTHOR_TAG that they performed worse than']","['results are given in table 2.', 'first of all, notice that tnt performs remarkably well across the 22 languages, closely followed by crf.', 'the bi - lstm tagger ( w ) without lower - level bi - lstm for subtokens falls short, outperforms the traditional taggers only on 3 languages.', 'the bi - lstm 2 https : / / sites. google. com / site / rmyeid / projects / polyglot 3 they found treetagger was closely followed by hunpos, a re - implementation of tnt, and stanford and clearnlp were lower ranked.', 'in an initial investigation, we compared tnt, hunpos and treetagger and found tnt to be consistently better than treetagger, hunpos followed closely but crashed on some languages ( e. g., arabic ).', 'model clearly benefits from character representations.', 'the model using characters alone ( c ) works remarkably well, it improves over tnt on 9 languages ( incl.', 'slavic and nordic languages ).', 'the combined word + character representation model is the best representation, outperforming the baseline on all except one language ( indonesian ), providing strong results already without pre - trained embeddings.', 'this model ( w + c ) reaches the biggest improvement ( more than + 2 % accuracy ) on hebrew and slovene.', 'initializing the word embeddings ( + polyglot ) with off - the - shelf languagespecific embeddings further improves accuracy.', 'the only system we are aware of that evaluates on ud is  #AUTHOR_TAG ( last column ).', 'however, note that these results are not strictly comparable as they use the earlier ud v1. 1 version.', 'the overall best system is the multi - task bi - lstm freqbin ( it uses w + c and polyglot initialization for w ).', 'while on macro average it is on par with bi - lstm w + c, it obtains the best results on 12 / 22 languages, and it is successful in predicting pos for oov tokens ( cf.', 'table 2 oov acc columns ), especially for languages like arabic, farsi, hebrew, finnish.', 'we examined simple rnns and confirm the finding of  #TAUTHOR_TAG that they performed worse than their lstm counterparts.', 'finally, the bi - lstm tagger is competitive on wsj, cf.', 'table 3.', 'rare words in order to evaluate the effect of modeling sub - token information, we examine accuracy rates at different frequency rates.', 'figure 2 shows absolute improvements in accuracy of bi - lstm w + c']",3
"['pos tagging are also reported in  #TAUTHOR_TAG, however,']","['pos tagging are also reported in  #TAUTHOR_TAG, however,']","['pos tagging are also reported in  #TAUTHOR_TAG, however,']","['embeddings were first introduced by  #AUTHOR_TAG for language modeling.', 'early applications include text classification ( chrupała, 2013 ;  #AUTHOR_TAG.', 'recently, these representations were successfully applied to a range of structured prediction tasks.', 'for pos tagging,  #AUTHOR_TAG were the first to propose character - based models.', 'they use a convolutional neural network ( cnn ; or convnet ) and evaluated their model on english ( ptb ) and portuguese, showing that the model achieves state - of - the - art performance close to taggers using carefully designed feature templates.', ' #AUTHOR_TAG extend this line and compare a novel bi - lstm model, learning word representations through character embeddings.', 'they evaluate their model on a language modeling and pos tagging setup, and show that bi - lstms outperform the cnn approach of  #AUTHOR_TAG.', ' #AUTHOR_TAG evaluate character embeddings for german.', 'bi - lstms for pos tagging are also reported in  #TAUTHOR_TAG, however, they only explore word embeddings, orthographic information and evaluate on wsj only.', ""a related study is  #AUTHOR_TAG who propose a multi - task rnn for named entity recognition by jointly predicting the next token and current token's name label."", 'our model is simpler, it uses a very coarse set of labels rather then integrating an entire language modeling task which is computationally more expensive.', 'an interesting recent study is  #AUTHOR_TAG, they build a single byte - to - span model for multiple languages based on a sequence - to - sequence rnn  #AUTHOR_TAG achieving impressive results.', 'we would like to extend this work in their direction']",4
"['estimates comparable features for the translation pairs in the translation model  #TAUTHOR_TAG.', 'ble also']","['estimates comparable features for the translation pairs in the translation model  #TAUTHOR_TAG.', 'ble also']","['estimates comparable features for the translation pairs in the translation model  #TAUTHOR_TAG.', 'ble also']","['the pioneering work of  #AUTHOR_TAG, ble from comparable corpora has been studied for a long time.', 'ble is based on the distributional hypothesis  #AUTHOR_TAG, stating that words with similar meaning have similar distributions across languages.', 'contextual similarity  #AUTHOR_TAG, topical similarity ( vulic et al., 2011 ) and temporal similarity  #AUTHOR_TAG can be important clues for ble.', 'orthographic similarity may also be used for ble for some similar language pairs  #AUTHOR_TAG.', 'moreover, some studies try to use the combinations of different similarities for ble ( irvine and callison -  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'to address the data sparseness problem of ble, smoothing technology has been proposed  #AUTHOR_TAG.', 'ble can be used to address the accuracy problem of smt, which estimates comparable features for the translation pairs in the translation model  #TAUTHOR_TAG.', 'ble also can be used to address the coverage problem of smt, which mines translations for the unknown words or phrases in the translation model from comparable corpora ( daume iii and  #AUTHOR_TAG.', 'moreover, studies have been conducted to address the accuracy and coverage problems of smt simultaneously with ble ( irvine and callison -  #AUTHOR_TAG a ).', 'our study focuses on addressing the accuracy problem of smt with ble.', 'we use paraphrases to address the data sparseness problem of ble for comparable feature estimation, which makes the comparable features more accurate']",0
"['dates  #TAUTHOR_TAG.', 'we estimate temporal feature following  #TAUTHOR_TAG.', 'for']","['dates  #TAUTHOR_TAG.', 'we estimate temporal feature following  #TAUTHOR_TAG.', 'for']","['dates  #TAUTHOR_TAG.', 'we estimate temporal feature following  #TAUTHOR_TAG.', 'for a phrase pair, we build source and target temporal occurrence vectors by counting']","['feature is the temporal similarity of a phrase pair.', 'the intuition of temporal similarity is that news stories across languages tend to discuss the same world events on the same day, and the occurrences of a translated phrase pair over time tend to spike on the same dates  #TAUTHOR_TAG.', 'we estimate temporal feature following  #TAUTHOR_TAG.', 'for a phrase pair, we build source and target temporal occurrence vectors by counting their occurrences in equally sized temporal bins, which are sorted from the set of time - stamped documents in the comparable corpus.', 'we set the window size of a bin to 1 day.', 'therefore the number of dimensions of the constructed vector is equal to the number of days spanned by the corpus, and each dimension is the number of times that the phrase appears in the corresponding bin.', 'the similarity of the phrase pair is computed as the similarity of the source and target vectors using cosine similarity ( equation 3 )']",0
"['estimates comparable features for the translation pairs in the translation model  #TAUTHOR_TAG.', 'ble also']","['estimates comparable features for the translation pairs in the translation model  #TAUTHOR_TAG.', 'ble also']","['estimates comparable features for the translation pairs in the translation model  #TAUTHOR_TAG.', 'ble also']","['the pioneering work of  #AUTHOR_TAG, ble from comparable corpora has been studied for a long time.', 'ble is based on the distributional hypothesis  #AUTHOR_TAG, stating that words with similar meaning have similar distributions across languages.', 'contextual similarity  #AUTHOR_TAG, topical similarity ( vulic et al., 2011 ) and temporal similarity  #AUTHOR_TAG can be important clues for ble.', 'orthographic similarity may also be used for ble for some similar language pairs  #AUTHOR_TAG.', 'moreover, some studies try to use the combinations of different similarities for ble ( irvine and callison -  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'to address the data sparseness problem of ble, smoothing technology has been proposed  #AUTHOR_TAG.', 'ble can be used to address the accuracy problem of smt, which estimates comparable features for the translation pairs in the translation model  #TAUTHOR_TAG.', 'ble also can be used to address the coverage problem of smt, which mines translations for the unknown words or phrases in the translation model from comparable corpora ( daume iii and  #AUTHOR_TAG.', 'moreover, studies have been conducted to address the accuracy and coverage problems of smt simultaneously with ble ( irvine and callison -  #AUTHOR_TAG a ).', 'our study focuses on addressing the accuracy problem of smt with ble.', 'we use paraphrases to address the data sparseness problem of ble for comparable feature estimation, which makes the comparable features more accurate']",1
"['estimate topical similarity.', 'however, this method is not scalable for large data sets.', 'in this paper, we estimate topical feature in a scalable way following  #TAUTHOR_TAG.', 'we']","['estimate topical similarity.', 'however, this method is not scalable for large data sets.', 'in this paper, we estimate topical feature in a scalable way following  #TAUTHOR_TAG.', 'we']","['estimate topical similarity.', 'however, this method is not scalable for large data sets.', 'in this paper, we estimate topical feature in a scalable way following  #TAUTHOR_TAG.', 'we treat an article pair aligned by interlanguage links in wikipedia as a topic aligned pair.', 'for a phrase pair, we build source and target topical occurrence vectors by counting']","['feature is the topical similarity of a phrase pair.', 'topical similarity uses the distributional hy - pothesis on topics, stating that two phrases are potential translation candidates if they are often present in the same cross - lingual topics and not observed in other cross - lingual topics ( vulic et al., 2011 ).', 'vulic et al. ( 2011 ) propose using bilingual topic model based method to estimate topical similarity.', 'however, this method is not scalable for large data sets.', 'in this paper, we estimate topical feature in a scalable way following  #TAUTHOR_TAG.', 'we treat an article pair aligned by interlanguage links in wikipedia as a topic aligned pair.', 'for a phrase pair, we build source and target topical occurrence vectors by counting their occurrences in its corresponding language articles.', 'the number of dimensions of the constructed vector is equal to the number of aligned article pairs, and each dimension is the number of times that the phrase appears in the corresponding article.', 'the similarity of the phrase pair is computed as the similarity of the source and target vectors using cosine similarity ( equation 3 )']",5
"['dates  #TAUTHOR_TAG.', 'we estimate temporal feature following  #TAUTHOR_TAG.', 'for']","['dates  #TAUTHOR_TAG.', 'we estimate temporal feature following  #TAUTHOR_TAG.', 'for']","['dates  #TAUTHOR_TAG.', 'we estimate temporal feature following  #TAUTHOR_TAG.', 'for a phrase pair, we build source and target temporal occurrence vectors by counting']","['feature is the temporal similarity of a phrase pair.', 'the intuition of temporal similarity is that news stories across languages tend to discuss the same world events on the same day, and the occurrences of a translated phrase pair over time tend to spike on the same dates  #TAUTHOR_TAG.', 'we estimate temporal feature following  #TAUTHOR_TAG.', 'for a phrase pair, we build source and target temporal occurrence vectors by counting their occurrences in equally sized temporal bins, which are sorted from the set of time - stamped documents in the comparable corpus.', 'we set the window size of a bin to 1 day.', 'therefore the number of dimensions of the constructed vector is equal to the number of days spanned by the corpus, and each dimension is the number of times that the phrase appears in the corresponding bin.', 'the similarity of the phrase pair is computed as the similarity of the source and target vectors using cosine similarity ( equation 3 )']",5
"['proposed method with  #TAUTHOR_TAG.', '']","['proposed method with  #TAUTHOR_TAG.', '']","['proposed method with  #TAUTHOR_TAG.', 'we estimated comparable features from comparable corpora using the method of  #TAUTHOR_TAG and']","['our experiments, we compared our proposed method with  #TAUTHOR_TAG.', 'we estimated comparable features from comparable corpora using the method of  #TAUTHOR_TAG and our proposed method respectively.', 'we appended the comparable features to the phrase table, and evaluated the two methods in the perspective of smt performance.', 'we conducted experiments on chinese - english data.', 'in all our experiments, we preprocessed the data by segmenting chinese sentences using a segmenter proposed by  #AUTHOR_TAG, and tokenizing english sentences']",5
"['proposed method with  #TAUTHOR_TAG.', '']","['proposed method with  #TAUTHOR_TAG.', '']","['proposed method with  #TAUTHOR_TAG.', 'we estimated comparable features from comparable corpora using the method of  #TAUTHOR_TAG and']","['our experiments, we compared our proposed method with  #TAUTHOR_TAG.', 'we estimated comparable features from comparable corpora using the method of  #TAUTHOR_TAG and our proposed method respectively.', 'we appended the comparable features to the phrase table, and evaluated the two methods in the perspective of smt performance.', 'we conducted experiments on chinese - english data.', 'in all our experiments, we preprocessed the data by segmenting chinese sentences using a segmenter proposed by  #AUTHOR_TAG, and tokenizing english sentences']",5
"['phrase length limit ( 7→3 ) following  #TAUTHOR_TAG.', 'we trained a 5 - gram language model on']","['phrase length limit ( 7→3 ) following  #TAUTHOR_TAG.', 'we trained a 5 - gram language model on']","['the phrase length limit ( 7→3 ) following  #TAUTHOR_TAG.', 'we trained a 5 - gram language model on the english side of the parallel corpus using the srilm toolkit 7 with interpolated kneser - ney discounting, and used it']","['conducted chinese - to - english translation experiments.', 'the parallel corpus we used is from chinese - english nist open mt.', '6 the "" nist "" column of table 4 shows the statistics of this parallel corpus.', 'for decoding, we used the state - of - theart pbsmt toolkit moses  #AUTHOR_TAG with default options, except for the phrase length limit ( 7→3 ) following  #TAUTHOR_TAG.', 'we trained a 5 - gram language model on the english side of the parallel corpus using the srilm toolkit 7 with interpolated kneser - ney discounting, and used it for all the experiments.', 'we used nist open mt 2002 and 2003 data sets for tuning and testing, containing 878 and 919 sentence pairs respectively.', 'note that both mt 2002 and 2003 data sets contain 4 references for each chinese sentence.', 'tuning was performed by minimum error rate training ( mert )  #AUTHOR_TAG, and it was re - run for every experiment.', 'table 4 : statistics of the comparable data used for comparable feature estimation']",5
"['sides of the parallel corpus as independent monolingual corpora, following  #TAUTHOR_TAG']","['sides of the parallel corpus as independent monolingual corpora, following  #TAUTHOR_TAG']","['feature was estimated on the parallel corpus.', 'we treated the two sides of the parallel corpus as independent monolingual corpora, following  #TAUTHOR_TAG we used an open - source python script 13 to extract and clean the text from the dumps.', 'we aligned the articles on the same topic in chinese - english wikipedia via the interlanguage links.', 'we estimated comparable features']","['feature was estimated on the parallel corpus.', 'we treated the two sides of the parallel corpus as independent monolingual corpora, following  #TAUTHOR_TAG we used an open - source python script 13 to extract and clean the text from the dumps.', 'we aligned the articles on the same topic in chinese - english wikipedia via the interlanguage links.', 'we estimated comparable features for the unique phrase pairs used for tuning and testing.', 'these phrase pairs were extracted from the entire phrase table constructed from the parallel corpus, by checking all the source phrases in the tuning and testing data sets.', '']",5
['following  #TAUTHOR_TAG to'],['following  #TAUTHOR_TAG to'],['appends the comparable features estimated following  #TAUTHOR_TAG to'],"['report results on the test set using caseinsensitive bleu - 4 score and four references.', 'table 7 shows the results of chinese - to - english translation experiments.', '"" baseline "" denotes the baseline system that does not use comparable features.', '"" klementiev + "" denotes the system that appends the comparable features estimated following  #TAUTHOR_TAG to the phrase table. "" proposed "" denotes the system that uses the comparable features estimated by our proposed method.', '"" + contex - tual "", "" + topical "" and "" + temporal "" denote the systems that append contextual, topical and temporal features respectively. "" + all "" denotes the system that appends all the three types of features.', 'the significance test was performed using the bootstrap resampling method proposed by  #AUTHOR_TAG.', '']",5
['following  #TAUTHOR_TAG to'],['following  #TAUTHOR_TAG to'],['appends the comparable features estimated following  #TAUTHOR_TAG to'],"['report results on the test set using caseinsensitive bleu - 4 score and four references.', 'table 7 shows the results of chinese - to - english translation experiments.', '"" baseline "" denotes the baseline system that does not use comparable features.', '"" klementiev + "" denotes the system that appends the comparable features estimated following  #TAUTHOR_TAG to the phrase table. "" proposed "" denotes the system that uses the comparable features estimated by our proposed method.', '"" + contex - tual "", "" + topical "" and "" + temporal "" denote the systems that append contextual, topical and temporal features respectively. "" + all "" denotes the system that appends all the three types of features.', 'the significance test was performed using the bootstrap resampling method proposed by  #AUTHOR_TAG.', '']",5
['following  #TAUTHOR_TAG to'],['following  #TAUTHOR_TAG to'],['appends the comparable features estimated following  #TAUTHOR_TAG to'],"['report results on the test set using caseinsensitive bleu - 4 score and four references.', 'table 7 shows the results of chinese - to - english translation experiments.', '"" baseline "" denotes the baseline system that does not use comparable features.', '"" klementiev + "" denotes the system that appends the comparable features estimated following  #TAUTHOR_TAG to the phrase table. "" proposed "" denotes the system that uses the comparable features estimated by our proposed method.', '"" + contex - tual "", "" + topical "" and "" + temporal "" denote the systems that append contextual, topical and temporal features respectively. "" + all "" denotes the system that appends all the three types of features.', 'the significance test was performed using the bootstrap resampling method proposed by  #AUTHOR_TAG.', '']",5
['following  #TAUTHOR_TAG to'],['following  #TAUTHOR_TAG to'],['appends the comparable features estimated following  #TAUTHOR_TAG to'],"['report results on the test set using caseinsensitive bleu - 4 score and four references.', 'table 7 shows the results of chinese - to - english translation experiments.', '"" baseline "" denotes the baseline system that does not use comparable features.', '"" klementiev + "" denotes the system that appends the comparable features estimated following  #TAUTHOR_TAG to the phrase table. "" proposed "" denotes the system that uses the comparable features estimated by our proposed method.', '"" + contex - tual "", "" + topical "" and "" + temporal "" denote the systems that append contextual, topical and temporal features respectively. "" + all "" denotes the system that appends all the three types of features.', 'the significance test was performed using the bootstrap resampling method proposed by  #AUTHOR_TAG.', '']",5
['following  #TAUTHOR_TAG to'],['following  #TAUTHOR_TAG to'],['appends the comparable features estimated following  #TAUTHOR_TAG to'],"['report results on the test set using caseinsensitive bleu - 4 score and four references.', 'table 7 shows the results of chinese - to - english translation experiments.', '"" baseline "" denotes the baseline system that does not use comparable features.', '"" klementiev + "" denotes the system that appends the comparable features estimated following  #TAUTHOR_TAG to the phrase table. "" proposed "" denotes the system that uses the comparable features estimated by our proposed method.', '"" + contex - tual "", "" + topical "" and "" + temporal "" denote the systems that append contextual, topical and temporal features respectively. "" + all "" denotes the system that appends all the three types of features.', 'the significance test was performed using the bootstrap resampling method proposed by  #AUTHOR_TAG.', '']",4
['following  #TAUTHOR_TAG to'],['following  #TAUTHOR_TAG to'],['appends the comparable features estimated following  #TAUTHOR_TAG to'],"['report results on the test set using caseinsensitive bleu - 4 score and four references.', 'table 7 shows the results of chinese - to - english translation experiments.', '"" baseline "" denotes the baseline system that does not use comparable features.', '"" klementiev + "" denotes the system that appends the comparable features estimated following  #TAUTHOR_TAG to the phrase table. "" proposed "" denotes the system that uses the comparable features estimated by our proposed method.', '"" + contex - tual "", "" + topical "" and "" + temporal "" denote the systems that append contextual, topical and temporal features respectively. "" + all "" denotes the system that appends all the three types of features.', 'the significance test was performed using the bootstrap resampling method proposed by  #AUTHOR_TAG.', '']",4
['following  #TAUTHOR_TAG to'],['following  #TAUTHOR_TAG to'],['appends the comparable features estimated following  #TAUTHOR_TAG to'],"['report results on the test set using caseinsensitive bleu - 4 score and four references.', 'table 7 shows the results of chinese - to - english translation experiments.', '"" baseline "" denotes the baseline system that does not use comparable features.', '"" klementiev + "" denotes the system that appends the comparable features estimated following  #TAUTHOR_TAG to the phrase table. "" proposed "" denotes the system that uses the comparable features estimated by our proposed method.', '"" + contex - tual "", "" + topical "" and "" + temporal "" denote the systems that append contextual, topical and temporal features respectively. "" + all "" denotes the system that appends all the three types of features.', 'the significance test was performed using the bootstrap resampling method proposed by  #AUTHOR_TAG.', '']",4
"[' #TAUTHOR_TAG.', 'when strong single - sentence cap']","[' #TAUTHOR_TAG.', 'when strong single - sentence']","[' #TAUTHOR_TAG.', 'when strong single - sentence captioning models are trained on this dataset,']","['captioning aims to describe the objects, actions, and details present in an image using natural language.', 'most image captioning research has focused on single - sentence captions, but the descriptive capacity of this form is limited ; a single sentence can only describe in detail a small aspect of an image.', 'recent work has argued instead for image paragraph captioning with the aim of generating a ( usually 5 - 8 sentence ) paragraph describing an image.', 'compared with single - sentence captioning, paragraph captioning is a relatively new task.', 'the main paragraph captioning dataset is the visual genome corpus, introduced by  #TAUTHOR_TAG.', 'when strong single - sentence captioning models are trained on this dataset, they produce repetitive paragraphs that are unable to describe diverse aspects of images.', 'the generated paragraphs repeat a slight variant of the same sentence multiple times, even when beam search is used.', 'prior work, discussed in the following section, tried to address this repetition with architectural changes, such as hierarchical lstms, which separate the generation of sentence topics and words.', 'in this work, we consider an approach for training paragraph captioning models that focuses on increasing the diversity of the output paragraph.', 'in particular, we note that self - critical sequence training ( scst )  #AUTHOR_TAG, a technique which uses policy gradient methods to directly optimize a target metric, has been successfully employed in standard captioning, but not in paragraph captioning.', 'we observe that during scst training the intermediate results of the system lack diversity, which makes it difficult for the model to improve.', 'we address this issue with a simple repetition penalty which downweights trigram overlap.', 'experiments show that this technique greatly improves the baseline model.', 'a simple baseline, non - hierarchical model trained with repetitionpenalized scst outperforms complex hierarchical models trained with both cross - entropy and customized adversarial losses.', 'we demonstrate that this strong performance gain comes from the combination of repetition - penalized search and scst, rather than from either individually, and discuss how this impacts the output paragraphs']",0
"['matches.', 'we discuss these metrics in greater detail when analyzing our experiments.', ' #TAUTHOR_TAG introduced']","['weighting n - grams by tf - idf ( termfrequency inverse - document - frequency ), and me - teor uses unigram overlap, incorporating synonym and paraphrase matches.', 'we discuss these metrics in greater detail when analyzing our experiments.', ' #TAUTHOR_TAG introduced']","['overlap, incorporating synonym and paraphrase matches.', 'we discuss these metrics in greater detail when analyzing our experiments.', ' #TAUTHOR_TAG introduced the first large - scale paragraph captioning dataset, a subset']","['all modern image captioning models employ variants of an encoder - decoder architecture.', 'as introduced by  #AUTHOR_TAG, the encoder is a cnn pre - trained for classification and the decoder is a lstm or gru.', 'following work in machine translation,  #AUTHOR_TAG added an attention mechanism over the encoder features.', ' #AUTHOR_TAG further improved single - sentence captioning performance by incorporating object detection in the encoder ( bottomup attention ) and adding an lstm layer before attending to spatial features in the decoder ( top - down attention ).', 'single - sentence and paragraph captioning models are evaluated with a number of metrics, including some designed specifically for captioning ( cider ) and some adopted from machine translation ( bleu, meteor ).', 'cider and bleu measure accuracy with n - gram overlaps, with cider weighting n - grams by tf - idf ( termfrequency inverse - document - frequency ), and me - teor uses unigram overlap, incorporating synonym and paraphrase matches.', 'we discuss these metrics in greater detail when analyzing our experiments.', ' #TAUTHOR_TAG introduced the first large - scale paragraph captioning dataset, a subset of the visual genome dataset, along with a number of models for paragraph captioning.', 'empirically, they showed that paragraphs contain significantly more pronouns, verbs, coreferences, and greater overall "" diversity "" than singlesentence captions.', 'whereas most single - sentence captions in the mscoco dataset describe only the most important object or action in an image, paragraph captions usually touch on multiple objects and actions']",0
['##ing models proposed by  #TAUTHOR_TAG included template - based ( nonneural ) approaches and'],['paragraph captioning models proposed by  #TAUTHOR_TAG included template - based ( nonneural ) approaches and'],['paragraph captioning models proposed by  #TAUTHOR_TAG included template - based ( nonneural ) approaches and'],"['paragraph captioning models proposed by  #TAUTHOR_TAG included template - based ( nonneural ) approaches and two encoder - decoder models.', 'in both neural models, the encoder is an object detector pre - trained for dense captioning.', '']",0
['##ing models proposed by  #TAUTHOR_TAG included template - based ( nonneural ) approaches and'],['paragraph captioning models proposed by  #TAUTHOR_TAG included template - based ( nonneural ) approaches and'],['paragraph captioning models proposed by  #TAUTHOR_TAG included template - based ( nonneural ) approaches and'],"['paragraph captioning models proposed by  #TAUTHOR_TAG included template - based ( nonneural ) approaches and two encoder - decoder models.', 'in both neural models, the encoder is an object detector pre - trained for dense captioning.', '']",0
['##ing models proposed by  #TAUTHOR_TAG included template - based ( nonneural ) approaches and'],['paragraph captioning models proposed by  #TAUTHOR_TAG included template - based ( nonneural ) approaches and'],['paragraph captioning models proposed by  #TAUTHOR_TAG included template - based ( nonneural ) approaches and'],"['paragraph captioning models proposed by  #TAUTHOR_TAG included template - based ( nonneural ) approaches and two encoder - decoder models.', 'in both neural models, the encoder is an object detector pre - trained for dense captioning.', '']",3
"['##ing, as in  #TAUTHOR_TAG and  #AUTHOR_TAG ).', 'meteor cid']","['( as opposed to dense captioning, as in  #TAUTHOR_TAG and  #AUTHOR_TAG ).', 'meteor cider bleu - 1 bleu - 2']","['##ing, as in  #TAUTHOR_TAG and  #AUTHOR_TAG ).', '']","['our paragraph captioning model we use the top - down model from  #AUTHOR_TAG.', 'our encoder is a convolutional network pretrained for object detection ( as opposed to dense captioning, as in  #TAUTHOR_TAG and  #AUTHOR_TAG ).', '']",4
"['##ing, as in  #TAUTHOR_TAG and  #AUTHOR_TAG ).', 'meteor cid']","['( as opposed to dense captioning, as in  #TAUTHOR_TAG and  #AUTHOR_TAG ).', 'meteor cider bleu - 1 bleu - 2']","['##ing, as in  #TAUTHOR_TAG and  #AUTHOR_TAG ).', '']","['our paragraph captioning model we use the top - down model from  #AUTHOR_TAG.', 'our encoder is a convolutional network pretrained for object detection ( as opposed to dense captioning, as in  #TAUTHOR_TAG and  #AUTHOR_TAG ).', '']",5
"['previously by  #TAUTHOR_TAG, offering a simpler setup and more relevant outputs.', 'we introduce the generation setting in section 2 and describe our generator architecture in']","['previously by  #TAUTHOR_TAG, offering a simpler setup and more relevant outputs.', 'we introduce the generation setting in section 2 and describe our generator architecture in']","['n - gram - based scores achieved previously by  #TAUTHOR_TAG, offering a simpler setup and more relevant outputs.', 'we introduce the generation setting in section 2 and describe our generator architecture in section 3.', 'section 4 details our experiments, section 5 analyzes the results.', '']","['', 'while some generators keep this division and use a two - step pipeline  #AUTHOR_TAG, others apply a joint model for both tasks  #AUTHOR_TAG.', 'we present a new, conceptually simple nlg system for sds that is able to operate in both modes : it either produces natural language strings or generates deep syntax dependency trees, which are subsequently processed by an external surface realizer.', 'this allows us to show a direct comparison of two - step generation, where sentence planning and surface realization are separated, with a joint, one - step approach.', 'our generator is based on the sequence - tosequence ( seq2seq ) generation technique  #AUTHOR_TAG, combined with beam search and an n - best list reranker to suppress irrelevant information in the outputs.', 'unlike most previous nlg systems for sds ( e. g.,  #AUTHOR_TAG, it is trainable from unaligned pairs of mr and sentences alone.', 'we experiment with using much less training data than recent systems based on recurrent neural networks ( rnn )  #AUTHOR_TAG b ;  #AUTHOR_TAG, and we find that our generator learns successfully to produce both strings and deep syntax trees on the bagel restaurant information dataset.', 'it is able to surpass n - gram - based scores achieved previously by  #TAUTHOR_TAG, offering a simpler setup and more relevant outputs.', 'we introduce the generation setting in section 2 and describe our generator architecture in section 3.', 'section 4 details our experiments, section 5 analyzes the results.', '']",4
"['##d domain  #TAUTHOR_TAG,', '']","['##d domain  #TAUTHOR_TAG,', '']","['works almost flawlessly on this lim - ited domain  #TAUTHOR_TAG,', 'leaving the seq2seq']","['we include bleu and nist scores and the number of semantic errors ( incorrect, missing, and', 'repeated information ), which we assessed manually on a sample of 42 output sentences ( outputs of two randomly selected cross - validation runs ). the outputs of direct string generation show that the models learn to produce fluent sentences in the domain', 'style ; 9 incoherent sentences are rare, but semantic errors are very frequent in the greedy search. most errors involve confusion of semantically close items, e. g., italian instead of french or', 'riverside area instead of city centre ( see table 2 ) ; items occurring more frequently are preferred regardless of their relevance. the beam search brings a bleu improvement but keeps most semantic errors in place. the rerank', '##er is able to reduce the number of semantic errors while increasing automatic scores considerably.', 'using a larger beam increases the effect of the reranker as expected, resulting in slightly improved', 'outputs. models generating deep syntax trees are also able to learn the domain style, and they have virtually no', 'problems producing valid trees. 10 the surface realizer works almost flawlessly on this lim - ited domain  #TAUTHOR_TAG,', '']",4
['generator of  #TAUTHOR_TAG while reducing'],['by a perceptron - based generator of  #TAUTHOR_TAG while reducing'],['by a perceptron - based generator of  #TAUTHOR_TAG while'],"['have presented a direct comparison of two - step generation via deep syntax trees with a direct generation into strings, both using the same nlg system based on the seq2seq approach.', 'while both approaches offer decent performance, their outputs are quite different.', 'the results show the direct approach as more favorable, with significantly higher n - gram based scores and a similar number of semantic errors in the output.', 'we also showed that our generator can learn to produce meaningful utterances using a much smaller amount of training data than what is typically used for rnn - based approaches.', 'the resulting models had virtually no problems with produc - ing fluent, coherent sentences or with generating valid structure of bracketed deep syntax trees.', 'our generator was able to surpass the best bleu / nist scores on the same dataset previously achieved by a perceptron - based generator of  #TAUTHOR_TAG while reducing the amount of irrelevant information on the output.', 'our generator is released on github at the following url :', 'https : / / github. com / ufal - dsg / tgen', 'we intend to apply it to other datasets for a broader comparison, and we plan further improvements, such as enhancing the reranker or including a bidirectional encoder  #AUTHOR_TAG and sequence level training  #AUTHOR_TAG']",4
"['times  #TAUTHOR_TAG, and the joint mode does not need to model structure explicitly and avoids accumulating errors along the pipeline  #AUTHOR_TAG']","['times  #TAUTHOR_TAG, and the joint mode does not need to model structure explicitly and avoids accumulating errors along the pipeline  #AUTHOR_TAG']","['into one step, producing natural language sentences directly.', 'both modes offer their advantages : the twostep mode simplifies generation by abstracting away from complex surface syntax and morphology, which can be handled by a handcrafted, domain - independent module to ensure grammatical correctness at all times  #TAUTHOR_TAG, and the joint mode does not need to model structure explicitly and avoids accumulating errors along the pipeline  #AUTHOR_TAG']","['input to our generator are dialogue acts ( da ) representing an action, such as inform or request, along with one or more attributes ( slots ) and their values.', 'our generator operates in two modes, producing either deep syntax trees ( dusek et al., 2012 ) or natural language strings ( see fig. 1 ).', 'the first mode corresponds to the sentence planning nlg stage as it decides the syntactic shape of the output sentence ; the resulting deep syntax tree involves content words ( lemmas ) and their syntactic form ( formemes, purple in fig. 1 ).', 'the trees are linearized to strings using a surface realizer from the tectomt translation system.', 'the second generator mode joins sentence planning and surface realization into one step, producing natural language sentences directly.', 'both modes offer their advantages : the twostep mode simplifies generation by abstracting away from complex surface syntax and morphology, which can be handled by a handcrafted, domain - independent module to ensure grammatical correctness at all times  #TAUTHOR_TAG, and the joint mode does not need to model structure explicitly and avoids accumulating errors along the pipeline  #AUTHOR_TAG']",0
"['- guided a * search  #TAUTHOR_TAG.', 'generators taking']","['generator  #AUTHOR_TAG or using perceptron - guided a * search  #TAUTHOR_TAG.', 'generators taking']","['to focus on sentence planning, improving a handcrafted generator  #AUTHOR_TAG or using perceptron - guided a * search  #TAUTHOR_TAG.', 'generators taking']","['most recent nlg systems attempt to learn generation from data, the choice of a particular approach - pipeline or joint - is often arbitrary and depends on system architecture or particular generation domain.', 'works using the pipeline approach in sds tend to focus on sentence planning, improving a handcrafted generator  #AUTHOR_TAG or using perceptron - guided a * search  #TAUTHOR_TAG.', 'generators taking the joint approach employ various methods, e. g., factored language models, inverted parsing  #AUTHOR_TAG, or a pipeline of discriminative classifiers  #AUTHOR_TAG.', '']",0
"['times  #TAUTHOR_TAG, and the joint mode does not need to model structure explicitly and avoids accumulating errors along the pipeline  #AUTHOR_TAG']","['times  #TAUTHOR_TAG, and the joint mode does not need to model structure explicitly and avoids accumulating errors along the pipeline  #AUTHOR_TAG']","['into one step, producing natural language sentences directly.', 'both modes offer their advantages : the twostep mode simplifies generation by abstracting away from complex surface syntax and morphology, which can be handled by a handcrafted, domain - independent module to ensure grammatical correctness at all times  #TAUTHOR_TAG, and the joint mode does not need to model structure explicitly and avoids accumulating errors along the pipeline  #AUTHOR_TAG']","['input to our generator are dialogue acts ( da ) representing an action, such as inform or request, along with one or more attributes ( slots ) and their values.', 'our generator operates in two modes, producing either deep syntax trees ( dusek et al., 2012 ) or natural language strings ( see fig. 1 ).', 'the first mode corresponds to the sentence planning nlg stage as it decides the syntactic shape of the output sentence ; the resulting deep syntax tree involves content words ( lemmas ) and their syntactic form ( formemes, purple in fig. 1 ).', 'the trees are linearized to strings using a surface realizer from the tectomt translation system.', 'the second generator mode joins sentence planning and surface realization into one step, producing natural language sentences directly.', 'both modes offer their advantages : the twostep mode simplifies generation by abstracting away from complex surface syntax and morphology, which can be handled by a handcrafted, domain - independent module to ensure grammatical correctness at all times  #TAUTHOR_TAG, and the joint mode does not need to model structure explicitly and avoids accumulating errors along the pipeline  #AUTHOR_TAG']",5
"['##d domain  #TAUTHOR_TAG,', '']","['##d domain  #TAUTHOR_TAG,', '']","['works almost flawlessly on this lim - ited domain  #TAUTHOR_TAG,', 'leaving the seq2seq']","['we include bleu and nist scores and the number of semantic errors ( incorrect, missing, and', 'repeated information ), which we assessed manually on a sample of 42 output sentences ( outputs of two randomly selected cross - validation runs ). the outputs of direct string generation show that the models learn to produce fluent sentences in the domain', 'style ; 9 incoherent sentences are rare, but semantic errors are very frequent in the greedy search. most errors involve confusion of semantically close items, e. g., italian instead of french or', 'riverside area instead of city centre ( see table 2 ) ; items occurring more frequently are preferred regardless of their relevance. the beam search brings a bleu improvement but keeps most semantic errors in place. the rerank', '##er is able to reduce the number of semantic errors while increasing automatic scores considerably.', 'using a larger beam increases the effect of the reranker as expected, resulting in slightly improved', 'outputs. models generating deep syntax trees are also able to learn the domain style, and they have virtually no', 'problems producing valid trees. 10 the surface realizer works almost flawlessly on this lim - ited domain  #TAUTHOR_TAG,', '']",5
"['- guided a * search  #TAUTHOR_TAG.', 'generators taking']","['generator  #AUTHOR_TAG or using perceptron - guided a * search  #TAUTHOR_TAG.', 'generators taking']","['to focus on sentence planning, improving a handcrafted generator  #AUTHOR_TAG or using perceptron - guided a * search  #TAUTHOR_TAG.', 'generators taking']","['most recent nlg systems attempt to learn generation from data, the choice of a particular approach - pipeline or joint - is often arbitrary and depends on system architecture or particular generation domain.', 'works using the pipeline approach in sds tend to focus on sentence planning, improving a handcrafted generator  #AUTHOR_TAG or using perceptron - guided a * search  #TAUTHOR_TAG.', 'generators taking the joint approach employ various methods, e. g., factored language models, inverted parsing  #AUTHOR_TAG, or a pipeline of discriminative classifiers  #AUTHOR_TAG.', '']",6
"['as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of']","['as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of 1086 paths during training and 1430 paths']","['##ll relation data as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of']","['##ll relation data as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of 1086 paths during training and 1430 paths during test time. we split', '']",5
"['as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of']","['as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of 1086 paths during training and 1430 paths']","['##ll relation data as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of']","['##ll relation data as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of 1086 paths during training and 1430 paths during test time. we split', '']",5
"['as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of']","['as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of 1086 paths during training and 1430 paths']","['##ll relation data as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of']","['##ll relation data as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of 1086 paths during training and 1430 paths during test time. we split', '']",5
"['as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of']","['as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of 1086 paths during training and 1430 paths']","['##ll relation data as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of']","['##ll relation data as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of 1086 paths during training and 1430 paths during test time. we split', '']",5
"['as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of']","['as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of 1086 paths during training and 1430 paths']","['##ll relation data as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of']","['##ll relation data as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of 1086 paths during training and 1430 paths during test time. we split', '']",5
"['as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of']","['as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of 1086 paths during training and 1430 paths']","['##ll relation data as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of']","['##ll relation data as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of 1086 paths during training and 1430 paths during test time. we split', '']",5
"['as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of']","['as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of 1086 paths during training and 1430 paths']","['##ll relation data as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of']","['##ll relation data as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of 1086 paths during training and 1430 paths during test time. we split', '']",5
"['as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of']","['as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of 1086 paths during training and 1430 paths']","['##ll relation data as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of']","['##ll relation data as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of 1086 paths during training and 1430 paths during test time. we split', '']",5
"['as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of']","['as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of 1086 paths during training and 1430 paths']","['##ll relation data as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of']","['##ll relation data as used in  #TAUTHOR_TAG. the augmentation resulted in', 'the addition of 1086 paths during training and 1430 paths during test time. we split', '']",3
"['two models by various ways (  #AUTHOR_TAG ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ; summers -  #AUTHOR_TAG ).', 'the empirical evidence shows that the']","['two models by various ways (  #AUTHOR_TAG ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ; summers -  #AUTHOR_TAG ).', 'the empirical evidence shows that the']","['(  #AUTHOR_TAG ) ; ( 3 ) combining the above two models by various ways (  #AUTHOR_TAG ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ; summers -  #AUTHOR_TAG ).', 'the empirical evidence shows that the word representations']","['', 'previous work addressing the problem can be roughly classified into three categories : ( 1 ) learning word embeddings from large collections of text using variants of neural networks (  #AUTHOR_TAG a ) ;  #AUTHOR_TAG b ) ;  #AUTHOR_TAG c ) ;  #AUTHOR_TAG ) or global matrix factorization (  #AUTHOR_TAG ;  #AUTHOR_TAG ) ; ( 2 ) extracting knowledge from existing semantic networks, such as wordnet (  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ) and conceptnet (  #AUTHOR_TAG ) ; ( 3 ) combining the above two models by various ways (  #AUTHOR_TAG ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ; summers -  #AUTHOR_TAG ).', 'the empirical evidence shows that the word representations learned from neural network models do an especially good job in capturing not only attributional similarities between words but also similarities between pairs of words (  #AUTHOR_TAG c ) ).', ' #AUTHOR_TAG generalize the skip - gram model with negative sampling to include arbitrary word contexts and present the dependency - based word embeddings, which are learned from syntactic contexts derived from dependency parse - trees.', '']",0
"['two models by various ways (  #AUTHOR_TAG ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ; summers -  #AUTHOR_TAG ).', 'the empirical evidence shows that the']","['two models by various ways (  #AUTHOR_TAG ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ; summers -  #AUTHOR_TAG ).', 'the empirical evidence shows that the']","['(  #AUTHOR_TAG ) ; ( 3 ) combining the above two models by various ways (  #AUTHOR_TAG ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ; summers -  #AUTHOR_TAG ).', 'the empirical evidence shows that the word representations']","['', 'previous work addressing the problem can be roughly classified into three categories : ( 1 ) learning word embeddings from large collections of text using variants of neural networks (  #AUTHOR_TAG a ) ;  #AUTHOR_TAG b ) ;  #AUTHOR_TAG c ) ;  #AUTHOR_TAG ) or global matrix factorization (  #AUTHOR_TAG ;  #AUTHOR_TAG ) ; ( 2 ) extracting knowledge from existing semantic networks, such as wordnet (  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ) and conceptnet (  #AUTHOR_TAG ) ; ( 3 ) combining the above two models by various ways (  #AUTHOR_TAG ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ; summers -  #AUTHOR_TAG ).', 'the empirical evidence shows that the word representations learned from neural network models do an especially good job in capturing not only attributional similarities between words but also similarities between pairs of words (  #AUTHOR_TAG c ) ).', ' #AUTHOR_TAG generalize the skip - gram model with negative sampling to include arbitrary word contexts and present the dependency - based word embeddings, which are learned from syntactic contexts derived from dependency parse - trees.', '']",0
['directional similarity model  #TAUTHOR_TAG explores the difference of two relationships in multiple topicality dimensions in the vector'],"['directional similarity model  #TAUTHOR_TAG explores the difference of two relationships in multiple topicality dimensions in the vector space.', 'however, it ignores the spatial distance between word vectors, which can reveal the function similarity of words in function space.', 'the dual - space model (  #AUTHOR_TAG ) can measure both']","['directional similarity model  #TAUTHOR_TAG explores the difference of two relationships in multiple topicality dimensions in the vector space.', 'however, it ignores the spatial distance between word vectors, which can reveal the function similarity of words in function space.', 'the dual - space model (  #AUTHOR_TAG ) can measure']","['directional similarity model  #TAUTHOR_TAG explores the difference of two relationships in multiple topicality dimensions in the vector space.', 'however, it ignores the spatial distance between word vectors, which can reveal the function similarity of words in function space.', 'the dual - space model (  #AUTHOR_TAG ) can measure both domain similarity and function similarity between words.', 'however, it only computes the domain similarity between two single word vectors and places less emphasis on the domain similarity between two relations.', ' #AUTHOR_TAG model uses only noun or verbbased patterns for contexts to model domain or function space.', 'in this paper, we propose a novel dualspace model, which combines the advantages of the above two existing models.', 'we use the word2vec - based directional similarity model to measure the similarity of two relations in domain space, and the dependency - based word embeddings to represent the word vector in function space.', 'the two embeddings used in our model consider broader contexts than those in the original dual - space model, which can provide richer information for measuring domain similarity and function similarity.', '']",0
"['two models by various ways (  #AUTHOR_TAG ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ; summers -  #AUTHOR_TAG ).', 'the empirical evidence shows that the']","['two models by various ways (  #AUTHOR_TAG ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ; summers -  #AUTHOR_TAG ).', 'the empirical evidence shows that the']","['(  #AUTHOR_TAG ) ; ( 3 ) combining the above two models by various ways (  #AUTHOR_TAG ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ; summers -  #AUTHOR_TAG ).', 'the empirical evidence shows that the word representations']","['', 'previous work addressing the problem can be roughly classified into three categories : ( 1 ) learning word embeddings from large collections of text using variants of neural networks (  #AUTHOR_TAG a ) ;  #AUTHOR_TAG b ) ;  #AUTHOR_TAG c ) ;  #AUTHOR_TAG ) or global matrix factorization (  #AUTHOR_TAG ;  #AUTHOR_TAG ) ; ( 2 ) extracting knowledge from existing semantic networks, such as wordnet (  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ) and conceptnet (  #AUTHOR_TAG ) ; ( 3 ) combining the above two models by various ways (  #AUTHOR_TAG ;  #TAUTHOR_TAG ;  #AUTHOR_TAG ; summers -  #AUTHOR_TAG ).', 'the empirical evidence shows that the word representations learned from neural network models do an especially good job in capturing not only attributional similarities between words but also similarities between pairs of words (  #AUTHOR_TAG c ) ).', ' #AUTHOR_TAG generalize the skip - gram model with negative sampling to include arbitrary word contexts and present the dependency - based word embeddings, which are learned from syntactic contexts derived from dependency parse - trees.', '']",1
['directional similarity model  #TAUTHOR_TAG explores the difference of two relationships in multiple topicality dimensions in the vector'],"['directional similarity model  #TAUTHOR_TAG explores the difference of two relationships in multiple topicality dimensions in the vector space.', 'however, it ignores the spatial distance between word vectors, which can reveal the function similarity of words in function space.', 'the dual - space model (  #AUTHOR_TAG ) can measure both']","['directional similarity model  #TAUTHOR_TAG explores the difference of two relationships in multiple topicality dimensions in the vector space.', 'however, it ignores the spatial distance between word vectors, which can reveal the function similarity of words in function space.', 'the dual - space model (  #AUTHOR_TAG ) can measure']","['directional similarity model  #TAUTHOR_TAG explores the difference of two relationships in multiple topicality dimensions in the vector space.', 'however, it ignores the spatial distance between word vectors, which can reveal the function similarity of words in function space.', 'the dual - space model (  #AUTHOR_TAG ) can measure both domain similarity and function similarity between words.', 'however, it only computes the domain similarity between two single word vectors and places less emphasis on the domain similarity between two relations.', ' #AUTHOR_TAG model uses only noun or verbbased patterns for contexts to model domain or function space.', 'in this paper, we propose a novel dualspace model, which combines the advantages of the above two existing models.', 'we use the word2vec - based directional similarity model to measure the similarity of two relations in domain space, and the dependency - based word embeddings to represent the word vector in function space.', 'the two embeddings used in our model consider broader contexts than those in the original dual - space model, which can provide richer information for measuring domain similarity and function similarity.', '']",1
['directional similarity model  #TAUTHOR_TAG explores the difference of two relationships in multiple topicality dimensions in the vector'],"['directional similarity model  #TAUTHOR_TAG explores the difference of two relationships in multiple topicality dimensions in the vector space.', 'however, it ignores the spatial distance between word vectors, which can reveal the function similarity of words in function space.', 'the dual - space model (  #AUTHOR_TAG ) can measure both']","['directional similarity model  #TAUTHOR_TAG explores the difference of two relationships in multiple topicality dimensions in the vector space.', 'however, it ignores the spatial distance between word vectors, which can reveal the function similarity of words in function space.', 'the dual - space model (  #AUTHOR_TAG ) can measure']","['directional similarity model  #TAUTHOR_TAG explores the difference of two relationships in multiple topicality dimensions in the vector space.', 'however, it ignores the spatial distance between word vectors, which can reveal the function similarity of words in function space.', 'the dual - space model (  #AUTHOR_TAG ) can measure both domain similarity and function similarity between words.', 'however, it only computes the domain similarity between two single word vectors and places less emphasis on the domain similarity between two relations.', ' #AUTHOR_TAG model uses only noun or verbbased patterns for contexts to model domain or function space.', 'in this paper, we propose a novel dualspace model, which combines the advantages of the above two existing models.', 'we use the word2vec - based directional similarity model to measure the similarity of two relations in domain space, and the dependency - based word embeddings to represent the word vector in function space.', 'the two embeddings used in our model consider broader contexts than those in the original dual - space model, which can provide richer information for measuring domain similarity and function similarity.', '']",5
"['1 ) lm  #TAUTHOR_TAG, where the whole']","['( trf 1 ) lm  #TAUTHOR_TAG, where the whole']","['2, 3 ].', 'we have recently introduced a new transdimensional random field ( trf 1 ) lm  #TAUTHOR_TAG, where the whole sentence is modeled as a random field.', 'as the random field approach avoids local normalization which is required in the conditional approach, it is computationally more efficient in computing sentence probabilities']","['modeling ( lm ) involves determining the joint probability of words in a sentence.', 'the conditional approach is dominant, representing the joint probability in terms of conditionals.', 'examples include n - gram lms [ 1 ] and neural network ( nn ) lms [ 2, 3 ].', 'we have recently introduced a new transdimensional random field ( trf 1 ) lm  #TAUTHOR_TAG, where the whole sentence is modeled as a random field.', 'as the random field approach avoids local normalization which is required in the conditional approach, it is computationally more efficient in computing sentence probabilities and has the potential advantage of being able to flexibly integrating a richer set of features.', 'we developed an effective training algorithm using joint stochastic approximation ( sa ) and trans - dimensional mixture sampling.', '']",0
"['1 ) lm  #TAUTHOR_TAG, where the whole']","['( trf 1 ) lm  #TAUTHOR_TAG, where the whole']","['2, 3 ].', 'we have recently introduced a new transdimensional random field ( trf 1 ) lm  #TAUTHOR_TAG, where the whole sentence is modeled as a random field.', 'as the random field approach avoids local normalization which is required in the conditional approach, it is computationally more efficient in computing sentence probabilities']","['modeling ( lm ) involves determining the joint probability of words in a sentence.', 'the conditional approach is dominant, representing the joint probability in terms of conditionals.', 'examples include n - gram lms [ 1 ] and neural network ( nn ) lms [ 2, 3 ].', 'we have recently introduced a new transdimensional random field ( trf 1 ) lm  #TAUTHOR_TAG, where the whole sentence is modeled as a random field.', 'as the random field approach avoids local normalization which is required in the conditional approach, it is computationally more efficient in computing sentence probabilities and has the potential advantage of being able to flexibly integrating a richer set of features.', 'we developed an effective training algorithm using joint stochastic approximation ( sa ) and trans - dimensional mixture sampling.', '']",0
"['l.', 'as defined in  #TAUTHOR_TAG, a trans - dimensional random field model represents the joint probability of the pair ( l, x l ) as', 'where n l / n is']","['l.', 'as defined in  #TAUTHOR_TAG, a trans - dimensional random field model represents the joint probability of the pair ( l, x l ) as', 'where n l / n is']","['d l denotes the collection of length l in the training corpus.', 'n l denotes the size of d l and n = m l = 1 n l.', 'as defined in  #TAUTHOR_TAG, a trans - dimensional random field model represents the joint probability of the pair ( l, x l ) as', 'where n l / n is the empirical probability of length l. f (', 't is the feature vector, which is usually defined to be position - independent']","[', we denote by x l = ( x1,..., x l ) a sentence ( i. e., word sequence ) of length l ranging from 1 to m. each element of x l corresponds to a single word.', 'd denotes the whole training corpus and d l denotes the collection of length l in the training corpus.', 'n l denotes the size of d l and n = m l = 1 n l.', 'as defined in  #TAUTHOR_TAG, a trans - dimensional random field model represents the joint probability of the pair ( l, x l ) as', 'where n l / n is the empirical probability of length l. f (', 't is the feature vector, which is usually defined to be position - independent and length - independent, e. g.', 'is the normalization constant of length l. by making explicit the role of length in model definition, it is clear that the model in ( 1 ) is a mixture of random fields on sentences of different lengths ( namely on subspaces of different dimensions ), and hence will be called a trans - dimensional random field ( trf ).', 'in the joint sa training algorithm  #TAUTHOR_TAG, we define another form of mixture distribution as follows :', 'where ζ = { ζ1,..., ζm } with ζ1 = 0 and ζ l is the hypothesized value of the log ratio of z l ( λ ) with respect to z1 ( λ ), namely log', '. z1 ( λ ) is chosen as the reference value and can be calculated exactly.', 'an important observation is that if and only if ζ were equal to the true log ratios, then the marginal probability of length l under distribution ( 2 ) equals to n l / n.', 'we then use this property to construct the joint sa algorithm, which jointly estimates the model parameters and normalization constants']",0
"['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired']","['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired']","['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired']","['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired from [ 6, 7 ], we propose a simpler but more effective method which directly uses the empirical variances to rescale the gradients.', 'the improved sa algorithm is described as follows.', 'at each iteration t ( from 1 to tmax ), we perform two steps.', 'step i : mcmc sampling : generate a sample set b ( t ) with p ( l, x l ; λ ( t−1 ), ζ ( t−1 ) ) as the stationary distribution, using the trans - dimensional mixture sampling method ( see section 3. 3 in  #TAUTHOR_TAG.', 'step ii : sa updating : compute', 'and', 'nm / n ( 4 )', 'where γ λ, t, γ ζ, t are the learning rate of λ and ζ.', 'is the relative frequency of length l appearing in b ( t ).', 'in eq. 3, σ = diag ( σ1,..., σ d ) is a diagonal matrix and the element σi ( 1 ≤ i ≤ d ) is the empirical variance of feature fi :', '. it is shown in [ 6 ] that the convergence speed of log - linear model training is improved when the means and variances of the input features are normalized.', 'our update rule in eq. 3 performs the normalization on model side instead of explicitly normalizing the features, which is similar to [ 7 ].', 'as the empirical variances are calculated offline, this also reduces computational and memory cost during the sa iterations.', 'our empirical result shows that compared to using the online estimated hessian elements which are noisy, using the empirical variances which are exactly calculated can improve the convergence significantly, especially on large dataset with millions of features.', 'fig. 1 show an example of convergence curves of the sa training algorithm in  #TAUTHOR_TAG and the new improved sa']",0
"['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired']","['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired']","['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired']","['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired from [ 6, 7 ], we propose a simpler but more effective method which directly uses the empirical variances to rescale the gradients.', 'the improved sa algorithm is described as follows.', 'at each iteration t ( from 1 to tmax ), we perform two steps.', 'step i : mcmc sampling : generate a sample set b ( t ) with p ( l, x l ; λ ( t−1 ), ζ ( t−1 ) ) as the stationary distribution, using the trans - dimensional mixture sampling method ( see section 3. 3 in  #TAUTHOR_TAG.', 'step ii : sa updating : compute', 'and', 'nm / n ( 4 )', 'where γ λ, t, γ ζ, t are the learning rate of λ and ζ.', 'is the relative frequency of length l appearing in b ( t ).', 'in eq. 3, σ = diag ( σ1,..., σ d ) is a diagonal matrix and the element σi ( 1 ≤ i ≤ d ) is the empirical variance of feature fi :', '. it is shown in [ 6 ] that the convergence speed of log - linear model training is improved when the means and variances of the input features are normalized.', 'our update rule in eq. 3 performs the normalization on model side instead of explicitly normalizing the features, which is similar to [ 7 ].', 'as the empirical variances are calculated offline, this also reduces computational and memory cost during the sa iterations.', 'our empirical result shows that compared to using the online estimated hessian elements which are noisy, using the empirical variances which are exactly calculated can improve the convergence significantly, especially on large dataset with millions of features.', 'fig. 1 show an example of convergence curves of the sa training algorithm in  #TAUTHOR_TAG and the new improved sa']",0
"['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired']","['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired']","['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired']","['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired from [ 6, 7 ], we propose a simpler but more effective method which directly uses the empirical variances to rescale the gradients.', 'the improved sa algorithm is described as follows.', 'at each iteration t ( from 1 to tmax ), we perform two steps.', 'step i : mcmc sampling : generate a sample set b ( t ) with p ( l, x l ; λ ( t−1 ), ζ ( t−1 ) ) as the stationary distribution, using the trans - dimensional mixture sampling method ( see section 3. 3 in  #TAUTHOR_TAG.', 'step ii : sa updating : compute', 'and', 'nm / n ( 4 )', 'where γ λ, t, γ ζ, t are the learning rate of λ and ζ.', 'is the relative frequency of length l appearing in b ( t ).', 'in eq. 3, σ = diag ( σ1,..., σ d ) is a diagonal matrix and the element σi ( 1 ≤ i ≤ d ) is the empirical variance of feature fi :', '. it is shown in [ 6 ] that the convergence speed of log - linear model training is improved when the means and variances of the input features are normalized.', 'our update rule in eq. 3 performs the normalization on model side instead of explicitly normalizing the features, which is similar to [ 7 ].', 'as the empirical variances are calculated offline, this also reduces computational and memory cost during the sa iterations.', 'our empirical result shows that compared to using the online estimated hessian elements which are noisy, using the empirical variances which are exactly calculated can improve the convergence significantly, especially on large dataset with millions of features.', 'fig. 1 show an example of convergence curves of the sa training algorithm in  #TAUTHOR_TAG and the new improved sa']",0
"['3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are']","['3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are']","['long distance context without increasing the model size.', 'note that for all the feature types in tab. 1, only the features observed in the training data are used.', 'the improved sa algorithm ( in section 2. 2 ) is used to train the trf lms, in conjunction with the trans - dimensional mixture sampling proposed in section 3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are set as suggested in  #TAUTHOR_TAG :', 'where tc, t0 are constants and']","['', 'third, as an example of supporting heterogenous features that combine different information, the crossing features "" cpw "" ( meaning class - predict - word ) are introduced.', 'in the end, we introduce the tied long - skip - bigram features "" tied "" [ 11 ], in which the skip - bigrams with skipping distances from 6 to 9 share the same parameter.', 'in this way we can leverage long distance context without increasing the model size.', 'note that for all the feature types in tab. 1, only the features observed in the training data are used.', 'the improved sa algorithm ( in section 2. 2 ) is used to train the trf lms, in conjunction with the trans - dimensional mixture sampling proposed in section 3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are set as suggested in  #TAUTHOR_TAG :', 'where tc, t0 are constants and 0. 5 < β λ, β ζ < 1.', 'the class information is also used to accelerate the sampling, and more than one cpu cores are used to parallelize the algorithm, as described in  #TAUTHOR_TAG.', 'also, we examine how the trf models can be interpolated with']",0
"['3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are']","['3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are']","['long distance context without increasing the model size.', 'note that for all the feature types in tab. 1, only the features observed in the training data are used.', 'the improved sa algorithm ( in section 2. 2 ) is used to train the trf lms, in conjunction with the trans - dimensional mixture sampling proposed in section 3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are set as suggested in  #TAUTHOR_TAG :', 'where tc, t0 are constants and']","['', 'third, as an example of supporting heterogenous features that combine different information, the crossing features "" cpw "" ( meaning class - predict - word ) are introduced.', 'in the end, we introduce the tied long - skip - bigram features "" tied "" [ 11 ], in which the skip - bigrams with skipping distances from 6 to 9 share the same parameter.', 'in this way we can leverage long distance context without increasing the model size.', 'note that for all the feature types in tab. 1, only the features observed in the training data are used.', 'the improved sa algorithm ( in section 2. 2 ) is used to train the trf lms, in conjunction with the trans - dimensional mixture sampling proposed in section 3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are set as suggested in  #TAUTHOR_TAG :', 'where tc, t0 are constants and 0. 5 < β λ, β ζ < 1.', 'the class information is also used to accelerate the sampling, and more than one cpu cores are used to parallelize the algorithm, as described in  #TAUTHOR_TAG.', 'also, we examine how the trf models can be interpolated with']",0
"['3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are']","['3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are']","['long distance context without increasing the model size.', 'note that for all the feature types in tab. 1, only the features observed in the training data are used.', 'the improved sa algorithm ( in section 2. 2 ) is used to train the trf lms, in conjunction with the trans - dimensional mixture sampling proposed in section 3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are set as suggested in  #TAUTHOR_TAG :', 'where tc, t0 are constants and']","['', 'third, as an example of supporting heterogenous features that combine different information, the crossing features "" cpw "" ( meaning class - predict - word ) are introduced.', 'in the end, we introduce the tied long - skip - bigram features "" tied "" [ 11 ], in which the skip - bigrams with skipping distances from 6 to 9 share the same parameter.', 'in this way we can leverage long distance context without increasing the model size.', 'note that for all the feature types in tab. 1, only the features observed in the training data are used.', 'the improved sa algorithm ( in section 2. 2 ) is used to train the trf lms, in conjunction with the trans - dimensional mixture sampling proposed in section 3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are set as suggested in  #TAUTHOR_TAG :', 'where tc, t0 are constants and 0. 5 < β λ, β ζ < 1.', 'the class information is also used to accelerate the sampling, and more than one cpu cores are used to parallelize the algorithm, as described in  #TAUTHOR_TAG.', 'also, we examine how the trf models can be interpolated with']",0
"['respectively ( no gpu used ).', 'equally importantly, evaluations in this paper and also in  #TAUTHOR_TAG have shown that trf lms are able to perform as good as nn lms ( either rnn']","['rnn respectively ( no gpu used ).', 'equally importantly, evaluations in this paper and also in  #TAUTHOR_TAG have shown that trf lms are able to perform as good as nn lms ( either rnn']","['respectively ( no gpu used ).', 'equally importantly, evaluations in this paper and also in  #TAUTHOR_TAG have shown that trf lms are able to perform as good as nn lms ( either rnn']","['', 'lots of studies aim to alleviate this deficiency.', 'initial efforts include using hierarchical output layer structure with word clustering [ 3 ], converting nns to n - gram lms [ 16 ].', 'recently a number of studies [ 17, 18, 19, 20 ] make use of noise contrastive estimation ( nce ) [ 21 ] to build unnormalized variants of nn lms through trickily avoiding local normalization in training and heuristically fixing the normalizing term in testing.', 'in contrast, trf lms eliminate local normalization from the root and thus are much more efficient in testing with theoretical guarantee.', 'empirically in our experiments reported in section 3. 3, the average time costs for re - ranking of the 1000 - best list for a sentence are 0. 16 sec vs. 40 sec, based on trf and rnn respectively ( no gpu used ).', 'equally importantly, evaluations in this paper and also in  #TAUTHOR_TAG have shown that trf lms are able to perform as good as nn lms ( either rnn or fnn ) on a variety of tasks.', 'encouragingly, the random field approach may open a new door to language modeling in addition to the dominant conditional approach, as once envisioned in [ 5 ].', 'integrating richer features and introducing hidden variables are worthwhile future works']",0
"['1 ) lm  #TAUTHOR_TAG, where the whole']","['( trf 1 ) lm  #TAUTHOR_TAG, where the whole']","['2, 3 ].', 'we have recently introduced a new transdimensional random field ( trf 1 ) lm  #TAUTHOR_TAG, where the whole sentence is modeled as a random field.', 'as the random field approach avoids local normalization which is required in the conditional approach, it is computationally more efficient in computing sentence probabilities']","['modeling ( lm ) involves determining the joint probability of words in a sentence.', 'the conditional approach is dominant, representing the joint probability in terms of conditionals.', 'examples include n - gram lms [ 1 ] and neural network ( nn ) lms [ 2, 3 ].', 'we have recently introduced a new transdimensional random field ( trf 1 ) lm  #TAUTHOR_TAG, where the whole sentence is modeled as a random field.', 'as the random field approach avoids local normalization which is required in the conditional approach, it is computationally more efficient in computing sentence probabilities and has the potential advantage of being able to flexibly integrating a richer set of features.', 'we developed an effective training algorithm using joint stochastic approximation ( sa ) and trans - dimensional mixture sampling.', '']",1
"['l.', 'as defined in  #TAUTHOR_TAG, a trans - dimensional random field model represents the joint probability of the pair ( l, x l ) as', 'where n l / n is']","['l.', 'as defined in  #TAUTHOR_TAG, a trans - dimensional random field model represents the joint probability of the pair ( l, x l ) as', 'where n l / n is']","['d l denotes the collection of length l in the training corpus.', 'n l denotes the size of d l and n = m l = 1 n l.', 'as defined in  #TAUTHOR_TAG, a trans - dimensional random field model represents the joint probability of the pair ( l, x l ) as', 'where n l / n is the empirical probability of length l. f (', 't is the feature vector, which is usually defined to be position - independent']","[', we denote by x l = ( x1,..., x l ) a sentence ( i. e., word sequence ) of length l ranging from 1 to m. each element of x l corresponds to a single word.', 'd denotes the whole training corpus and d l denotes the collection of length l in the training corpus.', 'n l denotes the size of d l and n = m l = 1 n l.', 'as defined in  #TAUTHOR_TAG, a trans - dimensional random field model represents the joint probability of the pair ( l, x l ) as', 'where n l / n is the empirical probability of length l. f (', 't is the feature vector, which is usually defined to be position - independent and length - independent, e. g.', 'is the normalization constant of length l. by making explicit the role of length in model definition, it is clear that the model in ( 1 ) is a mixture of random fields on sentences of different lengths ( namely on subspaces of different dimensions ), and hence will be called a trans - dimensional random field ( trf ).', 'in the joint sa training algorithm  #TAUTHOR_TAG, we define another form of mixture distribution as follows :', 'where ζ = { ζ1,..., ζm } with ζ1 = 0 and ζ l is the hypothesized value of the log ratio of z l ( λ ) with respect to z1 ( λ ), namely log', '. z1 ( λ ) is chosen as the reference value and can be calculated exactly.', 'an important observation is that if and only if ζ were equal to the true log ratios, then the marginal probability of length l under distribution ( 2 ) equals to n l / n.', 'we then use this property to construct the joint sa algorithm, which jointly estimates the model parameters and normalization constants']",6
"['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired']","['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired']","['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired']","['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired from [ 6, 7 ], we propose a simpler but more effective method which directly uses the empirical variances to rescale the gradients.', 'the improved sa algorithm is described as follows.', 'at each iteration t ( from 1 to tmax ), we perform two steps.', 'step i : mcmc sampling : generate a sample set b ( t ) with p ( l, x l ; λ ( t−1 ), ζ ( t−1 ) ) as the stationary distribution, using the trans - dimensional mixture sampling method ( see section 3. 3 in  #TAUTHOR_TAG.', 'step ii : sa updating : compute', 'and', 'nm / n ( 4 )', 'where γ λ, t, γ ζ, t are the learning rate of λ and ζ.', 'is the relative frequency of length l appearing in b ( t ).', 'in eq. 3, σ = diag ( σ1,..., σ d ) is a diagonal matrix and the element σi ( 1 ≤ i ≤ d ) is the empirical variance of feature fi :', '. it is shown in [ 6 ] that the convergence speed of log - linear model training is improved when the means and variances of the input features are normalized.', 'our update rule in eq. 3 performs the normalization on model side instead of explicitly normalizing the features, which is similar to [ 7 ].', 'as the empirical variances are calculated offline, this also reduces computational and memory cost during the sa iterations.', 'our empirical result shows that compared to using the online estimated hessian elements which are noisy, using the empirical variances which are exactly calculated can improve the convergence significantly, especially on large dataset with millions of features.', 'fig. 1 show an example of convergence curves of the sa training algorithm in  #TAUTHOR_TAG and the new improved sa']",5
"['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired']","['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired']","['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired']","['order to make use of hessian information in parameter optimization, we use the online estimated hessian diagonal elements to rescale the gradients in  #TAUTHOR_TAG.', 'in this paper, inspired from [ 6, 7 ], we propose a simpler but more effective method which directly uses the empirical variances to rescale the gradients.', 'the improved sa algorithm is described as follows.', 'at each iteration t ( from 1 to tmax ), we perform two steps.', 'step i : mcmc sampling : generate a sample set b ( t ) with p ( l, x l ; λ ( t−1 ), ζ ( t−1 ) ) as the stationary distribution, using the trans - dimensional mixture sampling method ( see section 3. 3 in  #TAUTHOR_TAG.', 'step ii : sa updating : compute', 'and', 'nm / n ( 4 )', 'where γ λ, t, γ ζ, t are the learning rate of λ and ζ.', 'is the relative frequency of length l appearing in b ( t ).', 'in eq. 3, σ = diag ( σ1,..., σ d ) is a diagonal matrix and the element σi ( 1 ≤ i ≤ d ) is the empirical variance of feature fi :', '. it is shown in [ 6 ] that the convergence speed of log - linear model training is improved when the means and variances of the input features are normalized.', 'our update rule in eq. 3 performs the normalization on model side instead of explicitly normalizing the features, which is similar to [ 7 ].', 'as the empirical variances are calculated offline, this also reduces computational and memory cost during the sa iterations.', 'our empirical result shows that compared to using the online estimated hessian elements which are noisy, using the empirical variances which are exactly calculated can improve the convergence significantly, especially on large dataset with millions of features.', 'fig. 1 show an example of convergence curves of the sa training algorithm in  #TAUTHOR_TAG and the new improved sa']",5
"['3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are']","['3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are']","['long distance context without increasing the model size.', 'note that for all the feature types in tab. 1, only the features observed in the training data are used.', 'the improved sa algorithm ( in section 2. 2 ) is used to train the trf lms, in conjunction with the trans - dimensional mixture sampling proposed in section 3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are set as suggested in  #TAUTHOR_TAG :', 'where tc, t0 are constants and']","['', 'third, as an example of supporting heterogenous features that combine different information, the crossing features "" cpw "" ( meaning class - predict - word ) are introduced.', 'in the end, we introduce the tied long - skip - bigram features "" tied "" [ 11 ], in which the skip - bigrams with skipping distances from 6 to 9 share the same parameter.', 'in this way we can leverage long distance context without increasing the model size.', 'note that for all the feature types in tab. 1, only the features observed in the training data are used.', 'the improved sa algorithm ( in section 2. 2 ) is used to train the trf lms, in conjunction with the trans - dimensional mixture sampling proposed in section 3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are set as suggested in  #TAUTHOR_TAG :', 'where tc, t0 are constants and 0. 5 < β λ, β ζ < 1.', 'the class information is also used to accelerate the sampling, and more than one cpu cores are used to parallelize the algorithm, as described in  #TAUTHOR_TAG.', 'also, we examine how the trf models can be interpolated with']",5
"['3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are']","['3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are']","['long distance context without increasing the model size.', 'note that for all the feature types in tab. 1, only the features observed in the training data are used.', 'the improved sa algorithm ( in section 2. 2 ) is used to train the trf lms, in conjunction with the trans - dimensional mixture sampling proposed in section 3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are set as suggested in  #TAUTHOR_TAG :', 'where tc, t0 are constants and']","['', 'third, as an example of supporting heterogenous features that combine different information, the crossing features "" cpw "" ( meaning class - predict - word ) are introduced.', 'in the end, we introduce the tied long - skip - bigram features "" tied "" [ 11 ], in which the skip - bigrams with skipping distances from 6 to 9 share the same parameter.', 'in this way we can leverage long distance context without increasing the model size.', 'note that for all the feature types in tab. 1, only the features observed in the training data are used.', 'the improved sa algorithm ( in section 2. 2 ) is used to train the trf lms, in conjunction with the trans - dimensional mixture sampling proposed in section 3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are set as suggested in  #TAUTHOR_TAG :', 'where tc, t0 are constants and 0. 5 < β λ, β ζ < 1.', 'the class information is also used to accelerate the sampling, and more than one cpu cores are used to parallelize the algorithm, as described in  #TAUTHOR_TAG.', 'also, we examine how the trf models can be interpolated with']",5
"['3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are']","['3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are']","['long distance context without increasing the model size.', 'note that for all the feature types in tab. 1, only the features observed in the training data are used.', 'the improved sa algorithm ( in section 2. 2 ) is used to train the trf lms, in conjunction with the trans - dimensional mixture sampling proposed in section 3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are set as suggested in  #TAUTHOR_TAG :', 'where tc, t0 are constants and']","['', 'third, as an example of supporting heterogenous features that combine different information, the crossing features "" cpw "" ( meaning class - predict - word ) are introduced.', 'in the end, we introduce the tied long - skip - bigram features "" tied "" [ 11 ], in which the skip - bigrams with skipping distances from 6 to 9 share the same parameter.', 'in this way we can leverage long distance context without increasing the model size.', 'note that for all the feature types in tab. 1, only the features observed in the training data are used.', 'the improved sa algorithm ( in section 2. 2 ) is used to train the trf lms, in conjunction with the trans - dimensional mixture sampling proposed in section 3. 3 of  #TAUTHOR_TAG.', 'the learning rates of λ and ζ are set as suggested in  #TAUTHOR_TAG :', 'where tc, t0 are constants and 0. 5 < β λ, β ζ < 1.', 'the class information is also used to accelerate the sampling, and more than one cpu cores are used to parallelize the algorithm, as described in  #TAUTHOR_TAG.', 'also, we examine how the trf models can be interpolated with']",5
"['- best list rescoring experiments are conducted as configured in  #TAUTHOR_TAG.', 'the maximum length of trfs is m = 82, which is equal to the maximum length of the training sentences.', 'the other configurations are : k = 300, β λ =']","['and 1000 - best list rescoring experiments are conducted as configured in  #TAUTHOR_TAG.', 'the maximum length of trfs is m = 82, which is equal to the maximum length of the training sentences.', 'the other configurations are : k = 300, β λ = 0. 8, β ζ = 0. 6, tc = 3000, t0 = 2000, tmax = 20, 000.', 'l2 regularization with constant 4 ×']","['- best list rescoring experiments are conducted as configured in  #TAUTHOR_TAG.', 'the maximum length of trfs is m = 82, which is equal to the maximum length of the training sentences.', 'the other configurations are : k = 300, β λ =']","['this section, speech recognition and 1000 - best list rescoring experiments are conducted as configured in  #TAUTHOR_TAG.', 'the maximum length of trfs is m = 82, which is equal to the maximum length of the training sentences.', 'the other configurations are : k = 300, β λ = 0. 8, β ζ = 0. 6, tc = 3000, t0 = 2000, tmax = 20, 000.', 'l2 regularization with constant 4 × 10 −5 is used to avoid over - fitting.', '6 cpu cores are used to parallelize the algorithm.', ""the word error rates ( wers ) and perplexities ( ppls ) on wsj'92 test set are shown in tab. 4."", 'the trf lms are compared with the classic kn n - gram lms, the rnn lm [ 3 ] and the results reported in  #TAUTHOR_TAG.', 'compared to the results in  #TAUTHOR_TAG, the improved sa proposed in section 2. 2 gives the same wers but lower ppls in using the same feature types.', 'introducing the tied skip - bigram features further reduce the wer.', 'combining trf and kn5 provides no wer reduction.', 'different schemes give close wers for combining trf and rnn.', 'the log - linear interpolation performs more stable when considering both english and chinese experiments ( as shown later ).', 'for english, the obtained wer 7. 57 % indicates 12. 1 % and 3. 6 % relative reductions, when compared to the result of using kn6 ( 8. 61 % ) and the best result of combining rnn and kn5 ( 7. 85 % ) respectively']",5
"['- best list rescoring experiments are conducted as configured in  #TAUTHOR_TAG.', 'the maximum length of trfs is m = 82, which is equal to the maximum length of the training sentences.', 'the other configurations are : k = 300, β λ =']","['and 1000 - best list rescoring experiments are conducted as configured in  #TAUTHOR_TAG.', 'the maximum length of trfs is m = 82, which is equal to the maximum length of the training sentences.', 'the other configurations are : k = 300, β λ = 0. 8, β ζ = 0. 6, tc = 3000, t0 = 2000, tmax = 20, 000.', 'l2 regularization with constant 4 ×']","['- best list rescoring experiments are conducted as configured in  #TAUTHOR_TAG.', 'the maximum length of trfs is m = 82, which is equal to the maximum length of the training sentences.', 'the other configurations are : k = 300, β λ =']","['this section, speech recognition and 1000 - best list rescoring experiments are conducted as configured in  #TAUTHOR_TAG.', 'the maximum length of trfs is m = 82, which is equal to the maximum length of the training sentences.', 'the other configurations are : k = 300, β λ = 0. 8, β ζ = 0. 6, tc = 3000, t0 = 2000, tmax = 20, 000.', 'l2 regularization with constant 4 × 10 −5 is used to avoid over - fitting.', '6 cpu cores are used to parallelize the algorithm.', ""the word error rates ( wers ) and perplexities ( ppls ) on wsj'92 test set are shown in tab. 4."", 'the trf lms are compared with the classic kn n - gram lms, the rnn lm [ 3 ] and the results reported in  #TAUTHOR_TAG.', 'compared to the results in  #TAUTHOR_TAG, the improved sa proposed in section 2. 2 gives the same wers but lower ppls in using the same feature types.', 'introducing the tied skip - bigram features further reduce the wer.', 'combining trf and kn5 provides no wer reduction.', 'different schemes give close wers for combining trf and rnn.', 'the log - linear interpolation performs more stable when considering both english and chinese experiments ( as shown later ).', 'for english, the obtained wer 7. 57 % indicates 12. 1 % and 3. 6 % relative reductions, when compared to the result of using kn6 ( 8. 61 % ) and the best result of combining rnn and kn5 ( 7. 85 % ) respectively']",7
"['- best list rescoring experiments are conducted as configured in  #TAUTHOR_TAG.', 'the maximum length of trfs is m = 82, which is equal to the maximum length of the training sentences.', 'the other configurations are : k = 300, β λ =']","['and 1000 - best list rescoring experiments are conducted as configured in  #TAUTHOR_TAG.', 'the maximum length of trfs is m = 82, which is equal to the maximum length of the training sentences.', 'the other configurations are : k = 300, β λ = 0. 8, β ζ = 0. 6, tc = 3000, t0 = 2000, tmax = 20, 000.', 'l2 regularization with constant 4 ×']","['- best list rescoring experiments are conducted as configured in  #TAUTHOR_TAG.', 'the maximum length of trfs is m = 82, which is equal to the maximum length of the training sentences.', 'the other configurations are : k = 300, β λ =']","['this section, speech recognition and 1000 - best list rescoring experiments are conducted as configured in  #TAUTHOR_TAG.', 'the maximum length of trfs is m = 82, which is equal to the maximum length of the training sentences.', 'the other configurations are : k = 300, β λ = 0. 8, β ζ = 0. 6, tc = 3000, t0 = 2000, tmax = 20, 000.', 'l2 regularization with constant 4 × 10 −5 is used to avoid over - fitting.', '6 cpu cores are used to parallelize the algorithm.', ""the word error rates ( wers ) and perplexities ( ppls ) on wsj'92 test set are shown in tab. 4."", 'the trf lms are compared with the classic kn n - gram lms, the rnn lm [ 3 ] and the results reported in  #TAUTHOR_TAG.', 'compared to the results in  #TAUTHOR_TAG, the improved sa proposed in section 2. 2 gives the same wers but lower ppls in using the same feature types.', 'introducing the tied skip - bigram features further reduce the wer.', 'combining trf and kn5 provides no wer reduction.', 'different schemes give close wers for combining trf and rnn.', 'the log - linear interpolation performs more stable when considering both english and chinese experiments ( as shown later ).', 'for english, the obtained wer 7. 57 % indicates 12. 1 % and 3. 6 % relative reductions, when compared to the result of using kn6 ( 8. 61 % ) and the best result of combining rnn and kn5 ( 7. 85 % ) respectively']",7
"['.  #TAUTHOR_TAG apply a', 'cnn -']","['.  #TAUTHOR_TAG apply a', 'cnn -']","['.  #TAUTHOR_TAG apply a', 'cnn -']","['', 'a multi - label classification problem to accommodate the cda scenario. qu et al.  #TAUTHOR_TAG apply a', 'cnn - based text classifier proposed by kim [ 8 ] using a', 'fixed window to represent the context. although capable of classifying utterances', ""with cdas, qu et al.  #TAUTHOR_TAG's model only concerns a strictly - local context range and thus cannot include distant information. in this paper, we present a novel neural model that is adapted from convolutional"", '']",7
['recognition  #TAUTHOR_TAG'],['cda recognition  #TAUTHOR_TAG'],"['cda recognition  #TAUTHOR_TAG.', '• cnn - kim [ 8 ] :']","['this section, three versions of our proposed model with incremental improvements are evaluated against a cnn baseline [ 8 ] and the state - of - the - art approach for cda recognition  #TAUTHOR_TAG.', '• cnn - kim [ 8 ] : one of the first attempts to apply cnn to text classification.', 'the cnn model consists of three convolutional layers with the same filter size.', '• cnn - cr  #TAUTHOR_TAG.', 'the cnn model incorporates context information with a window size of 3.', '• crnn ( v 1 ) : our base model that adapts crnn for cda recognition using bce loss and sigmoid activation function.', '• crnn ( v 2 ) : crnn ( v 1 ) with highway connections added between the convolutional layer and the fully connected layer.', '• crnn ( v 3 ) : crnn ( v 1 ) with highway connections and dynamic k - max pooling implemented']",7
"['.  #TAUTHOR_TAG apply a', 'cnn -']","['.  #TAUTHOR_TAG apply a', 'cnn -']","['.  #TAUTHOR_TAG apply a', 'cnn -']","['', 'a multi - label classification problem to accommodate the cda scenario. qu et al.  #TAUTHOR_TAG apply a', 'cnn - based text classifier proposed by kim [ 8 ] using a', 'fixed window to represent the context. although capable of classifying utterances', ""with cdas, qu et al.  #TAUTHOR_TAG's model only concerns a strictly - local context range and thus cannot include distant information. in this paper, we present a novel neural model that is adapted from convolutional"", '']",0
"['.  #TAUTHOR_TAG apply a', 'cnn -']","['.  #TAUTHOR_TAG apply a', 'cnn -']","['.  #TAUTHOR_TAG apply a', 'cnn -']","['', 'a multi - label classification problem to accommodate the cda scenario. qu et al.  #TAUTHOR_TAG apply a', 'cnn - based text classifier proposed by kim [ 8 ] using a', 'fixed window to represent the context. although capable of classifying utterances', ""with cdas, qu et al.  #TAUTHOR_TAG's model only concerns a strictly - local context range and thus cannot include distant information. in this paper, we present a novel neural model that is adapted from convolutional"", '']",1
"['use the msdialog - intent dataset  #TAUTHOR_TAG to conduct experiments.', 'in the dataset, each of the 10,']","['use the msdialog - intent dataset  #TAUTHOR_TAG to conduct experiments.', 'in the dataset, each of the 10, 020 utterances is annotated with a subset of 12 das.', 'the abundance of information in a single']","['use the msdialog - intent dataset  #TAUTHOR_TAG to conduct experiments.', 'in the dataset, each of the 10, 020 utterances is annotated with a subset of 12 das.', 'the abundance of information in a single utterance (']","['use the msdialog - intent dataset  #TAUTHOR_TAG to conduct experiments.', 'in the dataset, each of the 10, 020 utterances is annotated with a subset of 12 das.', 'the abundance of information in a single utterance ( avg. 72 tokens / utterance ) breeds cda ( avg. 1. 83 das / utterance ).', 'we observe a strong correlation between the number of das and utterance length, which necessitates a cda model for forum conversations.', 'the dataset includes plenty of metadata for each utterance, e. g., answer vote and user affiliation.', 'for generalizability, our model only incorporates textual content of the dialogues.', 'besides, unlike qu et al.  #TAUTHOR_TAG, we keep all the da annotations in the dataset to preserve the meaningful da structures within and across utterances.', '']",4
"[' #AUTHOR_TAG b ), dialogue control  #TAUTHOR_TAG, sentiment']","[' #AUTHOR_TAG b ), dialogue control  #TAUTHOR_TAG, sentiment classification, recommendation system,']","[', sentence generation  #AUTHOR_TAG b ), dialogue control  #TAUTHOR_TAG, sentiment classification, recommendation']","['tutorial introduces the advances in deep bayesian learning with abundant applications for natural language understanding ranging from speech recognition  #AUTHOR_TAG to document summarization  #AUTHOR_TAG, text classification  #AUTHOR_TAG, text segmentation  #AUTHOR_TAG, information extraction  #AUTHOR_TAG, image caption generation  #AUTHOR_TAG, sentence generation  #AUTHOR_TAG b ), dialogue control  #TAUTHOR_TAG, sentiment classification, recommendation system, question answering  #AUTHOR_TAG and machine translation, to name a few.', 'traditionally, "" deep learning "" is taken to be a learning process where the inference or optimization is based on the real - valued deterministic model.', 'the "" semantic structure "" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.', 'the "" distribution function "" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.', 'this tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced bayesian models and deep models including hierarchical dirichlet process, chinese restaurant process  #AUTHOR_TAG, hierarchical pitman - yor process  #AUTHOR_TAG, indian buffet process  #AUTHOR_TAG, recurrent neural network  #AUTHOR_TAG, long short - term memory  #AUTHOR_TAG sequence - to - sequence model  #AUTHOR_TAG, variational auto - encoder  #AUTHOR_TAG, generative adversarial network  #AUTHOR_TAG, attention mechanism  #AUTHOR_TAG, memory - augmented neural network  #AUTHOR_TAG, stochastic neural network  #AUTHOR_TAG, predictive state neural network  #AUTHOR_TAG, policy gradient  #AUTHOR_TAG and reinforcement learning  #AUTHOR_TAG.', '']",5
"[' #AUTHOR_TAG b ), dialogue control  #TAUTHOR_TAG, sentiment']","[' #AUTHOR_TAG b ), dialogue control  #TAUTHOR_TAG, sentiment classification, recommendation system,']","[', sentence generation  #AUTHOR_TAG b ), dialogue control  #TAUTHOR_TAG, sentiment classification, recommendation']","['tutorial introduces the advances in deep bayesian learning with abundant applications for natural language understanding ranging from speech recognition  #AUTHOR_TAG to document summarization  #AUTHOR_TAG, text classification  #AUTHOR_TAG, text segmentation  #AUTHOR_TAG, information extraction  #AUTHOR_TAG, image caption generation  #AUTHOR_TAG, sentence generation  #AUTHOR_TAG b ), dialogue control  #TAUTHOR_TAG, sentiment classification, recommendation system, question answering  #AUTHOR_TAG and machine translation, to name a few.', 'traditionally, "" deep learning "" is taken to be a learning process where the inference or optimization is based on the real - valued deterministic model.', 'the "" semantic structure "" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.', 'the "" distribution function "" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.', 'this tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced bayesian models and deep models including hierarchical dirichlet process, chinese restaurant process  #AUTHOR_TAG, hierarchical pitman - yor process  #AUTHOR_TAG, indian buffet process  #AUTHOR_TAG, recurrent neural network  #AUTHOR_TAG, long short - term memory  #AUTHOR_TAG sequence - to - sequence model  #AUTHOR_TAG, variational auto - encoder  #AUTHOR_TAG, generative adversarial network  #AUTHOR_TAG, attention mechanism  #AUTHOR_TAG, memory - augmented neural network  #AUTHOR_TAG, stochastic neural network  #AUTHOR_TAG, predictive state neural network  #AUTHOR_TAG, policy gradient  #AUTHOR_TAG and reinforcement learning  #AUTHOR_TAG.', '']",0
"['1 ],  #TAUTHOR_TAG.', 'these are corpus -']","['with morphologically complex languages like russian [ 1 ],  #TAUTHOR_TAG.', 'these are corpus - based approaches,']","['with morphologically complex languages like russian [ 1 ],  #TAUTHOR_TAG.', 'these are corpus - based approaches,']","['semantic similarity task is an important part of contemporary nlp.', 'it can be applied in many areas, like word sense disambiguation, information retrieval, information extraction and others.', 'it has long history of improvements, starting with simple models, like bag - of - words ( often weighted by tf - idf score ), continuing with more complex ones, like lsa [ 6 ], which attempts to find "" latent "" meanings of words and phrases, and even more abstract models, like nnlm [ 3 ].', 'latest results are based on neural network experience, but are far more simple : various versions of word2vec, skip - gram and cbow models [ 8 ], which currently show the state - of - the - art results and have proven success with morphologically complex languages like russian [ 1 ],  #TAUTHOR_TAG.', 'these are corpus - based approaches, where one computes or trains the model from a large corpus.', 'they usually consider some word context, like in bag - ofwords, where model is simple count of how often can some word be seen in context of a word being described.', 'this model anyhow does not use semantic information.', 'a step in semantic direction was made by lsa, which requires svd transformation of co - occurrence matrix and produces vectors with latent, unknown structure.', '']",0
"['. g. russe workshop  #TAUTHOR_TAG.', 'it was shown that type of corpus used']","['of verbal data.', 'traditional corpora for the word semantic similarity task are news, wikipedia, electronic libraries and others ( e. g. russe workshop  #TAUTHOR_TAG.', 'it was shown that type of corpus used']","['. g. russe workshop  #TAUTHOR_TAG.', 'it was shown that type of corpus used']","['primary goal of this paper is to prove usefulness of russian twitter stream as word semantic similarity resource.', 'twitter is a popular social network 1, or also called "" microblogging service "", which enables users to share and interact with short messages instantly and publicly ( although private accounts are also available ).', 'users all over the world generate hundreds of millions of tweets per day, all over the world, in many languages, generating enormous amount of verbal data.', 'traditional corpora for the word semantic similarity task are news, wikipedia, electronic libraries and others ( e. g. russe workshop  #TAUTHOR_TAG.', 'it was shown that type of corpus used for training affects the resulting accuracy.', 'twitter is not usually considered, and intuition behind this is that probably every - day language is too simple and too occasional to produce good results.', 'on the other hand, the real - time nature of this user message stream seems promising, as it may reveal what certain word means in this given moment.', 'the other counter - argument against twitter - as - dataset is the policy of twitter, which disallows publication of any dump of twitter messages larger than 50k 2.', 'however, this policy permits publication of twitter ids in any amount.', 'thus the secondary goal of this paper is to describe how to create this kind of dataset from scratch.', 'we provide the sample of twitter messages used, as well as set of twitter ids used during experiments 3']",0
"[' #TAUTHOR_TAG was conducted, producing russe human - judgements evaluation dataset ( we will']","['there was no such dataset for russian language.', 'to mitigate this the "" russe : the first workshop on russian semantic similarity ""  #TAUTHOR_TAG was conducted, producing russe human - judgements evaluation dataset ( we will']","['recent days there was no such dataset for russian language.', 'to mitigate this the "" russe : the first workshop on russian semantic similarity ""  #TAUTHOR_TAG was conducted, producing russe human - judgements evaluation dataset ( we will refer to']","['similarity and relatedness task received significant amount of attention.', 'several "" gold standard "" datasets were produced to facilitate the evaluation of algorithms and models, including wordsim353 [ 4 ], rg - 65 [ 11 ] for english language and others.', 'these datasets consist of several pairs of words, where each pair receives a score from human annotators.', 'the score represents the similarity between two words, from 0 % ( not similar ) to 100 % ( identical meaning, words are synonyms ).', 'usually these scores are filled out by a number of human annotators, for instance, 13 in case of wordsim353 4.', 'the inter - annotator agreement is measured and the mean value is put into dataset.', 'until recent days there was no such dataset for russian language.', 'to mitigate this the "" russe : the first workshop on russian semantic similarity ""  #TAUTHOR_TAG was conducted, producing russe human - judgements evaluation dataset ( we will refer to it as hj - dataset ).', 'russe dataset was constructed the following way.', 'firstly, datasets wordsim353, mc [ 9 ] and rg - 65 were combined and translated.', 'then human judgements were obtained by crowdsourcing ( using custom implementation ).', 'final size of the dataset is 333 word pairs, it is available online 5.', 'the russe contest was followed by paper from its organizers  #TAUTHOR_TAG and several participators [ 1 ], [ 5 ], thus filling the gap in word semantic similarity task for russian language.', 'in this paper we evaluate a word2vec model, trained on russian twitter corpus against russe hj - dataset, and show results comparable to top results of other russe competitors']",0
"[' #TAUTHOR_TAG was conducted, producing russe human - judgements evaluation dataset ( we will']","['there was no such dataset for russian language.', 'to mitigate this the "" russe : the first workshop on russian semantic similarity ""  #TAUTHOR_TAG was conducted, producing russe human - judgements evaluation dataset ( we will']","['recent days there was no such dataset for russian language.', 'to mitigate this the "" russe : the first workshop on russian semantic similarity ""  #TAUTHOR_TAG was conducted, producing russe human - judgements evaluation dataset ( we will refer to']","['similarity and relatedness task received significant amount of attention.', 'several "" gold standard "" datasets were produced to facilitate the evaluation of algorithms and models, including wordsim353 [ 4 ], rg - 65 [ 11 ] for english language and others.', 'these datasets consist of several pairs of words, where each pair receives a score from human annotators.', 'the score represents the similarity between two words, from 0 % ( not similar ) to 100 % ( identical meaning, words are synonyms ).', 'usually these scores are filled out by a number of human annotators, for instance, 13 in case of wordsim353 4.', 'the inter - annotator agreement is measured and the mean value is put into dataset.', 'until recent days there was no such dataset for russian language.', 'to mitigate this the "" russe : the first workshop on russian semantic similarity ""  #TAUTHOR_TAG was conducted, producing russe human - judgements evaluation dataset ( we will refer to it as hj - dataset ).', 'russe dataset was constructed the following way.', 'firstly, datasets wordsim353, mc [ 9 ] and rg - 65 were combined and translated.', 'then human judgements were obtained by crowdsourcing ( using custom implementation ).', 'final size of the dataset is 333 word pairs, it is available online 5.', 'the russe contest was followed by paper from its organizers  #TAUTHOR_TAG and several participators [ 1 ], [ 5 ], thus filling the gap in word semantic similarity task for russian language.', 'in this paper we evaluate a word2vec model, trained on russian twitter corpus against russe hj - dataset, and show results comparable to top results of other russe competitors']",0
['was used as accuracy measure in russe  #TAUTHOR_TAG'],['was used as accuracy measure in russe  #TAUTHOR_TAG'],"['the correlation with hj - dataset.', 'to compute correlation we use spearman coefficient, since it was used as accuracy measure in russe  #TAUTHOR_TAG']","['use word2vec to obtain word vectors from twitter corpus.', 'in this model word vectors are initialized randomly for each unique word and are fed to a sort of neural network.', 'authors of word2vec propose two different models : skipgram and cbow.', 'the first one is trained to predict the context of the word given just the word vector itself.', 'the second one is somewhat opposite : it is trained to predict the word vector given its context.', 'in our study cbow always performs worse than skip - gram, hence we describe only results with skip - gram model.', 'those models have several training parameters, namely : vector size, size of vocabulary ( or minimal frequency of a word ), context size, threshold of downsampling, amount of training epochs.', 'we choose vector size based on size of corpus.', 'we use "" context size "" as "" number of tokens before or after current token "".', 'in all experiments presented in this paper we use one training epoch.', 'there are several implementations of word2vec available, including original c utility 10 and a python library gensim 11.', 'we use the latter one as we find it more convenient.', 'output of tomita parser is fed directly line - by - line to the model.', 'it produces the set of vectors, which we then query to obtain similarity between word vectors, in order to compute the correlation with hj - dataset.', 'to compute correlation we use spearman coefficient, since it was used as accuracy measure in russe  #TAUTHOR_TAG']",3
"['corpus.', 'having in mind that best results in russe combine several corpora, it is reasonable to compare twitter results to other single - corpus results.', 'for convenience we replicate results for these corpora, originally presented in  #TAUTHOR_TAG, alongside with our result in table 5.', 'given these considerations we conclude that with size']","['corpus.', 'having in mind that best results in russe combine several corpora, it is reasonable to compare twitter results to other single - corpus results.', 'for convenience we replicate results for these corpora, originally presented in  #TAUTHOR_TAG, alongside with our result in table 5.', 'given these considerations we conclude that with size']","['our twitter corpus.', 'having in mind that best results in russe combine several corpora, it is reasonable to compare twitter results to other single - corpus results.', 'for convenience we replicate results for these corpora, originally presented in  #TAUTHOR_TAG, alongside with our result in table 5.', 'given these considerations we conclude that with size']","['##2vec model was designed to be trained on large corpora.', 'there are results of training it in reasonable time with corpus size of 1 billion of tokens [ 8 ].', 'it was mentioned that accuracy of estimated word vectors improves with size of corpus.', 'twitter provides an enormous amount of data, thus it is a perfect job for word2vec.', 'we fix parameters for the model with following values : vector size of 300, min - freq of 40, context size of 5 and downsampling of 1e - 3.', 'we train our table 3.', 'in this experiment the best result belongs to 7 - day corpus with 0. 56 correlation with hj - dataset, and 15 - day corpus has a little less, 0. 55.', 'this can be explained by following : in order to achieve better results with word2vec one should increase both corpus and vector sizes.', 'indeed, training model with vector size of 600 on full twitter corpus ( 15 days ) shows the best result of 0. 59.', 'it is also worth noting that number of "" missing "" pairs is negligible in 7 - days corpus : the only missing word ( and pair ) is "" иель "", yale, the name of university in the usa.', 'there are no "" missing "" words in 15 - days corpus.', 'training the model on 15 - days corpus took 8 hours on our machine with 2 cores and 4gb of ram.', 'we have an intuition that further improvements are possible with larger corpus.', 'comparing our results to ones reported by russe participants, we conclude that our best result of 0. 598 is comparable to other results, as it ( virtually ) encloses the top - 10 of results.', 'however, best submission of russe has huge gap in accuracy of 0. 16, compared to our twitter corpus.', 'having in mind that best results in russe combine several corpora, it is reasonable to compare twitter results to other single - corpus results.', 'for convenience we replicate results for these corpora, originally presented in  #TAUTHOR_TAG, alongside with our result in table 5.', 'given these considerations we conclude that with size of twitter corpus of 500m one can achieve reasonably good results on task of word semantic similarity']",3
['was used as accuracy measure in russe  #TAUTHOR_TAG'],['was used as accuracy measure in russe  #TAUTHOR_TAG'],"['the correlation with hj - dataset.', 'to compute correlation we use spearman coefficient, since it was used as accuracy measure in russe  #TAUTHOR_TAG']","['use word2vec to obtain word vectors from twitter corpus.', 'in this model word vectors are initialized randomly for each unique word and are fed to a sort of neural network.', 'authors of word2vec propose two different models : skipgram and cbow.', 'the first one is trained to predict the context of the word given just the word vector itself.', 'the second one is somewhat opposite : it is trained to predict the word vector given its context.', 'in our study cbow always performs worse than skip - gram, hence we describe only results with skip - gram model.', 'those models have several training parameters, namely : vector size, size of vocabulary ( or minimal frequency of a word ), context size, threshold of downsampling, amount of training epochs.', 'we choose vector size based on size of corpus.', 'we use "" context size "" as "" number of tokens before or after current token "".', 'in all experiments presented in this paper we use one training epoch.', 'there are several implementations of word2vec available, including original c utility 10 and a python library gensim 11.', 'we use the latter one as we find it more convenient.', 'output of tomita parser is fed directly line - by - line to the model.', 'it produces the set of vectors, which we then query to obtain similarity between word vectors, in order to compute the correlation with hj - dataset.', 'to compute correlation we use spearman coefficient, since it was used as accuracy measure in russe  #TAUTHOR_TAG']",5
"['corpus.', 'having in mind that best results in russe combine several corpora, it is reasonable to compare twitter results to other single - corpus results.', 'for convenience we replicate results for these corpora, originally presented in  #TAUTHOR_TAG, alongside with our result in table 5.', 'given these considerations we conclude that with size']","['corpus.', 'having in mind that best results in russe combine several corpora, it is reasonable to compare twitter results to other single - corpus results.', 'for convenience we replicate results for these corpora, originally presented in  #TAUTHOR_TAG, alongside with our result in table 5.', 'given these considerations we conclude that with size']","['our twitter corpus.', 'having in mind that best results in russe combine several corpora, it is reasonable to compare twitter results to other single - corpus results.', 'for convenience we replicate results for these corpora, originally presented in  #TAUTHOR_TAG, alongside with our result in table 5.', 'given these considerations we conclude that with size']","['##2vec model was designed to be trained on large corpora.', 'there are results of training it in reasonable time with corpus size of 1 billion of tokens [ 8 ].', 'it was mentioned that accuracy of estimated word vectors improves with size of corpus.', 'twitter provides an enormous amount of data, thus it is a perfect job for word2vec.', 'we fix parameters for the model with following values : vector size of 300, min - freq of 40, context size of 5 and downsampling of 1e - 3.', 'we train our table 3.', 'in this experiment the best result belongs to 7 - day corpus with 0. 56 correlation with hj - dataset, and 15 - day corpus has a little less, 0. 55.', 'this can be explained by following : in order to achieve better results with word2vec one should increase both corpus and vector sizes.', 'indeed, training model with vector size of 600 on full twitter corpus ( 15 days ) shows the best result of 0. 59.', 'it is also worth noting that number of "" missing "" pairs is negligible in 7 - days corpus : the only missing word ( and pair ) is "" иель "", yale, the name of university in the usa.', 'there are no "" missing "" words in 15 - days corpus.', 'training the model on 15 - days corpus took 8 hours on our machine with 2 cores and 4gb of ram.', 'we have an intuition that further improvements are possible with larger corpus.', 'comparing our results to ones reported by russe participants, we conclude that our best result of 0. 598 is comparable to other results, as it ( virtually ) encloses the top - 10 of results.', 'however, best submission of russe has huge gap in accuracy of 0. 16, compared to our twitter corpus.', 'having in mind that best results in russe combine several corpora, it is reasonable to compare twitter results to other single - corpus results.', 'for convenience we replicate results for these corpora, originally presented in  #TAUTHOR_TAG, alongside with our result in table 5.', 'given these considerations we conclude that with size of twitter corpus of 500m one can achieve reasonably good results on task of word semantic similarity']",5
"['task of word semantic similarity.', 'we described a method to obtain stream of twitter messages and prepare them for training.', 'we use hj - dataset, which was created for russe contest  #TAUTHOR_TAG to measure correlation between']","['task of word semantic similarity.', 'we described a method to obtain stream of twitter messages and prepare them for training.', 'we use hj - dataset, which was created for russe contest  #TAUTHOR_TAG to measure correlation between']","['task of word semantic similarity.', 'we described a method to obtain stream of twitter messages and prepare them for training.', 'we use hj - dataset, which was created for russe contest  #TAUTHOR_TAG to measure correlation between']","['this paper we investigated the use of twitter corpus for training word2vec model for task of word semantic similarity.', 'we described a method to obtain stream of twitter messages and prepare them for training.', 'we use hj - dataset, which was created for russe contest  #TAUTHOR_TAG to measure correlation between similarity of word vectors and human judgements on word pairs similarity.', 'we achieve results comparable with results obtained while training word2vec on traditional corpora, like wikipedia and web pages [ 1 ], [ 5 ].', 'this is especially important because twitter data is highly dynamic, and traditional sources are mostly static ( rarely change over time ).', 'thus verbal data acquired from twitter may be used to estimate word vectors for neologisms, or determine other changes in word semantic, as soon as they appear in human speech']",5
"['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['instances. for example, the amrs corresponding to examples ( 1 ) and ( 2 ) above are', 'given in figure 1. note that, due to the bracketing, the variable b encapsulates the whole entity person : name "" bob "" and not just person, i. e. b stands', 'for a person with the name bob. that there is a lot to gain in this area can be seen by applying the amr evaluation suite of  #AUTHOR_TAG, which calculates nine different metrics to evaluate amr parsing', ', reentrancy being one of them. out of', 'the four systems that made these scores available (', 'all scores reported in  #TAUTHOR_TAG, the reentrancy metric', 'obtained the lowest f - score for three of them. various methods have been proposed to automatically parse amrs, ranging from syntax - based approaches ( e. g.  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ) to the more recent neural approaches (  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #TAUTHOR_TAG. especially the neural approaches are interesting,', 'since they all use some sort of linearization method and therefore need a predefined way to handle reentrancy.  #AUTHOR_TAG and  #AUTHOR_TAG use a special character to indicate reentrancy and restore co - referring variables in a', 'post - processing step.  #AUTHOR_TAG simply replace reentrancy variables by their co - referring concept in the input and never outputs co - referring nodes.  #AUTHOR_TAG and  #TAUTHOR_TAG use', 'the same input transformation as  #AUTHOR_TAG, but do try to restore co - referring nodes by merging all equal concepts into a single concept in a post - processing step. all these methods have in common that they are not very sophisticated, but more', 'importantly, that it is not clear what the exact impact of these methods is on the final performance of the model, making it unclear what the best', '']",0
"['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['instances. for example, the amrs corresponding to examples ( 1 ) and ( 2 ) above are', 'given in figure 1. note that, due to the bracketing, the variable b encapsulates the whole entity person : name "" bob "" and not just person, i. e. b stands', 'for a person with the name bob. that there is a lot to gain in this area can be seen by applying the amr evaluation suite of  #AUTHOR_TAG, which calculates nine different metrics to evaluate amr parsing', ', reentrancy being one of them. out of', 'the four systems that made these scores available (', 'all scores reported in  #TAUTHOR_TAG, the reentrancy metric', 'obtained the lowest f - score for three of them. various methods have been proposed to automatically parse amrs, ranging from syntax - based approaches ( e. g.  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ) to the more recent neural approaches (  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #TAUTHOR_TAG. especially the neural approaches are interesting,', 'since they all use some sort of linearization method and therefore need a predefined way to handle reentrancy.  #AUTHOR_TAG and  #AUTHOR_TAG use a special character to indicate reentrancy and restore co - referring variables in a', 'post - processing step.  #AUTHOR_TAG simply replace reentrancy variables by their co - referring concept in the input and never outputs co - referring nodes.  #AUTHOR_TAG and  #TAUTHOR_TAG use', 'the same input transformation as  #AUTHOR_TAG, but do try to restore co - referring nodes by merging all equal concepts into a single concept in a post - processing step. all these methods have in common that they are not very sophisticated, but more', 'importantly, that it is not clear what the exact impact of these methods is on the final performance of the model, making it unclear what the best', '']",0
"['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['instances. for example, the amrs corresponding to examples ( 1 ) and ( 2 ) above are', 'given in figure 1. note that, due to the bracketing, the variable b encapsulates the whole entity person : name "" bob "" and not just person, i. e. b stands', 'for a person with the name bob. that there is a lot to gain in this area can be seen by applying the amr evaluation suite of  #AUTHOR_TAG, which calculates nine different metrics to evaluate amr parsing', ', reentrancy being one of them. out of', 'the four systems that made these scores available (', 'all scores reported in  #TAUTHOR_TAG, the reentrancy metric', 'obtained the lowest f - score for three of them. various methods have been proposed to automatically parse amrs, ranging from syntax - based approaches ( e. g.  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ) to the more recent neural approaches (  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #TAUTHOR_TAG. especially the neural approaches are interesting,', 'since they all use some sort of linearization method and therefore need a predefined way to handle reentrancy.  #AUTHOR_TAG and  #AUTHOR_TAG use a special character to indicate reentrancy and restore co - referring variables in a', 'post - processing step.  #AUTHOR_TAG simply replace reentrancy variables by their co - referring concept in the input and never outputs co - referring nodes.  #AUTHOR_TAG and  #TAUTHOR_TAG use', 'the same input transformation as  #AUTHOR_TAG, but do try to restore co - referring nodes by merging all equal concepts into a single concept in a post - processing step. all these methods have in common that they are not very sophisticated, but more', 'importantly, that it is not clear what the exact impact of these methods is on the final performance of the model, making it unclear what the best', '']",0
"['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['instances. for example, the amrs corresponding to examples ( 1 ) and ( 2 ) above are', 'given in figure 1. note that, due to the bracketing, the variable b encapsulates the whole entity person : name "" bob "" and not just person, i. e. b stands', 'for a person with the name bob. that there is a lot to gain in this area can be seen by applying the amr evaluation suite of  #AUTHOR_TAG, which calculates nine different metrics to evaluate amr parsing', ', reentrancy being one of them. out of', 'the four systems that made these scores available (', 'all scores reported in  #TAUTHOR_TAG, the reentrancy metric', 'obtained the lowest f - score for three of them. various methods have been proposed to automatically parse amrs, ranging from syntax - based approaches ( e. g.  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ) to the more recent neural approaches (  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #TAUTHOR_TAG. especially the neural approaches are interesting,', 'since they all use some sort of linearization method and therefore need a predefined way to handle reentrancy.  #AUTHOR_TAG and  #AUTHOR_TAG use a special character to indicate reentrancy and restore co - referring variables in a', 'post - processing step.  #AUTHOR_TAG simply replace reentrancy variables by their co - referring concept in the input and never outputs co - referring nodes.  #AUTHOR_TAG and  #TAUTHOR_TAG use', 'the same input transformation as  #AUTHOR_TAG, but do try to restore co - referring nodes by merging all equal concepts into a single concept in a post - processing step. all these methods have in common that they are not very sophisticated, but more', 'importantly, that it is not clear what the exact impact of these methods is on the final performance of the model, making it unclear what the best', '']",0
"['our earlier models, described in  #TAUTHOR_TAG.', 'the first is']","['our earlier models, described in  #TAUTHOR_TAG.', 'the first is']","['our earlier models, described in  #TAUTHOR_TAG.', 'the first is a simple baseline model that only takes the characters into account without any additional methods to improve performance.', 'this model is referred to']","['test the impact of the different methods on two of our earlier models, described in  #TAUTHOR_TAG.', 'the first is a simple baseline model that only takes the characters into account without any additional methods to improve performance.', 'this model is referred to as the char - only model.', 'the second is the approach that produced one of the best results so far in the literature.', 'this model uses pos - tagged input, clusters together groups of characters ( super characters ) and exploits 100, 000 "" silver "" amrs that were obtained by using the off - the - shelf amr parsers camr  #AUTHOR_TAG and jamr  #AUTHOR_TAG 4.', 'the added amrs are all camr - produced.', 'we must note that camr is not particularly keen on outputting coreference, as the 100, 000 silver amrs only produced 18, 865 new reentrancy nodes.', 'the second approach also employs the postprocessing methods wikification and pruning, as explained in  #TAUTHOR_TAG.', 'the wikification step simply adds wiki links to : name nodes, since those links were removed in the input.', 'pruning is used to remove erroneously produced duplicate output.', 'this is a common problem for sequence - to - sequence models, since the model does not keep track of what it has already output.', 'no pre - training or ensemble methods are used for both approaches']",0
"['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['instances. for example, the amrs corresponding to examples ( 1 ) and ( 2 ) above are', 'given in figure 1. note that, due to the bracketing, the variable b encapsulates the whole entity person : name "" bob "" and not just person, i. e. b stands', 'for a person with the name bob. that there is a lot to gain in this area can be seen by applying the amr evaluation suite of  #AUTHOR_TAG, which calculates nine different metrics to evaluate amr parsing', ', reentrancy being one of them. out of', 'the four systems that made these scores available (', 'all scores reported in  #TAUTHOR_TAG, the reentrancy metric', 'obtained the lowest f - score for three of them. various methods have been proposed to automatically parse amrs, ranging from syntax - based approaches ( e. g.  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ) to the more recent neural approaches (  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #TAUTHOR_TAG. especially the neural approaches are interesting,', 'since they all use some sort of linearization method and therefore need a predefined way to handle reentrancy.  #AUTHOR_TAG and  #AUTHOR_TAG use a special character to indicate reentrancy and restore co - referring variables in a', 'post - processing step.  #AUTHOR_TAG simply replace reentrancy variables by their co - referring concept in the input and never outputs co - referring nodes.  #AUTHOR_TAG and  #TAUTHOR_TAG use', 'the same input transformation as  #AUTHOR_TAG, but do try to restore co - referring nodes by merging all equal concepts into a single concept in a post - processing step. all these methods have in common that they are not very sophisticated, but more', 'importantly, that it is not clear what the exact impact of these methods is on the final performance of the model, making it unclear what the best', '']",1
"['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['instances. for example, the amrs corresponding to examples ( 1 ) and ( 2 ) above are', 'given in figure 1. note that, due to the bracketing, the variable b encapsulates the whole entity person : name "" bob "" and not just person, i. e. b stands', 'for a person with the name bob. that there is a lot to gain in this area can be seen by applying the amr evaluation suite of  #AUTHOR_TAG, which calculates nine different metrics to evaluate amr parsing', ', reentrancy being one of them. out of', 'the four systems that made these scores available (', 'all scores reported in  #TAUTHOR_TAG, the reentrancy metric', 'obtained the lowest f - score for three of them. various methods have been proposed to automatically parse amrs, ranging from syntax - based approaches ( e. g.  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ) to the more recent neural approaches (  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #TAUTHOR_TAG. especially the neural approaches are interesting,', 'since they all use some sort of linearization method and therefore need a predefined way to handle reentrancy.  #AUTHOR_TAG and  #AUTHOR_TAG use a special character to indicate reentrancy and restore co - referring variables in a', 'post - processing step.  #AUTHOR_TAG simply replace reentrancy variables by their co - referring concept in the input and never outputs co - referring nodes.  #AUTHOR_TAG and  #TAUTHOR_TAG use', 'the same input transformation as  #AUTHOR_TAG, but do try to restore co - referring nodes by merging all equal concepts into a single concept in a post - processing step. all these methods have in common that they are not very sophisticated, but more', 'importantly, that it is not clear what the exact impact of these methods is on the final performance of the model, making it unclear what the best', '']",1
"['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['these scores available (', 'all scores reported in  #TAUTHOR_TAG,']","['instances. for example, the amrs corresponding to examples ( 1 ) and ( 2 ) above are', 'given in figure 1. note that, due to the bracketing, the variable b encapsulates the whole entity person : name "" bob "" and not just person, i. e. b stands', 'for a person with the name bob. that there is a lot to gain in this area can be seen by applying the amr evaluation suite of  #AUTHOR_TAG, which calculates nine different metrics to evaluate amr parsing', ', reentrancy being one of them. out of', 'the four systems that made these scores available (', 'all scores reported in  #TAUTHOR_TAG, the reentrancy metric', 'obtained the lowest f - score for three of them. various methods have been proposed to automatically parse amrs, ranging from syntax - based approaches ( e. g.  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ) to the more recent neural approaches (  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #TAUTHOR_TAG. especially the neural approaches are interesting,', 'since they all use some sort of linearization method and therefore need a predefined way to handle reentrancy.  #AUTHOR_TAG and  #AUTHOR_TAG use a special character to indicate reentrancy and restore co - referring variables in a', 'post - processing step.  #AUTHOR_TAG simply replace reentrancy variables by their co - referring concept in the input and never outputs co - referring nodes.  #AUTHOR_TAG and  #TAUTHOR_TAG use', 'the same input transformation as  #AUTHOR_TAG, but do try to restore co - referring nodes by merging all equal concepts into a single concept in a post - processing step. all these methods have in common that they are not very sophisticated, but more', 'importantly, that it is not clear what the exact impact of these methods is on the final performance of the model, making it unclear what the best', '']",5
,,,,5
"['implement a bidirectional sequence - to - sequence model with general attention that takes characters as input, using the opennmt software  #AUTHOR_TAG.', 'the parameter settings are the same as in  #TAUTHOR_TAG and are shown in table 2.', 'it is trained for 20 epochs, after which the model that performs best on the development set is used to decode the test set']","['implement a bidirectional sequence - to - sequence model with general attention that takes characters as input, using the opennmt software  #AUTHOR_TAG.', 'the parameter settings are the same as in  #TAUTHOR_TAG and are shown in table 2.', 'it is trained for 20 epochs, after which the model that performs best on the development set is used to decode the test set']","['implement a bidirectional sequence - to - sequence model with general attention that takes characters as input, using the opennmt software  #AUTHOR_TAG.', 'the parameter settings are the same as in  #TAUTHOR_TAG and are shown in table 2.', 'it is trained for 20 epochs, after which the model that performs best on the development set is used to decode the test set']","['implement a bidirectional sequence - to - sequence model with general attention that takes characters as input, using the opennmt software  #AUTHOR_TAG.', 'the parameter settings are the same as in  #TAUTHOR_TAG and are shown in table 2.', 'it is trained for 20 epochs, after which the model that performs best on the development set is used to decode the test set']",5
"['our earlier models, described in  #TAUTHOR_TAG.', 'the first is']","['our earlier models, described in  #TAUTHOR_TAG.', 'the first is']","['our earlier models, described in  #TAUTHOR_TAG.', 'the first is a simple baseline model that only takes the characters into account without any additional methods to improve performance.', 'this model is referred to']","['test the impact of the different methods on two of our earlier models, described in  #TAUTHOR_TAG.', 'the first is a simple baseline model that only takes the characters into account without any additional methods to improve performance.', 'this model is referred to as the char - only model.', 'the second is the approach that produced one of the best results so far in the literature.', 'this model uses pos - tagged input, clusters together groups of characters ( super characters ) and exploits 100, 000 "" silver "" amrs that were obtained by using the off - the - shelf amr parsers camr  #AUTHOR_TAG and jamr  #AUTHOR_TAG 4.', 'the added amrs are all camr - produced.', 'we must note that camr is not particularly keen on outputting coreference, as the 100, 000 silver amrs only produced 18, 865 new reentrancy nodes.', 'the second approach also employs the postprocessing methods wikification and pruning, as explained in  #TAUTHOR_TAG.', 'the wikification step simply adds wiki links to : name nodes, since those links were removed in the input.', 'pruning is used to remove erroneously produced duplicate output.', 'this is a common problem for sequence - to - sequence models, since the model does not keep track of what it has already output.', 'no pre - training or ensemble methods are used for both approaches']",5
"['our earlier models, described in  #TAUTHOR_TAG.', 'the first is']","['our earlier models, described in  #TAUTHOR_TAG.', 'the first is']","['our earlier models, described in  #TAUTHOR_TAG.', 'the first is a simple baseline model that only takes the characters into account without any additional methods to improve performance.', 'this model is referred to']","['test the impact of the different methods on two of our earlier models, described in  #TAUTHOR_TAG.', 'the first is a simple baseline model that only takes the characters into account without any additional methods to improve performance.', 'this model is referred to as the char - only model.', 'the second is the approach that produced one of the best results so far in the literature.', 'this model uses pos - tagged input, clusters together groups of characters ( super characters ) and exploits 100, 000 "" silver "" amrs that were obtained by using the off - the - shelf amr parsers camr  #AUTHOR_TAG and jamr  #AUTHOR_TAG 4.', 'the added amrs are all camr - produced.', 'we must note that camr is not particularly keen on outputting coreference, as the 100, 000 silver amrs only produced 18, 865 new reentrancy nodes.', 'the second approach also employs the postprocessing methods wikification and pruning, as explained in  #TAUTHOR_TAG.', 'the wikification step simply adds wiki links to : name nodes, since those links were removed in the input.', 'pruning is used to remove erroneously produced duplicate output.', 'this is a common problem for sequence - to - sequence models, since the model does not keep track of what it has already output.', 'no pre - training or ensemble methods are used for both approaches']",5
['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],"['used the dataset provided by  #TAUTHOR_TAG to extract the othering lexicon content.', 'they used a crowdsourced hate speech lexicon to collect tweets containing different types of hate speech keywords.', 'they used crowdsourcing to divide a sample of these tweets into three categories : those containing hate speech, those with only offensive language, and those with neither.', 'their dataset contains different samples of different hate speech types.', 'we extracted the samples that contain hate speech or offensive language and then from the extracted samples we extracted those that only contain two - sided othering language, which resulted in 674 tweets.', 'those tweets are used for extracting the verbs, nouns and adjectives related to othering terms in hate speech for lexicon construction.', 'in addition to this sample, we extracted all nouns and adjectives that were mentioned in the whole hateful samples.', 'for testing, to demonstrate an improvement on the state of the art, we used four datasets from previous work [ 6 ].', ""the datasets were collected from twitter and annotated using the crowdflower human intelligence task service with a single question :'is this text antagonistic or hateful based on a protected characteristic?'the datasets comprise four different protected characteristics, as follows : sexual orientation aas 1803 tweets, with 183 instances of offensive or antagonistic content ( 10. 15 % of the annotated sample ) ; race 1876 tweets, with 70 instances of offensive or antagonistic content ( 3. 73 % of the annotated sample ) ; disability 1914 tweets, with 51 instances of offensive or antagonistic content ( 2. 66 % of the annotated sample ) ; and religion 1901 tweets, with 222 instances of offensive or antagonistic content ( 11. 68 % of the annotated sample )."", 'the authors conducted all of the necessary tests so as to ensure agreement between annotators for the gold - standard samples [ 6 ].', 'the amount of abusive or hate instances is small relative to the size of the sample.', 'however, these are random instances of the full datasets for each event and they are considered representative of the overall levels of cyberhate within the corpus of tweets']",3
['from negative samples of an annotated dataset  #TAUTHOR_TAG'],['from negative samples of an annotated dataset  #TAUTHOR_TAG'],"['##bj, det, compound ) > < pos ( vb, nn, jj ) > our lexicon content was extracted from negative samples of an annotated dataset  #TAUTHOR_TAG.', 'from each instance in the negative dataset we constitute one row in the lexicon, with']","['on the previous findings in section 3. 1, we evaluate the effectiveness of the othering language as features in hate speech detection by implementing a classification based on training in a negative lexicon.', 'as the othering language might appear in the benign and negative language, we construct the following basic expression in order to build a lexicon, with each row of the lexicon being as follows.', 'the lexicon assumes that two - sided pronouns appear with a specific pattern and offensive words in each row.', 'this row is then converted to vector space using the paragraph2vec algorithm : < othering terms permutation > < othering pattern > < hateful words > each part was extracted as following : < all two - sided pronouns > < dependency ( nsubj, dobj, det, compound ) > < pos ( vb, nn, jj ) > our lexicon content was extracted from negative samples of an annotated dataset  #TAUTHOR_TAG.', 'from each instance in the negative dataset we constitute one row in the lexicon, with each row in the lexicon containing three columns : ( 1 ) all combinations of two - sided english pronouns, ( 2 ) the othering pronoun - related patterns which were extracted using typed dependency parser, and ( 3 ) the negative words which were extracted by pos 3 tagging [ 9 ].', '']",3
['by training in the positive samples of the  #TAUTHOR_TAG'],"['. this method has previously been used for building', 'machine classifiers for short text [ 37 ]. in addition to that, we implemented semi - supervised classification by training in the positive samples of the  #TAUTHOR_TAG']",['by training in the positive samples of the  #TAUTHOR_TAG'],"['##k the contexts which are produced by using k windows size as k words before and the k', 'words after w. for example, for k = 2, the contexts of the target word comprise w - 2, w - 1,', 'w + 1, w + 2. in fact, we cannot assign a meaning to any particular dimension. we observed the different results being captured by the model by examining which dimension behaved better in which dataset. we report results for 600 dimension embeddings', 'and windows = 2, though similar trends were also observed with 100, 300, 800 and 1000 dimensions and windows = 3, 5, 6 and 10. we recorded the best performance for 600 dimensions and windows = 2 parameters. the final output is a vectorised dataset that is used as a feature set for developing a machine classification approach. 5. 3', 'machine classification ten - fold cross - validation was used due to the', 'volume of positive and negative in - stances for supervised classification. this method has previously been used for building', 'machine classifiers for short text [ 37 ]. in addition to that, we implemented semi - supervised classification by training in the positive samples of the  #TAUTHOR_TAG dataset and training in only the lexicon as negative', 'samples. the model was evaluated by testing it on the four datasets :', 'religion, disability, racism and sexual - orientation. two classifiers were applied : multilayer perception ( mlp ) and', 'logistic regression ( lr ). multilayer perceptron ( mlp ) is a feed - forward artificial neural network model which maps input datasets on an appropriate set', 'of outputs. mlp consists of multiple layers of nodes in a directed graph, with each', 'layer being fully connected to the next layer [ 13 ]. non - linear mlp classifiers work better with our vectorised features. two - layer mlp achieved better', 'performance for our vectors with 200 iterations. an lr model is a simplified model on which mlp is based. we also', 'implemented this model to test the potential for a much less computationally intensive approach to classification,', 'possibly something that could be utilised in real time to classify content in streamed data. for ten - fold cross - validation, we', 'learned the vector space of the negative samples and the othering lexicon ( as a negative feature set ) jointly with positive samples for supervised', 'learning ( see figure 5 ). for semi - supervised learning,', 'when we tested our lexicon on unseen datasets, we trained the embedding', 'algorithm in our othering lexicon as negative and the positive samples of', 'the  #TAUTHOR_TAG dataset ( see figure 9 ). then we labelled each instance individually, assigning the positive and negative labels as 0 and 1 respectively']",3
['by training in the positive samples of the  #TAUTHOR_TAG'],"['. this method has previously been used for building', 'machine classifiers for short text [ 37 ]. in addition to that, we implemented semi - supervised classification by training in the positive samples of the  #TAUTHOR_TAG']",['by training in the positive samples of the  #TAUTHOR_TAG'],"['##k the contexts which are produced by using k windows size as k words before and the k', 'words after w. for example, for k = 2, the contexts of the target word comprise w - 2, w - 1,', 'w + 1, w + 2. in fact, we cannot assign a meaning to any particular dimension. we observed the different results being captured by the model by examining which dimension behaved better in which dataset. we report results for 600 dimension embeddings', 'and windows = 2, though similar trends were also observed with 100, 300, 800 and 1000 dimensions and windows = 3, 5, 6 and 10. we recorded the best performance for 600 dimensions and windows = 2 parameters. the final output is a vectorised dataset that is used as a feature set for developing a machine classification approach. 5. 3', 'machine classification ten - fold cross - validation was used due to the', 'volume of positive and negative in - stances for supervised classification. this method has previously been used for building', 'machine classifiers for short text [ 37 ]. in addition to that, we implemented semi - supervised classification by training in the positive samples of the  #TAUTHOR_TAG dataset and training in only the lexicon as negative', 'samples. the model was evaluated by testing it on the four datasets :', 'religion, disability, racism and sexual - orientation. two classifiers were applied : multilayer perception ( mlp ) and', 'logistic regression ( lr ). multilayer perceptron ( mlp ) is a feed - forward artificial neural network model which maps input datasets on an appropriate set', 'of outputs. mlp consists of multiple layers of nodes in a directed graph, with each', 'layer being fully connected to the next layer [ 13 ]. non - linear mlp classifiers work better with our vectorised features. two - layer mlp achieved better', 'performance for our vectors with 200 iterations. an lr model is a simplified model on which mlp is based. we also', 'implemented this model to test the potential for a much less computationally intensive approach to classification,', 'possibly something that could be utilised in real time to classify content in streamed data. for ten - fold cross - validation, we', 'learned the vector space of the negative samples and the othering lexicon ( as a negative feature set ) jointly with positive samples for supervised', 'learning ( see figure 5 ). for semi - supervised learning,', 'when we tested our lexicon on unseen datasets, we trained the embedding', 'algorithm in our othering lexicon as negative and the positive samples of', 'the  #TAUTHOR_TAG dataset ( see figure 9 ). then we labelled each instance individually, assigning the positive and negative labels as 0 and 1 respectively']",3
['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],"['used the dataset provided by  #TAUTHOR_TAG to extract the othering lexicon content.', 'they used a crowdsourced hate speech lexicon to collect tweets containing different types of hate speech keywords.', 'they used crowdsourcing to divide a sample of these tweets into three categories : those containing hate speech, those with only offensive language, and those with neither.', 'their dataset contains different samples of different hate speech types.', 'we extracted the samples that contain hate speech or offensive language and then from the extracted samples we extracted those that only contain two - sided othering language, which resulted in 674 tweets.', 'those tweets are used for extracting the verbs, nouns and adjectives related to othering terms in hate speech for lexicon construction.', 'in addition to this sample, we extracted all nouns and adjectives that were mentioned in the whole hateful samples.', 'for testing, to demonstrate an improvement on the state of the art, we used four datasets from previous work [ 6 ].', ""the datasets were collected from twitter and annotated using the crowdflower human intelligence task service with a single question :'is this text antagonistic or hateful based on a protected characteristic?'the datasets comprise four different protected characteristics, as follows : sexual orientation aas 1803 tweets, with 183 instances of offensive or antagonistic content ( 10. 15 % of the annotated sample ) ; race 1876 tweets, with 70 instances of offensive or antagonistic content ( 3. 73 % of the annotated sample ) ; disability 1914 tweets, with 51 instances of offensive or antagonistic content ( 2. 66 % of the annotated sample ) ; and religion 1901 tweets, with 222 instances of offensive or antagonistic content ( 11. 68 % of the annotated sample )."", 'the authors conducted all of the necessary tests so as to ensure agreement between annotators for the gold - standard samples [ 6 ].', 'the amount of abusive or hate instances is small relative to the size of the sample.', 'however, these are random instances of the full datasets for each event and they are considered representative of the overall levels of cyberhate within the corpus of tweets']",5
"['pronoun permutations.', 'the dataset was provided by  #TAUTHOR_TAG.', ""an example of the previous method for the tweet '""]","['pronoun permutations.', 'the dataset was provided by  #TAUTHOR_TAG.', ""an example of the previous method for the tweet'we want""]","['of the two - sided pronoun permutations.', 'the dataset was provided by  #TAUTHOR_TAG.', ""an example of the previous method for the tweet '""]","['study discovers the effectiveness of a text portion which somehow could be considered a hate pointer, which is using two - sided othering pronouns.', 'to leverage the use of the two - sided othering terms as a new feature in hate speech classification, two core steps were implemented to build our model : ( a ) building the othering lexicon and ( b ) learning the vector space embedding of the lexicon jointly with the negative and benign samples.', 'the othering lexicon contains the othering terms and their patterns extracted by a dependency parser, as well as the verbs, nouns and adjectives that appear in all of the negative samples extracted by the pos tagger and all of the two - sided pronoun permutations.', 'the dataset was provided by  #TAUTHOR_TAG.', ""an example of the previous method for the tweet'we want to send them all home'contains two - sided pronouns and the verb'send ', which grammatically relates to the pronoun'them'as a subjective clause."", ""even though the verb'send'is not offensive, we include it in the lexicon."", '']",5
['from negative samples of an annotated dataset  #TAUTHOR_TAG'],['from negative samples of an annotated dataset  #TAUTHOR_TAG'],"['##bj, det, compound ) > < pos ( vb, nn, jj ) > our lexicon content was extracted from negative samples of an annotated dataset  #TAUTHOR_TAG.', 'from each instance in the negative dataset we constitute one row in the lexicon, with']","['on the previous findings in section 3. 1, we evaluate the effectiveness of the othering language as features in hate speech detection by implementing a classification based on training in a negative lexicon.', 'as the othering language might appear in the benign and negative language, we construct the following basic expression in order to build a lexicon, with each row of the lexicon being as follows.', 'the lexicon assumes that two - sided pronouns appear with a specific pattern and offensive words in each row.', 'this row is then converted to vector space using the paragraph2vec algorithm : < othering terms permutation > < othering pattern > < hateful words > each part was extracted as following : < all two - sided pronouns > < dependency ( nsubj, dobj, det, compound ) > < pos ( vb, nn, jj ) > our lexicon content was extracted from negative samples of an annotated dataset  #TAUTHOR_TAG.', 'from each instance in the negative dataset we constitute one row in the lexicon, with each row in the lexicon containing three columns : ( 1 ) all combinations of two - sided english pronouns, ( 2 ) the othering pronoun - related patterns which were extracted using typed dependency parser, and ( 3 ) the negative words which were extracted by pos 3 tagging [ 9 ].', '']",5
['by training in the positive samples of the  #TAUTHOR_TAG'],"['. this method has previously been used for building', 'machine classifiers for short text [ 37 ]. in addition to that, we implemented semi - supervised classification by training in the positive samples of the  #TAUTHOR_TAG']",['by training in the positive samples of the  #TAUTHOR_TAG'],"['##k the contexts which are produced by using k windows size as k words before and the k', 'words after w. for example, for k = 2, the contexts of the target word comprise w - 2, w - 1,', 'w + 1, w + 2. in fact, we cannot assign a meaning to any particular dimension. we observed the different results being captured by the model by examining which dimension behaved better in which dataset. we report results for 600 dimension embeddings', 'and windows = 2, though similar trends were also observed with 100, 300, 800 and 1000 dimensions and windows = 3, 5, 6 and 10. we recorded the best performance for 600 dimensions and windows = 2 parameters. the final output is a vectorised dataset that is used as a feature set for developing a machine classification approach. 5. 3', 'machine classification ten - fold cross - validation was used due to the', 'volume of positive and negative in - stances for supervised classification. this method has previously been used for building', 'machine classifiers for short text [ 37 ]. in addition to that, we implemented semi - supervised classification by training in the positive samples of the  #TAUTHOR_TAG dataset and training in only the lexicon as negative', 'samples. the model was evaluated by testing it on the four datasets :', 'religion, disability, racism and sexual - orientation. two classifiers were applied : multilayer perception ( mlp ) and', 'logistic regression ( lr ). multilayer perceptron ( mlp ) is a feed - forward artificial neural network model which maps input datasets on an appropriate set', 'of outputs. mlp consists of multiple layers of nodes in a directed graph, with each', 'layer being fully connected to the next layer [ 13 ]. non - linear mlp classifiers work better with our vectorised features. two - layer mlp achieved better', 'performance for our vectors with 200 iterations. an lr model is a simplified model on which mlp is based. we also', 'implemented this model to test the potential for a much less computationally intensive approach to classification,', 'possibly something that could be utilised in real time to classify content in streamed data. for ten - fold cross - validation, we', 'learned the vector space of the negative samples and the othering lexicon ( as a negative feature set ) jointly with positive samples for supervised', 'learning ( see figure 5 ). for semi - supervised learning,', 'when we tested our lexicon on unseen datasets, we trained the embedding', 'algorithm in our othering lexicon as negative and the positive samples of', 'the  #TAUTHOR_TAG dataset ( see figure 9 ). then we labelled each instance individually, assigning the positive and negative labels as 0 and 1 respectively']",5
"['extracting specific content ( verbs, nouns and adjectives ) for lexicon building.', ' #TAUTHOR_TAG prepared their data by stemming and then creating unigram, bigram and trigram features,']","['extracting specific content ( verbs, nouns and adjectives ) for lexicon building.', ' #TAUTHOR_TAG prepared their data by stemming and then creating unigram, bigram and trigram features,']","['extracting specific content ( verbs, nouns and adjectives ) for lexicon building.', ' #TAUTHOR_TAG prepared their data by stemming and then creating unigram, bigram and trigram features,']","['of the most basic forms of natural language processing is the bag of words ( bow ) feature extraction approach.', 'bow has been successfully applied as a feature extraction method for the automated detection of hate speech, relying largely on keywords relating to offence and antagonism [ 5, 29, 42 ].', ""however, the method also suffers from a high rate of false positives, since the presence of hateful words can lead to the misclassification of tweets being hateful when they are used in a different context ( e. g.'black') [ 16 ]."", 'similar to bow, n - grams consecutive words of varying sizes ( from1... n ) have been used to improve the accuracy of hate speech classification [ 14 ] [? ] by capturing context within a sentence that is lost in the bow model [ 3, 7, 28, 41 ].', '[ 41 ] found that character n - grams have been shown to be appropriate for abusive language tasks due to their ability to capture variations of words associated with hate.', 'in general, while previous studies addressed the difficulty of the definition of hateful language, their experiments led to better results when combined with a large set of features such as bow. they showed that bow, n - grams, part - ofspeech tagging, and data preprocessing ( stop word / punctuation removal ) provided a significant improvement in sentiment classification among different datasets ( blogs and movies ) when applied as a sophisticated combination of feature sets.', 'they speculated that engineering features based on deeper linguistic representations ( e. g. dependencies and parse tree ) may work for content on social media.', 'in general, lexical and syntactic features are useful when they are applied directly for automatic categorisation of annoying behaviours or topic detection [ 40 ], whereas in cyberhate detection we need a deep understanding of the posted text, which will be the focus of the current work.', 'according to [ 49 ], while part - of - speech tagging does not significantly improve classifier performance, we apply pos tagging for extracting specific content ( verbs, nouns and adjectives ) for lexicon building.', ' #TAUTHOR_TAG prepared their data by stemming and then creating unigram, bigram and trigram features, each of which was weighted by its tf - idf metric.', 'then they demonstrated how non - hateful content might be misclassified due to the fact that it contains words used in racist text.', 'in contrast, there were hateful instances misclassified because they did not contain any of the terms most strongly associated with cyberhate.', 'we']",0
"['after woolwich event, which contains language about muslim and african man ( religion and race ), according to  #TAUTHOR_TAG twitter users use this', 'type of language in their everyday communications which could be contained in the benign tweets']","['set collected after woolwich event, which contains language about muslim and african man ( religion and race ), according to  #TAUTHOR_TAG twitter users use this', 'type of language in their everyday communications which could be contained in the benign tweets']","['after woolwich event, which contains language about muslim and african man ( religion and race ), according to  #TAUTHOR_TAG twitter users use this', 'type of language in their everyday communications which could be contained in the benign tweets']","['', 'false negative detection. turning to true hate speech classified as offensive it appears that our framework success in increasing the true negative and true positive tweets. as we consider the othering language and the indirect', 'hateful content as feature, the improvement is returned to the the lexicon and the embedding learning. to show how', 'our lexicon improved the detection results, using only our lexicon resulted in decreasing the false negative than the previous word which use', 'm - gram, bow and hateful terms. looking at the tweets misclassified as hate speech, despite that our framework successfully advance the machine understanding of hateful content, the', 'misclassified means that the there are tweets contains content which considered benign yet the machine consider them as negative. for religion data set,', 'this data set collected after woolwich event, which contains language about muslim and african man ( religion and race ), according to  #TAUTHOR_TAG twitter users use this', 'type of language in their everyday communications which could be contained in the benign tweets']",0
"['2, 4, 5,  #TAUTHOR_TAG 10, 11, 13, 14 ].', 'the chunk']","['techniques [ 2, 4, 5,  #TAUTHOR_TAG 10, 11, 13, 14 ].', 'the chunking task was extended to the conll - 2000 shared task with standard datasets and evaluation metrics, which is now a standard evaluation task for text chunking [ 3 ].', '']","['2, 4, 5,  #TAUTHOR_TAG 10, 11, 13, 14 ].', 'the chunking task was extended to the conll - 2000 shared task with standard datasets and evaluation metrics, which is now a standard evaluation task for text chunking [ 3 ].', '']","['chunking is a process to identify non - recursive cores of various phrase types without conducting deep parsing of text [ 3 ].', 'abney first proposed it as an intermediate step toward full parsing [ 1 ].', 'since ramshaw and marcus approached np chunking using a machine learning method, many researchers have used various machine learning techniques [ 2, 4, 5,  #TAUTHOR_TAG 10, 11, 13, 14 ].', 'the chunking task was extended to the conll - 2000 shared task with standard datasets and evaluation metrics, which is now a standard evaluation task for text chunking [ 3 ].', 'most previous works with relatively high performance in english used machine learning methods for chunking [ 4, 13 ].', 'machine learning methods are mainly divided into the generative approach and conditional approach.', 'the generative approach relies on generative probabilistic models that assign a joint probability p ( x, y ) of paired input sequence and label sequence, x and y respectively.', 'it provides straightforward understanding of underlying distribution.', 'however, this approach is intractable in most domains without strong independence assumptions that each input element is independent from the other elements in input sequence, and is also difficult to use multiple interacting features and long - range dependencies between input elements.', '']",0
"['2, 4, 5,  #TAUTHOR_TAG 10, 11, 13, 14 ].', 'the chunk']","['techniques [ 2, 4, 5,  #TAUTHOR_TAG 10, 11, 13, 14 ].', 'the chunking task was extended to the conll - 2000 shared task with standard datasets and evaluation metrics, which is now a standard evaluation task for text chunking [ 3 ].', '']","['2, 4, 5,  #TAUTHOR_TAG 10, 11, 13, 14 ].', 'the chunking task was extended to the conll - 2000 shared task with standard datasets and evaluation metrics, which is now a standard evaluation task for text chunking [ 3 ].', '']","['chunking is a process to identify non - recursive cores of various phrase types without conducting deep parsing of text [ 3 ].', 'abney first proposed it as an intermediate step toward full parsing [ 1 ].', 'since ramshaw and marcus approached np chunking using a machine learning method, many researchers have used various machine learning techniques [ 2, 4, 5,  #TAUTHOR_TAG 10, 11, 13, 14 ].', 'the chunking task was extended to the conll - 2000 shared task with standard datasets and evaluation metrics, which is now a standard evaluation task for text chunking [ 3 ].', 'most previous works with relatively high performance in english used machine learning methods for chunking [ 4, 13 ].', 'machine learning methods are mainly divided into the generative approach and conditional approach.', 'the generative approach relies on generative probabilistic models that assign a joint probability p ( x, y ) of paired input sequence and label sequence, x and y respectively.', 'it provides straightforward understanding of underlying distribution.', 'however, this approach is intractable in most domains without strong independence assumptions that each input element is independent from the other elements in input sequence, and is also difficult to use multiple interacting features and long - range dependencies between input elements.', '']",0
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","['89 % accuracy  #TAUTHOR_TAG.', '']","['is an agglutinative language, in which a word unit ( called an eojeol ) is a composition of a content word and function word ( s ).', 'function words - postpositions and endings - give much information such as grammatical relation, case, tense, modal, etc.', 'well - developed function words in korean help with chunking, especially np and vp chunking.', 'for example, when the part - of - speech of current word is one of determiner, pronoun and noun, the following seven rules for np chunking in table 1 can find most np chunks in text, with about 89 % accuracy  #TAUTHOR_TAG.', 'for this reason, boundaries of chunks are easily found in korean, compared to other languages such as english or chinese.', 'this is why a rule - based chunking method is predominantly used.', 'however, with sophisticated rules, the rule - based chunking method has limitations when handling exceptional cases.', 'park et al. proposed a hybrid of the rule - based and the machine learning method to resolve this problem [ 5,  #TAUTHOR_TAG.', 'many recent machine learning techniques can capture hidden characteristics for classification.', 'despite its simplicity and efficiency, the rule - based method has recently been outdone by the machine learning method in various classification problems']",0
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","['89 % accuracy  #TAUTHOR_TAG.', '']","['is an agglutinative language, in which a word unit ( called an eojeol ) is a composition of a content word and function word ( s ).', 'function words - postpositions and endings - give much information such as grammatical relation, case, tense, modal, etc.', 'well - developed function words in korean help with chunking, especially np and vp chunking.', 'for example, when the part - of - speech of current word is one of determiner, pronoun and noun, the following seven rules for np chunking in table 1 can find most np chunks in text, with about 89 % accuracy  #TAUTHOR_TAG.', 'for this reason, boundaries of chunks are easily found in korean, compared to other languages such as english or chinese.', 'this is why a rule - based chunking method is predominantly used.', 'however, with sophisticated rules, the rule - based chunking method has limitations when handling exceptional cases.', 'park et al. proposed a hybrid of the rule - based and the machine learning method to resolve this problem [ 5,  #TAUTHOR_TAG.', 'many recent machine learning techniques can capture hidden characteristics for classification.', 'despite its simplicity and efficiency, the rule - based method has recently been outdone by the machine learning method in various classification problems']",0
"['independent phrase ( ip )  #TAUTHOR_TAG.', 'as function words']","['independent phrase ( ip )  #TAUTHOR_TAG.', 'as function words']","['independent phrase ( ip )  #TAUTHOR_TAG.', 'as function words']","[""##ney was the first to use the term'chunk'to represent a non - recursive core of an intra - clausal constituent, extending from the beginning of constituent to its head."", 'in korean, there are four basic phrases : noun phrase ( np ), verb phrase ( vp ), adverb phrase ( advp ), and independent phrase ( ip )  #TAUTHOR_TAG.', 'as function words such as postposition or ending are well - developed, the number of chunk types is small compared to other languages such as english or chinese.', 'like the conll - 2000 dataset, we use three types of chunk border tags, indicating whether a word is outside a chunk ( o ), starts a chunk ( b ), or continues a chunk ( i ).', 'each chunk type xp has two border tags : b - xp and i - xp.', 'xp should be one of np, vp, advp and ip.', 'there exist nine chunk tags in korean']",0
"['methods  #TAUTHOR_TAG.', 'we add the experimental results of the chunk']","['of various chunking methods  #TAUTHOR_TAG.', 'we add the experimental results of the chunking methods using hmms - bigram and crfs.', 'in']","['. reported the performance of various chunking methods  #TAUTHOR_TAG.', 'we add the experimental results of the chunking methods using hmms - bigram and crfs.', 'in table 6,']","['were performed with c + + implementation of crfs ( flexcrfs ) on linux with 2. 4 ghz pentium iv dual processors and 2. 0gbyte of main memory [ 18 ].', 'we use l - bfgs to train the parameters and use a gaussian prior regularization in order to avoid overfitting [ 20 ].', 'total number of crf features is 83, 264.', 'as shown in table 5, the performances of most chunk type are 96 ~ 100 %, very high performance.', 'however, the performance of np chunk type is lowest, 94. 27 % because the border of np chunk type is very ambiguous in case of consecutive nouns.', 'using more features such as previous chunk tag should be able to improve the performance of np chunk type.', 'park et al. reported the performance of various chunking methods  #TAUTHOR_TAG.', 'we add the experimental results of the chunking methods using hmms - bigram and crfs.', 'in table 6, f - score of chunking using crfs in korean texts is 97. 19 %, the highest performance of all.', 'it significantly outperforms all others, including machine learning methods, rule - based methods and hybrid methods.', 'it is because crfs have a global optimum solution hence overcoming the label bias problem.', 'they also can use many arbitrary, overlapping features.', '']",0
"['methods  #TAUTHOR_TAG.', 'we add the experimental results of the chunk']","['of various chunking methods  #TAUTHOR_TAG.', 'we add the experimental results of the chunking methods using hmms - bigram and crfs.', 'in']","['. reported the performance of various chunking methods  #TAUTHOR_TAG.', 'we add the experimental results of the chunking methods using hmms - bigram and crfs.', 'in table 6,']","['were performed with c + + implementation of crfs ( flexcrfs ) on linux with 2. 4 ghz pentium iv dual processors and 2. 0gbyte of main memory [ 18 ].', 'we use l - bfgs to train the parameters and use a gaussian prior regularization in order to avoid overfitting [ 20 ].', 'total number of crf features is 83, 264.', 'as shown in table 5, the performances of most chunk type are 96 ~ 100 %, very high performance.', 'however, the performance of np chunk type is lowest, 94. 27 % because the border of np chunk type is very ambiguous in case of consecutive nouns.', 'using more features such as previous chunk tag should be able to improve the performance of np chunk type.', 'park et al. reported the performance of various chunking methods  #TAUTHOR_TAG.', 'we add the experimental results of the chunking methods using hmms - bigram and crfs.', 'in table 6, f - score of chunking using crfs in korean texts is 97. 19 %, the highest performance of all.', 'it significantly outperforms all others, including machine learning methods, rule - based methods and hybrid methods.', 'it is because crfs have a global optimum solution hence overcoming the label bias problem.', 'they also can use many arbitrary, overlapping features.', '']",3
"['introduced by  #TAUTHOR_TAG, who describe']","['introduced by  #TAUTHOR_TAG, who describe']","['introduced by  #TAUTHOR_TAG, who describe']","['contrast to factoid question answering ( qa ), nonfactoid qa is concerned with questions whose answer is not easily expressed as an entity or list of entities and can instead be quite complex - compare, for example, the factoid question who is the secretary general of the un? with the non - factoid manner question how is the secretary general of the un chosen?', 'a significant amount of research has been carried out on factoid qa, with non - factoid questions receiving less attention.', 'this is changing, however, with the popularity of community - based question answering ( cqa ) sites such as yahoo! answers 1, quora 2 and the stackexchange 3 family of forums.', 'the ability of users to vote for their favourite answer makes these sites a valuable source of training data for open - domain non - factoid qa systems.', 'in this paper, we present a neural approach to open - domain non - factoid qa, focusing on the subtask of answer reranking, i. e. given a list of candidate answers to a question, order the answers according to their relevance to the question.', 'we test our approach on the yahoo! answers dataset of manner or how questions introduced by  #TAUTHOR_TAG, who describe answer reranking experiments on this dataset using a diverse range of features incorporating syntax, lexical semantics and discourse.', 'in particular, they show how discourse information ( obtained either via a discourse parser or using shallow techniques based on discourse markers ) can complement distributed lexical semantic information.', ' #AUTHOR_TAG show how discourse structure can be used to generate artificial questionanswer training pairs from documents, and test their approach on the same dataset.', 'the best performance on this dataset - 33. 01 p @ 1 and 53. 96 mrr - is reported by  #AUTHOR_TAG who improve on the lexical semantic models of  #TAUTHOR_TAG by exploiting indirect associations between words using higher - order models.', 'in contrast, our approach is very simple and requires no feature engineering.', 'question - answer pairs are represented by concatenated distributed representation vectors and a multilayer perceptron is used to compute the score for an answer ( the probability of an answer being the best answer to the question ).', '']",5
"[' #TAUTHOR_TAG, we also']","[' #TAUTHOR_TAG, we also']","['combination.', 'for comparison with recent work in answer reranking  #TAUTHOR_TAG, we also']","['', 'we experiment with both dm and dbow models, as well as their combination.', 'for comparison with recent work in answer reranking  #TAUTHOR_TAG, we also evaluate the averaged word embedding vectors obtained with the skip - gram model  #AUTHOR_TAG ( henceforth referred to as the skipavg model )']",5
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],5
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],5
"[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and the candidate retrieval ( cr ) baseline.', 'the cr baseline uses the same scoring as in  #TAUTHOR_TAG : the questions and the candidate answers are represented using tf - idf  #AUTHOR_TAG over lemmas ; the candidate answers are ranked according to their cosine similarity to the respective question.', 'we use the gensim 7 implementation of the dbow and dm paragraph vector models.', 'the word embeddings for the skipavg model are obtained with word2vec.', '8 the data was tokenized with the stanford tokenizer 9 and then lowercased.', 'to evaluate our models, we use standard imple - mentations of the p @ 1 and mean reciprocal rank ( mrr ) evaluation metrics.', 'to evaluate whether the difference between two models is statistically significant, statistical significance testing is performed using one - tailed bootstrap resampling with 10, 000 iterations.', 'improvements are considered to be statistically significant at the 5 % confidence level ( p < 0. 05 )']",5
"[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and the candidate retrieval ( cr ) baseline.', 'the cr baseline uses the same scoring as in  #TAUTHOR_TAG : the questions and the candidate answers are represented using tf - idf  #AUTHOR_TAG over lemmas ; the candidate answers are ranked according to their cosine similarity to the respective question.', 'we use the gensim 7 implementation of the dbow and dm paragraph vector models.', 'the word embeddings for the skipavg model are obtained with word2vec.', '8 the data was tokenized with the stanford tokenizer 9 and then lowercased.', 'to evaluate our models, we use standard imple - mentations of the p @ 1 and mean reciprocal rank ( mrr ) evaluation metrics.', 'to evaluate whether the difference between two models is statistically significant, statistical significance testing is performed using one - tailed bootstrap resampling with 10, 000 iterations.', 'improvements are considered to be statistically significant at the 5 % confidence level ( p < 0. 05 )']",5
['##o! answers  #TAUTHOR_TAG 200 outperform'],"['multilayer perceptron trained on yahoo! answers  #TAUTHOR_TAG 200 outperforming the rest by a small margin.', 'the multilayer perceptron with combinations of different distributed representations reach slightly higher p @ 1']","['##o! answers  #TAUTHOR_TAG 200 outperforming the rest by a small margin.', 'the multilayer perceptron with combinations of different distributed representations reach slightly higher p @ 1']","['table 1, we report best development p @ 1 and mrr of the multilayer perceptron trained on yahoo! answers  #TAUTHOR_TAG 200 outperforming the rest by a small margin.', 'the multilayer perceptron with combinations of different distributed representations reach slightly higher p @ 1 and mrr on the development set.', 'however, these improvements over the 200 - dimension dbow model are not statistically significant.', 'table 2 presents the results on the test set.', 'we only evaluate the 200 - dimension dbow model and its combinations with other models, comparing these to the baselines and the previous results on the same dataset ( we use the same train / dev / test split as  #TAUTHOR_TAG.', 'the dbow outperforms the baselines by a statistically significant margin.', 'the combination of the dbow, dm and skipavg models provides slightly better results, but the improvement over the dbow is not statistically significant.', ' #AUTHOR_TAG report that answer reranking benefits from lexical semantic models, and describe experiments using skipavg embeddings pretrained using the english gigaword corpus.', 'here we compare the performance of the reranker with distributed representations pretrained on a large "" outof - domain "" newswire corpus ( gigaword ), versus a smaller "" in - domain "" non - factoid qa one ( l6 yahoo ).', 'figure 2 shows the development p @ 1 and mrr of the multilayer perceptron with dbow model on the yahoo! answers dataset pretrained on 30m random paragraphs from the english gigaword corpus versus the multilayer perceptron with dbow model pretrained on the yahoo l6 corpus containing about 8. 5m paragraphs.', 'we also evaluate the com - bination of the two models.', 'the results highlight the importance of finding a suitable source of unlabelled training data since vectors pretrained on reasonably large amounts of yahoo! answers data are more beneficial than using a much larger gigaword dataset']",5
['##o! answers  #TAUTHOR_TAG 200 outperform'],"['multilayer perceptron trained on yahoo! answers  #TAUTHOR_TAG 200 outperforming the rest by a small margin.', 'the multilayer perceptron with combinations of different distributed representations reach slightly higher p @ 1']","['##o! answers  #TAUTHOR_TAG 200 outperforming the rest by a small margin.', 'the multilayer perceptron with combinations of different distributed representations reach slightly higher p @ 1']","['table 1, we report best development p @ 1 and mrr of the multilayer perceptron trained on yahoo! answers  #TAUTHOR_TAG 200 outperforming the rest by a small margin.', 'the multilayer perceptron with combinations of different distributed representations reach slightly higher p @ 1 and mrr on the development set.', 'however, these improvements over the 200 - dimension dbow model are not statistically significant.', 'table 2 presents the results on the test set.', 'we only evaluate the 200 - dimension dbow model and its combinations with other models, comparing these to the baselines and the previous results on the same dataset ( we use the same train / dev / test split as  #TAUTHOR_TAG.', 'the dbow outperforms the baselines by a statistically significant margin.', 'the combination of the dbow, dm and skipavg models provides slightly better results, but the improvement over the dbow is not statistically significant.', ' #AUTHOR_TAG report that answer reranking benefits from lexical semantic models, and describe experiments using skipavg embeddings pretrained using the english gigaword corpus.', 'here we compare the performance of the reranker with distributed representations pretrained on a large "" outof - domain "" newswire corpus ( gigaword ), versus a smaller "" in - domain "" non - factoid qa one ( l6 yahoo ).', 'figure 2 shows the development p @ 1 and mrr of the multilayer perceptron with dbow model on the yahoo! answers dataset pretrained on 30m random paragraphs from the english gigaword corpus versus the multilayer perceptron with dbow model pretrained on the yahoo l6 corpus containing about 8. 5m paragraphs.', 'we also evaluate the com - bination of the two models.', 'the results highlight the importance of finding a suitable source of unlabelled training data since vectors pretrained on reasonably large amounts of yahoo! answers data are more beneficial than using a much larger gigaword dataset']",5
"['introduced by  #TAUTHOR_TAG, who describe']","['introduced by  #TAUTHOR_TAG, who describe']","['introduced by  #TAUTHOR_TAG, who describe']","['contrast to factoid question answering ( qa ), nonfactoid qa is concerned with questions whose answer is not easily expressed as an entity or list of entities and can instead be quite complex - compare, for example, the factoid question who is the secretary general of the un? with the non - factoid manner question how is the secretary general of the un chosen?', 'a significant amount of research has been carried out on factoid qa, with non - factoid questions receiving less attention.', 'this is changing, however, with the popularity of community - based question answering ( cqa ) sites such as yahoo! answers 1, quora 2 and the stackexchange 3 family of forums.', 'the ability of users to vote for their favourite answer makes these sites a valuable source of training data for open - domain non - factoid qa systems.', 'in this paper, we present a neural approach to open - domain non - factoid qa, focusing on the subtask of answer reranking, i. e. given a list of candidate answers to a question, order the answers according to their relevance to the question.', 'we test our approach on the yahoo! answers dataset of manner or how questions introduced by  #TAUTHOR_TAG, who describe answer reranking experiments on this dataset using a diverse range of features incorporating syntax, lexical semantics and discourse.', 'in particular, they show how discourse information ( obtained either via a discourse parser or using shallow techniques based on discourse markers ) can complement distributed lexical semantic information.', ' #AUTHOR_TAG show how discourse structure can be used to generate artificial questionanswer training pairs from documents, and test their approach on the same dataset.', 'the best performance on this dataset - 33. 01 p @ 1 and 53. 96 mrr - is reported by  #AUTHOR_TAG who improve on the lexical semantic models of  #TAUTHOR_TAG by exploiting indirect associations between words using higher - order models.', 'in contrast, our approach is very simple and requires no feature engineering.', 'question - answer pairs are represented by concatenated distributed representation vectors and a multilayer perceptron is used to compute the score for an answer ( the probability of an answer being the best answer to the question ).', '']",0
"[' #TAUTHOR_TAG, we also']","[' #TAUTHOR_TAG, we also']","['combination.', 'for comparison with recent work in answer reranking  #TAUTHOR_TAG, we also']","['', 'we experiment with both dm and dbow models, as well as their combination.', 'for comparison with recent work in answer reranking  #TAUTHOR_TAG, we also evaluate the averaged word embedding vectors obtained with the skip - gram model  #AUTHOR_TAG ( henceforth referred to as the skipavg model )']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
"['game environments  #TAUTHOR_TAG.', 'lack of a well - defined structure in such negotiations allows humans or agents to express themselves more freely, which better emulates a realistic scenario.', 'interestingly, this also provides an exciting research']","['game environments  #TAUTHOR_TAG.', 'lack of a well - defined structure in such negotiations allows humans or agents to express themselves more freely, which better emulates a realistic scenario.', 'interestingly, this also provides an exciting research opportunity :']","['game environments  #TAUTHOR_TAG.', 'lack of a well - defined structure in such negotiations allows humans or agents to express themselves more freely, which better emulates a realistic scenario.', 'interestingly, this also provides an exciting research']","[', either between individuals or entities, are ubiquitous in everyday human interactions ranging from sales to legal proceedings.', ""being a good negotiator is a complex skill, requiring the ability to understand the partner's motives, ability to reason and to communicate effectively, making it a challenging task for an automated system."", 'while research in building automatically negotiating agents has primarily focused on agent - agent negotiations  #AUTHOR_TAG, there is a recent interest in agent - human negotiations  #AUTHOR_TAG as well.', 'such agents may act as mediators or can be helpful for pedagogical purposes  #AUTHOR_TAG.', 'efforts in agent - human negotiations involving free - form natural language as a means of communication are rather sparse.', 'researchers  #AUTHOR_TAG recently studied natural language negotiations in buyer - seller bargaining setup, which is comparatively less restricted than previously studied game environments  #TAUTHOR_TAG.', 'lack of a well - defined structure in such negotiations allows humans or agents to express themselves more freely, which better emulates a realistic scenario.', 'interestingly, this also provides an exciting research opportunity : how can an agent leverage the behavioral cues in natural language to direct its negotiation strategies? understanding the impact of natural language on negotiation outcomes through a data - driven neural framework is the primary objective of this work.', 'we focus on buyer - seller negotiations  #AUTHOR_TAG where two individuals negotiate the price of a given product.', 'leveraging the recent advancements  #AUTHOR_TAG in pre - trained language encoders, we attempt to predict negotiation outcomes early on in the conversation, in a completely data - driven manner ( figure 1 ).', 'early prediction of outcomes is essential for effective planning of an automatically negotiating agent.', 'although there have been attempts to gain insights into negotiations  #AUTHOR_TAG, to the best of our knowledge, we are the first to study early natural language cues through a datadriven neural system ( section 3 ).', 'our evaluations show that natural language allows the models to make better predictions by looking at only a fraction of the negotiation.', 'rather than just realizing the strategy in natural language, our empirical results suggest that language can be crucial in the planning as well.', 'we provide a sample negotiation from the test set  #AUTHOR_TAG along with our model predictions in table 1']",0
"['studied game environments  #TAUTHOR_TAG, the dataset considers a more realistic setup : negoti']","['studied game environments  #TAUTHOR_TAG, the dataset considers a more realistic setup : negotiating']","['studied game environments  #TAUTHOR_TAG, the dataset considers a more realistic setup : negotiating the price']","['study human - human negotiations in the buyerseller bargaining scenario, which has been a key research area in the literature  #AUTHOR_TAG.', 'in this section, we first describe our problem setup and key terminologies by discussing the dataset used.', 'later, we formalize our problem definition.', 'dataset : for our explorations, we use the craigslist bargaining dataset ( cb ) introduced by  #AUTHOR_TAG.', 'instead of focusing on the previously studied game environments  #TAUTHOR_TAG, the dataset considers a more realistic setup : negotiating the price of products listed on craigslist 1.', 'the dataset consists of 6682 dialogues between a buyer and a seller who converse in natural language to negotiate the price of a given product ( sample in table 1 ).', 'in total, 1402 product ad postings were scraped from craigslist, belonging to six categories : phones, bikes, housing, furniture, car and electronics.', 'each ad posting contains details such as product title, category type and a listing price.', 'moreover, a secret target price is also pre - decided for the buyer.', '']",4
"['planning of a negotiating agent.', 'this can be a viable middleground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning  #TAUTHOR_TAG']","['the planning of a negotiating agent.', 'this can be a viable middleground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning  #TAUTHOR_TAG']","['a negotiating agent.', 'this can be a viable middleground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning  #TAUTHOR_TAG']","['', 'as more information about the final price is revealed. paired bootstrap resampling  #AUTHOR_TAG with 10, 000 bootstraps shows that for a given f, bert - gru is better than its prices - only counterpart with 95 % statistical significance.', 'the prices discussed during the negotiation still play a crucial role in making the predictions. in fact, in only 65 % of the negotiations, the first price is quoted within the first 0', '. 4 fraction of the events. this is visible in higher performance as more events', 'are seen after this point. this number is lower than average for housing, bike and car, resulting in relative better performance of price', '##only model for these categories over others. the models also show evidence of capturing', 'buyer interest. by constructing artificial negotiations, we observe that the model predictions at f = 0. 2 increase when the buyer shows more interest in the', 'product, indicating more willingness to pay. with the', 'capability to incorporate cues from natural language, such a framework can be used in the future to get negotiation feedback, in order to guide the planning of a negotiating agent.', 'this can be a viable middleground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning  #TAUTHOR_TAG']",5
"['planning of a negotiating agent.', 'this can be a viable middleground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning  #TAUTHOR_TAG']","['the planning of a negotiating agent.', 'this can be a viable middleground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning  #TAUTHOR_TAG']","['a negotiating agent.', 'this can be a viable middleground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning  #TAUTHOR_TAG']","['', 'as more information about the final price is revealed. paired bootstrap resampling  #AUTHOR_TAG with 10, 000 bootstraps shows that for a given f, bert - gru is better than its prices - only counterpart with 95 % statistical significance.', 'the prices discussed during the negotiation still play a crucial role in making the predictions. in fact, in only 65 % of the negotiations, the first price is quoted within the first 0', '. 4 fraction of the events. this is visible in higher performance as more events', 'are seen after this point. this number is lower than average for housing, bike and car, resulting in relative better performance of price', '##only model for these categories over others. the models also show evidence of capturing', 'buyer interest. by constructing artificial negotiations, we observe that the model predictions at f = 0. 2 increase when the buyer shows more interest in the', 'product, indicating more willingness to pay. with the', 'capability to incorporate cues from natural language, such a framework can be used in the future to get negotiation feedback, in order to guide the planning of a negotiating agent.', 'this can be a viable middleground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning  #TAUTHOR_TAG']",2
"[""recent bilstm - max encoder of  #TAUTHOR_TAG, which served as our model's 1""]","[""recent bilstm - max encoder of  #TAUTHOR_TAG, which served as our model's 1 - layer starting""]","['', ""we also compare to the recent bilstm - max encoder of  #TAUTHOR_TAG, which served as our model's 1""]","['', ""we also compare to the recent bilstm - max encoder of  #TAUTHOR_TAG, which served as our model's 1 - layer starting point."", ""1 the results indicate that'our shortcut - stacked encoder'sur - passes all the previous state - of - the - art encoders, and achieves the new best encoding - based result on snli, suggesting the general effectiveness of simple shortcut - connected stacked layers in sentence encoders""]",5
['of  #AUTHOR_TAG and  #TAUTHOR_TAG. our simple short'],"[', contradiction, or neural ( similar to the classifier setup of  #AUTHOR_TAG and  #TAUTHOR_TAG. our simple shortcut - stacked encoders']",['of  #AUTHOR_TAG and  #TAUTHOR_TAG. our simple short'],"['##pm ) model  #AUTHOR_TAG. moreover, common sentence encoders can again be classified into tree - based encoders such as spinn in  #AUTHOR_TAG which', 'we mentioned before, or sequential encoders such as the bilstm model by  #AUTHOR_TAG. in this paper', ', we follow the former approach of encoding - based models, and propose a novel yet simple sequential sentence encoder for the multi - nli problem. our encoder does not require any syntactic information of the sentence. it also does not contain any attention or memory structure. it is basically a stacked ( multi - layered ) bidirectional lstm', ""- rnn with shortcut connections ( feeding all previous layers'outputs and word embeddings to each layer ) and word embedding fine - tuning. the overall supervised model uses these shortcutstacked encoders to encode two input sentences into two vectors, and then we use a classifier over the vector combination to label the relationship between these two sentences as that of entailment, contradiction, or neural ( similar to the classifier setup of  #AUTHOR_TAG and  #TAUTHOR_TAG. our simple shortcut - stacked encoders achieve strong improvements over existing encoders due to its multi - layered and shortcutconnected properties, on both matched and mis - matched evaluation settings for multi - domain natural"", 'language inference, as well as on the original snli dataset. it is the top single - model ( nonensemble ) result in the emnlp repeval 2017 multi - nli shared task', ', and the new state - of - the - art for encoding - based results on the snli dataset  #AUTHOR_TAG. github code link : https : /', '/ github. com / easonnie / multin', '##li _ encoder 2 model our model mainly consists of two', 'separate components, a sentence encoder and an entailment classifier. the sentence encoder compresses each source sentence into a vector representation and the classifier makes a three - way classification based on the two vectors of the two', ""source sentences. the model follows the'encoding - based rule ', i. e., the encoder will encode each source sentence into a fixed length vector without any information or function based on the other sentence ( e. g., cross - attention or memory comparing the two sentences ). in order"", 'to fully explore the generalization of the sentence encoder, the same encoder is applied to both the premise and the hypothesis with shared parameters projecting them into the same space. this setting follows the idea of siamese networks in  #AUTHOR_TAG. figure 1 shows the overview of our encoding model (', 'the standard classifier setup is not shown here ; see  #AUTHOR_TAG and  #TAUTHOR_TAG for that )']",5
"[', similar to  #TAUTHOR_TAG']","['bilstm layer, similar to  #TAUTHOR_TAG']","[', similar to  #TAUTHOR_TAG.', '']","['', 'we assume w i ∈ r d is a word embedding vector which are initialized using some pre - trained vector embeddings ( and is then fine - tuned end - to - end via the nli supervision ).', 'then, the input of ith bilstm layer at time t is defined as :', 'then, assuming we have m layers of bilstm, the final vector representation will be obtained by applying row - max - pool over the output of the last bilstm layer, similar to  #TAUTHOR_TAG.', 'the final layer is defined as :', '']",5
"[""recent bilstm - max encoder of  #TAUTHOR_TAG, which served as our model's 1""]","[""recent bilstm - max encoder of  #TAUTHOR_TAG, which served as our model's 1 - layer starting""]","['', ""we also compare to the recent bilstm - max encoder of  #TAUTHOR_TAG, which served as our model's 1""]","['', ""we also compare to the recent bilstm - max encoder of  #TAUTHOR_TAG, which served as our model's 1 - layer starting point."", ""1 the results indicate that'our shortcut - stacked encoder'sur - passes all the previous state - of - the - art encoders, and achieves the new best encoding - based result on snli, suggesting the general effectiveness of simple shortcut - connected stacked layers in sentence encoders""]",4
['layer bilstm in  #TAUTHOR_TAG'],['bilstm in  #TAUTHOR_TAG'],"['in  #TAUTHOR_TAG.', 'we only experimented with up to 3 layers with 512, 1024, 2048 dimensions each, so the model']",[' #TAUTHOR_TAG'],4
['of  #AUTHOR_TAG and  #TAUTHOR_TAG. our simple short'],"[', contradiction, or neural ( similar to the classifier setup of  #AUTHOR_TAG and  #TAUTHOR_TAG. our simple shortcut - stacked encoders']",['of  #AUTHOR_TAG and  #TAUTHOR_TAG. our simple short'],"['##pm ) model  #AUTHOR_TAG. moreover, common sentence encoders can again be classified into tree - based encoders such as spinn in  #AUTHOR_TAG which', 'we mentioned before, or sequential encoders such as the bilstm model by  #AUTHOR_TAG. in this paper', ', we follow the former approach of encoding - based models, and propose a novel yet simple sequential sentence encoder for the multi - nli problem. our encoder does not require any syntactic information of the sentence. it also does not contain any attention or memory structure. it is basically a stacked ( multi - layered ) bidirectional lstm', ""- rnn with shortcut connections ( feeding all previous layers'outputs and word embeddings to each layer ) and word embedding fine - tuning. the overall supervised model uses these shortcutstacked encoders to encode two input sentences into two vectors, and then we use a classifier over the vector combination to label the relationship between these two sentences as that of entailment, contradiction, or neural ( similar to the classifier setup of  #AUTHOR_TAG and  #TAUTHOR_TAG. our simple shortcut - stacked encoders achieve strong improvements over existing encoders due to its multi - layered and shortcutconnected properties, on both matched and mis - matched evaluation settings for multi - domain natural"", 'language inference, as well as on the original snli dataset. it is the top single - model ( nonensemble ) result in the emnlp repeval 2017 multi - nli shared task', ', and the new state - of - the - art for encoding - based results on the snli dataset  #AUTHOR_TAG. github code link : https : /', '/ github. com / easonnie / multin', '##li _ encoder 2 model our model mainly consists of two', 'separate components, a sentence encoder and an entailment classifier. the sentence encoder compresses each source sentence into a vector representation and the classifier makes a three - way classification based on the two vectors of the two', ""source sentences. the model follows the'encoding - based rule ', i. e., the encoder will encode each source sentence into a fixed length vector without any information or function based on the other sentence ( e. g., cross - attention or memory comparing the two sentences ). in order"", 'to fully explore the generalization of the sentence encoder, the same encoder is applied to both the premise and the hypothesis with shared parameters projecting them into the same space. this setting follows the idea of siamese networks in  #AUTHOR_TAG. figure 1 shows the overview of our encoding model (', 'the standard classifier setup is not shown here ; see  #AUTHOR_TAG and  #TAUTHOR_TAG for that )']",3
"[', similar to  #TAUTHOR_TAG']","['bilstm layer, similar to  #TAUTHOR_TAG']","[', similar to  #TAUTHOR_TAG.', '']","['', 'we assume w i ∈ r d is a word embedding vector which are initialized using some pre - trained vector embeddings ( and is then fine - tuned end - to - end via the nli supervision ).', 'then, the input of ith bilstm layer at time t is defined as :', 'then, assuming we have m layers of bilstm, the final vector representation will be obtained by applying row - max - pool over the output of the last bilstm layer, similar to  #TAUTHOR_TAG.', 'the final layer is defined as :', '']",3
"[', similar to  #TAUTHOR_TAG']","['bilstm layer, similar to  #TAUTHOR_TAG']","[', similar to  #TAUTHOR_TAG.', '']","['', 'we assume w i ∈ r d is a word embedding vector which are initialized using some pre - trained vector embeddings ( and is then fine - tuned end - to - end via the nli supervision ).', 'then, the input of ith bilstm layer at time t is defined as :', 'then, assuming we have m layers of bilstm, the final vector representation will be obtained by applying row - max - pool over the output of the last bilstm layer, similar to  #TAUTHOR_TAG.', 'the final layer is defined as :', '']",3
['of  #AUTHOR_TAG and  #TAUTHOR_TAG. our simple short'],"[', contradiction, or neural ( similar to the classifier setup of  #AUTHOR_TAG and  #TAUTHOR_TAG. our simple shortcut - stacked encoders']",['of  #AUTHOR_TAG and  #TAUTHOR_TAG. our simple short'],"['##pm ) model  #AUTHOR_TAG. moreover, common sentence encoders can again be classified into tree - based encoders such as spinn in  #AUTHOR_TAG which', 'we mentioned before, or sequential encoders such as the bilstm model by  #AUTHOR_TAG. in this paper', ', we follow the former approach of encoding - based models, and propose a novel yet simple sequential sentence encoder for the multi - nli problem. our encoder does not require any syntactic information of the sentence. it also does not contain any attention or memory structure. it is basically a stacked ( multi - layered ) bidirectional lstm', ""- rnn with shortcut connections ( feeding all previous layers'outputs and word embeddings to each layer ) and word embedding fine - tuning. the overall supervised model uses these shortcutstacked encoders to encode two input sentences into two vectors, and then we use a classifier over the vector combination to label the relationship between these two sentences as that of entailment, contradiction, or neural ( similar to the classifier setup of  #AUTHOR_TAG and  #TAUTHOR_TAG. our simple shortcut - stacked encoders achieve strong improvements over existing encoders due to its multi - layered and shortcutconnected properties, on both matched and mis - matched evaluation settings for multi - domain natural"", 'language inference, as well as on the original snli dataset. it is the top single - model ( nonensemble ) result in the emnlp repeval 2017 multi - nli shared task', ', and the new state - of - the - art for encoding - based results on the snli dataset  #AUTHOR_TAG. github code link : https : /', '/ github. com / easonnie / multin', '##li _ encoder 2 model our model mainly consists of two', 'separate components, a sentence encoder and an entailment classifier. the sentence encoder compresses each source sentence into a vector representation and the classifier makes a three - way classification based on the two vectors of the two', ""source sentences. the model follows the'encoding - based rule ', i. e., the encoder will encode each source sentence into a fixed length vector without any information or function based on the other sentence ( e. g., cross - attention or memory comparing the two sentences ). in order"", 'to fully explore the generalization of the sentence encoder, the same encoder is applied to both the premise and the hypothesis with shared parameters projecting them into the same space. this setting follows the idea of siamese networks in  #AUTHOR_TAG. figure 1 shows the overview of our encoding model (', 'the standard classifier setup is not shown here ; see  #AUTHOR_TAG and  #TAUTHOR_TAG for that )']",0
"[', similar to  #TAUTHOR_TAG']","['bilstm layer, similar to  #TAUTHOR_TAG']","[', similar to  #TAUTHOR_TAG.', '']","['', 'we assume w i ∈ r d is a word embedding vector which are initialized using some pre - trained vector embeddings ( and is then fine - tuned end - to - end via the nli supervision ).', 'then, the input of ith bilstm layer at time t is defined as :', 'then, assuming we have m layers of bilstm, the final vector representation will be obtained by applying row - max - pool over the output of the last bilstm layer, similar to  #TAUTHOR_TAG.', 'the final layer is defined as :', '']",6
"['.  #TAUTHOR_TAG showed the parsing', 'model using only supertagging probabilities could achieve']","['weighted cdg.  #TAUTHOR_TAG showed the parsing', 'model using only supertagging probabilities could achieve']","['.  #TAUTHOR_TAG showed the parsing', 'model using only supertagging probabilities could achieve accuracy as high as the probabilistic model']","[""##ag )  #AUTHOR_TAG. supertagging is a process where words in an input sentence are tagged with'supertags,'which are lexical entries in lexicalized grammars, e. g., elementary trees in ltag, lexical categories in ccg, and lexical entries in hpsg. the"", 'concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a ccg parser  #AUTHOR_TAG a )', 'with the result of a drastic improvement in the parsing speed.  #AUTHOR_TAG also demonstrated the effects of supertagging with a statistical constraint dependency grammar ( cd', '##g ) parser by showing accuracy as high as the state - of - the - art parsers, and and reported that accuracy was significantly improved by incorporating the supertagging probabilities into manually tuned weighted cdg.  #TAUTHOR_TAG showed the parsing', 'model using only supertagging probabilities could achieve accuracy as high as the probabilistic model for phrase structures. this means that syntactic structures are almost determined', 'by supertags as is claimed by  #AUTHOR_TAG. however, supertaggers themselves were heuristically used as an external tagger. they filter out unlikely lexical entries just to help parsing  #AUTHOR_TAG a ), or', ""the probabilistic models for phrase structures were trained independently of the supertagger's probabilistic models  #TAUTHOR_TAG. in the case of supertagging of weighted cdg, parameters for weighted cdg are manually tuned, i. e., their model is not a well - defined"", 'probabilistic model. we propose a log - linear model for probabilistic hpsg parsing in which the supertagging probabilities are introduced as a reference distribution for the probabilistic hpsg. the reference distribution is simply defined as the', 'product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and part - of - speech ( pos ) n - gram as defined in the ccg / hpsg / cdg supertagging. this is the first model which properly incorporates the', ""supertagging probabilities into parse tree's probabilistic model. we compared our model with the probabilistic model for phrase structures."", 'this model uses word and pos unigram for its reference distribution, i. e.,', 'the probabilities of unigram supertagging. our model can be regarded as an', 'extension of a unigram reference distribution to an n - gram reference distribution with features that are used in supertagging. we also compared with a probabilistic model in  #TAUTHOR_TAG. the', 'probabilities of  #TAUTHOR_TAG was trained independently of supertagging probabilities, i. e., the supertagging probabilities are not used for reference distributions']",7
['types of probabilistic models using a supertagger  #TAUTHOR_TAG. the first'],['types of probabilistic models using a supertagger  #TAUTHOR_TAG. the first one is'],"['the experiments, we compared', 'our model with other two types of probabilistic models using a supertagger  #TAUTHOR_TAG. the first']","['the log - likelihood of the training data. however, the above', 'model cannot be easily estimated because the estimation requires the computation of p ( t | w ) for all parse candidates assigned to sentence w. because', 'the number of parse candidates is exponentially related to the length of the sentence,', 'the estimation is intractable for long sentences. to make the model estimation tractable, geman and johnson  #AUTHOR_TAG and miyao and ts', '##ujii  #AUTHOR_TAG proposed a dynamic programming algorithm for estimating p ( t | w ). also introduced a', 'preliminary probabilistic model p 0 ( t | w ) whose estimation does not require the parsing of a treebank. this model is introduced as a reference', 'distribution  #AUTHOR_TAG of the probabilistic hpsg model ; i.', 'e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage', ', or a probabilistic model can be augmented by several distributions estimated from the larger and simpler', ""corpus. in, p 0 ( t | w ) is defined as the product of probabilities of selecting lexical entries with word and pos unigram features :'s model ) where l i is a lexical entry assigned to word w i in t and p ( l i | w i ) is the probability of selecting lexical entry l i for w i. in the experiments, we compared"", 'our model with other two types of probabilistic models using a supertagger  #TAUTHOR_TAG. the first one is the simplest probabilistic model, which is defined with', 'only the probabilities of lexical entry selection. it is defined simply as the product of the probabilities of selecting all lexical entries in', 'the sentence ; i. e., the model does not use the probabilities of phrase structures like the probabilistic models explained above. given a set of lexical entries, l,', 'a sentence, w = w 1,..., w n, and the probabilistic model of lexical entry selection, p ( l i ∈ l', '| w, i ), the first model is formally defined as follows : where l i is a lexical entry assigned to word w i in t and p ( l i | w, i ) is the', 'probability of selecting lexical entry l i for w i. the probabilities of lexical entry selection, p ( l i | w, i ), are defined as', 'follows : where z w is the sum over all possible lexical entries for the word w i. the second model is a hybrid', 'model of supertagging and the probabilistic hpsg. the probabilities are given as the product of  #TAUTHOR_TAG 1 and the probabilistic hpsg. (  #AUTHOR_TAG in the experiments, we compared our model with figure 2']",7
['types of probabilistic models using a supertagger  #TAUTHOR_TAG. the first'],['types of probabilistic models using a supertagger  #TAUTHOR_TAG. the first one is'],"['the experiments, we compared', 'our model with other two types of probabilistic models using a supertagger  #TAUTHOR_TAG. the first']","['the log - likelihood of the training data. however, the above', 'model cannot be easily estimated because the estimation requires the computation of p ( t | w ) for all parse candidates assigned to sentence w. because', 'the number of parse candidates is exponentially related to the length of the sentence,', 'the estimation is intractable for long sentences. to make the model estimation tractable, geman and johnson  #AUTHOR_TAG and miyao and ts', '##ujii  #AUTHOR_TAG proposed a dynamic programming algorithm for estimating p ( t | w ). also introduced a', 'preliminary probabilistic model p 0 ( t | w ) whose estimation does not require the parsing of a treebank. this model is introduced as a reference', 'distribution  #AUTHOR_TAG of the probabilistic hpsg model ; i.', 'e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage', ', or a probabilistic model can be augmented by several distributions estimated from the larger and simpler', ""corpus. in, p 0 ( t | w ) is defined as the product of probabilities of selecting lexical entries with word and pos unigram features :'s model ) where l i is a lexical entry assigned to word w i in t and p ( l i | w i ) is the probability of selecting lexical entry l i for w i. in the experiments, we compared"", 'our model with other two types of probabilistic models using a supertagger  #TAUTHOR_TAG. the first one is the simplest probabilistic model, which is defined with', 'only the probabilities of lexical entry selection. it is defined simply as the product of the probabilities of selecting all lexical entries in', 'the sentence ; i. e., the model does not use the probabilities of phrase structures like the probabilistic models explained above. given a set of lexical entries, l,', 'a sentence, w = w 1,..., w n, and the probabilistic model of lexical entry selection, p ( l i ∈ l', '| w, i ), the first model is formally defined as follows : where l i is a lexical entry assigned to word w i in t and p ( l i | w, i ) is the', 'probability of selecting lexical entry l i for w i. the probabilities of lexical entry selection, p ( l i | w, i ), are defined as', 'follows : where z w is the sum over all possible lexical entries for the word w i. the second model is a hybrid', 'model of supertagging and the probabilistic hpsg. the probabilities are given as the product of  #TAUTHOR_TAG 1 and the probabilistic hpsg. (  #AUTHOR_TAG in the experiments, we compared our model with figure 2']",7
"[',  #TAUTHOR_TAG uses the supertag']","[',  #TAUTHOR_TAG uses the supertagger as an external module. once']","[""parser. on the other hand,  #TAUTHOR_TAG uses the supertagger as an external module. once the parser acquires the supertagger's outputs, the"", 'n - gram information']","['', 'the n - gram reference distribution is', 'incorporated into the kernel of the parser, but the n - gram features and a maximum entropy estimator are defined in other modules ; n - gram features are defined in a grammar module, and a', 'maximum entropy estimator for the n - gram reference distribution is implemented with a general - purpose maximum entropy estimator module. consequently, strings that represent the ngram information are', 'very frequently changed into feature structures and vice versa when they go in and out of the kernel of', ""the parser. on the other hand,  #TAUTHOR_TAG uses the supertagger as an external module. once the parser acquires the supertagger's outputs, the"", 'n - gram information never goes in and out of the kernel. this advantage of  #TAUTHOR_TAG', 'can apparently be implemented in our model, but this requires many parts of rewriting of the implemented parser.', 'we estimate that the overhead of the interface is around from 50 to 80 ms / sentence. we think that re - implementation of the parser will improve the parsing', 'speed as estimated. in figure 3, the line of our model', 'crosses the line of  #TAUTHOR_TAG. if the estimation is correct, our model will be faster and more accurate so that the', 'lines in the figure do not cross. speed - up in our model is left as a future work']",7
"[',  #TAUTHOR_TAG uses the supertag']","[',  #TAUTHOR_TAG uses the supertagger as an external module. once']","[""parser. on the other hand,  #TAUTHOR_TAG uses the supertagger as an external module. once the parser acquires the supertagger's outputs, the"", 'n - gram information']","['', 'the n - gram reference distribution is', 'incorporated into the kernel of the parser, but the n - gram features and a maximum entropy estimator are defined in other modules ; n - gram features are defined in a grammar module, and a', 'maximum entropy estimator for the n - gram reference distribution is implemented with a general - purpose maximum entropy estimator module. consequently, strings that represent the ngram information are', 'very frequently changed into feature structures and vice versa when they go in and out of the kernel of', ""the parser. on the other hand,  #TAUTHOR_TAG uses the supertagger as an external module. once the parser acquires the supertagger's outputs, the"", 'n - gram information never goes in and out of the kernel. this advantage of  #TAUTHOR_TAG', 'can apparently be implemented in our model, but this requires many parts of rewriting of the implemented parser.', 'we estimate that the overhead of the interface is around from 50 to 80 ms / sentence. we think that re - implementation of the parser will improve the parsing', 'speed as estimated. in figure 3, the line of our model', 'crosses the line of  #TAUTHOR_TAG. if the estimation is correct, our model will be faster and more accurate so that the', 'lines in the figure do not cross. speed - up in our model is left as a future work']",7
"['with supertagging  #TAUTHOR_TAG.', 'though our model was not as fast']","['probabilistic hpsg with a unigram reference distribution and the probabilistic hpsg with supertagging  #TAUTHOR_TAG.', 'though our model was not as fast']","['with a unigram reference distribution and the probabilistic hpsg with supertagging  #TAUTHOR_TAG.', 'though our model was not as fast']","['proposed a probabilistic model in which supertagging is consistently integrated into the probabilistic model for hpsg.', 'in the model, the n - gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries with machine learning features of word and pos ngram as defined in the ccg / hpsg / cdg supertagging.', 'we conducted experiments on the penn treebank with a wide - coverage hpsg parser.', 'in the experiments, we compared our model with the probabilistic hpsg with a unigram reference distribution and the probabilistic hpsg with supertagging  #TAUTHOR_TAG.', '']",7
"[', hw, hp, hl, hp, hl ( probabilistic hpsg with an n - gram reference distribution )', 'in our model,  #TAUTHOR_TAG is used as a reference distribution.', 'the probabilistic']","['f root hw, hp, hl, hw, hp, hw, hl, sy, hw, hp, hl, hp, hl ( probabilistic hpsg with an n - gram reference distribution )', 'in our model,  #TAUTHOR_TAG is used as a reference distribution.', 'the probabilistic']","[', hw, hp, hl, hp, hl ( probabilistic hpsg with an n - gram reference distribution )', 'in our model,  #TAUTHOR_TAG is used as a reference distribution.', 'the probabilistic model of lexical entry selection']","['this section, we propose a probabilistic model with an n - gram reference distribution for probabilistic hpsg parsing.', ""this is an extension of's model by replacing the unigram reference distribution with an n - gram reference distribution."", 'our model is formally defined as follows :', 'combinations, d, c, hw, hp, hl, r, d, c, hw, hp, r, d, c, hw, hl, r, d, c, sy, hw, r, c, sp, hw, hp, hl, r, c, sp, hw, hp, r, c, sp, hw, hl, r, c, sp, sy, hw, r, d, c, hp, hl, r, d, c, hp, r, d, c, hl, r, d, c, sy, r, c, sp, hp, hl, r, c, sp, hp, r, c, sp, hl, r, c, sp, sy combinations of feature templates for funary r, hw, hp, hl, r, hw, hp, r, hw, hl, r, sy, hw, r, hp, hl, r, hp, r, hl, r, sy combinations of feature templates for f root hw, hp, hl, hw, hp, hw, hl, sy, hw, hp, hl, hp, hl ( probabilistic hpsg with an n - gram reference distribution )', 'in our model,  #TAUTHOR_TAG is used as a reference distribution.', 'the probabilistic model of lexical entry selection and its feature templates are the same as defined in  #TAUTHOR_TAG.', 'the formula of our model is the same as  #TAUTHOR_TAG is not a probabilistic model with a reference distribution.', 'both our model and their model consist of the probabilities for lexical entries ( = p model1 ( t | w ) ) and the probabilities for phrase structures ( = the rest of each formula ).', 'the only difference between our model and their model is the way of how to train model parameters for phrase structures.', 'in both our model and their model, the parameters for lexical entries ( = the parameters of p model1 ( t | w ) ) are first estimated from the word and pos sequences independently of the parameters for phrase structures.', 'that is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 ( t | w ) of both models are the same.', 'note that the parameters for lexical entries will never be updated after this estimation stage ; i. e']",5
"[', hw, hp, hl, hp, hl ( probabilistic hpsg with an n - gram reference distribution )', 'in our model,  #TAUTHOR_TAG is used as a reference distribution.', 'the probabilistic']","['f root hw, hp, hl, hw, hp, hw, hl, sy, hw, hp, hl, hp, hl ( probabilistic hpsg with an n - gram reference distribution )', 'in our model,  #TAUTHOR_TAG is used as a reference distribution.', 'the probabilistic']","[', hw, hp, hl, hp, hl ( probabilistic hpsg with an n - gram reference distribution )', 'in our model,  #TAUTHOR_TAG is used as a reference distribution.', 'the probabilistic model of lexical entry selection']","['this section, we propose a probabilistic model with an n - gram reference distribution for probabilistic hpsg parsing.', ""this is an extension of's model by replacing the unigram reference distribution with an n - gram reference distribution."", 'our model is formally defined as follows :', 'combinations, d, c, hw, hp, hl, r, d, c, hw, hp, r, d, c, hw, hl, r, d, c, sy, hw, r, c, sp, hw, hp, hl, r, c, sp, hw, hp, r, c, sp, hw, hl, r, c, sp, sy, hw, r, d, c, hp, hl, r, d, c, hp, r, d, c, hl, r, d, c, sy, r, c, sp, hp, hl, r, c, sp, hp, r, c, sp, hl, r, c, sp, sy combinations of feature templates for funary r, hw, hp, hl, r, hw, hp, r, hw, hl, r, sy, hw, r, hp, hl, r, hp, r, hl, r, sy combinations of feature templates for f root hw, hp, hl, hw, hp, hw, hl, sy, hw, hp, hl, hp, hl ( probabilistic hpsg with an n - gram reference distribution )', 'in our model,  #TAUTHOR_TAG is used as a reference distribution.', 'the probabilistic model of lexical entry selection and its feature templates are the same as defined in  #TAUTHOR_TAG.', 'the formula of our model is the same as  #TAUTHOR_TAG is not a probabilistic model with a reference distribution.', 'both our model and their model consist of the probabilities for lexical entries ( = p model1 ( t | w ) ) and the probabilities for phrase structures ( = the rest of each formula ).', 'the only difference between our model and their model is the way of how to train model parameters for phrase structures.', 'in both our model and their model, the parameters for lexical entries ( = the parameters of p model1 ( t | w ) ) are first estimated from the word and pos sequences independently of the parameters for phrase structures.', 'that is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 ( t | w ) of both models are the same.', 'note that the parameters for lexical entries will never be updated after this estimation stage ; i. e']",3
"[', hw, hp, hl, hp, hl ( probabilistic hpsg with an n - gram reference distribution )', 'in our model,  #TAUTHOR_TAG is used as a reference distribution.', 'the probabilistic']","['f root hw, hp, hl, hw, hp, hw, hl, sy, hw, hp, hl, hp, hl ( probabilistic hpsg with an n - gram reference distribution )', 'in our model,  #TAUTHOR_TAG is used as a reference distribution.', 'the probabilistic']","[', hw, hp, hl, hp, hl ( probabilistic hpsg with an n - gram reference distribution )', 'in our model,  #TAUTHOR_TAG is used as a reference distribution.', 'the probabilistic model of lexical entry selection']","['this section, we propose a probabilistic model with an n - gram reference distribution for probabilistic hpsg parsing.', ""this is an extension of's model by replacing the unigram reference distribution with an n - gram reference distribution."", 'our model is formally defined as follows :', 'combinations, d, c, hw, hp, hl, r, d, c, hw, hp, r, d, c, hw, hl, r, d, c, sy, hw, r, c, sp, hw, hp, hl, r, c, sp, hw, hp, r, c, sp, hw, hl, r, c, sp, sy, hw, r, d, c, hp, hl, r, d, c, hp, r, d, c, hl, r, d, c, sy, r, c, sp, hp, hl, r, c, sp, hp, r, c, sp, hl, r, c, sp, sy combinations of feature templates for funary r, hw, hp, hl, r, hw, hp, r, hw, hl, r, sy, hw, r, hp, hl, r, hp, r, hl, r, sy combinations of feature templates for f root hw, hp, hl, hw, hp, hw, hl, sy, hw, hp, hl, hp, hl ( probabilistic hpsg with an n - gram reference distribution )', 'in our model,  #TAUTHOR_TAG is used as a reference distribution.', 'the probabilistic model of lexical entry selection and its feature templates are the same as defined in  #TAUTHOR_TAG.', 'the formula of our model is the same as  #TAUTHOR_TAG is not a probabilistic model with a reference distribution.', 'both our model and their model consist of the probabilities for lexical entries ( = p model1 ( t | w ) ) and the probabilities for phrase structures ( = the rest of each formula ).', 'the only difference between our model and their model is the way of how to train model parameters for phrase structures.', 'in both our model and their model, the parameters for lexical entries ( = the parameters of p model1 ( t | w ) ) are first estimated from the word and pos sequences independently of the parameters for phrase structures.', 'that is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 ( t | w ) of both models are the same.', 'note that the parameters for lexical entries will never be updated after this estimation stage ; i. e']",3
"[', hw, hp, hl, hp, hl ( probabilistic hpsg with an n - gram reference distribution )', 'in our model,  #TAUTHOR_TAG is used as a reference distribution.', 'the probabilistic']","['f root hw, hp, hl, hw, hp, hw, hl, sy, hw, hp, hl, hp, hl ( probabilistic hpsg with an n - gram reference distribution )', 'in our model,  #TAUTHOR_TAG is used as a reference distribution.', 'the probabilistic']","[', hw, hp, hl, hp, hl ( probabilistic hpsg with an n - gram reference distribution )', 'in our model,  #TAUTHOR_TAG is used as a reference distribution.', 'the probabilistic model of lexical entry selection']","['this section, we propose a probabilistic model with an n - gram reference distribution for probabilistic hpsg parsing.', ""this is an extension of's model by replacing the unigram reference distribution with an n - gram reference distribution."", 'our model is formally defined as follows :', 'combinations, d, c, hw, hp, hl, r, d, c, hw, hp, r, d, c, hw, hl, r, d, c, sy, hw, r, c, sp, hw, hp, hl, r, c, sp, hw, hp, r, c, sp, hw, hl, r, c, sp, sy, hw, r, d, c, hp, hl, r, d, c, hp, r, d, c, hl, r, d, c, sy, r, c, sp, hp, hl, r, c, sp, hp, r, c, sp, hl, r, c, sp, sy combinations of feature templates for funary r, hw, hp, hl, r, hw, hp, r, hw, hl, r, sy, hw, r, hp, hl, r, hp, r, hl, r, sy combinations of feature templates for f root hw, hp, hl, hw, hp, hw, hl, sy, hw, hp, hl, hp, hl ( probabilistic hpsg with an n - gram reference distribution )', 'in our model,  #TAUTHOR_TAG is used as a reference distribution.', 'the probabilistic model of lexical entry selection and its feature templates are the same as defined in  #TAUTHOR_TAG.', 'the formula of our model is the same as  #TAUTHOR_TAG is not a probabilistic model with a reference distribution.', 'both our model and their model consist of the probabilities for lexical entries ( = p model1 ( t | w ) ) and the probabilities for phrase structures ( = the rest of each formula ).', 'the only difference between our model and their model is the way of how to train model parameters for phrase structures.', 'in both our model and their model, the parameters for lexical entries ( = the parameters of p model1 ( t | w ) ) are first estimated from the word and pos sequences independently of the parameters for phrase structures.', 'that is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of p model1 ( t | w ) of both models are the same.', 'note that the parameters for lexical entries will never be updated after this estimation stage ; i. e']",4
"['s model. we must admit that the difference between our models and  #TAUTHOR_TAG was not as', 'great as the difference']","['s model. we must admit that the difference between our models and  #TAUTHOR_TAG was not as', 'great as the difference']","['s model. we must admit that the difference between our models and  #TAUTHOR_TAG was not as', 'great as the difference']","['', ""are labeled f - score and unlabeled f - score. f - score is the harmonic mean of precision and recall. we evaluated our model in two settings. one is implemented with a narrow beam width ('our model 1'in the figure ), and the other is implemented"", ""with a wider beam width ('our model 2'in the figure ) 3.'our model figure 3 : f - score versus average parsing time"", ""for sentences in section 24 of ≤ 100 words. 1'was introduced to measure the performance with balanced f - score and speed, which we"", 'think appropriate for practical use', "".'our model 2'was introduced to measure how high the precision and recall could reach by sacrific"", ""##ing speed. our models increased the parsing accuracy.'our model 1'was around 2. 6 times faster and had around 2. 65"", ""points higher f - score than's model.'our model 2'was around 2"", "". 3 times slower but had around 2. 9 points higher f - score than's model. we must admit that the difference between our models and  #TAUTHOR_TAG was not as"", ""great as the difference from's model, but'our"", ""model 1'achieved 0. 56 points higher f - score, and'our model 2'achieved 0. 8 points higher f - score. when the automatic pos tagger was"", 'introduced, fscore dropped by around 2. 4 points for all models. we also', ""compared our model with  #AUTHOR_TAG's model.""]",4
"['with supertagging  #TAUTHOR_TAG.', 'though our model was not as fast']","['probabilistic hpsg with a unigram reference distribution and the probabilistic hpsg with supertagging  #TAUTHOR_TAG.', 'though our model was not as fast']","['with a unigram reference distribution and the probabilistic hpsg with supertagging  #TAUTHOR_TAG.', 'though our model was not as fast']","['proposed a probabilistic model in which supertagging is consistently integrated into the probabilistic model for hpsg.', 'in the model, the n - gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries with machine learning features of word and pos ngram as defined in the ccg / hpsg / cdg supertagging.', 'we conducted experiments on the penn treebank with a wide - coverage hpsg parser.', 'in the experiments, we compared our model with the probabilistic hpsg with a unigram reference distribution and the probabilistic hpsg with supertagging  #TAUTHOR_TAG.', '']",4
"['with supertagging  #TAUTHOR_TAG.', 'though our model was not as fast']","['probabilistic hpsg with a unigram reference distribution and the probabilistic hpsg with supertagging  #TAUTHOR_TAG.', 'though our model was not as fast']","['with a unigram reference distribution and the probabilistic hpsg with supertagging  #TAUTHOR_TAG.', 'though our model was not as fast']","['proposed a probabilistic model in which supertagging is consistently integrated into the probabilistic model for hpsg.', 'in the model, the n - gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries with machine learning features of word and pos ngram as defined in the ccg / hpsg / cdg supertagging.', 'we conducted experiments on the penn treebank with a wide - coverage hpsg parser.', 'in the experiments, we compared our model with the probabilistic hpsg with a unigram reference distribution and the probabilistic hpsg with supertagging  #TAUTHOR_TAG.', '']",4
"['with supertagging  #TAUTHOR_TAG.', 'though our model was not as fast']","['probabilistic hpsg with a unigram reference distribution and the probabilistic hpsg with supertagging  #TAUTHOR_TAG.', 'though our model was not as fast']","['with a unigram reference distribution and the probabilistic hpsg with supertagging  #TAUTHOR_TAG.', 'though our model was not as fast']","['proposed a probabilistic model in which supertagging is consistently integrated into the probabilistic model for hpsg.', 'in the model, the n - gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries with machine learning features of word and pos ngram as defined in the ccg / hpsg / cdg supertagging.', 'we conducted experiments on the penn treebank with a wide - coverage hpsg parser.', 'in the experiments, we compared our model with the probabilistic hpsg with a unigram reference distribution and the probabilistic hpsg with supertagging  #TAUTHOR_TAG.', '']",4
"['architecture solely based on', '"" attention "" mechanisms  #TAUTHOR_TAG. we later clarify the']","['architecture solely based on', '"" attention "" mechanisms  #TAUTHOR_TAG. we later clarify the']","['architecture solely based on', '"" attention "" mechanisms  #TAUTHOR_TAG. we later clarify the meaning of attention in this context']","['', 'have some drawbacks, of which the most relevant to real - world applications is the high number of sequential operations, which increases the processing time of both learning and inference. to address these limitations, vaswani et al. have proposed the transformer, a machine translation model that introduces a new deep learning architecture solely based on', '"" attention "" mechanisms  #TAUTHOR_TAG. we later clarify the meaning of attention in this context. inspired by the positive results of vaswani et al. in machine translation', '']",0
"['- art results in machine translation  #TAUTHOR_TAG.', 'in this section, we briefly discuss both types of attention models']","['model employs attention, but they all calculate it over the hidden states of an rnn.', 'vaswani et al. were the first to apply attention directly over the word - embeddings, and thus derived a new neural network architecture which, without any recurrence, achieved state - ofthe - art results in machine translation  #TAUTHOR_TAG.', 'in this section, we briefly discuss both types of attention models']","['- art results in machine translation  #TAUTHOR_TAG.', 'in this section, we briefly discuss both types of attention models']","['vast majority of papers that address the squad dataset have adopted rnn - based models [ 3 ] - [ 26 ].', 'they all follow a similar pipeline, with pre - trained word - embeddings that are processed by bidirectional rnns.', 'question and passage are processed independently, and their interaction is modeled by attention mechanisms [ 27 ] to produce an answer.', 'there are slight differences in how each model employs attention, but they all calculate it over the hidden states of an rnn.', 'vaswani et al. were the first to apply attention directly over the word - embeddings, and thus derived a new neural network architecture which, without any recurrence, achieved state - ofthe - art results in machine translation  #TAUTHOR_TAG.', 'in this section, we briefly discuss both types of attention models']",0
"['as machine translation  #TAUTHOR_TAG, [ 27 ]']","['as machine translation  #TAUTHOR_TAG, [ 27 ]']","['as machine translation  #TAUTHOR_TAG, [ 27 ]']","['recent years, attention mechanisms have been used with success in a variety of nlp tasks, such as machine translation  #TAUTHOR_TAG, [ 27 ] and natural language inference [ 28 ], [ 29 ].', 'indeed, most models that target the squad dataset use some form of attention to model the relationship between question and passage.', 'attention can be defined as a mechanism that gives a score α i to a vector p i from a set p = [ p 1,..., p m ] with respect to a vector q j from q = [ q 1,..., q n ].', 'this score is a function of both p and q and is shown in its most general form in ( 1 ).', 'where s i and α i are scalars and f is a score function that measures the importance of p i relative to q j.', 'intuitively, a large weight α i means that the vector p i is somehow strongly related to q. in the literature, two alternatives for f have been proposed, additive [ 27 ] and multiplicative [ 30 ] attentions :', 'where w 1, w 2 and w 3 are learnable parameters and g is a elementwise nonlinear function.', 'for small vectors, additive and multiplicative attention mechanisms have been shown to produce similar results [ 31 ].', 'in most models, the attention scores α are used to create a context vector c given by a weighted sum of p, which is processed by an rnn :', 'where v t is the hidden state of the rnn at time t. notably, in the squad dataset, p and q are the vectorial representations of passage and question, respectively']",0
['transformer is a machine translation model introduced in  #TAUTHOR_TAG that achieved'],['transformer is a machine translation model introduced in  #TAUTHOR_TAG that achieved state - of - the - art results by combining feedforward neural'],['transformer is a machine translation model introduced in  #TAUTHOR_TAG that achieved'],"['transformer is a machine translation model introduced in  #TAUTHOR_TAG that achieved state - of - the - art results by combining feedforward neural networks with a multiplicative attention mechanism applied over position - encoded embedding vectors.', 'it defines three different matrices u, k and v that are associated with queries, keys, and values, respectively.', 'every attention operation in the transformer is performed by multiplying these matrices as shown in ( 4 ).', 'where w u k, w v ∈ r d model ×d model are weight matrices and d model is the embedding size of each word.', 'additionally, vaswani et al.  #TAUTHOR_TAG suggest a multi - head attention, in which u, k and v are divided into n heads heads and the attention in the i th head is computed as', 'where w u, i, w k, i, w v, i ∈ r d model ×d head are again learnable weight matrices and d head is the embedding dimension of each head.', 'finally, attention is computed by the concatenation of every head attention att i, followed by an affine transformation :', 'where w o ∈ r n heads * d head ×d model.', 'if one wants to model the interdependence of words within a single piece of text, u, k and v are all equal and consist of the text of interest embedded in some vectorial space.', 'this type of attention is often called "" self - attention "" or "" self - alignment ""  #TAUTHOR_TAG, [ 21 ].', 'conversely, if one seeks the relationship between words from two different passages, then u represents one, while k and v represent the other.', 'in that case, we talk about "" cross - attention ""']",0
['transformer is a machine translation model introduced in  #TAUTHOR_TAG that achieved'],['transformer is a machine translation model introduced in  #TAUTHOR_TAG that achieved state - of - the - art results by combining feedforward neural'],['transformer is a machine translation model introduced in  #TAUTHOR_TAG that achieved'],"['transformer is a machine translation model introduced in  #TAUTHOR_TAG that achieved state - of - the - art results by combining feedforward neural networks with a multiplicative attention mechanism applied over position - encoded embedding vectors.', 'it defines three different matrices u, k and v that are associated with queries, keys, and values, respectively.', 'every attention operation in the transformer is performed by multiplying these matrices as shown in ( 4 ).', 'where w u k, w v ∈ r d model ×d model are weight matrices and d model is the embedding size of each word.', 'additionally, vaswani et al.  #TAUTHOR_TAG suggest a multi - head attention, in which u, k and v are divided into n heads heads and the attention in the i th head is computed as', 'where w u, i, w k, i, w v, i ∈ r d model ×d head are again learnable weight matrices and d head is the embedding dimension of each head.', 'finally, attention is computed by the concatenation of every head attention att i, followed by an affine transformation :', 'where w o ∈ r n heads * d head ×d model.', 'if one wants to model the interdependence of words within a single piece of text, u, k and v are all equal and consist of the text of interest embedded in some vectorial space.', 'this type of attention is often called "" self - attention "" or "" self - alignment ""  #TAUTHOR_TAG, [ 21 ].', 'conversely, if one seeks the relationship between words from two different passages, then u represents one, while k and v represent the other.', 'in that case, we talk about "" cross - attention ""']",0
['transformer is a machine translation model introduced in  #TAUTHOR_TAG that achieved'],['transformer is a machine translation model introduced in  #TAUTHOR_TAG that achieved state - of - the - art results by combining feedforward neural'],['transformer is a machine translation model introduced in  #TAUTHOR_TAG that achieved'],"['transformer is a machine translation model introduced in  #TAUTHOR_TAG that achieved state - of - the - art results by combining feedforward neural networks with a multiplicative attention mechanism applied over position - encoded embedding vectors.', 'it defines three different matrices u, k and v that are associated with queries, keys, and values, respectively.', 'every attention operation in the transformer is performed by multiplying these matrices as shown in ( 4 ).', 'where w u k, w v ∈ r d model ×d model are weight matrices and d model is the embedding size of each word.', 'additionally, vaswani et al.  #TAUTHOR_TAG suggest a multi - head attention, in which u, k and v are divided into n heads heads and the attention in the i th head is computed as', 'where w u, i, w k, i, w v, i ∈ r d model ×d head are again learnable weight matrices and d head is the embedding dimension of each head.', 'finally, attention is computed by the concatenation of every head attention att i, followed by an affine transformation :', 'where w o ∈ r n heads * d head ×d model.', 'if one wants to model the interdependence of words within a single piece of text, u, k and v are all equal and consist of the text of interest embedded in some vectorial space.', 'this type of attention is often called "" self - attention "" or "" self - alignment ""  #TAUTHOR_TAG, [ 21 ].', 'conversely, if one seeks the relationship between words from two different passages, then u represents one, while k and v represent the other.', 'in that case, we talk about "" cross - attention ""']",0
"['word embedding using a trigonometric encoder as proposed in  #TAUTHOR_TAG.', 'therefore, given a sequence of embedding vectors of even size']","['word embedding using a trigonometric encoder as proposed in  #TAUTHOR_TAG.', 'therefore, given a sequence of embedding vectors of even size']","['hence needs to model the position of each word in a sentence differently.', 'we add positional information to each word embedding using a trigonometric encoder as proposed in  #TAUTHOR_TAG.', 'therefore, given a sequence of embedding vectors of even size d model, the position of the i th word is encoded in']","['contrast to an rnn, fabir does not process words in sequence, and hence needs to model the position of each word in a sentence differently.', 'we add positional information to each word embedding using a trigonometric encoder as proposed in  #TAUTHOR_TAG.', 'therefore, given a sequence of embedding vectors of even size d model, the position of the i th word is encoded in a vector e i as follows :', 'where f k are scalars, which were chosen according to  #TAUTHOR_TAG.', 'the encoding of an embedding matrix ω is represented by e and the whole operation can be summarized as', 'where d model is the size of each position encoding, which is not necessarily equal to d input.', 'the encoding e can be summed to ω to include the information of the position of each word in the text.', 'indeed, in  #TAUTHOR_TAG, the final vectorial representation of a piece of text is defined by the sum of the embeddings ω with the position encoding e, which would require d model = d input.', 'however, we introduce a layer that processes embeddings and encodings separately before summing them up.', 'because we also use this layer to reduce the embedding size from d input to d model, we named it "" reduction layer "".', 'the architecture of this type of layer is addressed further on']",0
"['architecture solely based on', '"" attention "" mechanisms  #TAUTHOR_TAG. we later clarify the']","['architecture solely based on', '"" attention "" mechanisms  #TAUTHOR_TAG. we later clarify the']","['architecture solely based on', '"" attention "" mechanisms  #TAUTHOR_TAG. we later clarify the meaning of attention in this context']","['', 'have some drawbacks, of which the most relevant to real - world applications is the high number of sequential operations, which increases the processing time of both learning and inference. to address these limitations, vaswani et al. have proposed the transformer, a machine translation model that introduces a new deep learning architecture solely based on', '"" attention "" mechanisms  #TAUTHOR_TAG. we later clarify the meaning of attention in this context. inspired by the positive results of vaswani et al. in machine translation', '']",1
['.  #TAUTHOR_TAG is more'],['al.  #TAUTHOR_TAG is more'],['.  #TAUTHOR_TAG is more'],"['squad dataset is relatively small for the training of word embeddings, and pre - trained word vectors have been favored in the literature [ 21 ].', 'nonetheless, we observed that the new architecture introduced by vaswani et al.  #TAUTHOR_TAG is more susceptible to overfitting than rnns when presented with large embedding sizes.', 'hence, we needed a method to compress the word representations, and thus facilitate and speed up training by reducing the number of parameters.', '']",1
"['architecture solely based on', '"" attention "" mechanisms  #TAUTHOR_TAG. we later clarify the']","['architecture solely based on', '"" attention "" mechanisms  #TAUTHOR_TAG. we later clarify the']","['architecture solely based on', '"" attention "" mechanisms  #TAUTHOR_TAG. we later clarify the meaning of attention in this context']","['', 'have some drawbacks, of which the most relevant to real - world applications is the high number of sequential operations, which increases the processing time of both learning and inference. to address these limitations, vaswani et al. have proposed the transformer, a machine translation model that introduces a new deep learning architecture solely based on', '"" attention "" mechanisms  #TAUTHOR_TAG. we later clarify the meaning of attention in this context. inspired by the positive results of vaswani et al. in machine translation', '']",5
['transformer is a machine translation model introduced in  #TAUTHOR_TAG that achieved'],['transformer is a machine translation model introduced in  #TAUTHOR_TAG that achieved state - of - the - art results by combining feedforward neural'],['transformer is a machine translation model introduced in  #TAUTHOR_TAG that achieved'],"['transformer is a machine translation model introduced in  #TAUTHOR_TAG that achieved state - of - the - art results by combining feedforward neural networks with a multiplicative attention mechanism applied over position - encoded embedding vectors.', 'it defines three different matrices u, k and v that are associated with queries, keys, and values, respectively.', 'every attention operation in the transformer is performed by multiplying these matrices as shown in ( 4 ).', 'where w u k, w v ∈ r d model ×d model are weight matrices and d model is the embedding size of each word.', 'additionally, vaswani et al.  #TAUTHOR_TAG suggest a multi - head attention, in which u, k and v are divided into n heads heads and the attention in the i th head is computed as', 'where w u, i, w k, i, w v, i ∈ r d model ×d head are again learnable weight matrices and d head is the embedding dimension of each head.', 'finally, attention is computed by the concatenation of every head attention att i, followed by an affine transformation :', 'where w o ∈ r n heads * d head ×d model.', 'if one wants to model the interdependence of words within a single piece of text, u, k and v are all equal and consist of the text of interest embedded in some vectorial space.', 'this type of attention is often called "" self - attention "" or "" self - alignment ""  #TAUTHOR_TAG, [ 21 ].', 'conversely, if one seeks the relationship between words from two different passages, then u represents one, while k and v represent the other.', 'in that case, we talk about "" cross - attention ""']",5
"['word embedding using a trigonometric encoder as proposed in  #TAUTHOR_TAG.', 'therefore, given a sequence of embedding vectors of even size']","['word embedding using a trigonometric encoder as proposed in  #TAUTHOR_TAG.', 'therefore, given a sequence of embedding vectors of even size']","['hence needs to model the position of each word in a sentence differently.', 'we add positional information to each word embedding using a trigonometric encoder as proposed in  #TAUTHOR_TAG.', 'therefore, given a sequence of embedding vectors of even size d model, the position of the i th word is encoded in']","['contrast to an rnn, fabir does not process words in sequence, and hence needs to model the position of each word in a sentence differently.', 'we add positional information to each word embedding using a trigonometric encoder as proposed in  #TAUTHOR_TAG.', 'therefore, given a sequence of embedding vectors of even size d model, the position of the i th word is encoded in a vector e i as follows :', 'where f k are scalars, which were chosen according to  #TAUTHOR_TAG.', 'the encoding of an embedding matrix ω is represented by e and the whole operation can be summarized as', 'where d model is the size of each position encoding, which is not necessarily equal to d input.', 'the encoding e can be summed to ω to include the information of the position of each word in the text.', 'indeed, in  #TAUTHOR_TAG, the final vectorial representation of a piece of text is defined by the sum of the embeddings ω with the position encoding e, which would require d model = d input.', 'however, we introduce a layer that processes embeddings and encodings separately before summing them up.', 'because we also use this layer to reduce the embedding size from d input to d model, we named it "" reduction layer "".', 'the architecture of this type of layer is addressed further on']",5
"['word embedding using a trigonometric encoder as proposed in  #TAUTHOR_TAG.', 'therefore, given a sequence of embedding vectors of even size']","['word embedding using a trigonometric encoder as proposed in  #TAUTHOR_TAG.', 'therefore, given a sequence of embedding vectors of even size']","['hence needs to model the position of each word in a sentence differently.', 'we add positional information to each word embedding using a trigonometric encoder as proposed in  #TAUTHOR_TAG.', 'therefore, given a sequence of embedding vectors of even size d model, the position of the i th word is encoded in']","['contrast to an rnn, fabir does not process words in sequence, and hence needs to model the position of each word in a sentence differently.', 'we add positional information to each word embedding using a trigonometric encoder as proposed in  #TAUTHOR_TAG.', 'therefore, given a sequence of embedding vectors of even size d model, the position of the i th word is encoded in a vector e i as follows :', 'where f k are scalars, which were chosen according to  #TAUTHOR_TAG.', 'the encoding of an embedding matrix ω is represented by e and the whole operation can be summarized as', 'where d model is the size of each position encoding, which is not necessarily equal to d input.', 'the encoding e can be summed to ω to include the information of the position of each word in the text.', 'indeed, in  #TAUTHOR_TAG, the final vectorial representation of a piece of text is defined by the sum of the embeddings ω with the position encoding e, which would require d model = d input.', 'however, we introduce a layer that processes embeddings and encodings separately before summing them up.', 'because we also use this layer to reduce the embedding size from d input to d model, we named it "" reduction layer "".', 'the architecture of this type of layer is addressed further on']",5
"['.  #TAUTHOR_TAG, where the softmax in ( 12d ) is']","['al.  #TAUTHOR_TAG, where the softmax in ( 12d ) is']","['.  #TAUTHOR_TAG, where the softmax in ( 12d ) is applied in a row - wise manner, we suggest column']","['', 'in this section, we introduce each of these operations.', '1 ) self - attention : self - attention ( att self ) is the mechanism that models the interdependence between words in the same piece of text.', 'it has been proven to help relating distant words, which is crucial to understand the long sentences that appear in context paragraphs in squad.', 'in fabir, self - attention is a sublayer that applies such operation via convolutional attention and is defined as att self ( p ) = att conv ( p, p, p ).', '2 ) column - wise cross - attention : cross - attention ( att cross ) differs from other types of attention by relating two different pieces of text.', 'given p and q, cross - attention of q over p is defined as', 'in contrast to vaswani et al.  #TAUTHOR_TAG, where the softmax in ( 12d ) is applied in a row - wise manner, we suggest column - wise cross - attention.', 'more precisely, we sum over i instead of j in ( 1b ).', '']",5
"['40 ].', 'as suggested in  #TAUTHOR_TAG, we have chosen the adam']","['for replicability.', 'we pre - processed the texts with the nltk tokenizer [ 40 ].', 'as suggested in  #TAUTHOR_TAG, we have chosen the adam']","['for replicability.', 'we pre - processed the texts with the nltk tokenizer [ 40 ].', 'as suggested in  #TAUTHOR_TAG, we have chosen']","['have trained our fabir model during 54 epochs with a batch size of 75 in a gpu nvidia titan x with 12 gb of ram.', 'we developed our model in tensorflow [ 39 ] and made it available at https : / / worksheets. codalab. org / worksheets / 0xee647ea284674396831ecb5aae9ca297 / for replicability.', 'we pre - processed the texts with the nltk tokenizer [ 40 ].', 'as suggested in  #TAUTHOR_TAG, we have chosen the adam optimizer [ 41 ] with the same hyperparameters, except for the learning rate, which was divided by two in our implementation.', 'for regularization, we applied residual and attention dropout  #TAUTHOR_TAG of 0. 9 in processing layers and of 0. 8 in the reduction layer.', 'in the character - level embedding process, a dropout of 0. 75 was added before the convolution.', 'additionally, a dropout of 0. 8 was added before each convolutional layer in the answer selector.', 'we set processing layers dimension d model to 100, the number of heads n heads in each attention sublayer to 4, the feed - forward hidden size to 200 in processing layers and 400 in the reduction layer.', 'convolution kernels in attention sublayers had spatial dimensions 1 × 5']",5
"['40 ].', 'as suggested in  #TAUTHOR_TAG, we have chosen the adam']","['for replicability.', 'we pre - processed the texts with the nltk tokenizer [ 40 ].', 'as suggested in  #TAUTHOR_TAG, we have chosen the adam']","['for replicability.', 'we pre - processed the texts with the nltk tokenizer [ 40 ].', 'as suggested in  #TAUTHOR_TAG, we have chosen']","['have trained our fabir model during 54 epochs with a batch size of 75 in a gpu nvidia titan x with 12 gb of ram.', 'we developed our model in tensorflow [ 39 ] and made it available at https : / / worksheets. codalab. org / worksheets / 0xee647ea284674396831ecb5aae9ca297 / for replicability.', 'we pre - processed the texts with the nltk tokenizer [ 40 ].', 'as suggested in  #TAUTHOR_TAG, we have chosen the adam optimizer [ 41 ] with the same hyperparameters, except for the learning rate, which was divided by two in our implementation.', 'for regularization, we applied residual and attention dropout  #TAUTHOR_TAG of 0. 9 in processing layers and of 0. 8 in the reduction layer.', 'in the character - level embedding process, a dropout of 0. 75 was added before the convolution.', 'additionally, a dropout of 0. 8 was added before each convolutional layer in the answer selector.', 'we set processing layers dimension d model to 100, the number of heads n heads in each attention sublayer to 4, the feed - forward hidden size to 200 in processing layers and 400 in the reduction layer.', 'convolution kernels in attention sublayers had spatial dimensions 1 × 5']",5
"['in  #TAUTHOR_TAG, the']","['in  #TAUTHOR_TAG, the']","['in  #TAUTHOR_TAG, the performance dropped']",[' #TAUTHOR_TAG'],5
"['being thoroughly compatible with the transformer  #TAUTHOR_TAG, these new mechanisms']","['2 %.', 'moreover, being thoroughly compatible with the transformer  #TAUTHOR_TAG, these new mechanisms']","['', 'moreover, being thoroughly compatible with the transformer  #TAUTHOR_TAG, these new mechanisms']","['experiments validate that attention mechanisms alone are enough to power an effective question - answering model.', 'above all, fabir proved roughly five times faster at both training and inference than bidaf, a competing rnn - based model with similar performance [ 21 ].', ""these results strengthen some of fabir's compelling advantages, notably, an architecture that is both more parallelizable and lighter, with half of the number of parameters in comparison to bidaf [ 21 ]."", 'fabir also brings three significant contributions to this new class of neural network architectures.', ""the convolutional attention, the reduction layer, and the column - wise crossattention individually increased the model's f1 and em scores by more than 2 %."", 'moreover, being thoroughly compatible with the transformer  #TAUTHOR_TAG, these new mechanisms are valuable assets to further developments in attention models.', 'in fact, an intriguing line for future research is to evaluate their impact on other nlp tasks, such as machine translation or parsing.', 'although fabir is still far from surpassing the models at the top of the squad leaderboard ( table iii ), we believe that its faster and lighter architecture already make it an attractive alternative to rnn - based models, especially for applications with limited processing power or that require low - latency.', 'also, being a distinct technique, fabir might have low correlation with existing rnn - based models, increasing the potential of ensemble methods.', 'how to combine fabir with other systems is then an interesting topic for future research in diverse nlp applications']",5
"['word embedding using a trigonometric encoder as proposed in  #TAUTHOR_TAG.', 'therefore, given a sequence of embedding vectors of even size']","['word embedding using a trigonometric encoder as proposed in  #TAUTHOR_TAG.', 'therefore, given a sequence of embedding vectors of even size']","['hence needs to model the position of each word in a sentence differently.', 'we add positional information to each word embedding using a trigonometric encoder as proposed in  #TAUTHOR_TAG.', 'therefore, given a sequence of embedding vectors of even size d model, the position of the i th word is encoded in']","['contrast to an rnn, fabir does not process words in sequence, and hence needs to model the position of each word in a sentence differently.', 'we add positional information to each word embedding using a trigonometric encoder as proposed in  #TAUTHOR_TAG.', 'therefore, given a sequence of embedding vectors of even size d model, the position of the i th word is encoded in a vector e i as follows :', 'where f k are scalars, which were chosen according to  #TAUTHOR_TAG.', 'the encoding of an embedding matrix ω is represented by e and the whole operation can be summarized as', 'where d model is the size of each position encoding, which is not necessarily equal to d input.', 'the encoding e can be summed to ω to include the information of the position of each word in the text.', 'indeed, in  #TAUTHOR_TAG, the final vectorial representation of a piece of text is defined by the sum of the embeddings ω with the position encoding e, which would require d model = d input.', 'however, we introduce a layer that processes embeddings and encodings separately before summing them up.', 'because we also use this layer to reduce the embedding size from d input to d model, we named it "" reduction layer "".', 'the architecture of this type of layer is addressed further on']",4
"['.  #TAUTHOR_TAG, where the softmax in ( 12d ) is']","['al.  #TAUTHOR_TAG, where the softmax in ( 12d ) is']","['.  #TAUTHOR_TAG, where the softmax in ( 12d ) is applied in a row - wise manner, we suggest column']","['', 'in this section, we introduce each of these operations.', '1 ) self - attention : self - attention ( att self ) is the mechanism that models the interdependence between words in the same piece of text.', 'it has been proven to help relating distant words, which is crucial to understand the long sentences that appear in context paragraphs in squad.', 'in fabir, self - attention is a sublayer that applies such operation via convolutional attention and is defined as att self ( p ) = att conv ( p, p, p ).', '2 ) column - wise cross - attention : cross - attention ( att cross ) differs from other types of attention by relating two different pieces of text.', 'given p and q, cross - attention of q over p is defined as', 'in contrast to vaswani et al.  #TAUTHOR_TAG, where the softmax in ( 12d ) is applied in a row - wise manner, we suggest column - wise cross - attention.', 'more precisely, we sum over i instead of j in ( 1b ).', '']",4
"['40 ].', 'as suggested in  #TAUTHOR_TAG, we have chosen the adam']","['for replicability.', 'we pre - processed the texts with the nltk tokenizer [ 40 ].', 'as suggested in  #TAUTHOR_TAG, we have chosen the adam']","['for replicability.', 'we pre - processed the texts with the nltk tokenizer [ 40 ].', 'as suggested in  #TAUTHOR_TAG, we have chosen']","['have trained our fabir model during 54 epochs with a batch size of 75 in a gpu nvidia titan x with 12 gb of ram.', 'we developed our model in tensorflow [ 39 ] and made it available at https : / / worksheets. codalab. org / worksheets / 0xee647ea284674396831ecb5aae9ca297 / for replicability.', 'we pre - processed the texts with the nltk tokenizer [ 40 ].', 'as suggested in  #TAUTHOR_TAG, we have chosen the adam optimizer [ 41 ] with the same hyperparameters, except for the learning rate, which was divided by two in our implementation.', 'for regularization, we applied residual and attention dropout  #TAUTHOR_TAG of 0. 9 in processing layers and of 0. 8 in the reduction layer.', 'in the character - level embedding process, a dropout of 0. 75 was added before the convolution.', 'additionally, a dropout of 0. 8 was added before each convolutional layer in the answer selector.', 'we set processing layers dimension d model to 100, the number of heads n heads in each attention sublayer to 4, the feed - forward hidden size to 200 in processing layers and 400 in the reduction layer.', 'convolution kernels in attention sublayers had spatial dimensions 1 × 5']",4
"['a model in the lowresource language  #TAUTHOR_TAG.', 'these methods overall have limited']","['a model in the lowresource language  #TAUTHOR_TAG.', 'these methods overall have limited']","['a model in the lowresource language  #TAUTHOR_TAG.', 'these methods overall have limited']","['- of - speech ( pos ) tagging is an important first step in most natural language processing ( nlp ) applications.', 'typically this is modelled using sequence labelling methods to predict the conditional probability of taggings given word sequences, using linear graphical models  #AUTHOR_TAG, or neural network models, such as recurrent neural networks ( rnn )  #AUTHOR_TAG.', 'these supervised learning algorithms rely on large labelled corpora ; this is particularly true for state - of - the - art neural network models.', 'due to the expense of annotating sufficient data, such techniques are not well suited to applications in low - resource languages.', 'prior work on low - resource nlp has primarily focused on exploiting parallel corpora to project information between a high - and low - resource language  #AUTHOR_TAG tackstrom et al., 2013 ;  #AUTHOR_TAG agic et al., 2016 ;  #AUTHOR_TAG.', 'for example, pos tags can be projected via word alignments, and the projected pos is then used to train a model in the lowresource language  #TAUTHOR_TAG.', 'these methods overall have limited effectiveness due to errors in the alignment and fundamental differences between the languages.', 'they also assume a large parallel corpus, which may not be available for many low - resource languages.', 'to address these limitations, we propose a new technique for low resource tagging, with more modest resource requirements : 1 ) a bilingual dictionary ; 2 ) monolingual corpora in the high and low resource languages ; and 3 ) a small annotated corpus of around 1, 000 tokens in the low - resource language.', 'the first two resources are used as a form of distant supervision through learning crosslingual word embeddings over the monolingual corpora and bilingual dictionary  #AUTHOR_TAG.', 'additionally, our model jointly incorporates the language - dependent information from the small set of gold annotations.', 'our approach combines these two sources of supervision using multi - task learning, such that the kinds of errors that occur in cross - lingual transfer can be accounted for, and corrected automatically.', 'we empirically demonstrate the validity of our observation by using distant supervision to improve pos tagging performance with little supervision.', 'experimental results show the effectiveness of our approach across several low - resource languages, including both simulated and true lowresource settings.', 'furthermore, given the clear superiority of training with manual annotations, we compare several active learning heuristics.', 'active learning using uncertainty sampling with a word - type bias leads to substantial gains over benchmark methods such as token or sentence level uncertainty sampling']",0
"['developing nlp systems in low - resource languages  #AUTHOR_TAG tackstrom et al., 2013 ;  #TAUTHOR_TAG.', ' #AUTHOR_TAG pioneered the use of parallel data']","['developing nlp systems in low - resource languages  #AUTHOR_TAG tackstrom et al., 2013 ;  #TAUTHOR_TAG.', ' #AUTHOR_TAG pioneered the use of parallel data']","['developing nlp systems in low - resource languages  #AUTHOR_TAG tackstrom et al., 2013 ;  #TAUTHOR_TAG.', ' #AUTHOR_TAG pioneered the use of parallel data']","['tagging has been studied for many years.', 'traditionally, probabilistic models are a popular choice, such as hidden markov models ( hmm ) and conditional random fields ( crf )  #AUTHOR_TAG.', 'recently, neural network models have been developed for pos tagging and achieved good performance, such as rnn and bidirectional long short - term memory ( bilstm ) and crf - bilstm models  #AUTHOR_TAG.', 'for example, the crfbilstm pos tagger obtained the state - of - theart performance on penn treebank wsj corpus  #AUTHOR_TAG.', 'however, in low - resource languages, these models are seldom used because of limited labelled data.', 'parallel data therefore appears to be the most realistic additional source of information for developing nlp systems in low - resource languages  #AUTHOR_TAG tackstrom et al., 2013 ;  #TAUTHOR_TAG.', ' #AUTHOR_TAG pioneered the use of parallel data for projecting pos tag information from one language to another language.', 'used parallel data and exploited graph - based label propagation to expand the coverage of labelled tokens.', 'tackstrom et al. ( 2013 ) constructed tag dictionaries by projecting tag information from a highresource language to a low - resource language via alignments in the parallel text.', ' #AUTHOR_TAG used parallel data to obtain projected tags as distant labels and proposed a joint bilstm model trained on both the distant data and 1, 000 tagged tokens.', ' #AUTHOR_TAG used a few word translations pairs to find a linear transformation between two language embeddings.', 'then they used unsupervised learning to refine embedding transformations and model parameters.', ""instead we use minimal supervision to refine'distant'labels through modelling the tag transformation, based on a small set of annotations""]",0
"['a model in the lowresource language  #TAUTHOR_TAG.', 'these methods overall have limited']","['a model in the lowresource language  #TAUTHOR_TAG.', 'these methods overall have limited']","['a model in the lowresource language  #TAUTHOR_TAG.', 'these methods overall have limited']","['- of - speech ( pos ) tagging is an important first step in most natural language processing ( nlp ) applications.', 'typically this is modelled using sequence labelling methods to predict the conditional probability of taggings given word sequences, using linear graphical models  #AUTHOR_TAG, or neural network models, such as recurrent neural networks ( rnn )  #AUTHOR_TAG.', 'these supervised learning algorithms rely on large labelled corpora ; this is particularly true for state - of - the - art neural network models.', 'due to the expense of annotating sufficient data, such techniques are not well suited to applications in low - resource languages.', 'prior work on low - resource nlp has primarily focused on exploiting parallel corpora to project information between a high - and low - resource language  #AUTHOR_TAG tackstrom et al., 2013 ;  #AUTHOR_TAG agic et al., 2016 ;  #AUTHOR_TAG.', 'for example, pos tags can be projected via word alignments, and the projected pos is then used to train a model in the lowresource language  #TAUTHOR_TAG.', 'these methods overall have limited effectiveness due to errors in the alignment and fundamental differences between the languages.', 'they also assume a large parallel corpus, which may not be available for many low - resource languages.', 'to address these limitations, we propose a new technique for low resource tagging, with more modest resource requirements : 1 ) a bilingual dictionary ; 2 ) monolingual corpora in the high and low resource languages ; and 3 ) a small annotated corpus of around 1, 000 tokens in the low - resource language.', 'the first two resources are used as a form of distant supervision through learning crosslingual word embeddings over the monolingual corpora and bilingual dictionary  #AUTHOR_TAG.', 'additionally, our model jointly incorporates the language - dependent information from the small set of gold annotations.', 'our approach combines these two sources of supervision using multi - task learning, such that the kinds of errors that occur in cross - lingual transfer can be accounted for, and corrected automatically.', 'we empirically demonstrate the validity of our observation by using distant supervision to improve pos tagging performance with little supervision.', 'experimental results show the effectiveness of our approach across several low - resource languages, including both simulated and true lowresource settings.', 'furthermore, given the clear superiority of training with manual annotations, we compare several active learning heuristics.', 'active learning using uncertainty sampling with a word - type bias leads to substantial gains over benchmark methods such as token or sentence level uncertainty sampling']",4
"[""a more expressive label mapping than  #TAUTHOR_TAG's linear matrix translation""]","[""a more expressive label mapping than  #TAUTHOR_TAG's linear matrix translation. joint"", 'multi - task learning to combine the two']","[""allows for a more expressive label mapping than  #TAUTHOR_TAG's linear matrix translation. joint"", 'multi - task learning to combine the two']","['∼ categorial ( o t ), with parameters o t = softmax ( w h t + b )', 'as a linear classifier over a sentence encoding, h t, which is the output of a bidirectional lstm encoder over the words. ground truth supervision the second component of the model is manually', 'labelled text in the low - resource language. to model this data we employ the same model structure as above but augmented with a second perceptron output layer', ', as illustrated in figure 1 ( right ). formally, y t ∼ categorial ( o t', ') whereo t = mlp ( o t ) is a single hidden layer perceptron with tanh activation and softmax output transformation. this', ""component allows for a more expressive label mapping than  #TAUTHOR_TAG's linear matrix translation. joint"", 'multi - task learning to combine the two sources of information, we use a joint objective', ', where n and m index the token positions in the distant and ground truth corpora, respectively, and γ is a constant balancing the two components which we set for uniform weighting, γ = | m | | n |. consider the training effect of the true pos tags : when performing', 'error backpropagation, the cross - entropy error signal must pass through the transformation linkingo with o, which can be seen as a language - specific step,', 'after which the generalised error signal can be further backpropagated to the rest of the model. active learning given the scarcity of ground truth labels and the high cost of annotation, a natural', 'question is whether we can optimise which text to be annotated in order achieve the high accuracy for the lowest cost. we now outline a range of active learning approaches based on the following heuristics, which are', 'used to select the instances for annotation from a pool of candidates : token select the token x t with the highest uncertainty, h ( x, t ) = − y p (', 'y | x, t ) log p ( y | x, t ) ; sent select the sentence x with the highest aggregate uncertainty, h ( x ) = t h ( x', ', t ) ; freqtype select the most frequent unannotated word type  #AUTHOR_TAG, in which case all token instances are annotated with the most frequent label for the type in the training corpus ; 1 sumtype select a word type, z, for annotation with the highest aggregate uncertainty over token occurrences, h ( z ) = i', '##∈d x i, t = z h ( x i, t ), which effectively combines uncertainty', 'sampling with a bias towards high frequency types ; and random select word types randomly']",4
"['include bilstm -', 'debias  #TAUTHOR_TAG, applied using']","['not use cross - lingual supervision. for a more direct comparison, we include bilstm -', 'debias  #TAUTHOR_TAG, applied using our proposed cross - lingual supervision based on dictionaries, instead of parallel corpora ; accordingly the key difference is their linear transformation for the distant data, versus']","['not use cross - lingual supervision. for a more direct comparison, we include bilstm -', 'debias  #TAUTHOR_TAG, applied using our proposed cross - lingual supervision based on dictionaries, instead of parallel corpora ; accordingly the key difference is']","['data. note these methods do not use cross - lingual supervision. for a more direct comparison, we include bilstm -', 'debias  #TAUTHOR_TAG, applied using our proposed cross - lingual supervision based on dictionaries, instead of parallel corpora ; accordingly the key difference is their linear transformation for the distant data, versus our non - linear transformation to the gold data. results table 1 reports the tagging accuracy, showing that', 'our models consistently outperform the baseline techniques', '. the poor performance of the supervised methods suggests they are overfitting the small training set, however this is much less of a problem for our approach ( labelled joint )', '. note that distant supervision alone gives reasonable performance ( labelled', 'distant ) however the joint modelling of the ground truth and distant data yields significant improvements in almost all cases. bilstm - debias  #TAUTHOR_TAG performs worse than our proposed method, indicating that', 'a linear transformation is insufficient for modelling distant supervision. the accuracies are higher overall for the european cf. turkic languages, presumably because these languages are', 'table 1 : pos tagging accuracy on over the ten target languages, showing first approaches using only the gold data ; next methods using only distant cross - lingual supervision, and lastly joint multi - task learning. english', 'is used as the source language and columns correspond to a specific target language. closer to english, have higher quality dictionaries and in', 'most cases are morphologically simpler. finally, note the difference between cca', 'and cluster methods for learning word embeddings which arise from the differing quality of distant supervision between the languages. figure 3 compares various active learning', 'heuristics ( see § 3 ) based on different taggers, either a supervised bilstm ( labelled trad ) or our multi', '- task model which also includes crosslingual supervision ( joint ). traditional uncertainty - based sampling strategies ( token ( trad ) and sent ( trad ) ) do not work well because models based on limited supervision do not', 'provide accurate uncertainty information, 5 and moreover, annotating at', 'the type rather than token level provides a significantly stronger supervision signal. the difference is apparent from the', 'decent performance of random sampling over word types. overall, sumtype ( joint ) outperforms', 'the other heuristics consistently, underlining the importance of cross - lingual distant super - vision, as well as combining the benefits of uncertainty sampling, type selection and a frequency bias. comparing the amount of annotation required between the best traditional active learning method sumtype ( trad ) and our best method sumtype ( joint ), we achieve the same performance with an', 'order of magnitude less annotated data ( 100 vs. 1,', '000 labelled words )']",4
"['include bilstm -', 'debias  #TAUTHOR_TAG, applied using']","['not use cross - lingual supervision. for a more direct comparison, we include bilstm -', 'debias  #TAUTHOR_TAG, applied using our proposed cross - lingual supervision based on dictionaries, instead of parallel corpora ; accordingly the key difference is their linear transformation for the distant data, versus']","['not use cross - lingual supervision. for a more direct comparison, we include bilstm -', 'debias  #TAUTHOR_TAG, applied using our proposed cross - lingual supervision based on dictionaries, instead of parallel corpora ; accordingly the key difference is']","['data. note these methods do not use cross - lingual supervision. for a more direct comparison, we include bilstm -', 'debias  #TAUTHOR_TAG, applied using our proposed cross - lingual supervision based on dictionaries, instead of parallel corpora ; accordingly the key difference is their linear transformation for the distant data, versus our non - linear transformation to the gold data. results table 1 reports the tagging accuracy, showing that', 'our models consistently outperform the baseline techniques', '. the poor performance of the supervised methods suggests they are overfitting the small training set, however this is much less of a problem for our approach ( labelled joint )', '. note that distant supervision alone gives reasonable performance ( labelled', 'distant ) however the joint modelling of the ground truth and distant data yields significant improvements in almost all cases. bilstm - debias  #TAUTHOR_TAG performs worse than our proposed method, indicating that', 'a linear transformation is insufficient for modelling distant supervision. the accuracies are higher overall for the european cf. turkic languages, presumably because these languages are', 'table 1 : pos tagging accuracy on over the ten target languages, showing first approaches using only the gold data ; next methods using only distant cross - lingual supervision, and lastly joint multi - task learning. english', 'is used as the source language and columns correspond to a specific target language. closer to english, have higher quality dictionaries and in', 'most cases are morphologically simpler. finally, note the difference between cca', 'and cluster methods for learning word embeddings which arise from the differing quality of distant supervision between the languages. figure 3 compares various active learning', 'heuristics ( see § 3 ) based on different taggers, either a supervised bilstm ( labelled trad ) or our multi', '- task model which also includes crosslingual supervision ( joint ). traditional uncertainty - based sampling strategies ( token ( trad ) and sent ( trad ) ) do not work well because models based on limited supervision do not', 'provide accurate uncertainty information, 5 and moreover, annotating at', 'the type rather than token level provides a significantly stronger supervision signal. the difference is apparent from the', 'decent performance of random sampling over word types. overall, sumtype ( joint ) outperforms', 'the other heuristics consistently, underlining the importance of cross - lingual distant super - vision, as well as combining the benefits of uncertainty sampling, type selection and a frequency bias. comparing the amount of annotation required between the best traditional active learning method sumtype ( trad ) and our best method sumtype ( joint ), we achieve the same performance with an', 'order of magnitude less annotated data ( 100 vs. 1,', '000 labelled words )']",4
"['a model in the lowresource language  #TAUTHOR_TAG.', 'these methods overall have limited']","['a model in the lowresource language  #TAUTHOR_TAG.', 'these methods overall have limited']","['a model in the lowresource language  #TAUTHOR_TAG.', 'these methods overall have limited']","['- of - speech ( pos ) tagging is an important first step in most natural language processing ( nlp ) applications.', 'typically this is modelled using sequence labelling methods to predict the conditional probability of taggings given word sequences, using linear graphical models  #AUTHOR_TAG, or neural network models, such as recurrent neural networks ( rnn )  #AUTHOR_TAG.', 'these supervised learning algorithms rely on large labelled corpora ; this is particularly true for state - of - the - art neural network models.', 'due to the expense of annotating sufficient data, such techniques are not well suited to applications in low - resource languages.', 'prior work on low - resource nlp has primarily focused on exploiting parallel corpora to project information between a high - and low - resource language  #AUTHOR_TAG tackstrom et al., 2013 ;  #AUTHOR_TAG agic et al., 2016 ;  #AUTHOR_TAG.', 'for example, pos tags can be projected via word alignments, and the projected pos is then used to train a model in the lowresource language  #TAUTHOR_TAG.', 'these methods overall have limited effectiveness due to errors in the alignment and fundamental differences between the languages.', 'they also assume a large parallel corpus, which may not be available for many low - resource languages.', 'to address these limitations, we propose a new technique for low resource tagging, with more modest resource requirements : 1 ) a bilingual dictionary ; 2 ) monolingual corpora in the high and low resource languages ; and 3 ) a small annotated corpus of around 1, 000 tokens in the low - resource language.', 'the first two resources are used as a form of distant supervision through learning crosslingual word embeddings over the monolingual corpora and bilingual dictionary  #AUTHOR_TAG.', 'additionally, our model jointly incorporates the language - dependent information from the small set of gold annotations.', 'our approach combines these two sources of supervision using multi - task learning, such that the kinds of errors that occur in cross - lingual transfer can be accounted for, and corrected automatically.', 'we empirically demonstrate the validity of our observation by using distant supervision to improve pos tagging performance with little supervision.', 'experimental results show the effectiveness of our approach across several low - resource languages, including both simulated and true lowresource settings.', 'furthermore, given the clear superiority of training with manual annotations, we compare several active learning heuristics.', 'active learning using uncertainty sampling with a word - type bias leads to substantial gains over benchmark methods such as token or sentence level uncertainty sampling']",1
"[""a more expressive label mapping than  #TAUTHOR_TAG's linear matrix translation""]","[""a more expressive label mapping than  #TAUTHOR_TAG's linear matrix translation. joint"", 'multi - task learning to combine the two']","[""allows for a more expressive label mapping than  #TAUTHOR_TAG's linear matrix translation. joint"", 'multi - task learning to combine the two']","['∼ categorial ( o t ), with parameters o t = softmax ( w h t + b )', 'as a linear classifier over a sentence encoding, h t, which is the output of a bidirectional lstm encoder over the words. ground truth supervision the second component of the model is manually', 'labelled text in the low - resource language. to model this data we employ the same model structure as above but augmented with a second perceptron output layer', ', as illustrated in figure 1 ( right ). formally, y t ∼ categorial ( o t', ') whereo t = mlp ( o t ) is a single hidden layer perceptron with tanh activation and softmax output transformation. this', ""component allows for a more expressive label mapping than  #TAUTHOR_TAG's linear matrix translation. joint"", 'multi - task learning to combine the two sources of information, we use a joint objective', ', where n and m index the token positions in the distant and ground truth corpora, respectively, and γ is a constant balancing the two components which we set for uniform weighting, γ = | m | | n |. consider the training effect of the true pos tags : when performing', 'error backpropagation, the cross - entropy error signal must pass through the transformation linkingo with o, which can be seen as a language - specific step,', 'after which the generalised error signal can be further backpropagated to the rest of the model. active learning given the scarcity of ground truth labels and the high cost of annotation, a natural', 'question is whether we can optimise which text to be annotated in order achieve the high accuracy for the lowest cost. we now outline a range of active learning approaches based on the following heuristics, which are', 'used to select the instances for annotation from a pool of candidates : token select the token x t with the highest uncertainty, h ( x, t ) = − y p (', 'y | x, t ) log p ( y | x, t ) ; sent select the sentence x with the highest aggregate uncertainty, h ( x ) = t h ( x', ', t ) ; freqtype select the most frequent unannotated word type  #AUTHOR_TAG, in which case all token instances are annotated with the most frequent label for the type in the training corpus ; 1 sumtype select a word type, z, for annotation with the highest aggregate uncertainty over token occurrences, h ( z ) = i', '##∈d x i, t = z h ( x i, t ), which effectively combines uncertainty', 'sampling with a bias towards high frequency types ; and random select word types randomly']",6
"['include bilstm -', 'debias  #TAUTHOR_TAG, applied using']","['not use cross - lingual supervision. for a more direct comparison, we include bilstm -', 'debias  #TAUTHOR_TAG, applied using our proposed cross - lingual supervision based on dictionaries, instead of parallel corpora ; accordingly the key difference is their linear transformation for the distant data, versus']","['not use cross - lingual supervision. for a more direct comparison, we include bilstm -', 'debias  #TAUTHOR_TAG, applied using our proposed cross - lingual supervision based on dictionaries, instead of parallel corpora ; accordingly the key difference is']","['data. note these methods do not use cross - lingual supervision. for a more direct comparison, we include bilstm -', 'debias  #TAUTHOR_TAG, applied using our proposed cross - lingual supervision based on dictionaries, instead of parallel corpora ; accordingly the key difference is their linear transformation for the distant data, versus our non - linear transformation to the gold data. results table 1 reports the tagging accuracy, showing that', 'our models consistently outperform the baseline techniques', '. the poor performance of the supervised methods suggests they are overfitting the small training set, however this is much less of a problem for our approach ( labelled joint )', '. note that distant supervision alone gives reasonable performance ( labelled', 'distant ) however the joint modelling of the ground truth and distant data yields significant improvements in almost all cases. bilstm - debias  #TAUTHOR_TAG performs worse than our proposed method, indicating that', 'a linear transformation is insufficient for modelling distant supervision. the accuracies are higher overall for the european cf. turkic languages, presumably because these languages are', 'table 1 : pos tagging accuracy on over the ten target languages, showing first approaches using only the gold data ; next methods using only distant cross - lingual supervision, and lastly joint multi - task learning. english', 'is used as the source language and columns correspond to a specific target language. closer to english, have higher quality dictionaries and in', 'most cases are morphologically simpler. finally, note the difference between cca', 'and cluster methods for learning word embeddings which arise from the differing quality of distant supervision between the languages. figure 3 compares various active learning', 'heuristics ( see § 3 ) based on different taggers, either a supervised bilstm ( labelled trad ) or our multi', '- task model which also includes crosslingual supervision ( joint ). traditional uncertainty - based sampling strategies ( token ( trad ) and sent ( trad ) ) do not work well because models based on limited supervision do not', 'provide accurate uncertainty information, 5 and moreover, annotating at', 'the type rather than token level provides a significantly stronger supervision signal. the difference is apparent from the', 'decent performance of random sampling over word types. overall, sumtype ( joint ) outperforms', 'the other heuristics consistently, underlining the importance of cross - lingual distant super - vision, as well as combining the benefits of uncertainty sampling, type selection and a frequency bias. comparing the amount of annotation required between the best traditional active learning method sumtype ( trad ) and our best method sumtype ( joint ), we achieve the same performance with an', 'order of magnitude less annotated data ( 100 vs. 1,', '000 labelled words )']",5
"['are designed to protect.', 'our study focuses on racial bias in hate speech and abusive language detection datasets  #TAUTHOR_TAG, all of which use data']","['are designed to protect.', 'our study focuses on racial bias in hate speech and abusive language detection datasets  #TAUTHOR_TAG, all of which use data']","['the same groups they are designed to protect.', 'our study focuses on racial bias in hate speech and abusive language detection datasets  #TAUTHOR_TAG, all of which use data']","['', 'the aim of these models is to identify abusive language that directly targets certain individuals or groups, particularly people belonging to protected categories  #AUTHOR_TAG.', 'bias may reduce the accuracy of these models, and at worst, will mean that the models actively discriminate against the same groups they are designed to protect.', 'our study focuses on racial bias in hate speech and abusive language detection datasets  #TAUTHOR_TAG, all of which use data collected from twitter.', 'we train classifiers using each of the datasets and use a corpus of tweets with demographic information to compare how each classifier performs on tweets written in african - american english ( aae ) versus standard american english ( sae )  #AUTHOR_TAG.', 'we use bootstrap sampling  #AUTHOR_TAG to estimate the proportion of tweets in each group that each classifier assigns to each class.', 'we find evidence of systematic racial biases across all of the classifiers, with aae tweets predicted as belonging to negative classes like hate speech or harassment significantly more frequently than sae tweets.', 'in most cases the bias decreases in magnitude when we condition on particular keywords which may indicate membership in negative classes, yet it still persists.', 'we expect that these biases will result in racial discrimination if classifiers trained on any of these datasets are deployed in the field']",1
"['are designed to protect.', 'our study focuses on racial bias in hate speech and abusive language detection datasets  #TAUTHOR_TAG, all of which use data']","['are designed to protect.', 'our study focuses on racial bias in hate speech and abusive language detection datasets  #TAUTHOR_TAG, all of which use data']","['the same groups they are designed to protect.', 'our study focuses on racial bias in hate speech and abusive language detection datasets  #TAUTHOR_TAG, all of which use data']","['', 'the aim of these models is to identify abusive language that directly targets certain individuals or groups, particularly people belonging to protected categories  #AUTHOR_TAG.', 'bias may reduce the accuracy of these models, and at worst, will mean that the models actively discriminate against the same groups they are designed to protect.', 'our study focuses on racial bias in hate speech and abusive language detection datasets  #TAUTHOR_TAG, all of which use data collected from twitter.', 'we train classifiers using each of the datasets and use a corpus of tweets with demographic information to compare how each classifier performs on tweets written in african - american english ( aae ) versus standard american english ( sae )  #AUTHOR_TAG.', 'we use bootstrap sampling  #AUTHOR_TAG to estimate the proportion of tweets in each group that each classifier assigns to each class.', 'we find evidence of systematic racial biases across all of the classifiers, with aae tweets predicted as belonging to negative classes like hate speech or harassment significantly more frequently than sae tweets.', 'in most cases the bias decreases in magnitude when we condition on particular keywords which may indicate membership in negative classes, yet it still persists.', 'we expect that these biases will result in racial discrimination if classifiers trained on any of these datasets are deployed in the field']",0
"['of these datasets in chronological order.', ' #TAUTHOR_TAG collected 130k tweets containing one of seventeen different terms or phrases']","['of these datasets in chronological order.', ' #TAUTHOR_TAG collected 130k tweets containing one of seventeen different terms or phrases']","['of these datasets in chronological order.', ' #TAUTHOR_TAG collected 130k tweets containing one of seventeen different terms or phrases they considered to be hat']","['focus on twitter, the most widely used data source in abusive language research.', 'we use all available datasets where tweets are labeled as various types of abuse and are written in english.', 'we now briefly describe each of these datasets in chronological order.', ' #TAUTHOR_TAG collected 130k tweets containing one of seventeen different terms or phrases they considered to be hateful.', 'they then annotated a sample of these tweets themselves, using guidelines inspired by critical race theory.', 'these annotators were then reviewed by "" a 25 year old woman studying gender studies and a nonactivist feminist "" to check for bias.', '']",0
['on  #TAUTHOR_TAG and  #AUTHOR_TAG are'],"['n * gga "", classifiers trained on  #TAUTHOR_TAG and  #AUTHOR_TAG are']","['on  #TAUTHOR_TAG and  #AUTHOR_TAG are both predict black - aligned', 'tweets to be instances of sexism approximately 1']","['', 'changes. table 3 shows that for tweets containing the word "" n * gga "", classifiers trained on  #TAUTHOR_TAG and  #AUTHOR_TAG are both predict black - aligned', 'tweets to be instances of sexism approximately 1. 5 times as often as white - aligned tweets. the classifier trained on the data is significantly less likely to classify black - aligned tweets as hate speech, although', 'it is more likely to classify them as offensive.  #AUTHOR_TAG classifies black - aligned tweets as harassment at a higher rate for both groups than in the previous experiment, although the', 'disparity is narrower. for the  #AUTHOR_TAG classifier we see that black - aligned tweets are slightly less frequently considered to be hate speech but are much', 'more frequently classified as abusive. the results for the second variation of experiment 2 where we conditioned on the word "" b * tch "" are', 'shown in table 4. we see similar results for  #TAUTHOR_TAG and  #AUTHOR_TAG. in both cases the classifiers trained upon their data are still more likely to flag black - aligned tweets as sexism. the  #TAUTHOR_TAG classifier', 'is particularly sensitive to the word "" b * tch "" with 96 % of black -', 'aligned and 94 % of white - aligned tweets predicted to belong to this class. for almost all', 'of these tweets are classified as offensive, however those in the blackaligned corpus are 1. 15 times as frequently', '']",0
"['the authors  #TAUTHOR_TAG, an']","['the authors  #TAUTHOR_TAG, an']","['the authors  #TAUTHOR_TAG, an approach demonstrated']","['', 'some studies sampled tweets using small, ad hoc sets of keywords created by the authors  #TAUTHOR_TAG, an approach demonstrated to produce poor results  #AUTHOR_TAG.', '']",0
"['the authors  #TAUTHOR_TAG, an']","['the authors  #TAUTHOR_TAG, an']","['the authors  #TAUTHOR_TAG, an approach demonstrated']","['', 'some studies sampled tweets using small, ad hoc sets of keywords created by the authors  #TAUTHOR_TAG, an approach demonstrated to produce poor results  #AUTHOR_TAG.', '']",0
['on  #TAUTHOR_TAG and  #AUTHOR_TAG are'],"['n * gga "", classifiers trained on  #TAUTHOR_TAG and  #AUTHOR_TAG are']","['on  #TAUTHOR_TAG and  #AUTHOR_TAG are both predict black - aligned', 'tweets to be instances of sexism approximately 1']","['', 'changes. table 3 shows that for tweets containing the word "" n * gga "", classifiers trained on  #TAUTHOR_TAG and  #AUTHOR_TAG are both predict black - aligned', 'tweets to be instances of sexism approximately 1. 5 times as often as white - aligned tweets. the classifier trained on the data is significantly less likely to classify black - aligned tweets as hate speech, although', 'it is more likely to classify them as offensive.  #AUTHOR_TAG classifies black - aligned tweets as harassment at a higher rate for both groups than in the previous experiment, although the', 'disparity is narrower. for the  #AUTHOR_TAG classifier we see that black - aligned tweets are slightly less frequently considered to be hate speech but are much', 'more frequently classified as abusive. the results for the second variation of experiment 2 where we conditioned on the word "" b * tch "" are', 'shown in table 4. we see similar results for  #TAUTHOR_TAG and  #AUTHOR_TAG. in both cases the classifiers trained upon their data are still more likely to flag black - aligned tweets as sexism. the  #TAUTHOR_TAG classifier', 'is particularly sensitive to the word "" b * tch "" with 96 % of black -', 'aligned and 94 % of white - aligned tweets predicted to belong to this class. for almost all', 'of these tweets are classified as offensive, however those in the blackaligned corpus are 1. 15 times as frequently', '']",4
['from  #TAUTHOR_TAG and  #AUTHOR_TAG only predicted'],['from  #TAUTHOR_TAG and  #AUTHOR_TAG only predicted'],['from  #TAUTHOR_TAG and  #AUTHOR_TAG only predicted'],"['', 'classifiers trained on data from  #TAUTHOR_TAG and  #AUTHOR_TAG only predicted a small fraction of the tweets to be racism.', 'we suspect that this is due to the composition of their dataset, since the majority of the racist training examples consist of anti - muslim rather than anti - table 4 : experiment 2, t = "" b * tch "" black language.', 'across both datasets the words "" n * gger "" and "" n * gga "" appear in 4 and 10 tweets respectively.', 'looking at the sexism class on the other hand, we see that both models were consistently classifying tweets in the black - aligned corpus as sexism at a substantially higher rate than those in the white - aligned corpus.', 'given this result, and the gender biases identified in these data by  #AUTHOR_TAG, it not apparent that the purportedly expert annotators were any less biased than amateur annotators  #AUTHOR_TAG.', 'the classifier trained on shows the largest disparities in experiment 1, with tweets in the black - aligned corpus classified as hate speech and offensive language at substantially higher rates than white - aligned tweets.', '']",4
['on  #TAUTHOR_TAG and  #AUTHOR_TAG are'],"['n * gga "", classifiers trained on  #TAUTHOR_TAG and  #AUTHOR_TAG are']","['on  #TAUTHOR_TAG and  #AUTHOR_TAG are both predict black - aligned', 'tweets to be instances of sexism approximately 1']","['', 'changes. table 3 shows that for tweets containing the word "" n * gga "", classifiers trained on  #TAUTHOR_TAG and  #AUTHOR_TAG are both predict black - aligned', 'tweets to be instances of sexism approximately 1. 5 times as often as white - aligned tweets. the classifier trained on the data is significantly less likely to classify black - aligned tweets as hate speech, although', 'it is more likely to classify them as offensive.  #AUTHOR_TAG classifies black - aligned tweets as harassment at a higher rate for both groups than in the previous experiment, although the', 'disparity is narrower. for the  #AUTHOR_TAG classifier we see that black - aligned tweets are slightly less frequently considered to be hate speech but are much', 'more frequently classified as abusive. the results for the second variation of experiment 2 where we conditioned on the word "" b * tch "" are', 'shown in table 4. we see similar results for  #TAUTHOR_TAG and  #AUTHOR_TAG. in both cases the classifiers trained upon their data are still more likely to flag black - aligned tweets as sexism. the  #TAUTHOR_TAG classifier', 'is particularly sensitive to the word "" b * tch "" with 96 % of black -', 'aligned and 94 % of white - aligned tweets predicted to belong to this class. for almost all', 'of these tweets are classified as offensive, however those in the blackaligned corpus are 1. 15 times as frequently', '']",3
['on  #TAUTHOR_TAG and  #AUTHOR_TAG are'],"['n * gga "", classifiers trained on  #TAUTHOR_TAG and  #AUTHOR_TAG are']","['on  #TAUTHOR_TAG and  #AUTHOR_TAG are both predict black - aligned', 'tweets to be instances of sexism approximately 1']","['', 'changes. table 3 shows that for tweets containing the word "" n * gga "", classifiers trained on  #TAUTHOR_TAG and  #AUTHOR_TAG are both predict black - aligned', 'tweets to be instances of sexism approximately 1. 5 times as often as white - aligned tweets. the classifier trained on the data is significantly less likely to classify black - aligned tweets as hate speech, although', 'it is more likely to classify them as offensive.  #AUTHOR_TAG classifies black - aligned tweets as harassment at a higher rate for both groups than in the previous experiment, although the', 'disparity is narrower. for the  #AUTHOR_TAG classifier we see that black - aligned tweets are slightly less frequently considered to be hate speech but are much', 'more frequently classified as abusive. the results for the second variation of experiment 2 where we conditioned on the word "" b * tch "" are', 'shown in table 4. we see similar results for  #TAUTHOR_TAG and  #AUTHOR_TAG. in both cases the classifiers trained upon their data are still more likely to flag black - aligned tweets as sexism. the  #TAUTHOR_TAG classifier', 'is particularly sensitive to the word "" b * tch "" with 96 % of black -', 'aligned and 94 % of white - aligned tweets predicted to belong to this class. for almost all', 'of these tweets are classified as offensive, however those in the blackaligned corpus are 1. 15 times as frequently', '']",3
['on  #TAUTHOR_TAG and  #AUTHOR_TAG are'],"['n * gga "", classifiers trained on  #TAUTHOR_TAG and  #AUTHOR_TAG are']","['on  #TAUTHOR_TAG and  #AUTHOR_TAG are both predict black - aligned', 'tweets to be instances of sexism approximately 1']","['', 'changes. table 3 shows that for tweets containing the word "" n * gga "", classifiers trained on  #TAUTHOR_TAG and  #AUTHOR_TAG are both predict black - aligned', 'tweets to be instances of sexism approximately 1. 5 times as often as white - aligned tweets. the classifier trained on the data is significantly less likely to classify black - aligned tweets as hate speech, although', 'it is more likely to classify them as offensive.  #AUTHOR_TAG classifies black - aligned tweets as harassment at a higher rate for both groups than in the previous experiment, although the', 'disparity is narrower. for the  #AUTHOR_TAG classifier we see that black - aligned tweets are slightly less frequently considered to be hate speech but are much', 'more frequently classified as abusive. the results for the second variation of experiment 2 where we conditioned on the word "" b * tch "" are', 'shown in table 4. we see similar results for  #TAUTHOR_TAG and  #AUTHOR_TAG. in both cases the classifiers trained upon their data are still more likely to flag black - aligned tweets as sexism. the  #TAUTHOR_TAG classifier', 'is particularly sensitive to the word "" b * tch "" with 96 % of black -', 'aligned and 94 % of white - aligned tweets predicted to belong to this class. for almost all', 'of these tweets are classified as offensive, however those in the blackaligned corpus are 1. 15 times as frequently', '']",5
"['especially deep learning techniques, researchers are starting to bring more data -', 'driven approaches to this field  #TAUTHOR_TAG. in this paper, we present our approach to nonstan']","['especially deep learning techniques, researchers are starting to bring more data -', 'driven approaches to this field  #TAUTHOR_TAG. in this paper, we present our approach to nonstandar', '##d text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively']","['especially deep learning techniques, researchers are starting to bring more data -', 'driven approaches to this field  #TAUTHOR_TAG. in this paper, we present our approach to nonstandar', '##d text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively']","['', 'then generate the audio signal. normalizing the written form text to its spoken form is difficult due to the following bottlenecks : 1. lack of supervision - there is no incentive for people to produce spoken form text. thus, it is hard', 'to obtain a supervised dataset for training machine learning models ; 2. ambiguity - for written text', ', a change in context may require a different normalization. for example, "" 2 / 3', '"" can be verbalized as a date or fraction depending on the meaning of the', 'sentence. traditionally, the task of nsw normalization has been approached by manually authoring grammars in', 'the form of finite - state transducers  #AUTHOR_TAG such as integer grammars ( e. g., "" 26 "" → "" twenty six', '"" ) or time grammars ( e. g., "" 5 : 26 "" → "" five twenty six "" ). constructing such grammars is time consuming and error - prone and requires extensive linguistic knowledge and programming proficiency. recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data -', 'driven approaches to this field  #TAUTHOR_TAG. in this paper, we present our approach to nonstandar', '##d text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively']",0
"['##n  #TAUTHOR_TAG.', 'to overcome']","['to tn and itn  #TAUTHOR_TAG.', 'to overcome']","['##n  #TAUTHOR_TAG.', 'to overcome']","[', methods based on neural networks have been applied to tn and itn  #TAUTHOR_TAG.', 'to overcome one of the biggest problems - a lack of supervision, wfsts have been used to transform large amounts of written - form text to its spoken form.', 'researchers hope a vast amount of such data can counteract the errors inherited in wfst - based models.', 'recent data - driven approaches examine window - based sequence - to - sequence ( seq2seq ) models and convolutional neural networks ( cnn ) to normalize a central piece of text with the help of context  #TAUTHOR_TAG.', 'window - based methods have the advantage of limiting the output vocabulary size, as most tokens that do not need to be transformed are labeled with a special < self > token.', 'hybrid neural / wfst models have also been proposed and applied to the text normalization problem  #AUTHOR_TAG.', 'tokens in the input are first tagged with labels using machine learned models whereupon a handcrafted grammar corresponding to each label conducts conversion.', 'in both methods, a tagger is needed to first segment / label the input tokens and conversion must be applied to each segment to normalize a full sentence.', 'our seq2seq model does not require the aforementioned tagger ( although could benefit from the tagger as we will show later ) and directly translates a written - form sentence to its spoken form without grammars']",0
"['##n  #TAUTHOR_TAG.', 'to overcome']","['to tn and itn  #TAUTHOR_TAG.', 'to overcome']","['##n  #TAUTHOR_TAG.', 'to overcome']","[', methods based on neural networks have been applied to tn and itn  #TAUTHOR_TAG.', 'to overcome one of the biggest problems - a lack of supervision, wfsts have been used to transform large amounts of written - form text to its spoken form.', 'researchers hope a vast amount of such data can counteract the errors inherited in wfst - based models.', 'recent data - driven approaches examine window - based sequence - to - sequence ( seq2seq ) models and convolutional neural networks ( cnn ) to normalize a central piece of text with the help of context  #TAUTHOR_TAG.', 'window - based methods have the advantage of limiting the output vocabulary size, as most tokens that do not need to be transformed are labeled with a special < self > token.', 'hybrid neural / wfst models have also been proposed and applied to the text normalization problem  #AUTHOR_TAG.', 'tokens in the input are first tagged with labels using machine learned models whereupon a handcrafted grammar corresponding to each label conducts conversion.', 'in both methods, a tagger is needed to first segment / label the input tokens and conversion must be applied to each segment to normalize a full sentence.', 'our seq2seq model does not require the aforementioned tagger ( although could benefit from the tagger as we will show later ) and directly translates a written - form sentence to its spoken form without grammars']",0
"['especially deep learning techniques, researchers are starting to bring more data -', 'driven approaches to this field  #TAUTHOR_TAG. in this paper, we present our approach to nonstan']","['especially deep learning techniques, researchers are starting to bring more data -', 'driven approaches to this field  #TAUTHOR_TAG. in this paper, we present our approach to nonstandar', '##d text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively']","['especially deep learning techniques, researchers are starting to bring more data -', 'driven approaches to this field  #TAUTHOR_TAG. in this paper, we present our approach to nonstandar', '##d text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively']","['', 'then generate the audio signal. normalizing the written form text to its spoken form is difficult due to the following bottlenecks : 1. lack of supervision - there is no incentive for people to produce spoken form text. thus, it is hard', 'to obtain a supervised dataset for training machine learning models ; 2. ambiguity - for written text', ', a change in context may require a different normalization. for example, "" 2 / 3', '"" can be verbalized as a date or fraction depending on the meaning of the', 'sentence. traditionally, the task of nsw normalization has been approached by manually authoring grammars in', 'the form of finite - state transducers  #AUTHOR_TAG such as integer grammars ( e. g., "" 26 "" → "" twenty six', '"" ) or time grammars ( e. g., "" 5 : 26 "" → "" five twenty six "" ). constructing such grammars is time consuming and error - prone and requires extensive linguistic knowledge and programming proficiency. recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data -', 'driven approaches to this field  #TAUTHOR_TAG. in this paper, we present our approach to nonstandar', '##d text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively']",1
"[' #TAUTHOR_TAG, we implement a seq2seq']","[' #TAUTHOR_TAG, we implement a seq2seq']","[' #TAUTHOR_TAG, we implement a seq2seq model']","[' #TAUTHOR_TAG, we implement a seq2seq model trained on window - based data.', 'table 1 illustrates the window - based model\'s training examples corresponding to one sentence "" wake me up at 8 am. "" which is broken down into 6 pairs.', '< n > and < / n > indicate the center of the window.', 'a window center might contain 1 or more words ( e. g., "" 8 am "" ) and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as time, date, ordinal  #TAUTHOR_TAG.', '']",5
"[' #TAUTHOR_TAG, we implement a seq2seq']","[' #TAUTHOR_TAG, we implement a seq2seq']","[' #TAUTHOR_TAG, we implement a seq2seq model']","[' #TAUTHOR_TAG, we implement a seq2seq model trained on window - based data.', 'table 1 illustrates the window - based model\'s training examples corresponding to one sentence "" wake me up at 8 am. "" which is broken down into 6 pairs.', '< n > and < / n > indicate the center of the window.', 'a window center might contain 1 or more words ( e. g., "" 8 am "" ) and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as time, date, ordinal  #TAUTHOR_TAG.', '']",5
"['##ted text from  #TAUTHOR_TAG.', 'the set']","['of parallel written / speech formatted text from  #TAUTHOR_TAG.', 'the set']","['##ted text from  #TAUTHOR_TAG.', 'the set']","['data for the window - based seq2seq model and full sentence seq2seq were generated from the publicly available release of parallel written / speech formatted text from  #TAUTHOR_TAG.', ""the set consists of wikipedia text which was processed through google tts's kestrel text normalization system relying primarily on handcrafted rules to produce speech - formatted text."", 'although a large parallel dataset is available for english, we consider the feasibility of developing neural models for other languages which may not have text normalization systems in place.', 'therefore, we choose to scale the training data size to a limited set of text which could be generated by annotators in a reasonable time frame.', 'as summarized in table 2, both window - based and sentencebased models are trained with 500k training instances.', 'our datasets were randomly sampled from a set of 4. 9m sentences in the training data portion of the  #TAUTHOR_TAG data release and split into training, validation, and test data.', 'however, the training data for window - based and sentencebased models are not identical due to differences in input configurations.', 'while the window - based model uses 500k randomly sampled windows, the sentence - based models use 500k sentences.', 'for testing, 62. 5k identical test sentences are used across all models.', 'in order to decode sentences with the window - based model, sentences are first segmented into windows before inference.', 'among 16 edit labels available in the dataset release, we found the normalization target for table 2 : size of training, validation, and test datasets.', 'for the window - baseline, the data are pairs of windows and the normalization of the central piece of the window.', 'for the sent - baseline and subword models, the data are pairs of sentences but in different formats - sent - baseline : ( character sequence, word sequence ) ; subword : ( subword sequence, subword sequence ).', 'all models are evaluated on the same set of 62. 5k sentences']",5
"['##ted text from  #TAUTHOR_TAG.', 'the set']","['of parallel written / speech formatted text from  #TAUTHOR_TAG.', 'the set']","['##ted text from  #TAUTHOR_TAG.', 'the set']","['data for the window - based seq2seq model and full sentence seq2seq were generated from the publicly available release of parallel written / speech formatted text from  #TAUTHOR_TAG.', ""the set consists of wikipedia text which was processed through google tts's kestrel text normalization system relying primarily on handcrafted rules to produce speech - formatted text."", 'although a large parallel dataset is available for english, we consider the feasibility of developing neural models for other languages which may not have text normalization systems in place.', 'therefore, we choose to scale the training data size to a limited set of text which could be generated by annotators in a reasonable time frame.', 'as summarized in table 2, both window - based and sentencebased models are trained with 500k training instances.', 'our datasets were randomly sampled from a set of 4. 9m sentences in the training data portion of the  #TAUTHOR_TAG data release and split into training, validation, and test data.', 'however, the training data for window - based and sentencebased models are not identical due to differences in input configurations.', 'while the window - based model uses 500k randomly sampled windows, the sentence - based models use 500k sentences.', 'for testing, 62. 5k identical test sentences are used across all models.', 'in order to decode sentences with the window - based model, sentences are first segmented into windows before inference.', 'among 16 edit labels available in the dataset release, we found the normalization target for table 2 : size of training, validation, and test datasets.', 'for the window - baseline, the data are pairs of windows and the normalization of the central piece of the window.', 'for the sent - baseline and subword models, the data are pairs of sentences but in different formats - sent - baseline : ( character sequence, word sequence ) ; subword : ( subword sequence, subword sequence ).', 'all models are evaluated on the same set of 62. 5k sentences']",5
"['forbes dot com "" ).', 'therefore, we exclude electronic data in our experiments.', 'there are large numbers of < self > tokens present in the dataset.', 'we follow  #TAUTHOR_TAG "" tokens to']","['r b e s dot c o m "" ( as opposed to "" forbes dot com "" ).', 'therefore, we exclude electronic data in our experiments.', 'there are large numbers of < self > tokens present in the dataset.', 'we follow  #TAUTHOR_TAG "" tokens to 10 % of the data.', '']","['forbes dot com "" ).', 'therefore, we exclude electronic data in our experiments.', 'there are large numbers of < self > tokens present in the dataset.', 'we follow  #TAUTHOR_TAG "" tokens to']","['electronic text is not suitable for our system as it primarily reads out urls letter by letter, e. g., "" forbes. com "" → "" f o r b e s dot c o m "" ( as opposed to "" forbes dot com "" ).', 'therefore, we exclude electronic data in our experiments.', 'there are large numbers of < self > tokens present in the dataset.', 'we follow  #TAUTHOR_TAG "" tokens to 10 % of the data.', 'for training sentence - based models, the source sentence is segmented into characters while the target sentence is broken into tokens.', 'for the subword model, both the source and target sentences are segmented into subword sequences.', 'subword units are concatenated to words for evaluation']",5
"['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input vocabulary consists of 250 common characters including letters, digits and symbols ( e. g., $ ).', 'the decoder vocabulary consists of 1k tokens including < self > and < sil >, the latter of which is used to normalize punctuation.', ' #AUTHOR_TAG, we use a stacked ( 2 - layer ) bi - directional long short term memory network ( bi - lstm ) as encoder and a stacked ( 2 - layer ) lstm as decoder.', 'we use 512 hidden states for the ( bi - ) lstm.', 'a softmax output distribution is computed over output vocabulary at each decoding step.', 'decoding uses the attention mechanism from  #AUTHOR_TAG and a beam size of 5.', 'word and character embeddings are trained from scratch.', 'we use the opennmt toolkit  #AUTHOR_TAG to train our models on a single p2. 8xlarge amazon ec2 instance.', 'models were trained with stochastic gradient descent ( sgd ) on 200k timesteps ( approximately 13 epochs ).', '']",5
"['capitalization :', 'upper, lower, mixed, nonalphanumerical, foreign characters ; 2 ) position :  #AUTHOR_TAG.', 'edit labels are the most expensive to obtain in real life.', 'our labels are generated directly from the google fst  #TAUTHOR_TAG.', '']","['capitalization :', 'upper, lower, mixed, nonalphanumerical, foreign characters ; 2 ) position :  #AUTHOR_TAG.', 'edit labels are the most expensive to obtain in real life.', 'our labels are generated directly from the google fst  #TAUTHOR_TAG.', '']","['use the following linguistic features : 1 ) capitalization :', 'upper, lower, mixed, nonalphanumerical, foreign characters ; 2 ) position :  #AUTHOR_TAG.', 'edit labels are the most expensive to obtain in real life.', 'our labels are generated directly from the google fst  #TAUTHOR_TAG.', 'each type of feature is represented by a one - hot encoding.', 'to combine linguistic features with sub']","['use the following linguistic features : 1 ) capitalization :', 'upper, lower, mixed, nonalphanumerical, foreign characters ; 2 ) position :  #AUTHOR_TAG.', 'edit labels are the most expensive to obtain in real life.', 'our labels are generated directly from the google fst  #TAUTHOR_TAG.', 'each type of feature is represented by a one - hot encoding.', ""to combine linguistic features with subword units, one can add or concatenate each subword's embedding with its corresponding linguistic feature embedding and feed a combined embedding to the bi - lstm encoder."", 'or, a multi - layer perceptron ( mlp ) can be applied to combine information in a non - linear way.', 'our experiments find that concatenation outperforms the other two methods.', 'in table 4 we can see that the subword model with linguistic features produces the lowest ser ( 0. 78 % ) and wer ( 0. 17 % ).', 'in addition, results from the ablation study show that each feature makes a positive contribution to the model.', 'however, edit labels seem to make the strongest contribution.', 'we acknowledge that edit labels may not always be readily available.', 'the model which utilizes all linguistic features except for edit labels still shows a 16 % relative ser reduction and 14 % wer reduction over the subword model without linguistic features']",5
"['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input vocabulary consists of 250 common characters including letters, digits and symbols ( e. g., $ ).', 'the decoder vocabulary consists of 1k tokens including < self > and < sil >, the latter of which is used to normalize punctuation.', ' #AUTHOR_TAG, we use a stacked ( 2 - layer ) bi - directional long short term memory network ( bi - lstm ) as encoder and a stacked ( 2 - layer ) lstm as decoder.', 'we use 512 hidden states for the ( bi - ) lstm.', 'a softmax output distribution is computed over output vocabulary at each decoding step.', 'decoding uses the attention mechanism from  #AUTHOR_TAG and a beam size of 5.', 'word and character embeddings are trained from scratch.', 'we use the opennmt toolkit  #AUTHOR_TAG to train our models on a single p2. 8xlarge amazon ec2 instance.', 'models were trained with stochastic gradient descent ( sgd ) on 200k timesteps ( approximately 13 epochs ).', '']",4
"['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input vocabulary consists of 250 common characters including letters, digits and symbols ( e. g., $ ).', 'the decoder vocabulary consists of 1k tokens including < self > and < sil >, the latter of which is used to normalize punctuation.', ' #AUTHOR_TAG, we use a stacked ( 2 - layer ) bi - directional long short term memory network ( bi - lstm ) as encoder and a stacked ( 2 - layer ) lstm as decoder.', 'we use 512 hidden states for the ( bi - ) lstm.', 'a softmax output distribution is computed over output vocabulary at each decoding step.', 'decoding uses the attention mechanism from  #AUTHOR_TAG and a beam size of 5.', 'word and character embeddings are trained from scratch.', 'we use the opennmt toolkit  #AUTHOR_TAG to train our models on a single p2. 8xlarge amazon ec2 instance.', 'models were trained with stochastic gradient descent ( sgd ) on 200k timesteps ( approximately 13 epochs ).', '']",4
"['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input vocabulary consists of 250 common characters including letters, digits and symbols ( e. g., $ ).', 'the decoder vocabulary consists of 1k tokens including < self > and < sil >, the latter of which is used to normalize punctuation.', ' #AUTHOR_TAG, we use a stacked ( 2 - layer ) bi - directional long short term memory network ( bi - lstm ) as encoder and a stacked ( 2 - layer ) lstm as decoder.', 'we use 512 hidden states for the ( bi - ) lstm.', 'a softmax output distribution is computed over output vocabulary at each decoding step.', 'decoding uses the attention mechanism from  #AUTHOR_TAG and a beam size of 5.', 'word and character embeddings are trained from scratch.', 'we use the opennmt toolkit  #AUTHOR_TAG to train our models on a single p2. 8xlarge amazon ec2 instance.', 'models were trained with stochastic gradient descent ( sgd ) on 200k timesteps ( approximately 13 epochs ).', '']",3
"['##er and van  #AUTHOR_TAG.', 'this approach is well suited for non - interactive, static contexts, but recently, there has been increased interest in generation for situated dialog  #TAUTHOR_TAG ;.', 'most human language use']","['to be the problem of producing a single noun phrase that uniquely identifies a referent ( krahmer and van  #AUTHOR_TAG.', 'this approach is well suited for non - interactive, static contexts, but recently, there has been increased interest in generation for situated dialog  #TAUTHOR_TAG ;.', 'most human language use']","['##er and van  #AUTHOR_TAG.', 'this approach is well suited for non - interactive, static contexts, but recently, there has been increased interest in generation for situated dialog  #TAUTHOR_TAG ;.', 'most human language use']","['expression generation is classically considered to be the problem of producing a single noun phrase that uniquely identifies a referent ( krahmer and van  #AUTHOR_TAG.', 'this approach is well suited for non - interactive, static contexts, but recently, there has been increased interest in generation for situated dialog  #TAUTHOR_TAG ;.', 'most human language use takes place in dynamic situations, and psycholinguistic research on humanhuman dialog has proposed that the production of referring expressions should rather be seen as a process that not only depends on the context and the choices of the speaker, but also on the reactions of the addressee.', 'thus the result is often not a single noun phrase but a sequence of installments ( clark and wilkes -  #AUTHOR_TAG, consisting of multiple utterances which may be interleaved with feedback from the addressee.', ""in a setting where the dialog partners have access to a common workspace, they, furthermore, carefully monitor each other's non - linugistic actions, which often replace verbal feedback  #AUTHOR_TAG."", 'the following example from our data illustrates this.', '']",0
"['##er and van  #AUTHOR_TAG.', 'this approach is well suited for non - interactive, static contexts, but recently, there has been increased interest in generation for situated dialog  #TAUTHOR_TAG ;.', 'most human language use']","['to be the problem of producing a single noun phrase that uniquely identifies a referent ( krahmer and van  #AUTHOR_TAG.', 'this approach is well suited for non - interactive, static contexts, but recently, there has been increased interest in generation for situated dialog  #TAUTHOR_TAG ;.', 'most human language use']","['##er and van  #AUTHOR_TAG.', 'this approach is well suited for non - interactive, static contexts, but recently, there has been increased interest in generation for situated dialog  #TAUTHOR_TAG ;.', 'most human language use']","['expression generation is classically considered to be the problem of producing a single noun phrase that uniquely identifies a referent ( krahmer and van  #AUTHOR_TAG.', 'this approach is well suited for non - interactive, static contexts, but recently, there has been increased interest in generation for situated dialog  #TAUTHOR_TAG ;.', 'most human language use takes place in dynamic situations, and psycholinguistic research on humanhuman dialog has proposed that the production of referring expressions should rather be seen as a process that not only depends on the context and the choices of the speaker, but also on the reactions of the addressee.', 'thus the result is often not a single noun phrase but a sequence of installments ( clark and wilkes -  #AUTHOR_TAG, consisting of multiple utterances which may be interleaved with feedback from the addressee.', ""in a setting where the dialog partners have access to a common workspace, they, furthermore, carefully monitor each other's non - linugistic actions, which often replace verbal feedback  #AUTHOR_TAG."", 'the following example from our data illustrates this.', '']",0
"['picks out the target.', ' #TAUTHOR_TAG observed that igs']","['picks out the target.', ' #TAUTHOR_TAG observed that igs']","['picks out the target.', ' #TAUTHOR_TAG observed that igs']","['', 'the second utterance then picks out the target.', "" #TAUTHOR_TAG observed that igs use move instructions to focus the if's attention on a particular area."", 'this is also common in our data.', 'for instance in ( 5 ), the if is asked to turn to directly face the group of buttons containing the target.', ""( 5 ) also shows how igs monitor their partners'actions and respond to them."", 'the if is moving towards the wrong button causing the ig to repeat part of the previous description.', 'similarly, in ( 6 ) the ig produces an elaboration when the if stops moving towards the target, indicating her confusion.', 'in ( 7 ) the ig inserts affirmative feedback when the if reacts correctly to a portion of his utterance.', 'as can be seen in table 2, reference utterances are relatively often followed by affirmative feedback.', 'igs can also take advantage of if actions that are not in direct response to an utterance.', 'this happens in ( 8 ).', 'the if enters a new room and looks around.', 'when she looks towards the target, the ig seizes the opportunity and produces affirmative feedback']",0
"['picks out the target.', ' #TAUTHOR_TAG observed that igs']","['picks out the target.', ' #TAUTHOR_TAG observed that igs']","['picks out the target.', ' #TAUTHOR_TAG observed that igs']","['', 'the second utterance then picks out the target.', "" #TAUTHOR_TAG observed that igs use move instructions to focus the if's attention on a particular area."", 'this is also common in our data.', 'for instance in ( 5 ), the if is asked to turn to directly face the group of buttons containing the target.', ""( 5 ) also shows how igs monitor their partners'actions and respond to them."", 'the if is moving towards the wrong button causing the ig to repeat part of the previous description.', 'similarly, in ( 6 ) the ig produces an elaboration when the if stops moving towards the target, indicating her confusion.', 'in ( 7 ) the ig inserts affirmative feedback when the if reacts correctly to a portion of his utterance.', 'as can be seen in table 2, reference utterances are relatively often followed by affirmative feedback.', 'igs can also take advantage of if actions that are not in direct response to an utterance.', 'this happens in ( 8 ).', 'the if enters a new room and looks around.', 'when she looks towards the target, the ig seizes the opportunity and produces affirmative feedback']",3
"['a description in installments is more effective.', ' #TAUTHOR_TAG as well as have addressed this']","['a description in installments is more effective.', ' #TAUTHOR_TAG as well as have addressed this question, but']","['a description in installments is more effective.', ' #TAUTHOR_TAG as well as have addressed this question, but']","['', ' #TAUTHOR_TAG as well as have addressed this question, but their approaches only make a choice between generating an instruction to move or a uniquely identifying referring expression.', '']",4
"['a description in installments is more effective.', ' #TAUTHOR_TAG as well as have addressed this']","['a description in installments is more effective.', ' #TAUTHOR_TAG as well as have addressed this question, but']","['a description in installments is more effective.', ' #TAUTHOR_TAG as well as have addressed this question, but']","['', ' #TAUTHOR_TAG as well as have addressed this question, but their approaches only make a choice between generating an instruction to move or a uniquely identifying referring expression.', '']",2
"['a description in installments is more effective.', ' #TAUTHOR_TAG as well as have addressed this']","['a description in installments is more effective.', ' #TAUTHOR_TAG as well as have addressed this question, but']","['a description in installments is more effective.', ' #TAUTHOR_TAG as well as have addressed this question, but']","['', ' #TAUTHOR_TAG as well as have addressed this question, but their approaches only make a choice between generating an instruction to move or a uniquely identifying referring expression.', '']",6
"[' #TAUTHOR_TAG -', 'the']","[' #TAUTHOR_TAG -', 'the']","[' #TAUTHOR_TAG -', 'the document planning phase of']","['', 'lies the notion of synchronic and diachronic relations ( sdrs ) whose aim is the identification of the similarities and differences that exist between the documents in the synchronic and diachronic axes. the end result of this methodology is a graph whose vertices are the sdrs and whose nodes are some', 'structures which we call messages. the creation of this graph can be considered as completing - as we have previously argued  #TAUTHOR_TAG -', '']",5
"['1 ] and  #TAUTHOR_TAG.', 'the use of an ontology']","['more fully exposed in [ 1 ] and  #TAUTHOR_TAG.', 'the use of an ontology']","['1 ] and  #TAUTHOR_TAG.', 'the use of an ontology']","['', 'the claim that the same trivial information might be contained in all the documents and thus such trivial information will have a high probability of being included in the final summary - this claim is rebuffed by the nature of the methodology that we have briefly presented in section 2 and more fully exposed in [ 1 ] and  #TAUTHOR_TAG.', 'the use of an ontology and especially the use of the messages guarantee that the system will try to extract information whose nature, we know beforehand, will be non - trivial.', 'of course, this beneficial situation has its drawbacks as well.', 'as we have argued in  #TAUTHOR_TAG the creation of the ontology and the specifications of the messages require a considerable amount of human labor.', 'nevertheless, in section 9 of  #TAUTHOR_TAG we present specific propositions of how this problem can be alleviated.', 'let us now come to the second objection.', '']",5
"['1 ] and  #TAUTHOR_TAG.', 'the use of an ontology']","['more fully exposed in [ 1 ] and  #TAUTHOR_TAG.', 'the use of an ontology']","['1 ] and  #TAUTHOR_TAG.', 'the use of an ontology']","['', 'the claim that the same trivial information might be contained in all the documents and thus such trivial information will have a high probability of being included in the final summary - this claim is rebuffed by the nature of the methodology that we have briefly presented in section 2 and more fully exposed in [ 1 ] and  #TAUTHOR_TAG.', 'the use of an ontology and especially the use of the messages guarantee that the system will try to extract information whose nature, we know beforehand, will be non - trivial.', 'of course, this beneficial situation has its drawbacks as well.', 'as we have argued in  #TAUTHOR_TAG the creation of the ontology and the specifications of the messages require a considerable amount of human labor.', 'nevertheless, in section 9 of  #TAUTHOR_TAG we present specific propositions of how this problem can be alleviated.', 'let us now come to the second objection.', '']",5
,,,,1
"['1 ] and  #TAUTHOR_TAG.', 'the use of an ontology']","['more fully exposed in [ 1 ] and  #TAUTHOR_TAG.', 'the use of an ontology']","['1 ] and  #TAUTHOR_TAG.', 'the use of an ontology']","['', 'the claim that the same trivial information might be contained in all the documents and thus such trivial information will have a high probability of being included in the final summary - this claim is rebuffed by the nature of the methodology that we have briefly presented in section 2 and more fully exposed in [ 1 ] and  #TAUTHOR_TAG.', 'the use of an ontology and especially the use of the messages guarantee that the system will try to extract information whose nature, we know beforehand, will be non - trivial.', 'of course, this beneficial situation has its drawbacks as well.', 'as we have argued in  #TAUTHOR_TAG the creation of the ontology and the specifications of the messages require a considerable amount of human labor.', 'nevertheless, in section 9 of  #TAUTHOR_TAG we present specific propositions of how this problem can be alleviated.', 'let us now come to the second objection.', '']",1
['1 ] and  #TAUTHOR_TAG we thoroughly presented a methodology ('],['[ 1 ] and  #TAUTHOR_TAG we thoroughly presented a methodology ( and applied'],['1 ] and  #TAUTHOR_TAG we thoroughly presented a methodology ('],"['[ 1 ] and  #TAUTHOR_TAG we thoroughly presented a methodology ( and applied it in two different case studies ) which aims towards the creation of summaries from descriptions of evolving events which are emitted from multiple sources.', 'the end result of this methodology is the computational extraction of a structure, which we called a grid.', 'this structure is a directed acyclic graph ( dag ) whose nodes are the messages extracted from the input documents and whose vertices are the synchronic and diachronic relations that connect those messages.', 'the creation of the grid, as we have argued, completes the document planning stage of a typical nlg architecture.', 'nevertheless, it can be the case that the created grid can prove to be large enough in order for the final summary to exceed the required compression rate.', 'in this paper we have presented a probabilistic model which can be applied to the content determination stage of the document planning phase.', 'the application of that model 8 to the extracted grid will have the effect of creating a subset of the original grid ( a sub - grid in other words ) which will contain just the messages that confront to this model as well as the sdrs that connect only the selected messages.', '']",1
['of  #TAUTHOR_TAG using'],['of  #TAUTHOR_TAG using'],['of  #TAUTHOR_TAG using'],"['this paper we present an extension of a successful simple and effective method for extracting parallel sentences from comparable corpora and we apply it to an arabic / english nist system.', 'we experiment with a new terp filter, along with wer and ter filters.', 'we also report a comparison of our approach with that of  #TAUTHOR_TAG using exactly the same corpora and show performance gain by using much lesser data.', 'our approach employs an smt system built from small amounts of parallel texts to translate the source side of the nonparallel corpus.', 'the target side texts are used, along with other corpora, in the language model of this smt system.', 'we then use information retrieval techniques and simple filters to create parallel data from a comparable news corpora.', 'we evaluate the quality of the extracted data by showing that it significantly improves the performance of an smt systems']",4
['by  #TAUTHOR_TAG uses a bilingual lexicon'],['by  #TAUTHOR_TAG uses a bilingual lexicon'],['by  #TAUTHOR_TAG uses a bilingual lexicon'],"['document pair.  #AUTHOR_TAG approach the problem by using a cosine similarity measure to match foreign and english documents. they work on "" very non - parallel corpora "". they then generate all possible sentence pairs and select the best ones based on', 'a threshold on cosine similarity scores. using the extracted sentences they learn a dictionary and iterate over', 'with more sentence pairs. recent work by  #TAUTHOR_TAG uses a bilingual lexicon to translate some of the', '']",4
['as in  #TAUTHOR_TAG which used small'],['as in  #TAUTHOR_TAG which used small'],['as in  #TAUTHOR_TAG which used small amounts'],[' #TAUTHOR_TAG'],4
['of  #TAUTHOR_TAG using'],['of  #TAUTHOR_TAG using'],['of  #TAUTHOR_TAG using'],"['this paper we present an extension of a successful simple and effective method for extracting parallel sentences from comparable corpora and we apply it to an arabic / english nist system.', 'we experiment with a new terp filter, along with wer and ter filters.', 'we also report a comparison of our approach with that of  #TAUTHOR_TAG using exactly the same corpora and show performance gain by using much lesser data.', 'our approach employs an smt system built from small amounts of parallel texts to translate the source side of the nonparallel corpus.', 'the target side texts are used, along with other corpora, in the language model of this smt system.', 'we then use information retrieval techniques and simple filters to create parallel data from a comparable news corpora.', 'we evaluate the quality of the extracted data by showing that it significantly improves the performance of an smt systems']",3
['by  #TAUTHOR_TAG uses a bilingual lexicon'],['by  #TAUTHOR_TAG uses a bilingual lexicon'],['by  #TAUTHOR_TAG uses a bilingual lexicon'],"['document pair.  #AUTHOR_TAG approach the problem by using a cosine similarity measure to match foreign and english documents. they work on "" very non - parallel corpora "". they then generate all possible sentence pairs and select the best ones based on', 'a threshold on cosine similarity scores. using the extracted sentences they learn a dictionary and iterate over', 'with more sentence pairs. recent work by  #TAUTHOR_TAG uses a bilingual lexicon to translate some of the', '']",3
['by  #TAUTHOR_TAG uses a bilingual lexicon'],['by  #TAUTHOR_TAG uses a bilingual lexicon'],['by  #TAUTHOR_TAG uses a bilingual lexicon'],"['document pair.  #AUTHOR_TAG approach the problem by using a cosine similarity measure to match foreign and english documents. they work on "" very non - parallel corpora "". they then generate all possible sentence pairs and select the best ones based on', 'a threshold on cosine similarity scores. using the extracted sentences they learn a dictionary and iterate over', 'with more sentence pairs. recent work by  #TAUTHOR_TAG uses a bilingual lexicon to translate some of the', '']",3
"['algorithm published by  #TAUTHOR_TAG.', 'this corpus contains 1']","['algorithm published by  #TAUTHOR_TAG.', 'this corpus contains 1. 1m sentence pairs ( about 35m words ) which were']","['with the algorithm published by  #TAUTHOR_TAG.', 'this corpus contains 1']","['##c provides extracted parallel texts extracted with the algorithm published by  #TAUTHOR_TAG.', 'this corpus contains 1. 1m sentence pairs ( about 35m words ) which were automatically extracted and aligned from the monolingual arabic and english gigaword corpora, a confidence score being provided for each sentence pair.', 'we also applied our approach on data provided by ldc, but on a different subset.', ""since we had used the recent data sets our corpora were till year 2006, whereas isi's data were till year 2004."", 'we filtered our data according to the time interval of their data ( date information was provided for each sentence pair ) and used them to compare the two data sets.', ""both afp and xin were used in these comparison experiments since the available isi's data was comprised of these two collections."", 'to perform the comparison, we have, firstly, the isi parallel sentences and secondly the parallel sentences extracted by using our approach using the same time frame and comparable corpora as isi.', 'we used our sentences as filtered by the ter filter and added them to the already available 5. 8m of human - translated ( as done in previous experiments ).', 'the result is shown graphically in figure 5.', '']",3
['by  #TAUTHOR_TAG uses a bilingual lexicon'],['by  #TAUTHOR_TAG uses a bilingual lexicon'],['by  #TAUTHOR_TAG uses a bilingual lexicon'],"['document pair.  #AUTHOR_TAG approach the problem by using a cosine similarity measure to match foreign and english documents. they work on "" very non - parallel corpora "". they then generate all possible sentence pairs and select the best ones based on', 'a threshold on cosine similarity scores. using the extracted sentences they learn a dictionary and iterate over', 'with more sentence pairs. recent work by  #TAUTHOR_TAG uses a bilingual lexicon to translate some of the', '']",0
['by  #TAUTHOR_TAG uses a bilingual lexicon'],['by  #TAUTHOR_TAG uses a bilingual lexicon'],['by  #TAUTHOR_TAG uses a bilingual lexicon'],"['document pair.  #AUTHOR_TAG approach the problem by using a cosine similarity measure to match foreign and english documents. they work on "" very non - parallel corpora "". they then generate all possible sentence pairs and select the best ones based on', 'a threshold on cosine similarity scores. using the extracted sentences they learn a dictionary and iterate over', 'with more sentence pairs. recent work by  #TAUTHOR_TAG uses a bilingual lexicon to translate some of the', '']",0
['by  #TAUTHOR_TAG uses a bilingual lexicon'],['by  #TAUTHOR_TAG uses a bilingual lexicon'],['by  #TAUTHOR_TAG uses a bilingual lexicon'],"['document pair.  #AUTHOR_TAG approach the problem by using a cosine similarity measure to match foreign and english documents. they work on "" very non - parallel corpora "". they then generate all possible sentence pairs and select the best ones based on', 'a threshold on cosine similarity scores. using the extracted sentences they learn a dictionary and iterate over', 'with more sentence pairs. recent work by  #TAUTHOR_TAG uses a bilingual lexicon to translate some of the', '']",5
"['. g.  #TAUTHOR_TAG.', 'aligned text segments suggest a boundary - based model of cooccurrence, illustrated in']","['researchers interested in co - occurrence of mutual translations have relied on bitexts where sentence boundaries ( or other text unit boundaries ) were easy to find ( e. g.  #TAUTHOR_TAG.', 'aligned text segments suggest a boundary - based model of cooccurrence, illustrated in']","['. g.  #TAUTHOR_TAG.', 'aligned text segments suggest a boundary - based model of cooccurrence, illustrated in figure 2.', 'for bite']","['', ' #AUTHOR_TAG were the first to use a distance - based model of co - occurrence, although they measured the distance in words rather than in characters.', 'general bitext mapping algorithms are a recent invention.', 'so far, most researchers interested in co - occurrence of mutual translations have relied on bitexts where sentence boundaries ( or other text unit boundaries ) were easy to find ( e. g.  #TAUTHOR_TAG.', 'aligned text segments suggest a boundary - based model of cooccurrence, illustrated in figure 2.', 'for bitexts involving languages with similar word order, a more accurate combined model of co - occurrence can be built using both segment boundary information and the map - distance threshold.', 'as shown in figure 3, each of these constraints eliminates the noise from a characteristic region of the bitext space.', '']",0
"[', by  #TAUTHOR_TAG turns out to be unsound.', 'the problem is easiest to illustrate under the boundary - based']","['( given, e. g., by  #TAUTHOR_TAG turns out to be unsound.', 'the problem is easiest to illustrate under the boundary - based']","['common answer ( given, e. g., by  #TAUTHOR_TAG turns out to be unsound.', 'the problem is easiest to illustrate under the boundary - based model of co - occurrence.', 'given two aligned text segments, the naive way to count co - occurrences is', '( 1 )']","['the boundary - based and distance - based constraints restrict the region of the bitext space where tokens may be considered to co - occur.', 'yet, these constraints do not answer the question of how to count co - occurrences within the restricted regions.', 'it is somewhat surprising that this is a question at all, and most authors ignore it.', 'however, when authors specify their algorithms in sufficient detail to answer this question, the most common answer ( given, e. g., by  #TAUTHOR_TAG turns out to be unsound.', 'the problem is easiest to illustrate under the boundary - based model of co - occurrence.', 'given two aligned text segments, the naive way to count co - occurrences is', '']",0
"['##s.', 'other preconditions may be imposed if certain language - specific resources are available  #TAUTHOR_TAG.', 'for example, parts of speech tend to be preserved in translation  #AUTHOR_TAG.', 'if part - of - speech taggers are available for both languages in']","['- occurrence is a universal precondition for translational equivalence among word tokens in bitexts.', 'other preconditions may be imposed if certain language - specific resources are available  #TAUTHOR_TAG.', 'for example, parts of speech tend to be preserved in translation  #AUTHOR_TAG.', 'if part - of - speech taggers are available for both languages in']","['##s.', 'other preconditions may be imposed if certain language - specific resources are available  #TAUTHOR_TAG.', 'for example, parts of speech tend to be preserved in translation  #AUTHOR_TAG.', 'if part - of - speech taggers are available for both languages in a bitext, and if cases where one part of speech is translated to another are not important']","['- occurrence is a universal precondition for translational equivalence among word tokens in bitexts.', 'other preconditions may be imposed if certain language - specific resources are available  #TAUTHOR_TAG.', 'for example, parts of speech tend to be preserved in translation  #AUTHOR_TAG.', 'if part - of - speech taggers are available for both languages in a bitext, and if cases where one part of speech is translated to another are not important for the intended application, then we can rule out the possibility of translational equivalence for all token pairs involving different parts of speech.', 'a more obvious source of language - specific information is a machine - readable bilingual dictionary ( mrbd ).', 'if token a in one half of the bitext is found to co - occur with token b in the other half, and ( a, b ) is an entry in the mrbd, then it is highly likely that the tokens a and b are indeed mutual translations.', 'in this case, there is no point considering the co - occurrence of a or b with any other token.', 'similarly exclusive candidacy can be granted to cognate token pairs  #AUTHOR_TAG.', 'most published translation models treat co - occurrence counts as counts of potential link tokens  #AUTHOR_TAG.', 'more accurate models may result if the co - occurrence counts are biased with language - specific knowledge.', 'without loss of generality, whenever translation models refer to cooccurrence counts, they can refer to co - occurrence counts that have been filtered using whatever language - specific resources happen to be available.', 'it does not matter if there are dependencies among the different knowledge sources, as long as each is used as a simple filter on the co - occurrence relation  #TAUTHOR_TAG']",0
"['##s.', 'other preconditions may be imposed if certain language - specific resources are available  #TAUTHOR_TAG.', 'for example, parts of speech tend to be preserved in translation  #AUTHOR_TAG.', 'if part - of - speech taggers are available for both languages in']","['- occurrence is a universal precondition for translational equivalence among word tokens in bitexts.', 'other preconditions may be imposed if certain language - specific resources are available  #TAUTHOR_TAG.', 'for example, parts of speech tend to be preserved in translation  #AUTHOR_TAG.', 'if part - of - speech taggers are available for both languages in']","['##s.', 'other preconditions may be imposed if certain language - specific resources are available  #TAUTHOR_TAG.', 'for example, parts of speech tend to be preserved in translation  #AUTHOR_TAG.', 'if part - of - speech taggers are available for both languages in a bitext, and if cases where one part of speech is translated to another are not important']","['- occurrence is a universal precondition for translational equivalence among word tokens in bitexts.', 'other preconditions may be imposed if certain language - specific resources are available  #TAUTHOR_TAG.', 'for example, parts of speech tend to be preserved in translation  #AUTHOR_TAG.', 'if part - of - speech taggers are available for both languages in a bitext, and if cases where one part of speech is translated to another are not important for the intended application, then we can rule out the possibility of translational equivalence for all token pairs involving different parts of speech.', 'a more obvious source of language - specific information is a machine - readable bilingual dictionary ( mrbd ).', 'if token a in one half of the bitext is found to co - occur with token b in the other half, and ( a, b ) is an entry in the mrbd, then it is highly likely that the tokens a and b are indeed mutual translations.', 'in this case, there is no point considering the co - occurrence of a or b with any other token.', 'similarly exclusive candidacy can be granted to cognate token pairs  #AUTHOR_TAG.', 'most published translation models treat co - occurrence counts as counts of potential link tokens  #AUTHOR_TAG.', 'more accurate models may result if the co - occurrence counts are biased with language - specific knowledge.', 'without loss of generality, whenever translation models refer to cooccurrence counts, they can refer to co - occurrence counts that have been filtered using whatever language - specific resources happen to be available.', 'it does not matter if there are dependencies among the different knowledge sources, as long as each is used as a simple filter on the co - occurrence relation  #TAUTHOR_TAG']",0
"['recognition results  #TAUTHOR_TAG 4 ] ).', 'it can be considered as a constraint on candidate selection so that']","['recognition results  #TAUTHOR_TAG 4 ] ).', 'it can be considered as a constraint on candidate selection so that']","['collocation is one source of information that has been proposed as a useful tool to post - process word recognition results  #TAUTHOR_TAG 4 ] ).', 'it can be considered as a constraint on candidate selection so that the word candidate selection problem can be formalized as an instance of constraint satisfaction.', 'relaxation is a typical method for constraint satisfaction problems.', '']","['collocation is one source of information that has been proposed as a useful tool to post - process word recognition results  #TAUTHOR_TAG 4 ] ).', 'it can be considered as a constraint on candidate selection so that the word candidate selection problem can be formalized as an instance of constraint satisfaction.', 'relaxation is a typical method for constraint satisfaction problems.', 'one of the advantages of relaxation is that it can achieve a global effect by using local constraints.', 'previously, a probabilistic relaxation algorithm was proposed for word candidate re - evaluation and selection ( [ 2 ] ).', 'the basic idea of the algorithm is to use word collocation constraints to select the word candidates that have a high probability of occurring simultaneously with word candidates at other nearby locations.', 'the algorithm runs iteratively.', 'in each iteration, the probability of each word candidate is upgraded based on its previous probability, the probabilities of its neighbors and word collocation data.', '']",0
"['1, 2,  #TAUTHOR_TAG 4 ], which can']","['1, 2,  #TAUTHOR_TAG 4 ], which can']","['uses lightweight neural networks [ 1, 2,  #TAUTHOR_TAG 4 ], which can perform inference in real - time even', '']","['enter keyword spotting systems. they solve the aforementioned issues by implementing an on - device mechanism to "" wake up "" the intelligent agent, e. g.,', '"" okay, google "" for triggering the android assistant. this then allows the device to record and transmit a limited segment of relevant speech only', ', obviating the need to be always - listening. specifically, the task of keyword spotting ( kws ) is to detect the presence of pre - specified phrases in', 'a stream of audio, often with the end goal of wake - word detection or simple command recognition on device. currently', ', state of the art uses lightweight neural networks [ 1, 2,  #TAUTHOR_TAG 4 ], which can perform inference in real - time even', '']",0
"['1, 2,  #TAUTHOR_TAG 4 ], which can']","['1, 2,  #TAUTHOR_TAG 4 ], which can']","['uses lightweight neural networks [ 1, 2,  #TAUTHOR_TAG 4 ], which can perform inference in real - time even', '']","['enter keyword spotting systems. they solve the aforementioned issues by implementing an on - device mechanism to "" wake up "" the intelligent agent, e. g.,', '"" okay, google "" for triggering the android assistant. this then allows the device to record and transmit a limited segment of relevant speech only', ', obviating the need to be always - listening. specifically, the task of keyword spotting ( kws ) is to detect the presence of pre - specified phrases in', 'a stream of audio, often with the end goal of wake - word detection or simple command recognition on device. currently', ', state of the art uses lightweight neural networks [ 1, 2,  #TAUTHOR_TAG 4 ], which can perform inference in real - time even', '']",0
"[' #TAUTHOR_TAG 10 ] and wake - word detection [ 2, 1 ].', 'a typical requirement is that such']","[' #TAUTHOR_TAG 10 ] and wake - word detection [ 2, 1 ].', 'a typical requirement is that such']","['spotting.', 'kws is the task of detecting a spoken phrase in audio, applicable to simple command recognition  #TAUTHOR_TAG 10 ] and wake - word detection [ 2, 1 ].', 'a typical requirement is that such a kws system must be small - footprint']","['spotting.', 'kws is the task of detecting a spoken phrase in audio, applicable to simple command recognition  #TAUTHOR_TAG 10 ] and wake - word detection [ 2, 1 ].', 'a typical requirement is that such a kws system must be small - footprint at inference time, since the target platforms are mobile phones, internet - of - things ( iot ) devices, and other portable electronics.', 'to achieve this goal, resource - efficient architectures using convolutional neural networks ( cnns )  #TAUTHOR_TAG 1 ] and recurrent neural networks ( rnns ) [ 2 ] have been proposed, while other works make use of low - bitwidth weights [ 4, 9 ].', 'however, despite the pervasiveness of modern web browsers in devices from smartphones to desktops, and in spite of the availability of javascript - based deep learning toolkits, implementing on - device kws systems in web applications has never been done before.', 'compressing neural networks.', 'sparse matrix storage leads to inefficient computation and storage in general - purpose hardware ; thus, inducing structured sparsity in neural networks, e. g., on entire rows and columns, has been the cornerstone of various compression techniques [ 6, 8 ].', 'network slimming [ 6 ] is one such state - of - the - art approach that have been applied successfully to cnns : first, models are trained with an l 1 penalty on the scale parameters in 2d batch normalization [ 11 ] layers, which encourages entire channels to approach zero.', 'then, a fixed percentage of smallest and hence unimportant scale parameters are removed, along with the correspondent preceding and succeeding filters in the convolution layers ( see figure 1 ).', 'finally, the entire network is fine - tuned on the training set - this entire process can optionally be repeated multiple times']",0
"[' #TAUTHOR_TAG 10 ] and wake - word detection [ 2, 1 ].', 'a typical requirement is that such']","[' #TAUTHOR_TAG 10 ] and wake - word detection [ 2, 1 ].', 'a typical requirement is that such']","['spotting.', 'kws is the task of detecting a spoken phrase in audio, applicable to simple command recognition  #TAUTHOR_TAG 10 ] and wake - word detection [ 2, 1 ].', 'a typical requirement is that such a kws system must be small - footprint']","['spotting.', 'kws is the task of detecting a spoken phrase in audio, applicable to simple command recognition  #TAUTHOR_TAG 10 ] and wake - word detection [ 2, 1 ].', 'a typical requirement is that such a kws system must be small - footprint at inference time, since the target platforms are mobile phones, internet - of - things ( iot ) devices, and other portable electronics.', 'to achieve this goal, resource - efficient architectures using convolutional neural networks ( cnns )  #TAUTHOR_TAG 1 ] and recurrent neural networks ( rnns ) [ 2 ] have been proposed, while other works make use of low - bitwidth weights [ 4, 9 ].', 'however, despite the pervasiveness of modern web browsers in devices from smartphones to desktops, and in spite of the availability of javascript - based deep learning toolkits, implementing on - device kws systems in web applications has never been done before.', 'compressing neural networks.', 'sparse matrix storage leads to inefficient computation and storage in general - purpose hardware ; thus, inducing structured sparsity in neural networks, e. g., on entire rows and columns, has been the cornerstone of various compression techniques [ 6, 8 ].', 'network slimming [ 6 ] is one such state - of - the - art approach that have been applied successfully to cnns : first, models are trained with an l 1 penalty on the scale parameters in 2d batch normalization [ 11 ] layers, which encourages entire channels to approach zero.', 'then, a fixed percentage of smallest and hence unimportant scale parameters are removed, along with the correspondent preceding and succeeding filters in the convolution layers ( see figure 1 ).', 'finally, the entire network is fine - tuned on the training set - this entire process can optionally be repeated multiple times']",0
"['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first']","['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first']","['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first version of the google speech commands dataset [ 10 ], which comprises a total of 65, 000 spoken utterances for 30 short, one - second phrases.', 'to compare with past work  #TAUTHOR_TAG, we pick the following twelve classes : "" yes, "" "" no, "" "" stop, "" "" go, "" "" left, "" "" right, "" "" on, "" "" off, "" unknown, and silence.', 'it contains roughly 2']","['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first version of the google speech commands dataset [ 10 ], which comprises a total of 65, 000 spoken utterances for 30 short, one - second phrases.', 'to compare with past work  #TAUTHOR_TAG, we pick the following twelve classes : "" yes, "" "" no, "" "" stop, "" "" go, "" "" left, "" "" right, "" "" on, "" "" off, "" unknown, and silence.', 'it contains roughly 2, 000 examples per class, including a few background noise samples of both man - made and artificial noise, e. g., washing dishes and white noise.', 'as is standard in speech processing literature, all audio is in 16 - bit pcm, 16khz mono - channel wav format.', 'we use the standard 80 %, 10 %, and 10 % splits for the training, validation, and test sets, respectively  #TAUTHOR_TAG 10 ]']",3
"['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first']","['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first']","['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first version of the google speech commands dataset [ 10 ], which comprises a total of 65, 000 spoken utterances for 30 short, one - second phrases.', 'to compare with past work  #TAUTHOR_TAG, we pick the following twelve classes : "" yes, "" "" no, "" "" stop, "" "" go, "" "" left, "" "" right, "" "" on, "" "" off, "" unknown, and silence.', 'it contains roughly 2']","['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first version of the google speech commands dataset [ 10 ], which comprises a total of 65, 000 spoken utterances for 30 short, one - second phrases.', 'to compare with past work  #TAUTHOR_TAG, we pick the following twelve classes : "" yes, "" "" no, "" "" stop, "" "" go, "" "" left, "" "" right, "" "" on, "" "" off, "" unknown, and silence.', 'it contains roughly 2, 000 examples per class, including a few background noise samples of both man - made and artificial noise, e. g., washing dishes and white noise.', 'as is standard in speech processing literature, all audio is in 16 - bit pcm, 16khz mono - channel wav format.', 'we use the standard 80 %, 10 %, and 10 % splits for the training, validation, and test sets, respectively  #TAUTHOR_TAG 10 ]']",3
"['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first']","['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first']","['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first version of the google speech commands dataset [ 10 ], which comprises a total of 65, 000 spoken utterances for 30 short, one - second phrases.', 'to compare with past work  #TAUTHOR_TAG, we pick the following twelve classes : "" yes, "" "" no, "" "" stop, "" "" go, "" "" left, "" "" right, "" "" on, "" "" off, "" unknown, and silence.', 'it contains roughly 2']","['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first version of the google speech commands dataset [ 10 ], which comprises a total of 65, 000 spoken utterances for 30 short, one - second phrases.', 'to compare with past work  #TAUTHOR_TAG, we pick the following twelve classes : "" yes, "" "" no, "" "" stop, "" "" go, "" "" left, "" "" right, "" "" on, "" "" off, "" unknown, and silence.', 'it contains roughly 2, 000 examples per class, including a few background noise samples of both man - made and artificial noise, e. g., washing dishes and white noise.', 'as is standard in speech processing literature, all audio is in 16 - bit pcm, 16khz mono - channel wav format.', 'we use the standard 80 %, 10 %, and 10 % splits for the training, validation, and test sets, respectively  #TAUTHOR_TAG 10 ]']",3
"['- narrow architectures from tang and lin  #TAUTHOR_TAG as a starting point, which represent prior state of the']","['res8 - narrow architectures from tang and lin  #TAUTHOR_TAG as a starting point, which represent prior state of the']","['- narrow architectures from tang and lin  #TAUTHOR_TAG as a starting point, which represent prior state of the art in residual cnns [ 13 ] for kws.', 'in both models, given the input x ∈ r 101×40, we first expand the input channel - wise']","['use the res8 and res8 - narrow architectures from tang and lin  #TAUTHOR_TAG as a starting point, which represent prior state of the art in residual cnns [ 13 ] for kws.', 'in both models, given the input x ∈ r 101×40, we first expand the input channel - wise by applying a 2d convolution layer with weights w ∈ r cout×1× ( 3×3 ) and padding of one on all sides.', 'this step results in an output ofx ∈ r cout×101×40, which we then downsample using an average pooling layer with a kernel size of ( 4, 3 ).', 'next, inspired by insights in image classification [ 13 ], the output is passed through a series of three residual blocks comprising convolution and batch normalization [ 11 ] layers - figure 2 illustrates one such block.', 'finally, we average - pool across the channels and pass the features through a softmax layer across the twelve classes.', 'in the previous description, we are free to choose c out to dictate the expressiveness and computational footprint of the model.', 'res8 and res8 - narrow choose 45 and 19, respectively, for c out.', 'in total, res8 contains 110k parameters and incurs 30 million multiplies per second of audio, while res8 - narrow uses 19. 9k parameters and incurs 5. 65 million multiplies per second']",3
"['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first']","['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first']","['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first version of the google speech commands dataset [ 10 ], which comprises a total of 65, 000 spoken utterances for 30 short, one - second phrases.', 'to compare with past work  #TAUTHOR_TAG, we pick the following twelve classes : "" yes, "" "" no, "" "" stop, "" "" go, "" "" left, "" "" right, "" "" on, "" "" off, "" unknown, and silence.', 'it contains roughly 2']","['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first version of the google speech commands dataset [ 10 ], which comprises a total of 65, 000 spoken utterances for 30 short, one - second phrases.', 'to compare with past work  #TAUTHOR_TAG, we pick the following twelve classes : "" yes, "" "" no, "" "" stop, "" "" go, "" "" left, "" "" right, "" "" on, "" "" off, "" unknown, and silence.', 'it contains roughly 2, 000 examples per class, including a few background noise samples of both man - made and artificial noise, e. g., washing dishes and white noise.', 'as is standard in speech processing literature, all audio is in 16 - bit pcm, 16khz mono - channel wav format.', 'we use the standard 80 %, 10 %, and 10 % splits for the training, validation, and test sets, respectively  #TAUTHOR_TAG 10 ]']",5
"['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first']","['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first']","['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first version of the google speech commands dataset [ 10 ], which comprises a total of 65, 000 spoken utterances for 30 short, one - second phrases.', 'to compare with past work  #TAUTHOR_TAG, we pick the following twelve classes : "" yes, "" "" no, "" "" stop, "" "" go, "" "" left, "" "" right, "" "" on, "" "" off, "" unknown, and silence.', 'it contains roughly 2']","['consistency with past results  #TAUTHOR_TAG 5 ], we train our models on the first version of the google speech commands dataset [ 10 ], which comprises a total of 65, 000 spoken utterances for 30 short, one - second phrases.', 'to compare with past work  #TAUTHOR_TAG, we pick the following twelve classes : "" yes, "" "" no, "" "" stop, "" "" go, "" "" left, "" "" right, "" "" on, "" "" off, "" unknown, and silence.', 'it contains roughly 2, 000 examples per class, including a few background noise samples of both man - made and artificial noise, e. g., washing dishes and white noise.', 'as is standard in speech processing literature, all audio is in 16 - bit pcm, 16khz mono - channel wav format.', 'we use the standard 80 %, 10 %, and 10 % splits for the training, validation, and test sets, respectively  #TAUTHOR_TAG 10 ]']",5
"['- narrow architectures from tang and lin  #TAUTHOR_TAG as a starting point, which represent prior state of the']","['res8 - narrow architectures from tang and lin  #TAUTHOR_TAG as a starting point, which represent prior state of the']","['- narrow architectures from tang and lin  #TAUTHOR_TAG as a starting point, which represent prior state of the art in residual cnns [ 13 ] for kws.', 'in both models, given the input x ∈ r 101×40, we first expand the input channel - wise']","['use the res8 and res8 - narrow architectures from tang and lin  #TAUTHOR_TAG as a starting point, which represent prior state of the art in residual cnns [ 13 ] for kws.', 'in both models, given the input x ∈ r 101×40, we first expand the input channel - wise by applying a 2d convolution layer with weights w ∈ r cout×1× ( 3×3 ) and padding of one on all sides.', 'this step results in an output ofx ∈ r cout×101×40, which we then downsample using an average pooling layer with a kernel size of ( 4, 3 ).', 'next, inspired by insights in image classification [ 13 ], the output is passed through a series of three residual blocks comprising convolution and batch normalization [ 11 ] layers - figure 2 illustrates one such block.', 'finally, we average - pool across the channels and pass the features through a softmax layer across the twelve classes.', 'in the previous description, we are free to choose c out to dictate the expressiveness and computational footprint of the model.', 'res8 and res8 - narrow choose 45 and 19, respectively, for c out.', 'in total, res8 contains 110k parameters and incurs 30 million multiplies per second of audio, while res8 - narrow uses 19. 9k parameters and incurs 5. 65 million multiplies per second']",5
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","['online support service  #TAUTHOR_TAG.', '']","['', 'we apply various probabilistic methods to improve discourse modelling in the support services domain.', 'in previous work, we collected a small corpus of task - oriented dialogues between customers and support representatives from the msn shopping online support service  #TAUTHOR_TAG.', '']",0
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","['online support service  #TAUTHOR_TAG.', '']","['', 'we apply various probabilistic methods to improve discourse modelling in the support services domain.', 'in previous work, we collected a small corpus of task - oriented dialogues between customers and support representatives from the msn shopping online support service  #TAUTHOR_TAG.', '']",0
"['dialogue act tag set contains 12 dialogue acts, which are intended to represent the illocutionary force of an utterance.', 'the tags were derived in  #TAUTHOR_TAG by manually labelling the msn shopping corpus using the tags that seemed appropriate from a list of 42 tags in  #AUTHOR_TAG.', 'the msn shopping corpus we use comprises']","['dialogue act tag set contains 12 dialogue acts, which are intended to represent the illocutionary force of an utterance.', 'the tags were derived in  #TAUTHOR_TAG by manually labelling the msn shopping corpus using the tags that seemed appropriate from a list of 42 tags in  #AUTHOR_TAG.', 'the msn shopping corpus we use comprises']","['dialogue act tag set contains 12 dialogue acts, which are intended to represent the illocutionary force of an utterance.', 'the tags were derived in  #TAUTHOR_TAG by manually labelling the msn shopping corpus using the tags that seemed appropriate from a list of 42 tags in  #AUTHOR_TAG.', 'the msn shopping corpus we use comprises']","['dialogue act tag set contains 12 dialogue acts, which are intended to represent the illocutionary force of an utterance.', 'the tags were derived in  #TAUTHOR_TAG by manually labelling the msn shopping corpus using the tags that seemed appropriate from a list of 42 tags in  #AUTHOR_TAG.', 'the msn shopping corpus we use comprises approximately 550 utterances and 6, 500 words.', ' #TAUTHOR_TAG describes the manual process of segmenting the messages into utterances and labelling the utterances with dialogue act tags to produce a gold standard version of the data.', 'kappa analysis on both the labelling and segmentation tasks was conducted with results showing high interannotator agreement  #AUTHOR_TAG a )']",0
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","['online support service  #TAUTHOR_TAG.', '']","['', 'we apply various probabilistic methods to improve discourse modelling in the support services domain.', 'in previous work, we collected a small corpus of task - oriented dialogues between customers and support representatives from the msn shopping online support service  #TAUTHOR_TAG.', '']",5
"['dialogue act tag set contains 12 dialogue acts, which are intended to represent the illocutionary force of an utterance.', 'the tags were derived in  #TAUTHOR_TAG by manually labelling the msn shopping corpus using the tags that seemed appropriate from a list of 42 tags in  #AUTHOR_TAG.', 'the msn shopping corpus we use comprises']","['dialogue act tag set contains 12 dialogue acts, which are intended to represent the illocutionary force of an utterance.', 'the tags were derived in  #TAUTHOR_TAG by manually labelling the msn shopping corpus using the tags that seemed appropriate from a list of 42 tags in  #AUTHOR_TAG.', 'the msn shopping corpus we use comprises']","['dialogue act tag set contains 12 dialogue acts, which are intended to represent the illocutionary force of an utterance.', 'the tags were derived in  #TAUTHOR_TAG by manually labelling the msn shopping corpus using the tags that seemed appropriate from a list of 42 tags in  #AUTHOR_TAG.', 'the msn shopping corpus we use comprises']","['dialogue act tag set contains 12 dialogue acts, which are intended to represent the illocutionary force of an utterance.', 'the tags were derived in  #TAUTHOR_TAG by manually labelling the msn shopping corpus using the tags that seemed appropriate from a list of 42 tags in  #AUTHOR_TAG.', 'the msn shopping corpus we use comprises approximately 550 utterances and 6, 500 words.', ' #TAUTHOR_TAG describes the manual process of segmenting the messages into utterances and labelling the utterances with dialogue act tags to produce a gold standard version of the data.', 'kappa analysis on both the labelling and segmentation tasks was conducted with results showing high interannotator agreement  #AUTHOR_TAG a )']",5
['in  #TAUTHOR_TAG by a large margin without impairing'],['in  #TAUTHOR_TAG by a large margin without impairing'],['in  #TAUTHOR_TAG by a large margin without impair'],"['paper aims to recognize bridging anaphora in written text.', 'we develop discourse structure, lexicosemantic and genericity features based on linguistic intuition and corpus research.', 'by using a cascading minority preference system, we show that our approach outperforms the bridging recognition in  #TAUTHOR_TAG by a large margin without impairing the performance on other is classes']",4
"['handled as part of information status ( is ) classification  #TAUTHOR_TAG.', '']","['handled as part of information status ( is ) classification  #TAUTHOR_TAG.', '']","[') recognition that a bridging anaphor is present and ( ii ) identification of the antecedent and contiguity relation.', 'in recent work, these two tasks have been tackled separately, with bridging recognition handled as part of information status ( is ) classification  #TAUTHOR_TAG.', 'each mention in a text gets assigned one is class that describes']","['bridging or associative anaphora  #AUTHOR_TAG, the antecedent and anaphor are not coreferent but are linked via a variety of contiguity relations.', '1 in example 1, the phrases a resident, the stairs and the lobby are bridging anaphors with the antecedent one building.', '2 ( 1 ) one building was upgraded to red status while people were taking things out, and a resident called up the stairs to his girlfriend, telling her to keep sending things down to the lobby.', 'bridging is an important problem as it affects linguistic theory and applications alike.', 'for example, without bridging resolution, entity coherence between the first and second coordinated clause in example 1 cannot be established.', 'this is a problem both for coherence theories such as centering  #AUTHOR_TAG ( where bridging is therefore incorporated as an indirect realization of previous entities ) as well as applications relying on entity coherence modelling, such as readability assessment or sentence ordering  #AUTHOR_TAG.', 'full bridging resolution needs ( i ) recognition that a bridging anaphor is present and ( ii ) identification of the antecedent and contiguity relation.', 'in recent work, these two tasks have been tackled separately, with bridging recognition handled as part of information status ( is ) classification  #TAUTHOR_TAG.', 'each mention in a text gets assigned one is class that describes its accessibility to the reader at a given point in a text, bridging being one possible class.', 'we stay within this framework.', 'bridging recognition is a difficult task, so that we had to report very low results on this is class in previous work  #TAUTHOR_TAG.', ""this is due to the phenomenon's variety, leading to a lack of clear surface features for recognition."", 'instead, we formulate in this paper novel discourse structure and lexicosemantic features as well as features that distinguish bridging from generics ( see section 3 ).', 'in addition, making up between 5 % and 20 % of definite descriptions ( gardent and manuelian, 2005 ;  #AUTHOR_TAG and around 6 % of all nps  #TAUTHOR_TAG, bridging is still less frequent than many other is classes and recognition of minority classes is well known to be more difficult.', 'we therefore use a cascaded classification algorithm to address this problem  #AUTHOR_TAG']",0
"['handled as part of information status ( is ) classification  #TAUTHOR_TAG.', '']","['handled as part of information status ( is ) classification  #TAUTHOR_TAG.', '']","[') recognition that a bridging anaphor is present and ( ii ) identification of the antecedent and contiguity relation.', 'in recent work, these two tasks have been tackled separately, with bridging recognition handled as part of information status ( is ) classification  #TAUTHOR_TAG.', 'each mention in a text gets assigned one is class that describes']","['bridging or associative anaphora  #AUTHOR_TAG, the antecedent and anaphor are not coreferent but are linked via a variety of contiguity relations.', '1 in example 1, the phrases a resident, the stairs and the lobby are bridging anaphors with the antecedent one building.', '2 ( 1 ) one building was upgraded to red status while people were taking things out, and a resident called up the stairs to his girlfriend, telling her to keep sending things down to the lobby.', 'bridging is an important problem as it affects linguistic theory and applications alike.', 'for example, without bridging resolution, entity coherence between the first and second coordinated clause in example 1 cannot be established.', 'this is a problem both for coherence theories such as centering  #AUTHOR_TAG ( where bridging is therefore incorporated as an indirect realization of previous entities ) as well as applications relying on entity coherence modelling, such as readability assessment or sentence ordering  #AUTHOR_TAG.', 'full bridging resolution needs ( i ) recognition that a bridging anaphor is present and ( ii ) identification of the antecedent and contiguity relation.', 'in recent work, these two tasks have been tackled separately, with bridging recognition handled as part of information status ( is ) classification  #TAUTHOR_TAG.', 'each mention in a text gets assigned one is class that describes its accessibility to the reader at a given point in a text, bridging being one possible class.', 'we stay within this framework.', 'bridging recognition is a difficult task, so that we had to report very low results on this is class in previous work  #TAUTHOR_TAG.', ""this is due to the phenomenon's variety, leading to a lack of clear surface features for recognition."", 'instead, we formulate in this paper novel discourse structure and lexicosemantic features as well as features that distinguish bridging from generics ( see section 3 ).', 'in addition, making up between 5 % and 20 % of definite descriptions ( gardent and manuelian, 2005 ;  #AUTHOR_TAG and around 6 % of all nps  #TAUTHOR_TAG, bridging is still less frequent than many other is classes and recognition of minority classes is well known to be more difficult.', 'we therefore use a cascaded classification algorithm to address this problem  #AUTHOR_TAG']",0
"['handled as part of information status ( is ) classification  #TAUTHOR_TAG.', '']","['handled as part of information status ( is ) classification  #TAUTHOR_TAG.', '']","[') recognition that a bridging anaphor is present and ( ii ) identification of the antecedent and contiguity relation.', 'in recent work, these two tasks have been tackled separately, with bridging recognition handled as part of information status ( is ) classification  #TAUTHOR_TAG.', 'each mention in a text gets assigned one is class that describes']","['bridging or associative anaphora  #AUTHOR_TAG, the antecedent and anaphor are not coreferent but are linked via a variety of contiguity relations.', '1 in example 1, the phrases a resident, the stairs and the lobby are bridging anaphors with the antecedent one building.', '2 ( 1 ) one building was upgraded to red status while people were taking things out, and a resident called up the stairs to his girlfriend, telling her to keep sending things down to the lobby.', 'bridging is an important problem as it affects linguistic theory and applications alike.', 'for example, without bridging resolution, entity coherence between the first and second coordinated clause in example 1 cannot be established.', 'this is a problem both for coherence theories such as centering  #AUTHOR_TAG ( where bridging is therefore incorporated as an indirect realization of previous entities ) as well as applications relying on entity coherence modelling, such as readability assessment or sentence ordering  #AUTHOR_TAG.', 'full bridging resolution needs ( i ) recognition that a bridging anaphor is present and ( ii ) identification of the antecedent and contiguity relation.', 'in recent work, these two tasks have been tackled separately, with bridging recognition handled as part of information status ( is ) classification  #TAUTHOR_TAG.', 'each mention in a text gets assigned one is class that describes its accessibility to the reader at a given point in a text, bridging being one possible class.', 'we stay within this framework.', 'bridging recognition is a difficult task, so that we had to report very low results on this is class in previous work  #TAUTHOR_TAG.', ""this is due to the phenomenon's variety, leading to a lack of clear surface features for recognition."", 'instead, we formulate in this paper novel discourse structure and lexicosemantic features as well as features that distinguish bridging from generics ( see section 3 ).', 'in addition, making up between 5 % and 20 % of definite descriptions ( gardent and manuelian, 2005 ;  #AUTHOR_TAG and around 6 % of all nps  #TAUTHOR_TAG, bridging is still less frequent than many other is classes and recognition of minority classes is well known to be more difficult.', 'we therefore use a cascaded classification algorithm to address this problem  #AUTHOR_TAG']",0
"['##ed is  #TAUTHOR_TAG.', 'results within this latter']","['as a subtask of learning fine - grained is  #TAUTHOR_TAG.', 'results within this latter']","['as a subtask of learning fine - grained is  #TAUTHOR_TAG.', 'results within this latter framework']","['bridging research concentrates on antecedent selection only  #AUTHOR_TAG a ;  #AUTHOR_TAG, assuming that bridging recognition has already been performed.', 'previous work on recognition is either limited to definite nps based on heuristics evaluated on small datasets  #AUTHOR_TAG, or models it as a subtask of learning fine - grained is  #TAUTHOR_TAG.', 'results within this latter framework for bridging have been mixed : we reported in  #AUTHOR_TAG low results for bridging in written news text whereas  #AUTHOR_TAG report high results for the four subcategories of bridging annotated in the switchboard dialogue corpus by  #AUTHOR_TAG.', 'we believe this discrepancy to be due to differences in corpus size and genre as well as in bridging definition.', ""bridging in switchboard includes non - anaphoric, syntactically linked part - of and set - member relationships ( such as the building's lobby ), as well as comparative anaphora, the latter being marked by surface indicators such as other, another etc."", 'both types are much easier to identify than anaphoric bridging cases.', '3 in addition, many non - anaphoric lexical cohesion cases have been annotated as bridging in switchbard as well.', 'we also separate bridging recognition and antecedent selection.', '']",0
"[' #TAUTHOR_TAG we classify eight finegrained is categories for nps in written text : old, new and 6 mediated categories ( syntactic, worldknowledge, bridging, comparative, aggregate and function ).', 'this feature set ( table 1,']","[' #TAUTHOR_TAG we classify eight finegrained is categories for nps in written text : old, new and 6 mediated categories ( syntactic, worldknowledge, bridging, comparative, aggregate and function ).', 'this feature set ( table 1,']","[' #TAUTHOR_TAG we classify eight finegrained is categories for nps in written text : old, new and 6 mediated categories ( syntactic, worldknowledge, bridging, comparative, aggregate and function ).', 'this feature set ( table 1,']","[' #TAUTHOR_TAG we classify eight finegrained is categories for nps in written text : old, new and 6 mediated categories ( syntactic, worldknowledge, bridging, comparative, aggregate and function ).', 'this feature set ( table 1, f 1 - f 13 ) works well to identify old, new and several mediated categories.', 'however, it fails to recognize most bridging anaphora which we try to remedy in this work by including more diverse features.', 'discourse structure features ( table 2, f 1 - f 3 ).', 'bridging occurs frequently in sentences where otherwise there would no entity coherence to previous sentences / clauses ( see  #AUTHOR_TAG and  #AUTHOR_TAG b ) for discussions about bridging, entity coherence and centering transitions in the centering framework ).', 'this is especially true for topic nps  #AUTHOR_TAG in such sentences.', 'we follow these insights by identifying coherence gap sentences ( see examples 1, 4, 5, 6, 7, 9 and also 2 ) : a sentence has a coherence gap ( f 1 ) if it has none new local features for bridging discourse of the following three coherence elements : ( 1 ) entity coreference to previous sentences, as approximated via string match or presence of pronouns, ( 2 ) comparative anaphora approximated by mentions modified via a small set of comparative markers ( see also table 1, f 10 premodbycompmarker ), or ( 3 ) proper names.', '']",0
"['handled as part of information status ( is ) classification  #TAUTHOR_TAG.', '']","['handled as part of information status ( is ) classification  #TAUTHOR_TAG.', '']","[') recognition that a bridging anaphor is present and ( ii ) identification of the antecedent and contiguity relation.', 'in recent work, these two tasks have been tackled separately, with bridging recognition handled as part of information status ( is ) classification  #TAUTHOR_TAG.', 'each mention in a text gets assigned one is class that describes']","['bridging or associative anaphora  #AUTHOR_TAG, the antecedent and anaphor are not coreferent but are linked via a variety of contiguity relations.', '1 in example 1, the phrases a resident, the stairs and the lobby are bridging anaphors with the antecedent one building.', '2 ( 1 ) one building was upgraded to red status while people were taking things out, and a resident called up the stairs to his girlfriend, telling her to keep sending things down to the lobby.', 'bridging is an important problem as it affects linguistic theory and applications alike.', 'for example, without bridging resolution, entity coherence between the first and second coordinated clause in example 1 cannot be established.', 'this is a problem both for coherence theories such as centering  #AUTHOR_TAG ( where bridging is therefore incorporated as an indirect realization of previous entities ) as well as applications relying on entity coherence modelling, such as readability assessment or sentence ordering  #AUTHOR_TAG.', 'full bridging resolution needs ( i ) recognition that a bridging anaphor is present and ( ii ) identification of the antecedent and contiguity relation.', 'in recent work, these two tasks have been tackled separately, with bridging recognition handled as part of information status ( is ) classification  #TAUTHOR_TAG.', 'each mention in a text gets assigned one is class that describes its accessibility to the reader at a given point in a text, bridging being one possible class.', 'we stay within this framework.', 'bridging recognition is a difficult task, so that we had to report very low results on this is class in previous work  #TAUTHOR_TAG.', ""this is due to the phenomenon's variety, leading to a lack of clear surface features for recognition."", 'instead, we formulate in this paper novel discourse structure and lexicosemantic features as well as features that distinguish bridging from generics ( see section 3 ).', 'in addition, making up between 5 % and 20 % of definite descriptions ( gardent and manuelian, 2005 ;  #AUTHOR_TAG and around 6 % of all nps  #TAUTHOR_TAG, bridging is still less frequent than many other is classes and recognition of minority classes is well known to be more difficult.', 'we therefore use a cascaded classification algorithm to address this problem  #AUTHOR_TAG']",1
"['handled as part of information status ( is ) classification  #TAUTHOR_TAG.', '']","['handled as part of information status ( is ) classification  #TAUTHOR_TAG.', '']","[') recognition that a bridging anaphor is present and ( ii ) identification of the antecedent and contiguity relation.', 'in recent work, these two tasks have been tackled separately, with bridging recognition handled as part of information status ( is ) classification  #TAUTHOR_TAG.', 'each mention in a text gets assigned one is class that describes']","['bridging or associative anaphora  #AUTHOR_TAG, the antecedent and anaphor are not coreferent but are linked via a variety of contiguity relations.', '1 in example 1, the phrases a resident, the stairs and the lobby are bridging anaphors with the antecedent one building.', '2 ( 1 ) one building was upgraded to red status while people were taking things out, and a resident called up the stairs to his girlfriend, telling her to keep sending things down to the lobby.', 'bridging is an important problem as it affects linguistic theory and applications alike.', 'for example, without bridging resolution, entity coherence between the first and second coordinated clause in example 1 cannot be established.', 'this is a problem both for coherence theories such as centering  #AUTHOR_TAG ( where bridging is therefore incorporated as an indirect realization of previous entities ) as well as applications relying on entity coherence modelling, such as readability assessment or sentence ordering  #AUTHOR_TAG.', 'full bridging resolution needs ( i ) recognition that a bridging anaphor is present and ( ii ) identification of the antecedent and contiguity relation.', 'in recent work, these two tasks have been tackled separately, with bridging recognition handled as part of information status ( is ) classification  #TAUTHOR_TAG.', 'each mention in a text gets assigned one is class that describes its accessibility to the reader at a given point in a text, bridging being one possible class.', 'we stay within this framework.', 'bridging recognition is a difficult task, so that we had to report very low results on this is class in previous work  #TAUTHOR_TAG.', ""this is due to the phenomenon's variety, leading to a lack of clear surface features for recognition."", 'instead, we formulate in this paper novel discourse structure and lexicosemantic features as well as features that distinguish bridging from generics ( see section 3 ).', 'in addition, making up between 5 % and 20 % of definite descriptions ( gardent and manuelian, 2005 ;  #AUTHOR_TAG and around 6 % of all nps  #TAUTHOR_TAG, bridging is still less frequent than many other is classes and recognition of minority classes is well known to be more difficult.', 'we therefore use a cascaded classification algorithm to address this problem  #AUTHOR_TAG']",1
"['##ed is  #TAUTHOR_TAG.', 'results within this latter']","['as a subtask of learning fine - grained is  #TAUTHOR_TAG.', 'results within this latter']","['as a subtask of learning fine - grained is  #TAUTHOR_TAG.', 'results within this latter framework']","['bridging research concentrates on antecedent selection only  #AUTHOR_TAG a ;  #AUTHOR_TAG, assuming that bridging recognition has already been performed.', 'previous work on recognition is either limited to definite nps based on heuristics evaluated on small datasets  #AUTHOR_TAG, or models it as a subtask of learning fine - grained is  #TAUTHOR_TAG.', 'results within this latter framework for bridging have been mixed : we reported in  #AUTHOR_TAG low results for bridging in written news text whereas  #AUTHOR_TAG report high results for the four subcategories of bridging annotated in the switchboard dialogue corpus by  #AUTHOR_TAG.', 'we believe this discrepancy to be due to differences in corpus size and genre as well as in bridging definition.', ""bridging in switchboard includes non - anaphoric, syntactically linked part - of and set - member relationships ( such as the building's lobby ), as well as comparative anaphora, the latter being marked by surface indicators such as other, another etc."", 'both types are much easier to identify than anaphoric bridging cases.', '3 in addition, many non - anaphoric lexical cohesion cases have been annotated as bridging in switchbard as well.', 'we also separate bridging recognition and antecedent selection.', '']",1
"[' #TAUTHOR_TAG we classify eight finegrained is categories for nps in written text : old, new and 6 mediated categories ( syntactic, worldknowledge, bridging, comparative, aggregate and function ).', 'this feature set ( table 1,']","[' #TAUTHOR_TAG we classify eight finegrained is categories for nps in written text : old, new and 6 mediated categories ( syntactic, worldknowledge, bridging, comparative, aggregate and function ).', 'this feature set ( table 1,']","[' #TAUTHOR_TAG we classify eight finegrained is categories for nps in written text : old, new and 6 mediated categories ( syntactic, worldknowledge, bridging, comparative, aggregate and function ).', 'this feature set ( table 1,']","[' #TAUTHOR_TAG we classify eight finegrained is categories for nps in written text : old, new and 6 mediated categories ( syntactic, worldknowledge, bridging, comparative, aggregate and function ).', 'this feature set ( table 1, f 1 - f 13 ) works well to identify old, new and several mediated categories.', 'however, it fails to recognize most bridging anaphora which we try to remedy in this work by including more diverse features.', 'discourse structure features ( table 2, f 1 - f 3 ).', 'bridging occurs frequently in sentences where otherwise there would no entity coherence to previous sentences / clauses ( see  #AUTHOR_TAG and  #AUTHOR_TAG b ) for discussions about bridging, entity coherence and centering transitions in the centering framework ).', 'this is especially true for topic nps  #AUTHOR_TAG in such sentences.', 'we follow these insights by identifying coherence gap sentences ( see examples 1, 4, 5, 6, 7, 9 and also 2 ) : a sentence has a coherence gap ( f 1 ) if it has none new local features for bridging discourse of the following three coherence elements : ( 1 ) entity coreference to previous sentences, as approximated via string match or presence of pronouns, ( 2 ) comparative anaphora approximated by mentions modified via a small set of comparative markers ( see also table 1, f 10 premodbycompmarker ), or ( 3 ) proper names.', '']",1
"[' #TAUTHOR_TAG', '']","[' #TAUTHOR_TAG', '']","['##less variation.', 'however, we observe that bridging anaphors are often licensed because of discourse structure  #TAUTHOR_TAG', 'f 13 precedes']","['', '( 9 )... josephine baker... friends pitched in.', '( 10 ) friends are part of the glue that holds life and faith together.', 'bridging anaphora can have almost limitless variation.', 'however, we observe that bridging anaphors are often licensed because of discourse structure  #TAUTHOR_TAG', 'f 13 precedes ( r ) and / or lexical or world knowledge.', 'with regard to discourse structure,  #AUTHOR_TAG observe that bridging is often needed to establish entity coherence between two adjacent sentences ( examples 1, 2, 4, 5, 6, 7 and 9 ).', 'with regard to lexical and world knowledge, relational noun phrases ( examples 3, 4, 8 and 9 ), building parts ( example 1 ), set membership elements ( example 7 ), or, more rarely, temporal / spatial modification ( example 6 ) may favor a bridging reading.', 'motivated by these observations, we develop discourse structure and lexico - semantic features indicating bridging anaphora as well as features designed to separate genericity from bridging']",5
"['experiments on the corpus provided in  #TAUTHOR_TAG 6.', 'it consists of 50 texts taken from']","['experiments on the corpus provided in  #TAUTHOR_TAG 6.', 'it consists of 50 texts taken from']","['setup.', 'we perform experiments on the corpus provided in  #TAUTHOR_TAG 6.', 'it consists of 50 texts taken from the wsj portion of the ontonotes corpus  #AUTHOR_TAG with almost 11, 000 nps annotated for information status including 663 bridging nps and']","['setup.', 'we perform experiments on the corpus provided in  #TAUTHOR_TAG 6.', 'it consists of 50 texts taken from the wsj portion of the ontonotes corpus  #AUTHOR_TAG with almost 11, 000 nps annotated for information status including 663 bridging nps and their antecedents.', 'all experiments are performed via 10 - fold crossvalidation on documents.', '']",5
"['experiments on the corpus provided in  #TAUTHOR_TAG 6.', 'it consists of 50 texts taken from']","['experiments on the corpus provided in  #TAUTHOR_TAG 6.', 'it consists of 50 texts taken from']","['setup.', 'we perform experiments on the corpus provided in  #TAUTHOR_TAG 6.', 'it consists of 50 texts taken from the wsj portion of the ontonotes corpus  #AUTHOR_TAG with almost 11, 000 nps annotated for information status including 663 bridging nps and']","['setup.', 'we perform experiments on the corpus provided in  #TAUTHOR_TAG 6.', 'it consists of 50 texts taken from the wsj portion of the ontonotes corpus  #AUTHOR_TAG with almost 11, 000 nps annotated for information status including 663 bridging nps and their antecedents.', 'all experiments are performed via 10 - fold crossvalidation on documents.', '']",6
"['combined with additional features, in particular, the discourse features presented by  #TAUTHOR_TAG']","['combined with additional features, in particular, the discourse features presented by  #TAUTHOR_TAG']","['with additional features, in particular, the discourse features presented by  #TAUTHOR_TAG.', 'our neural approach']","['show that a neural approach to the task of non - factoid answer reranking can benefit from the inclusion of tried - and - tested handcrafted features.', 'we present a novel neural network architecture based on a combination of recurrent neural networks that are used to encode questions and answers, and a multilayer perceptron.', 'we show how this approach can be combined with additional features, in particular, the discourse features presented by  #TAUTHOR_TAG.', 'our neural approach achieves state - of - the - art performance on a public dataset from yahoo! answers and its performance is further improved by incorporating the discourse features.', 'additionally, we present a new dataset of ask ubuntu questions where the hybrid approach also achieves good results']",5
['by  #TAUTHOR_TAG and later'],['by  #TAUTHOR_TAG'],['introduced by  #TAUTHOR_TAG and later'],[' #TAUTHOR_TAG'],5
"['be useful,  #TAUTHOR_TAG propose an']","['be useful,  #TAUTHOR_TAG propose an']","['on the intuition that modelling questionanswer structure both within and across sentences could be useful,  #TAUTHOR_TAG propose an answer reranking model based on discourse features combined with lexical semantics.', 'we experimentally evaluate these discourse features - added to']","['on the intuition that modelling questionanswer structure both within and across sentences could be useful,  #TAUTHOR_TAG propose an answer reranking model based on discourse features combined with lexical semantics.', 'we experimentally evaluate these discourse features - added to our model described in section 3 ( the additional features x ext ) and on their own.', '']",5
"['be useful,  #TAUTHOR_TAG propose an']","['be useful,  #TAUTHOR_TAG propose an']","['on the intuition that modelling questionanswer structure both within and across sentences could be useful,  #TAUTHOR_TAG propose an answer reranking model based on discourse features combined with lexical semantics.', 'we experimentally evaluate these discourse features - added to']","['on the intuition that modelling questionanswer structure both within and across sentences could be useful,  #TAUTHOR_TAG propose an answer reranking model based on discourse features combined with lexical semantics.', 'we experimentally evaluate these discourse features - added to our model described in section 3 ( the additional features x ext ) and on their own.', '']",5
['created by  #TAUTHOR_TAG which contains'],['created by  #TAUTHOR_TAG which contains'],"['our experiments, we use two datasets from different cqas.', 'for comparability, we use the dataset created by  #TAUTHOR_TAG which contains']","['our experiments, we use two datasets from different cqas.', 'for comparability, we use the dataset created by  #TAUTHOR_TAG which contains 10k how questions from yahoo! answers. 50 % of it is used for training, 25 % for development and 25 % for testing.', 'each question in this dataset contains at least four user - generated answers.', 'some examples can be found in table 1.', 'further details about this dataset can be found in  #TAUTHOR_TAG.', 'to evaluate our approach on a more technical domain, we create a dataset of ask ubuntu ( au ) questions containing 13k questions, of which 10k are used for training, 0. 5k for development and 2. 5k for testing.', 'the ask ubuntu community is a part of the stack exchange family of forums.', 'forums of this family share the same interface and guidelines.', 'they allow users to post questions and answers and to vote them up and down, resulting in every question and every answer having a score representing the votes it received.', 'the author of the question may select the best answer to their question.', '']",5
['created by  #TAUTHOR_TAG which contains'],['created by  #TAUTHOR_TAG which contains'],"['our experiments, we use two datasets from different cqas.', 'for comparability, we use the dataset created by  #TAUTHOR_TAG which contains']","['our experiments, we use two datasets from different cqas.', 'for comparability, we use the dataset created by  #TAUTHOR_TAG which contains 10k how questions from yahoo! answers. 50 % of it is used for training, 25 % for development and 25 % for testing.', 'each question in this dataset contains at least four user - generated answers.', 'some examples can be found in table 1.', 'further details about this dataset can be found in  #TAUTHOR_TAG.', 'to evaluate our approach on a more technical domain, we create a dataset of ask ubuntu ( au ) questions containing 13k questions, of which 10k are used for training, 0. 5k for development and 2. 5k for testing.', 'the ask ubuntu community is a part of the stack exchange family of forums.', 'forums of this family share the same interface and guidelines.', 'they allow users to post questions and answers and to vote them up and down, resulting in every question and every answer having a score representing the votes it received.', 'the author of the question may select the best answer to their question.', '']",5
"[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines :']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines :']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and the candidate retrieval ( cr ) baseline.', 'the cr baseline uses the same scoring as in  #TAUTHOR_TAG : the questions and the candidate answers are represented using tf - idf over lemmas ; the candidate answers are ranked according to their cosine similarity to the respective question.', 'additionally, we evaluate the discourse features described in section 4 alone : we use them as the representation of the question - answer pairs that are then used as the input to a multilayer perceptron with five hidden layers.', 'on the ya dataset, we also compare our results to the ones reported by  #TAUTHOR_TAG and by  #AUTHOR_TAG.', 'the model described in section 3 is regularized with l2 - regularization and dropout.', 'the development sets are used solely for early stopping and hyperparameter selection.', 'we tune the hyperparameters ( learning rate, l2 regularization rate, dropout probabilities, dimensionality of the embeddings, the network architecture ( the number of hidden layers and units, the use of gru versus lstm ) ) on the development sets.', '']",5
"[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines :']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines :']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and the candidate retrieval ( cr ) baseline.', 'the cr baseline uses the same scoring as in  #TAUTHOR_TAG : the questions and the candidate answers are represented using tf - idf over lemmas ; the candidate answers are ranked according to their cosine similarity to the respective question.', 'additionally, we evaluate the discourse features described in section 4 alone : we use them as the representation of the question - answer pairs that are then used as the input to a multilayer perceptron with five hidden layers.', 'on the ya dataset, we also compare our results to the ones reported by  #TAUTHOR_TAG and by  #AUTHOR_TAG.', 'the model described in section 3 is regularized with l2 - regularization and dropout.', 'the development sets are used solely for early stopping and hyperparameter selection.', 'we tune the hyperparameters ( learning rate, l2 regularization rate, dropout probabilities, dimensionality of the embeddings, the network architecture ( the number of hidden layers and units, the use of gru versus lstm ) ) on the development sets.', '']",5
"[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines :']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines :']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and the candidate retrieval ( cr ) baseline.', 'the cr baseline uses the same scoring as in  #TAUTHOR_TAG : the questions and the candidate answers are represented using tf - idf over lemmas ; the candidate answers are ranked according to their cosine similarity to the respective question.', 'additionally, we evaluate the discourse features described in section 4 alone : we use them as the representation of the question - answer pairs that are then used as the input to a multilayer perceptron with five hidden layers.', 'on the ya dataset, we also compare our results to the ones reported by  #TAUTHOR_TAG and by  #AUTHOR_TAG.', 'the model described in section 3 is regularized with l2 - regularization and dropout.', 'the development sets are used solely for early stopping and hyperparameter selection.', 'we tune the hyperparameters ( learning rate, l2 regularization rate, dropout probabilities, dimensionality of the embeddings, the network architecture ( the number of hidden layers and units, the use of gru versus lstm ) ) on the development sets.', '']",5
"[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines :']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines :']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and the candidate retrieval ( cr ) baseline.', 'the cr baseline uses the same scoring as in  #TAUTHOR_TAG : the questions and the candidate answers are represented using tf - idf over lemmas ; the candidate answers are ranked according to their cosine similarity to the respective question.', 'additionally, we evaluate the discourse features described in section 4 alone : we use them as the representation of the question - answer pairs that are then used as the input to a multilayer perceptron with five hidden layers.', 'on the ya dataset, we also compare our results to the ones reported by  #TAUTHOR_TAG and by  #AUTHOR_TAG.', 'the model described in section 3 is regularized with l2 - regularization and dropout.', 'the development sets are used solely for early stopping and hyperparameter selection.', 'we tune the hyperparameters ( learning rate, l2 regularization rate, dropout probabilities, dimensionality of the embeddings, the network architecture ( the number of hidden layers and units, the use of gru versus lstm ) ) on the development sets.', '']",5
"[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines :']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines :']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and the candidate retrieval ( cr ) baseline.', 'the cr baseline uses the same scoring as in  #TAUTHOR_TAG : the questions and the candidate answers are represented using tf - idf over lemmas ; the candidate answers are ranked according to their cosine similarity to the respective question.', 'additionally, we evaluate the discourse features described in section 4 alone : we use them as the representation of the question - answer pairs that are then used as the input to a multilayer perceptron with five hidden layers.', 'on the ya dataset, we also compare our results to the ones reported by  #TAUTHOR_TAG and by  #AUTHOR_TAG.', 'the model described in section 3 is regularized with l2 - regularization and dropout.', 'the development sets are used solely for early stopping and hyperparameter selection.', 'we tune the hyperparameters ( learning rate, l2 regularization rate, dropout probabilities, dimensionality of the embeddings, the network architecture ( the number of hidden layers and units, the use of gru versus lstm ) ) on the development sets.', '']",5
"['by  #TAUTHOR_TAG.', 'the combined']","['by  #TAUTHOR_TAG.', 'the combined']","['this task by  #TAUTHOR_TAG.', 'the combined approach']","['', '( gold ) warning : this answer is outdated.', 'as of writing this warning ( 6. 10. 2013 ) the kernel - ppa used here is no longer updated. please disregard this answer.', ""sudo apt - add - repository ppa : kernel - ppa / ppa sudo apt - get update sudo apt - get install packagename ( prediction ) since the kernel ppa is not really maintained anymore, here's a semi - automatic script : https : / / github. com / medigeek / kmp - downloader ( q2 ) which language is ubuntu - desktop mostly coded in? i heard it is python ( gold ) poked around in launchpad : ubuntu - desktop to and browsed the source for a few mins."", 'it appears to be a mix of python and shell scripts.', '( prediction ) i think the question referred to the language used to write the applications running on the default installation.', ""it's hard to say which language is used the most, but i would guess c or c + +."", ""this is just a guess and since all languages are pretty equal in terms of outcome, it doesn't really matter."", 'table 5 : example incorrect predictions of the system on the ask ubuntu dataset.', 'recurrent neural networks, then the interaction matrix is calculated, concatenated with external features, and passed as an input to a multilayer perceptron.', 'as external features, we evaluate the discourse features that were found useful for this task by  #TAUTHOR_TAG.', 'the combined approach achieves new state - of - the - art results on two cqa datasets.', 'however, despite these encouraging results, the p @ 1 is still below 40 %.', 'as']",5
['by  #TAUTHOR_TAG and later'],['by  #TAUTHOR_TAG'],['introduced by  #TAUTHOR_TAG and later'],[' #TAUTHOR_TAG'],0
['by  #TAUTHOR_TAG and later'],['by  #TAUTHOR_TAG'],['introduced by  #TAUTHOR_TAG and later'],[' #TAUTHOR_TAG'],0
['the lexical semantic models of  #TAUTHOR_TAG'],['the lexical semantic models of  #TAUTHOR_TAG'],['the lexical semantic models of  #TAUTHOR_TAG'],"['', ' #AUTHOR_TAG describe answer reranking experiments on ya using a diverse range of lexical, syntactic and discourse features.', 'in particular, they show how discourse information can complement distributed lexical semantic information obtained with a skip - gram model  #AUTHOR_TAG.', 'in this paper we use their features ( discussed in detail in section 4 ) in combination with a neural approach.', ' #AUTHOR_TAG improve on the lexical semantic models of  #TAUTHOR_TAG by exploiting indirect associations between words using higher - order models.', 'methods based purely on neural models have gained popularity in various areas of nlp in recent years.', 'the main advantage of these models is that they are often able to achieve state - ofthe - art results while obviating the need for manual feature engineering.', '']",0
"['be useful,  #TAUTHOR_TAG propose an']","['be useful,  #TAUTHOR_TAG propose an']","['on the intuition that modelling questionanswer structure both within and across sentences could be useful,  #TAUTHOR_TAG propose an answer reranking model based on discourse features combined with lexical semantics.', 'we experimentally evaluate these discourse features - added to']","['on the intuition that modelling questionanswer structure both within and across sentences could be useful,  #TAUTHOR_TAG propose an answer reranking model based on discourse features combined with lexical semantics.', 'we experimentally evaluate these discourse features - added to our model described in section 3 ( the additional features x ext ) and on their own.', '']",0
"['be useful,  #TAUTHOR_TAG propose an']","['be useful,  #TAUTHOR_TAG propose an']","['on the intuition that modelling questionanswer structure both within and across sentences could be useful,  #TAUTHOR_TAG propose an answer reranking model based on discourse features combined with lexical semantics.', 'we experimentally evaluate these discourse features - added to']","['on the intuition that modelling questionanswer structure both within and across sentences could be useful,  #TAUTHOR_TAG propose an answer reranking model based on discourse features combined with lexical semantics.', 'we experimentally evaluate these discourse features - added to our model described in section 3 ( the additional features x ext ) and on their own.', '']",0
"[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines :']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines :']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and']","[' #TAUTHOR_TAG and  #AUTHOR_TAG, we implement two baselines : the baseline that selects an answer randomly and the candidate retrieval ( cr ) baseline.', 'the cr baseline uses the same scoring as in  #TAUTHOR_TAG : the questions and the candidate answers are represented using tf - idf over lemmas ; the candidate answers are ranked according to their cosine similarity to the respective question.', 'additionally, we evaluate the discourse features described in section 4 alone : we use them as the representation of the question - answer pairs that are then used as the input to a multilayer perceptron with five hidden layers.', 'on the ya dataset, we also compare our results to the ones reported by  #TAUTHOR_TAG and by  #AUTHOR_TAG.', 'the model described in section 3 is regularized with l2 - regularization and dropout.', 'the development sets are used solely for early stopping and hyperparameter selection.', 'we tune the hyperparameters ( learning rate, l2 regularization rate, dropout probabilities, dimensionality of the embeddings, the network architecture ( the number of hidden layers and units, the use of gru versus lstm ) ) on the development sets.', '']",0
['by  #TAUTHOR_TAG and later'],['by  #TAUTHOR_TAG'],['introduced by  #TAUTHOR_TAG and later'],[' #TAUTHOR_TAG'],6
"[', the results are better than  #TAUTHOR_TAG']","['ya dataset, the results are better than  #TAUTHOR_TAG']","[', the results are better than  #TAUTHOR_TAG and very similar to  #AUTHOR_TAG']","['', '000 iterations. the gru - mlp systems does not use any external data, and learns only from the small training set. the system enriched with the interaction matrix, gr', '##u - mlp - sim, clearly outperforms all the baselines on both datasets, including the mlpdiscourse system. on the ya dataset, the results are better than  #TAUTHOR_TAG and very similar to  #AUTHOR_TAG. on the au dataset the improvement over the cr and the mlp - discourse systems is less remarkable, yet statistically significant.', 'this indicates the benefit of explicitly providing the interaction features to the', 'mlp. the same approach with the additional discourse features described in section 4, referred', 'to as gru - mlp - sim - discourse in table 3, achieves the highest p @ 1 and mrr on the ya dataset and the au dataset. surprisingly, the discourse features are very helpful on the au dataset which is highly technical, with significant parts of the information represented as commands and code. even though the results achieved on both datasets are similar in absolute values,', 'the datasets are very different and the errors might be of a different nature. we provide some insights into the challenges raised by the two datasets in the next section']",4
"[', the results are better than  #TAUTHOR_TAG']","['ya dataset, the results are better than  #TAUTHOR_TAG']","[', the results are better than  #TAUTHOR_TAG and very similar to  #AUTHOR_TAG']","['', '000 iterations. the gru - mlp systems does not use any external data, and learns only from the small training set. the system enriched with the interaction matrix, gr', '##u - mlp - sim, clearly outperforms all the baselines on both datasets, including the mlpdiscourse system. on the ya dataset, the results are better than  #TAUTHOR_TAG and very similar to  #AUTHOR_TAG. on the au dataset the improvement over the cr and the mlp - discourse systems is less remarkable, yet statistically significant.', 'this indicates the benefit of explicitly providing the interaction features to the', 'mlp. the same approach with the additional discourse features described in section 4, referred', 'to as gru - mlp - sim - discourse in table 3, achieves the highest p @ 1 and mrr on the ya dataset and the au dataset. surprisingly, the discourse features are very helpful on the au dataset which is highly technical, with significant parts of the information represented as commands and code. even though the results achieved on both datasets are similar in absolute values,', 'the datasets are very different and the errors might be of a different nature. we provide some insights into the challenges raised by the two datasets in the next section']",4
"['fine - tune a sequence of se type predictions over clauses  #TAUTHOR_TAG, which however is not our']","['top of our paragraph - level model to fine - tune a sequence of se type predictions over clauses  #TAUTHOR_TAG, which however is not our']","['- level model to fine - tune a sequence of se type predictions over clauses  #TAUTHOR_TAG, which however is not our contribution']","['', 'crf layer at the top of our paragraph - level model to fine - tune a sequence of se type predictions over clauses  #TAUTHOR_TAG, which however is not our contribution. experimental results show that our paragraphlevel neural network model greatly improves the performance of se type classification on the same masc + wiki  #TAUTHOR_TAG corpus and achieves robust performance close to human level. in addition, the crf layer further', '']",5
"['adjacent clauses.', 'for example,  #TAUTHOR_TAG reported the fact that generic sentences usually occur together in a paragraph.', 'following  #TAUTHOR_TAG, in']","['adjacent clauses.', 'for example,  #TAUTHOR_TAG reported the fact that generic sentences usually occur together in a paragraph.', 'following  #TAUTHOR_TAG, in']","['adjacent clauses.', 'for example,  #TAUTHOR_TAG reported the fact that generic sentences usually occur together in a paragraph.', 'following  #TAUTHOR_TAG, in']","['studies  #AUTHOR_TAG show that there exist common se label patterns between adjacent clauses.', 'for example,  #TAUTHOR_TAG reported the fact that generic sentences usually occur together in a paragraph.', 'following  #TAUTHOR_TAG, in order to capture se label patterns in our hierarchical recurrent neural network model, we add a crf layer at the top of the softmax prediction layer ( shown in figure 2 ) to fine - tune predicted situation entity types.', 'the crf layer will update a state - transition matrix, which can effectively adjust the current label depending on its preceding and following labels.', 'both the training and decoding procedures of the crf layer can be conducted efficiently using the viterbi algorithm.', 'with the crf layer, the model jointly assigns a sequence of se labels, one label per clause, by considering individual clause representations as well as common se label patterns']",5
"['model on the masc + wiki corpus 7  #TAUTHOR_TAG  #AUTHOR_TAG, we used the same 80 : 20 traintest split with balanced genre distributions.', 'preprocessing : as described in  #TAUTHOR_TAG, texts were split into clauses using spade  #AUTHOR_TAG.', 'there are 4,']","['model on the masc + wiki corpus 7  #TAUTHOR_TAG  #AUTHOR_TAG, we used the same 80 : 20 traintest split with balanced genre distributions.', 'preprocessing : as described in  #TAUTHOR_TAG, texts were split into clauses using spade  #AUTHOR_TAG.', 'there are']","['masc + wiki corpus : we evaluated our neural network model on the masc + wiki corpus 7  #TAUTHOR_TAG  #AUTHOR_TAG, we used the same 80 : 20 traintest split with balanced genre distributions.', 'preprocessing : as described in  #TAUTHOR_TAG, texts were split into clauses using spade  #AUTHOR_TAG.', 'there are 4,']","['masc + wiki corpus : we evaluated our neural network model on the masc + wiki corpus 7  #TAUTHOR_TAG  #AUTHOR_TAG, we used the same 80 : 20 traintest split with balanced genre distributions.', 'preprocessing : as described in  #TAUTHOR_TAG, texts were split into clauses using spade  #AUTHOR_TAG.', 'there are 4, 784 paragraphs in total in the corpus ; and on average, each paragraph contains 9. 6 clauses.', 'in figure 4, the horizontal axis shows the distribution of paragraphs based on the number of clauses in a paragraph.', 'the annotations of clauses are stored in separate files from the text files.', 'to recover the paragraph contexts for each clause, we matched its content with the corresponding raw document']",5
"['model on the masc + wiki corpus 7  #TAUTHOR_TAG  #AUTHOR_TAG, we used the same 80 : 20 traintest split with balanced genre distributions.', 'preprocessing : as described in  #TAUTHOR_TAG, texts were split into clauses using spade  #AUTHOR_TAG.', 'there are 4,']","['model on the masc + wiki corpus 7  #TAUTHOR_TAG  #AUTHOR_TAG, we used the same 80 : 20 traintest split with balanced genre distributions.', 'preprocessing : as described in  #TAUTHOR_TAG, texts were split into clauses using spade  #AUTHOR_TAG.', 'there are']","['masc + wiki corpus : we evaluated our neural network model on the masc + wiki corpus 7  #TAUTHOR_TAG  #AUTHOR_TAG, we used the same 80 : 20 traintest split with balanced genre distributions.', 'preprocessing : as described in  #TAUTHOR_TAG, texts were split into clauses using spade  #AUTHOR_TAG.', 'there are 4,']","['masc + wiki corpus : we evaluated our neural network model on the masc + wiki corpus 7  #TAUTHOR_TAG  #AUTHOR_TAG, we used the same 80 : 20 traintest split with balanced genre distributions.', 'preprocessing : as described in  #TAUTHOR_TAG, texts were split into clauses using spade  #AUTHOR_TAG.', 'there are 4, 784 paragraphs in total in the corpus ; and on average, each paragraph contains 9. 6 clauses.', 'in figure 4, the horizontal axis shows the distribution of paragraphs based on the number of clauses in a paragraph.', 'the annotations of clauses are stored in separate files from the text files.', 'to recover the paragraph contexts for each clause, we matched its content with the corresponding raw document']",5
"['work  #TAUTHOR_TAG on the same task and dataset, we report accuracy']","['work  #TAUTHOR_TAG on the same task and dataset, we report accuracy']","['the previous work  #TAUTHOR_TAG on the same task and dataset, we report accuracy']","['the previous work  #TAUTHOR_TAG on the same task and dataset, we report accuracy and macro - average f1 - score across se types on the test set of masc + wiki.', 'the first section of table 3 shows the results of the previous works.', 'the second section shows the result of our implemented clause - level bi - lstm baseline, which already outperforms the previous best model.', 'this result proves the effectiveness of the bi - lstm + max pooling approach in clause representation learning  #AUTHOR_TAG.', 'the third section reports the performance of the paragraph - level models that uses paragraph - wide contexts as input.', '']",5
['noticed that the previous work  #TAUTHOR_TAG did not publish the'],['noticed that the previous work  #TAUTHOR_TAG did not publish the'],['noticed that the previous work  #TAUTHOR_TAG did not publish'],"['noticed that the previous work  #TAUTHOR_TAG did not publish the class - wise performance of their model on the test set, instead, they reported the detailed performance on the training set using 10 - fold cross - validation.', 'for direct comparisons, we also report our 10 - fold cross - validation results 8 on the training set of masc + wiki.', 'table 2 reports the cross - validation classification results.', '']",5
"['fine - tune a sequence of se type predictions over clauses  #TAUTHOR_TAG, which however is not our']","['top of our paragraph - level model to fine - tune a sequence of se type predictions over clauses  #TAUTHOR_TAG, which however is not our']","['- level model to fine - tune a sequence of se type predictions over clauses  #TAUTHOR_TAG, which however is not our contribution']","['', 'crf layer at the top of our paragraph - level model to fine - tune a sequence of se type predictions over clauses  #TAUTHOR_TAG, which however is not our contribution. experimental results show that our paragraphlevel neural network model greatly improves the performance of se type classification on the same masc + wiki  #TAUTHOR_TAG corpus and achieves robust performance close to human level. in addition, the crf layer further', '']",6
"['fine - tune a sequence of se type predictions over clauses  #TAUTHOR_TAG, which however is not our']","['top of our paragraph - level model to fine - tune a sequence of se type predictions over clauses  #TAUTHOR_TAG, which however is not our']","['- level model to fine - tune a sequence of se type predictions over clauses  #TAUTHOR_TAG, which however is not our contribution']","['', 'crf layer at the top of our paragraph - level model to fine - tune a sequence of se type predictions over clauses  #TAUTHOR_TAG, which however is not our contribution. experimental results show that our paragraphlevel neural network model greatly improves the performance of se type classification on the same masc + wiki  #TAUTHOR_TAG corpus and achieves robust performance close to human level. in addition, the crf layer further', '']",4
"[').', 'to bridge the gap,  #TAUTHOR_TAG created a much larger dataset masc + wiki ( more than']","['accuracy ).', 'to bridge the gap,  #TAUTHOR_TAG created a much larger dataset masc + wiki ( more than 40, 000 clauses )']","[').', 'to bridge the gap,  #TAUTHOR_TAG created a much larger dataset masc + wiki ( more than']","['', 'to bridge the gap,  #TAUTHOR_TAG created a much larger dataset masc + wiki ( more than 40, 000 clauses ) and achieved better se type classification performance ( around 75 % accuracy ) by using rich features extracted from the target clause.', 'the feature sets include pos tags, brown cluster features, syntactic and semantic features of the main verb and main referent as well as features indicating the aspectual nature of a clause.', ' #TAUTHOR_TAG further improved the performance by implementing a sequence labeling ( crf ) model to fine - tune a sequence of se type predictions and noted that much of the performance gain came from modeling the label pattern that generic clauses often occur together.', 'in contrast, we focus on deriving dynamic clause representations informed by paragraph - level contexts and model context influences more extensively.', ' #AUTHOR_TAG proposed a gru based neural network model that predicts the se type for one clause each time, by encoding the content of the target clause using a gru and incorporating several sources of context information, including contents and labels of preceding clauses as well as genre information, using additional separate grus  #AUTHOR_TAG.', 'this model is different from our approach that processes one paragraph ( with a sequence of clauses ) at a time and extensively models inter - dependencies of clauses.', 'other related tasks include predicting aspectual classes of verbs  #AUTHOR_TAG a ), classifying genericity of noun phrases  #AUTHOR_TAG and predicting clause habituality  #AUTHOR_TAG']",4
['the previous system  #TAUTHOR_TAG and the baseline model by a large'],"['the previous system  #TAUTHOR_TAG and the baseline model by a large margin.', 'as shown in']","['the previous system  #TAUTHOR_TAG and the baseline model by a large margin.', 'as shown in']","['that masc + wiki is rich in written genres, we additionally conduct cross - genre classification experiments, where we use one genre of documents for testing and the other genres of documents for training.', 'the purpose of cross - genre experiment is to see whether the model can work robustly across genres.', 'table 4 shows cross - genre experimental results of our neural network models on the training set of masc + wiki by treating each genre as one crossvalidation fold.', 'as we expected, both the macroaverage f1 - score and class - wise f1 scores are lower compared with the results in table 2 where in - genre data were used for model training as well. but the performance drop on the paragraph - level models is little, which clearly outperform the previous system  #TAUTHOR_TAG and the baseline model by a large margin.', 'as shown in table 5, benefited from modeling wider contexts and common se label patterns, our full paragraphlevel model improves performance across almost all the genres.', 'the high performance in the crossgenre setting demonstrates the robustness of our paragraph - level model across genres']",4
"['( ag )  #TAUTHOR_TAG.', 'we experiment with four languages from the uto -']","['( ag )  #TAUTHOR_TAG.', 'we experiment with four languages from the uto - aztecan family.', 'our ag - based approaches']","['of these polysynthetic languages are low - resource.', 'we propose unsupervised approaches for morphological segmentation of low - resource polysynthetic languages based on adaptor grammars ( ag )  #TAUTHOR_TAG.', 'we experiment with four languages from the uto - aztecan family.', 'our ag - based approaches']","['##ynthetic languages pose a challenge for morphological analysis due to the rootmorpheme complexity and to the word class "" squish "".', 'in addition, many of these polysynthetic languages are low - resource.', 'we propose unsupervised approaches for morphological segmentation of low - resource polysynthetic languages based on adaptor grammars ( ag )  #TAUTHOR_TAG.', 'we experiment with four languages from the uto - aztecan family.', 'our ag - based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages']",5
['structure  #TAUTHOR_TAG'],"['structure  #TAUTHOR_TAG eskander et al.,, 2018.', 'our main']","['unsupervised morphological segmentation, where a pcfg is a morphological grammar that specifies word structure  #TAUTHOR_TAG eskander et al.,, 2018.', 'our main goal is to examine the success of adaptor grammars']","['morphology of polysynthetic languages is an emerging field of research.', 'polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations  #AUTHOR_TAG d ;  #AUTHOR_TAG a ).', 'previous approaches include rule - based methods based on finite state transducers  #AUTHOR_TAG, hybrid models  #AUTHOR_TAG b ;  #AUTHOR_TAG, and supervised machine learning, particularly deep learning approaches  #AUTHOR_TAG.', 'while each rule - based method is developed for a specific language ( inuktitut  #AUTHOR_TAG, or arapaho  #AUTHOR_TAG ), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages.', 'we propose an unsupervised approach for morphological segmentation of polysynthetic languages based on adaptor grammars  #AUTHOR_TAG.', 'we experiment with four utoaztecan languages : mexicanero ( mx ), nahuatl ( nh ), wixarika ( wx ) and yorem nokki ( yn )  #AUTHOR_TAG.', 'adaptor grammars ( ags ) are nonparametric bayesian models that generalize probabilistic context free grammars ( pcfg ), and have proven to be successful for unsupervised morphological segmentation, where a pcfg is a morphological grammar that specifies word structure  #TAUTHOR_TAG eskander et al.,, 2018.', 'our main goal is to examine the success of adaptor grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex ( not simply agglutinative ), and where resources are minimal.', 'we use the datasets introduced by  #AUTHOR_TAG in an unsupervised fashion ( unsegmented words ).', 'we design several ag learning setups : 1 ) use the best - on - average ag setup from  #TAUTHOR_TAG ; 2 ) optimize for language using just the small training vocabulary ( unsegmented ) and dev vocabulary ( segmented ) from  #AUTHOR_TAG ; 3 ) approximate the effect of having some linguistic knowledge ; 4 ) learn from all languages at once and 5 ) add additional unsupervised data for nh and wx ( section 3 ).', 'we show that the ag - based approaches outperform other unsupervised methods - m orf essor  #AUTHOR_TAG and m orphochain  #AUTHOR_TAG ) -, and that for two of the languages ( nh and yn ), the best ag - based approaches outperform the best supervised methods ( section 4 )']",5
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['adaptor grammar is typically composed of a pcfg and an adaptor that adapts the probabilities of individual subtrees.', 'for morphological segmentation, a pcfg is a morphological grammar that specifies word structure, where ags learn latent tree structures given a list of words.', 'in this paper, we experiment with the grammars and the learning setups proposed by  #TAUTHOR_TAG, which we outline briefly below.', 'grammars.', 'we use the nine grammars from  #TAUTHOR_TAG eskander et al. (, 2018 that were designed based on three dimensions : 1 ) how the grammar models word structure ( e. g., prefix - stem - suffix vs. morphemes ), 2 ) the level of abstraction in nonterminals ( e. g., compounds, morphemes and submorphemes ) and 3 ) how the output boundaries are specified ( see table 2 for a sample grammars ).', 'for example, the prstsu + sm grammar models the table 2 : sample grammar setups used by  #AUTHOR_TAG  #TAUTHOR_TAG.', '']",5
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['adaptor grammar is typically composed of a pcfg and an adaptor that adapts the probabilities of individual subtrees.', 'for morphological segmentation, a pcfg is a morphological grammar that specifies word structure, where ags learn latent tree structures given a list of words.', 'in this paper, we experiment with the grammars and the learning setups proposed by  #TAUTHOR_TAG, which we outline briefly below.', 'grammars.', 'we use the nine grammars from  #TAUTHOR_TAG eskander et al. (, 2018 that were designed based on three dimensions : 1 ) how the grammar models word structure ( e. g., prefix - stem - suffix vs. morphemes ), 2 ) the level of abstraction in nonterminals ( e. g., compounds, morphemes and submorphemes ) and 3 ) how the output boundaries are specified ( see table 2 for a sample grammars ).', 'for example, the prstsu + sm grammar models the table 2 : sample grammar setups used by  #AUTHOR_TAG  #TAUTHOR_TAG.', '']",5
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['adaptor grammar is typically composed of a pcfg and an adaptor that adapts the probabilities of individual subtrees.', 'for morphological segmentation, a pcfg is a morphological grammar that specifies word structure, where ags learn latent tree structures given a list of words.', 'in this paper, we experiment with the grammars and the learning setups proposed by  #TAUTHOR_TAG, which we outline briefly below.', 'grammars.', 'we use the nine grammars from  #TAUTHOR_TAG eskander et al. (, 2018 that were designed based on three dimensions : 1 ) how the grammar models word structure ( e. g., prefix - stem - suffix vs. morphemes ), 2 ) the level of abstraction in nonterminals ( e. g., compounds, morphemes and submorphemes ) and 3 ) how the output boundaries are specified ( see table 2 for a sample grammars ).', 'for example, the prstsu + sm grammar models the table 2 : sample grammar setups used by  #AUTHOR_TAG  #TAUTHOR_TAG.', '']",5
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['adaptor grammar is typically composed of a pcfg and an adaptor that adapts the probabilities of individual subtrees.', 'for morphological segmentation, a pcfg is a morphological grammar that specifies word structure, where ags learn latent tree structures given a list of words.', 'in this paper, we experiment with the grammars and the learning setups proposed by  #TAUTHOR_TAG, which we outline briefly below.', 'grammars.', 'we use the nine grammars from  #TAUTHOR_TAG eskander et al. (, 2018 that were designed based on three dimensions : 1 ) how the grammar models word structure ( e. g., prefix - stem - suffix vs. morphemes ), 2 ) the level of abstraction in nonterminals ( e. g., compounds, morphemes and submorphemes ) and 3 ) how the output boundaries are specified ( see table 2 for a sample grammars ).', 'for example, the prstsu + sm grammar models the table 2 : sample grammar setups used by  #AUTHOR_TAG  #TAUTHOR_TAG.', '']",5
"['by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is']","['by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is']","['by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is the cascaded prstsu + sm configuration.', 'we use this ag setup for']","['experimented with several setups using ags for unsupervised segmentation.', 'language - independent morphological segmenter.', 'lims is the best - on - average ag setup obtained by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is the cascaded prstsu + sm configuration.', 'we use this ag setup for each of the four languages.', 'we refer to this system as ag lim s.', 'best ag configuration per language.', 'in this experimental setup, we consider all nine grammars from  #TAUTHOR_TAG using both the standard and the cascaded approaches and choosing the one that is best for each polysynthetic language by training on the training set and evaluating on the development set.', '']",5
"['by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is']","['by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is']","['by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is the cascaded prstsu + sm configuration.', 'we use this ag setup for']","['experimented with several setups using ags for unsupervised segmentation.', 'language - independent morphological segmenter.', 'lims is the best - on - average ag setup obtained by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is the cascaded prstsu + sm configuration.', 'we use this ag setup for each of the four languages.', 'we refer to this system as ag lim s.', 'best ag configuration per language.', 'in this experimental setup, we consider all nine grammars from  #TAUTHOR_TAG using both the standard and the cascaded approaches and choosing the one that is best for each polysynthetic language by training on the training set and evaluating on the development set.', '']",5
"[' #TAUTHOR_TAG eskander et al. (, 2018 for languages that are not polysynthetic.', 'we showed that even when using these approaches and very little data, we can obtain encouraging results, and that using additional unsupervised data is a promising path']","[' #TAUTHOR_TAG eskander et al. (, 2018 for languages that are not polysynthetic.', 'we showed that even when using these approaches and very little data, we can obtain encouraging results, and that using additional unsupervised data is a promising path']","[' #TAUTHOR_TAG eskander et al. (, 2018 for languages that are not polysynthetic.', 'we showed that even when using these approaches and very little data, we can obtain encouraging results, and that using additional unsupervised data is a promising path']","['approaches based on adaptor grammars show promise for morphological segmentation of low - resource polysynthetic languages.', 'we worked with the ag grammars developed by  #TAUTHOR_TAG eskander et al. (, 2018 for languages that are not polysynthetic.', 'we showed that even when using these approaches and very little data, we can obtain encouraging results, and that using additional unsupervised data is a promising path']",5
"['( ag )  #TAUTHOR_TAG.', 'we experiment with four languages from the uto -']","['( ag )  #TAUTHOR_TAG.', 'we experiment with four languages from the uto - aztecan family.', 'our ag - based approaches']","['of these polysynthetic languages are low - resource.', 'we propose unsupervised approaches for morphological segmentation of low - resource polysynthetic languages based on adaptor grammars ( ag )  #TAUTHOR_TAG.', 'we experiment with four languages from the uto - aztecan family.', 'our ag - based approaches']","['##ynthetic languages pose a challenge for morphological analysis due to the rootmorpheme complexity and to the word class "" squish "".', 'in addition, many of these polysynthetic languages are low - resource.', 'we propose unsupervised approaches for morphological segmentation of low - resource polysynthetic languages based on adaptor grammars ( ag )  #TAUTHOR_TAG.', 'we experiment with four languages from the uto - aztecan family.', 'our ag - based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages']",6
"['by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is']","['by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is']","['by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is the cascaded prstsu + sm configuration.', 'we use this ag setup for']","['experimented with several setups using ags for unsupervised segmentation.', 'language - independent morphological segmenter.', 'lims is the best - on - average ag setup obtained by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is the cascaded prstsu + sm configuration.', 'we use this ag setup for each of the four languages.', 'we refer to this system as ag lim s.', 'best ag configuration per language.', 'in this experimental setup, we consider all nine grammars from  #TAUTHOR_TAG using both the standard and the cascaded approaches and choosing the one that is best for each polysynthetic language by training on the training set and evaluating on the development set.', '']",6
['structure  #TAUTHOR_TAG'],"['structure  #TAUTHOR_TAG eskander et al.,, 2018.', 'our main']","['unsupervised morphological segmentation, where a pcfg is a morphological grammar that specifies word structure  #TAUTHOR_TAG eskander et al.,, 2018.', 'our main goal is to examine the success of adaptor grammars']","['morphology of polysynthetic languages is an emerging field of research.', 'polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations  #AUTHOR_TAG d ;  #AUTHOR_TAG a ).', 'previous approaches include rule - based methods based on finite state transducers  #AUTHOR_TAG, hybrid models  #AUTHOR_TAG b ;  #AUTHOR_TAG, and supervised machine learning, particularly deep learning approaches  #AUTHOR_TAG.', 'while each rule - based method is developed for a specific language ( inuktitut  #AUTHOR_TAG, or arapaho  #AUTHOR_TAG ), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages.', 'we propose an unsupervised approach for morphological segmentation of polysynthetic languages based on adaptor grammars  #AUTHOR_TAG.', 'we experiment with four utoaztecan languages : mexicanero ( mx ), nahuatl ( nh ), wixarika ( wx ) and yorem nokki ( yn )  #AUTHOR_TAG.', 'adaptor grammars ( ags ) are nonparametric bayesian models that generalize probabilistic context free grammars ( pcfg ), and have proven to be successful for unsupervised morphological segmentation, where a pcfg is a morphological grammar that specifies word structure  #TAUTHOR_TAG eskander et al.,, 2018.', 'our main goal is to examine the success of adaptor grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex ( not simply agglutinative ), and where resources are minimal.', 'we use the datasets introduced by  #AUTHOR_TAG in an unsupervised fashion ( unsegmented words ).', 'we design several ag learning setups : 1 ) use the best - on - average ag setup from  #TAUTHOR_TAG ; 2 ) optimize for language using just the small training vocabulary ( unsegmented ) and dev vocabulary ( segmented ) from  #AUTHOR_TAG ; 3 ) approximate the effect of having some linguistic knowledge ; 4 ) learn from all languages at once and 5 ) add additional unsupervised data for nh and wx ( section 3 ).', 'we show that the ag - based approaches outperform other unsupervised methods - m orf essor  #AUTHOR_TAG and m orphochain  #AUTHOR_TAG ) -, and that for two of the languages ( nh and yn ), the best ag - based approaches outperform the best supervised methods ( section 4 )']",0
"['by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is']","['by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is']","['by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is the cascaded prstsu + sm configuration.', 'we use this ag setup for']","['experimented with several setups using ags for unsupervised segmentation.', 'language - independent morphological segmenter.', 'lims is the best - on - average ag setup obtained by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is the cascaded prstsu + sm configuration.', 'we use this ag setup for each of the four languages.', 'we refer to this system as ag lim s.', 'best ag configuration per language.', 'in this experimental setup, we consider all nine grammars from  #TAUTHOR_TAG using both the standard and the cascaded approaches and choosing the one that is best for each polysynthetic language by training on the training set and evaluating on the development set.', '']",0
['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG table 4 : best ag results compared'],"['evaluate the different ag setups on the blind test set from  #AUTHOR_TAG and compare our ag approaches to state - of - the - art unsupervised systems as well as supervised models including the best supervised deep learning models from  #AUTHOR_TAG.', 'as the metric, we use the segmentation - boundary f1 - score, which is standard for this task  #AUTHOR_TAG.', 'evaluating different ag setups.', 'table 3 shows the performance of our ag setups on the four languages.', 'the best ag setup learned for each of the four polysynthetic languages ( ag bestl ) is the prstsu + sm grammar using the cascaded learning setup.', 'this is an interesting finding as the cascaded prstsu + sm setup is in fact ag lim s - the best - on - average ag setup obtained by  #TAUTHOR_TAG table 4 : best ag results compared to supervised approaches from  #AUTHOR_TAG.', 'bold indicates best scores.', 'wx and yn, respectively.', '']",0
"['by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is']","['by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is']","['by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is the cascaded prstsu + sm configuration.', 'we use this ag setup for']","['experimented with several setups using ags for unsupervised segmentation.', 'language - independent morphological segmenter.', 'lims is the best - on - average ag setup obtained by  #TAUTHOR_TAG when trained on six languages ( english, german, finnish, estonian, turkish and zulu ), which is the cascaded prstsu + sm configuration.', 'we use this ag setup for each of the four languages.', 'we refer to this system as ag lim s.', 'best ag configuration per language.', 'in this experimental setup, we consider all nine grammars from  #TAUTHOR_TAG using both the standard and the cascaded approaches and choosing the one that is best for each polysynthetic language by training on the training set and evaluating on the development set.', '']",3
['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG table 4 : best ag results compared'],"['evaluate the different ag setups on the blind test set from  #AUTHOR_TAG and compare our ag approaches to state - of - the - art unsupervised systems as well as supervised models including the best supervised deep learning models from  #AUTHOR_TAG.', 'as the metric, we use the segmentation - boundary f1 - score, which is standard for this task  #AUTHOR_TAG.', 'evaluating different ag setups.', 'table 3 shows the performance of our ag setups on the four languages.', 'the best ag setup learned for each of the four polysynthetic languages ( ag bestl ) is the prstsu + sm grammar using the cascaded learning setup.', 'this is an interesting finding as the cascaded prstsu + sm setup is in fact ag lim s - the best - on - average ag setup obtained by  #TAUTHOR_TAG table 4 : best ag results compared to supervised approaches from  #AUTHOR_TAG.', 'bold indicates best scores.', 'wx and yn, respectively.', '']",3
['touchdown dataset  #TAUTHOR_TAG provides instructions by human annotators'],['touchdown dataset  #TAUTHOR_TAG provides instructions by human annotators'],['touchdown dataset  #TAUTHOR_TAG provides instructions by human annotators'],"['touchdown dataset  #TAUTHOR_TAG provides instructions by human annotators for navigation through new york city streets and for resolving spatial descriptions at a given location.', 'to enable the wider research community to work effectively with the touchdown tasks, we are publicly releasing the 29k raw street view panoramas needed for touchdown.', 'we follow the process used for the streetlearn data release  #AUTHOR_TAG to check panoramas for personally identifiable information and blur them as necessary.', 'these have been added to the streetlearn dataset and can be obtained via the same process as used previously for streetlearn.', 'we also provide a reference implementation for both of the touchdown tasks : vision and language navigation ( vln ) and spatial description resolution ( sdr ).', 'we compare our model results to those given in  #TAUTHOR_TAG and show that the panoramas we have added to streetlearn fully support both touchdown tasks and can be used effectively for further research and comparison']",0
"[' #TAUTHOR_TAG,']","['touchdown  #TAUTHOR_TAG,']","['touchdown  #TAUTHOR_TAG,']","['natural language navigation instructions in visual environments requires addressing multiple challenges in dynamic, continuously changing environments, including language understanding, object recognition, grounding and spatial reasoning.', 'until recently, the most commonly studied domains were map - based  #AUTHOR_TAG or game - like  #AUTHOR_TAG misra et al.,, 2018  #AUTHOR_TAG.', 'these environments enabled substantial progress, but the complexity and diversity of the visual input they provide is limited.', 'this greatly simplifies both the language and vision challenges.', 'to address this, recent tasks based on simulated environments include photo - realistic visual input, such as room - to - room ( r2r ;  #AUTHOR_TAG, talk - the - walk ( de  #AUTHOR_TAG and touchdown  #TAUTHOR_TAG, all of which rely on panorama photos.', 'a major challenge of creating simulations that use realworld photographs is they at times capture bystanders and their property.', '']",0
['spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to'],['spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to'],['spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to'],"['re - implement the best - reported models on the navigation and spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to the original touchdown paper.', 'the key difference between the two settings is that our released panoramas contain additional blurred patches ( section 2 ).', 'another minor difference is that we use a word - piece tokenizer  #AUTHOR_TAG instead of a full - word tokenizer.', 'spatial description resolution.', 'sdr results are given in table 1.', 'following  #TAUTHOR_TAG, we report mean distance error and accuracy with different thresholds ( 40px, 80px, and 120px ), which measures the proportion of evaluation items where the pixel chosen by the model is within the specified pixel distance.', 'our retouchdown reimplementation of lingunet obtains better performance on the accuracy measures, but worse performance on mean distance error.', 'to check whether this is a consequence of the blur - ring, we ran our model with features retrieved from original panoramas and obtained similar results as those listed in table 1.', 'given this, the performance difference between our model and the original paper are likely not due to the additional blurring.', 'as such, the touchdown panoramas available through streetlearn can be reliably used as direct replacement for those used in  #TAUTHOR_TAG.', 'vision - and - language navigation.', 'we use the following metrics to evaluate vln performance :', '• task completion ( tc ) : the accuracy of navigating to the correct location.', 'the correct location is defined as the exact goal panorama or one of its neighboring panoramas.', 'this is the equivalent of the success rate metric ( sr ) used commonly in vln for r2r.', ""• shortest - path distance ( spd ) : the mean of the distances over all executions of the agent's final panorama position and the goal panorama."", '• success weighted by edit distance ( sed ) : normalized graph edit distance between the agent path and true path, with points only awarded for successful paths.', '• normalized dynamic time warping ( ndtw ) : a minimized cumulative distance between the agent path and true path, normalized by path length.', '• success weighted dynamic time warping ( sdtw ) : ndtw, with points awarded only for successful paths.', 'tc, spd, and sed are defined in  #TAUTHOR_TAG and ndtw and sdtw are defined in  #AUTHOR_TAG.', 'vln results are given in table 2.', 'our retouchdown reimplementation of the rconcat model improves over the results given in  #TAUTHOR_TAG for all metrics.', 'we also establish benchmark scores for nd']",0
['touchdown dataset  #TAUTHOR_TAG provides instructions by human annotators'],['touchdown dataset  #TAUTHOR_TAG provides instructions by human annotators'],['touchdown dataset  #TAUTHOR_TAG provides instructions by human annotators'],"['touchdown dataset  #TAUTHOR_TAG provides instructions by human annotators for navigation through new york city streets and for resolving spatial descriptions at a given location.', 'to enable the wider research community to work effectively with the touchdown tasks, we are publicly releasing the 29k raw street view panoramas needed for touchdown.', 'we follow the process used for the streetlearn data release  #AUTHOR_TAG to check panoramas for personally identifiable information and blur them as necessary.', 'these have been added to the streetlearn dataset and can be obtained via the same process as used previously for streetlearn.', 'we also provide a reference implementation for both of the touchdown tasks : vision and language navigation ( vln ) and spatial description resolution ( sdr ).', 'we compare our model results to those given in  #TAUTHOR_TAG and show that the panoramas we have added to streetlearn fully support both touchdown tasks and can be used effectively for further research and comparison']",5
['spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to'],['spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to'],['spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to'],"['re - implement the best - reported models on the navigation and spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to the original touchdown paper.', 'the key difference between the two settings is that our released panoramas contain additional blurred patches ( section 2 ).', 'another minor difference is that we use a word - piece tokenizer  #AUTHOR_TAG instead of a full - word tokenizer.', 'spatial description resolution.', 'sdr results are given in table 1.', 'following  #TAUTHOR_TAG, we report mean distance error and accuracy with different thresholds ( 40px, 80px, and 120px ), which measures the proportion of evaluation items where the pixel chosen by the model is within the specified pixel distance.', 'our retouchdown reimplementation of lingunet obtains better performance on the accuracy measures, but worse performance on mean distance error.', 'to check whether this is a consequence of the blur - ring, we ran our model with features retrieved from original panoramas and obtained similar results as those listed in table 1.', 'given this, the performance difference between our model and the original paper are likely not due to the additional blurring.', 'as such, the touchdown panoramas available through streetlearn can be reliably used as direct replacement for those used in  #TAUTHOR_TAG.', 'vision - and - language navigation.', 'we use the following metrics to evaluate vln performance :', '• task completion ( tc ) : the accuracy of navigating to the correct location.', 'the correct location is defined as the exact goal panorama or one of its neighboring panoramas.', 'this is the equivalent of the success rate metric ( sr ) used commonly in vln for r2r.', ""• shortest - path distance ( spd ) : the mean of the distances over all executions of the agent's final panorama position and the goal panorama."", '• success weighted by edit distance ( sed ) : normalized graph edit distance between the agent path and true path, with points only awarded for successful paths.', '• normalized dynamic time warping ( ndtw ) : a minimized cumulative distance between the agent path and true path, normalized by path length.', '• success weighted dynamic time warping ( sdtw ) : ndtw, with points awarded only for successful paths.', 'tc, spd, and sed are defined in  #TAUTHOR_TAG and ndtw and sdtw are defined in  #AUTHOR_TAG.', 'vln results are given in table 2.', 'our retouchdown reimplementation of the rconcat model improves over the results given in  #TAUTHOR_TAG for all metrics.', 'we also establish benchmark scores for nd']",5
['spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to'],['spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to'],['spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to'],"['re - implement the best - reported models on the navigation and spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to the original touchdown paper.', 'the key difference between the two settings is that our released panoramas contain additional blurred patches ( section 2 ).', 'another minor difference is that we use a word - piece tokenizer  #AUTHOR_TAG instead of a full - word tokenizer.', 'spatial description resolution.', 'sdr results are given in table 1.', 'following  #TAUTHOR_TAG, we report mean distance error and accuracy with different thresholds ( 40px, 80px, and 120px ), which measures the proportion of evaluation items where the pixel chosen by the model is within the specified pixel distance.', 'our retouchdown reimplementation of lingunet obtains better performance on the accuracy measures, but worse performance on mean distance error.', 'to check whether this is a consequence of the blur - ring, we ran our model with features retrieved from original panoramas and obtained similar results as those listed in table 1.', 'given this, the performance difference between our model and the original paper are likely not due to the additional blurring.', 'as such, the touchdown panoramas available through streetlearn can be reliably used as direct replacement for those used in  #TAUTHOR_TAG.', 'vision - and - language navigation.', 'we use the following metrics to evaluate vln performance :', '• task completion ( tc ) : the accuracy of navigating to the correct location.', 'the correct location is defined as the exact goal panorama or one of its neighboring panoramas.', 'this is the equivalent of the success rate metric ( sr ) used commonly in vln for r2r.', ""• shortest - path distance ( spd ) : the mean of the distances over all executions of the agent's final panorama position and the goal panorama."", '• success weighted by edit distance ( sed ) : normalized graph edit distance between the agent path and true path, with points only awarded for successful paths.', '• normalized dynamic time warping ( ndtw ) : a minimized cumulative distance between the agent path and true path, normalized by path length.', '• success weighted dynamic time warping ( sdtw ) : ndtw, with points awarded only for successful paths.', 'tc, spd, and sed are defined in  #TAUTHOR_TAG and ndtw and sdtw are defined in  #AUTHOR_TAG.', 'vln results are given in table 2.', 'our retouchdown reimplementation of the rconcat model improves over the results given in  #TAUTHOR_TAG for all metrics.', 'we also establish benchmark scores for nd']",5
['spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to'],['spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to'],['spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to'],"['re - implement the best - reported models on the navigation and spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to the original touchdown paper.', 'the key difference between the two settings is that our released panoramas contain additional blurred patches ( section 2 ).', 'another minor difference is that we use a word - piece tokenizer  #AUTHOR_TAG instead of a full - word tokenizer.', 'spatial description resolution.', 'sdr results are given in table 1.', 'following  #TAUTHOR_TAG, we report mean distance error and accuracy with different thresholds ( 40px, 80px, and 120px ), which measures the proportion of evaluation items where the pixel chosen by the model is within the specified pixel distance.', 'our retouchdown reimplementation of lingunet obtains better performance on the accuracy measures, but worse performance on mean distance error.', 'to check whether this is a consequence of the blur - ring, we ran our model with features retrieved from original panoramas and obtained similar results as those listed in table 1.', 'given this, the performance difference between our model and the original paper are likely not due to the additional blurring.', 'as such, the touchdown panoramas available through streetlearn can be reliably used as direct replacement for those used in  #TAUTHOR_TAG.', 'vision - and - language navigation.', 'we use the following metrics to evaluate vln performance :', '• task completion ( tc ) : the accuracy of navigating to the correct location.', 'the correct location is defined as the exact goal panorama or one of its neighboring panoramas.', 'this is the equivalent of the success rate metric ( sr ) used commonly in vln for r2r.', ""• shortest - path distance ( spd ) : the mean of the distances over all executions of the agent's final panorama position and the goal panorama."", '• success weighted by edit distance ( sed ) : normalized graph edit distance between the agent path and true path, with points only awarded for successful paths.', '• normalized dynamic time warping ( ndtw ) : a minimized cumulative distance between the agent path and true path, normalized by path length.', '• success weighted dynamic time warping ( sdtw ) : ndtw, with points awarded only for successful paths.', 'tc, spd, and sed are defined in  #TAUTHOR_TAG and ndtw and sdtw are defined in  #AUTHOR_TAG.', 'vln results are given in table 2.', 'our retouchdown reimplementation of the rconcat model improves over the results given in  #TAUTHOR_TAG for all metrics.', 'we also establish benchmark scores for nd']",7
['spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to'],['spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to'],['spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to'],"['re - implement the best - reported models on the navigation and spatial description resolution tasks from  #TAUTHOR_TAG to compare performance with our data release to the original touchdown paper.', 'the key difference between the two settings is that our released panoramas contain additional blurred patches ( section 2 ).', 'another minor difference is that we use a word - piece tokenizer  #AUTHOR_TAG instead of a full - word tokenizer.', 'spatial description resolution.', 'sdr results are given in table 1.', 'following  #TAUTHOR_TAG, we report mean distance error and accuracy with different thresholds ( 40px, 80px, and 120px ), which measures the proportion of evaluation items where the pixel chosen by the model is within the specified pixel distance.', 'our retouchdown reimplementation of lingunet obtains better performance on the accuracy measures, but worse performance on mean distance error.', 'to check whether this is a consequence of the blur - ring, we ran our model with features retrieved from original panoramas and obtained similar results as those listed in table 1.', 'given this, the performance difference between our model and the original paper are likely not due to the additional blurring.', 'as such, the touchdown panoramas available through streetlearn can be reliably used as direct replacement for those used in  #TAUTHOR_TAG.', 'vision - and - language navigation.', 'we use the following metrics to evaluate vln performance :', '• task completion ( tc ) : the accuracy of navigating to the correct location.', 'the correct location is defined as the exact goal panorama or one of its neighboring panoramas.', 'this is the equivalent of the success rate metric ( sr ) used commonly in vln for r2r.', ""• shortest - path distance ( spd ) : the mean of the distances over all executions of the agent's final panorama position and the goal panorama."", '• success weighted by edit distance ( sed ) : normalized graph edit distance between the agent path and true path, with points only awarded for successful paths.', '• normalized dynamic time warping ( ndtw ) : a minimized cumulative distance between the agent path and true path, normalized by path length.', '• success weighted dynamic time warping ( sdtw ) : ndtw, with points awarded only for successful paths.', 'tc, spd, and sed are defined in  #TAUTHOR_TAG and ndtw and sdtw are defined in  #AUTHOR_TAG.', 'vln results are given in table 2.', 'our retouchdown reimplementation of the rconcat model improves over the results given in  #TAUTHOR_TAG for all metrics.', 'we also establish benchmark scores for nd']",7
"['recently, gre algorithms have focussed on the generation of distinguishing descriptions that are either as short as possible ( e. g.  #AUTHOR_TAG ) or almost as short as possible ( e. g.  #TAUTHOR_TAG.', 'since reductions in ambiguity']","['recently, gre algorithms have focussed on the generation of distinguishing descriptions that are either as short as possible ( e. g.  #AUTHOR_TAG ) or almost as short as possible ( e. g.  #TAUTHOR_TAG.', 'since reductions in ambiguity']","['recently, gre algorithms have focussed on the generation of distinguishing descriptions that are either as short as possible ( e. g.  #AUTHOR_TAG ) or almost as short as possible ( e. g.  #TAUTHOR_TAG.', 'since reductions in ambiguity']","['recently, gre algorithms have focussed on the generation of distinguishing descriptions that are either as short as possible ( e. g.  #AUTHOR_TAG ) or almost as short as possible ( e. g.  #TAUTHOR_TAG.', 'since reductions in ambiguity are achieved by increases in length, there is a tension between these factors, and algorithms usually resolve this in some fixed way.', 'however, the need for a distinguishing description is usually assumed, and typically built in to gre algorithms.', 'we will suggest a way to make explicit this balance between clarity ( i. e. lack of ambiguity ) and brevity, and we indicate some phenomena which we believe may be illuminated by this approach.', 'the ideas in this paper can be seen as a loosening of some of the many simplifying assumptions often made in gre work.', '* this work is supported by a university of aberdeen sixth century studentship, and the tuna project ( epsrc, uk ) under grant number gr / s13330 / 01.', 'we thank ielka van der sluis and albert gatt for valuable comments']",0
[' #TAUTHOR_TAG as'],[' #TAUTHOR_TAG as'],[' #TAUTHOR_TAG as'],"['consider only simple gre, where the aim is to construct a conjunction of unary properties which distinguish a single target object from a set of potential distractors.', 'our notation is as follows.', 'a domain consists of a set d of objects, and a set p of properties applicable to objects in d. a description is a subset of p. the denotation of s, written', ' #AUTHOR_TAG describe an approach to gre in which a cost function guides search for a suitable description, and show that some existing gre algorithms fit into this framework.', 'however, they follow the practice of concentrating solely on distinguishing descriptions, treating cost as a matter of brevity.', 'we suggest that decomposing cost into two components, for the clarity and brevity of descriptions, permits the examination of tradeoffs.', 'for now, we will take the cost of a description s to be the sum of two terms :', 'where f c counts ambiguity ( lack of clarity ) and f b counts size ( lack of brevity ).', 'even with this decomposition of cost, some existing algorithms can still be seen as cost - minimisation.', 'for example, the cost functions :', 'allow the full brevity algorithm  #AUTHOR_TAG to be viewed as minimising cost ( s ), and the incremental algorithm  #TAUTHOR_TAG as hill - climbing ( strictly, hill - descending ), guided by the property - ordering which that algorithm requires.', ""whereas krahmer et al.'s cost functions are ( brevity - based ) heuristic guidance functions, our alternative here is a global quantity for optimisation."", 'hence their simulation of full brevity relies on the details of their algorithm ( rather than cost ) to ensure clarity, while our own cost function ensures both brevity and clarity., there is a curve where the cost drops more steeply as the more undesirable distractors are excluded.', 'for example, each object could be assigned a numerical rating of how undesirable it is, with the target having a score of zero, and the f c value for a set a could be the maximum rating of any element of a. ( this would, of course, require a suitably rich domain model. )', 'the brevity cost function f b could still be a relatively simple linear function, providing f b values do not mask the effect of the shape of the f c curve']",0
"['knows  #TAUTHOR_TAG.', 'in practice, speakers can often only guess.', 'it has been observed that speakers sometimes produce referring']","['knows  #TAUTHOR_TAG.', 'in practice, speakers can often only guess.', 'it has been observed that speakers sometimes produce referring']","['knows  #TAUTHOR_TAG.', 'in practice, speakers can often only guess.', 'it has been observed that speakers sometimes produce referring']","['gre algorithms assume that the speaker knows what the hearer knows  #TAUTHOR_TAG.', 'in practice, speakers can often only guess.', 'it has been observed that speakers sometimes produce referring expressions that are only disambiguated through negotiation with the hearer, as exemplified in the following excerpt ( quoted in  #AUTHOR_TAG ).', 'a and b are in the same room, in an informal setting, so a can be relatively interactive in conveying information.', 'also, the situation does not appear to be highly critical, in comparison to a military officer directing gunfire, or a surgeon guiding an incision.', 'initially, a produces an expression which is not very detailed.', ""it may be that he thinks this is adequate ( the object is sufficiently salient that b will uniquely determine the referent ), or he doesn't really know, but is willing to make an opening bid in a negotiation to reach the goal of reference."", ""in the former case, a gre algorithm which took account of salience ( e. g.  #AUTHOR_TAG ), operating with a's model of b's knowledge, should produce this sort of effect."", '( a dialogue model might also be needed. ) in the latter case, we need an algorithm which can relax the need for complete clarity.', 'this could be arranged by having f c give similar scores to denotations where there are no distractors and to denotations where there are just a few distractors, with f b making a large contribution to the cost']",0
['of capturing hierarchical structure without being equipped with explicit structural representations  #TAUTHOR_TAG'],['of capturing hierarchical structure without being equipped with explicit structural representations  #TAUTHOR_TAG'],"['of capturing hierarchical structure without being equipped with explicit structural representations  #TAUTHOR_TAG.', 'we choose transformer as a non - recurrent model to study in this paper.', 'we refer']","['neural networks ( rnns ), in particular long short - term memory networks ( lstms ), have become a dominant tool in natural language processing.', 'while lstms appear to be a natural choice for modeling sequential data, recently a class of non - recurrent models  #AUTHOR_TAG have shown competitive performance on sequence modeling.', ' #AUTHOR_TAG propose a fully convolutional sequence - tosequence model that achieves state - of - the - art performance in machine translation.', ' #AUTHOR_TAG introduce transformer networks that do not use any convolution or recurrent connections while obtaining the best translation performance.', 'these non - recurrent models are appealing due to their highly parallelizable computations on modern gpus. but do they have the same ability to exploit hierarchical structures implicitly in comparison to rnns?', 'in this work, we provide a first answer to this question.', 'our interest here is the ability of capturing hierarchical structure without being equipped with explicit structural representations  #TAUTHOR_TAG.', 'we choose transformer as a non - recurrent model to study in this paper.', '']",5
"['.', 'the second task was introduced by  #TAUTHOR_TAG to compare tree']","['in natural language.', 'the second task was introduced by  #TAUTHOR_TAG to compare']","['capture syntactic dependencies in natural language.', 'the second task was introduced by  #TAUTHOR_TAG to compare tree']",[' #TAUTHOR_TAG'],5
"[' #TAUTHOR_TAG.', 'the vocabulary of this']","[' #TAUTHOR_TAG.', 'the vocabulary of this']","[' #TAUTHOR_TAG.', 'the vocabulary of this language includes six word types { a, b, c, d, e, f } and three logical operators { or, and, not }. the task consists of predicting one of seven mutually exclusive logical relations']","['this task, we choose the artificial language introduced by  #TAUTHOR_TAG.', 'the vocabulary of this language includes six word types { a, b, c, d, e, f } and three logical operators { or, and, not }. the task consists of predicting one of seven mutually exclusive logical relations that describe the relationship between a pair of sentences : entailment (, ), equivalence ( ≡ ), exhaustive and non - exhaustive contradiction ( ∧, | ), and two types of semantic independence ( #, ).', 'we generate 60, 000 samples 3 with the number of logical operations ranging from 1 to 12.', 'the train / dev / test dataset ratios are set to 0. 8 / 0. 1 / 0. 1.', 'here are some samples of the training data :', '']",5
['in  #TAUTHOR_TAG : premise and hypothesis sentences are encoded by fixed - size vectors'],"['in  #TAUTHOR_TAG : premise and hypothesis sentences are encoded by fixed - size vectors.', 'these two vectors']","['in  #TAUTHOR_TAG : premise and hypothesis sentences are encoded by fixed - size vectors.', 'these two vectors']","['follow the general architecture proposed in  #TAUTHOR_TAG : premise and hypothesis sentences are encoded by fixed - size vectors.', '']",5
"['the experimental protocol of  #TAUTHOR_TAG, the']","['the experimental protocol of  #TAUTHOR_TAG, the']","['the experimental protocol of  #TAUTHOR_TAG, the data is divided into 13 bins based on the number of logical operators.', 'both fans and lstms are trained on samples with']","['the experimental protocol of  #TAUTHOR_TAG, the data is divided into 13 bins based on the number of logical operators.', 'both fans and lstms are trained on samples with at most n logical operators and tested on all bins.', 'figure 4 shows the result of the experiments with n ≤ 6 and n ≤ 12.', 'we see that fans and lstms perform similarly when trained on the whole dataset ( figure 4a ).', 'however when trained on a subset of the data ( figure 4b ), lstms obtain better accuracies on similar examples ( n ≤ 6 ) and generalize better on longer examples ( 6 < n ≤ 12 )']",5
['in  #TAUTHOR_TAG : premise and hypothesis sentences are encoded by fixed - size vectors'],"['in  #TAUTHOR_TAG : premise and hypothesis sentences are encoded by fixed - size vectors.', 'these two vectors']","['in  #TAUTHOR_TAG : premise and hypothesis sentences are encoded by fixed - size vectors.', 'these two vectors']","['follow the general architecture proposed in  #TAUTHOR_TAG : premise and hypothesis sentences are encoded by fixed - size vectors.', '']",3
"['direct object position, are a common type of semantically - idiomatic mwe in english and cross - lingually  #TAUTHOR_TAG.', 'many vn']","['direct object position, are a common type of semantically - idiomatic mwe in english and cross - lingually  #TAUTHOR_TAG.', 'many vncs are ambiguous between mwes and literal combinations, as in']","['direct object position, are a common type of semantically - idiomatic mwe in english and cross - lingually  #TAUTHOR_TAG.', 'many vncs are ambiguous between mw']","['##word expressions ( mwes ) are combinations of multiple words that exhibit some degree of idiomaticity  #AUTHOR_TAG.', 'verb - noun combinations ( vncs ), consisting of a verb with a noun in its direct object position, are a common type of semantically - idiomatic mwe in english and cross - lingually  #TAUTHOR_TAG.', 'many vncs are ambiguous between mwes and literal combinations, as in the following examples of see stars, in which 1 is an idiomatic usage ( i. e., an mwe ), while 2 is a literal combination.', '1 1. hereford united were seeing stars at gillingham after letting in 2 early goals 2. look into the night sky to see the stars mwe identification is the task of automatically determining which word combinations at the token - level form mwes  #AUTHOR_TAG, and must be able to make such distinctions.', 'this is particularly important for applications such as machine translation  #AUTHOR_TAG, where the appropriate meaning of word combinations in context must be preserved for accurate translation.', 'in this paper, following prior work ( e. g.,  #AUTHOR_TAG, we frame token - level identification of vncs as a supervised binary classification problem, i. e., idiomatic vs. literal.', 'we consider a range of approaches to forming distributed representations of the context in which a vnc occurs, including word embeddings  #AUTHOR_TAG, word embeddings tailored to representing sentences  #AUTHOR_TAG, and skip - thoughts sentence embeddings  #AUTHOR_TAG.', '']",0
"['g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent']","['on mwe identification has focused on specific kinds of mwes ( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent']","['g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent work has considered the identification of a broad range of kinds of mwes ( e. g.,  #AUTHOR_TAG.', '']","['research on mwe identification has focused on specific kinds of mwes ( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent work has considered the identification of a broad range of kinds of mwes ( e. g.,  #AUTHOR_TAG.', 'work on mwe identification has leveraged rich linguistic knowledge of the constructions under consideration ( e. g.,  #TAUTHOR_TAG, treated literal and idiomatic as two senses of an expression and applied approaches similar to word - sense disambiguation ( e. g.,  #AUTHOR_TAG, incorporated topic models ( e. g.,  #AUTHOR_TAG, and made use of distributed representations of words  #AUTHOR_TAG.', 'in the most closely related work to ours,  #AUTHOR_TAG represent token instances of vncs by embedding the sentence that they occur in using skip - thoughts  #AUTHOR_TAG - an encoderdecoder model that can be viewed as a sentencelevel counterpart to the word2vec  #AUTHOR_TAG skip - gram model.', 'during training the target sentence is encoded using a recurrent neural network, and is used to predict the previous and next sentences.', 'salton et al. then use these sentence embeddings, representing vnc token instances, as features in a supervised classifier.', 'we treat this skip - thoughts based approach as a strong baseline to compare against.', ' #TAUTHOR_TAG formed a set of eleven lexicosyntactic patterns for vnc instances capturing the voice of the verb ( active or passive ), determiner ( e. g., a, the ), and number of the noun ( singular or plural ).', '']",0
"['g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent']","['on mwe identification has focused on specific kinds of mwes ( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent']","['g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent work has considered the identification of a broad range of kinds of mwes ( e. g.,  #AUTHOR_TAG.', '']","['research on mwe identification has focused on specific kinds of mwes ( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent work has considered the identification of a broad range of kinds of mwes ( e. g.,  #AUTHOR_TAG.', 'work on mwe identification has leveraged rich linguistic knowledge of the constructions under consideration ( e. g.,  #TAUTHOR_TAG, treated literal and idiomatic as two senses of an expression and applied approaches similar to word - sense disambiguation ( e. g.,  #AUTHOR_TAG, incorporated topic models ( e. g.,  #AUTHOR_TAG, and made use of distributed representations of words  #AUTHOR_TAG.', 'in the most closely related work to ours,  #AUTHOR_TAG represent token instances of vncs by embedding the sentence that they occur in using skip - thoughts  #AUTHOR_TAG - an encoderdecoder model that can be viewed as a sentencelevel counterpart to the word2vec  #AUTHOR_TAG skip - gram model.', 'during training the target sentence is encoded using a recurrent neural network, and is used to predict the previous and next sentences.', 'salton et al. then use these sentence embeddings, representing vnc token instances, as features in a supervised classifier.', 'we treat this skip - thoughts based approach as a strong baseline to compare against.', ' #TAUTHOR_TAG formed a set of eleven lexicosyntactic patterns for vnc instances capturing the voice of the verb ( active or passive ), determiner ( e. g., a, the ), and number of the noun ( singular or plural ).', '']",0
"['g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent']","['on mwe identification has focused on specific kinds of mwes ( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent']","['g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent work has considered the identification of a broad range of kinds of mwes ( e. g.,  #AUTHOR_TAG.', '']","['research on mwe identification has focused on specific kinds of mwes ( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent work has considered the identification of a broad range of kinds of mwes ( e. g.,  #AUTHOR_TAG.', 'work on mwe identification has leveraged rich linguistic knowledge of the constructions under consideration ( e. g.,  #TAUTHOR_TAG, treated literal and idiomatic as two senses of an expression and applied approaches similar to word - sense disambiguation ( e. g.,  #AUTHOR_TAG, incorporated topic models ( e. g.,  #AUTHOR_TAG, and made use of distributed representations of words  #AUTHOR_TAG.', 'in the most closely related work to ours,  #AUTHOR_TAG represent token instances of vncs by embedding the sentence that they occur in using skip - thoughts  #AUTHOR_TAG - an encoderdecoder model that can be viewed as a sentencelevel counterpart to the word2vec  #AUTHOR_TAG skip - gram model.', 'during training the target sentence is encoded using a recurrent neural network, and is used to predict the previous and next sentences.', 'salton et al. then use these sentence embeddings, representing vnc token instances, as features in a supervised classifier.', 'we treat this skip - thoughts based approach as a strong baseline to compare against.', ' #TAUTHOR_TAG formed a set of eleven lexicosyntactic patterns for vnc instances capturing the voice of the verb ( active or passive ), determiner ( e. g., a, the ), and number of the noun ( singular or plural ).', '']",0
"['g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent']","['on mwe identification has focused on specific kinds of mwes ( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent']","['g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent work has considered the identification of a broad range of kinds of mwes ( e. g.,  #AUTHOR_TAG.', '']","['research on mwe identification has focused on specific kinds of mwes ( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent work has considered the identification of a broad range of kinds of mwes ( e. g.,  #AUTHOR_TAG.', 'work on mwe identification has leveraged rich linguistic knowledge of the constructions under consideration ( e. g.,  #TAUTHOR_TAG, treated literal and idiomatic as two senses of an expression and applied approaches similar to word - sense disambiguation ( e. g.,  #AUTHOR_TAG, incorporated topic models ( e. g.,  #AUTHOR_TAG, and made use of distributed representations of words  #AUTHOR_TAG.', 'in the most closely related work to ours,  #AUTHOR_TAG represent token instances of vncs by embedding the sentence that they occur in using skip - thoughts  #AUTHOR_TAG - an encoderdecoder model that can be viewed as a sentencelevel counterpart to the word2vec  #AUTHOR_TAG skip - gram model.', 'during training the target sentence is encoded using a recurrent neural network, and is used to predict the previous and next sentences.', 'salton et al. then use these sentence embeddings, representing vnc token instances, as features in a supervised classifier.', 'we treat this skip - thoughts based approach as a strong baseline to compare against.', ' #TAUTHOR_TAG formed a set of eleven lexicosyntactic patterns for vnc instances capturing the voice of the verb ( active or passive ), determiner ( e. g., a, the ), and number of the noun ( singular or plural ).', '']",0
[' #TAUTHOR_TAG and  #AUTHOR_TAG - to train and'],[' #TAUTHOR_TAG and  #AUTHOR_TAG - to train and'],[' #TAUTHOR_TAG and  #AUTHOR_TAG - to train and'],"['use the vnc - tokens dataset  #AUTHOR_TAG - the same dataset used by  #TAUTHOR_TAG and  #AUTHOR_TAG - to train and evaluate our models.', 'this dataset consists of sentences containing vnc usages drawn from the british national corpus  #AUTHOR_TAG, 7 along with a label indicating whether the vnc is an idiomatic or literal usage ( or whether this cannot be determined, in which case it is labelled "" unknown "" ).', 'vnc - tokens is divided into dev and test sets that each include fourteen vnc types and a total of roughly six hundred instances of these types annotated as literal or idiomatic.', ' #AUTHOR_TAG, we use dev and test, and ignore all token instances annotated as "" unknown "".', ' #TAUTHOR_TAG and  #AUTHOR_TAG structured their experiments differently.', ' #TAUTHOR_TAG report results over dev and test separately.', 'in this setup test consists of expressions that were not seen during model development ( done on dev ).', 'salton et al., on the other hand, merge dev and test, and create new training and testing sets, such that each expression is present in the training and testing data, and the ratio of idiomatic to literal usages of each expression in the training data is roughly equal to that in the testing data.', 'we borrowed ideas from both of these approaches in structuring our experiments.', 'we retain we then divide each of these into training and testing sets, using the same ratios of idiomatic to literal usages for each expression as  #AUTHOR_TAG.', 'this allows us to develop and tune a model on dev, and then determine whether, when retrained on instances of unseen vncs in ( the training portion of ) test, that model is able to generalize to new vncs without further tuning to the specific expressions in test']",0
[' #TAUTHOR_TAG and  #AUTHOR_TAG - to train and'],[' #TAUTHOR_TAG and  #AUTHOR_TAG - to train and'],[' #TAUTHOR_TAG and  #AUTHOR_TAG - to train and'],"['use the vnc - tokens dataset  #AUTHOR_TAG - the same dataset used by  #TAUTHOR_TAG and  #AUTHOR_TAG - to train and evaluate our models.', 'this dataset consists of sentences containing vnc usages drawn from the british national corpus  #AUTHOR_TAG, 7 along with a label indicating whether the vnc is an idiomatic or literal usage ( or whether this cannot be determined, in which case it is labelled "" unknown "" ).', 'vnc - tokens is divided into dev and test sets that each include fourteen vnc types and a total of roughly six hundred instances of these types annotated as literal or idiomatic.', ' #AUTHOR_TAG, we use dev and test, and ignore all token instances annotated as "" unknown "".', ' #TAUTHOR_TAG and  #AUTHOR_TAG structured their experiments differently.', ' #TAUTHOR_TAG report results over dev and test separately.', 'in this setup test consists of expressions that were not seen during model development ( done on dev ).', 'salton et al., on the other hand, merge dev and test, and create new training and testing sets, such that each expression is present in the training and testing data, and the ratio of idiomatic to literal usages of each expression in the training data is roughly equal to that in the testing data.', 'we borrowed ideas from both of these approaches in structuring our experiments.', 'we retain we then divide each of these into training and testing sets, using the same ratios of idiomatic to literal usages for each expression as  #AUTHOR_TAG.', 'this allows us to develop and tune a model on dev, and then determine whether, when retrained on instances of unseen vncs in ( the training portion of ) test, that model is able to generalize to new vncs without further tuning to the specific expressions in test']",0
"['direct object position, are a common type of semantically - idiomatic mwe in english and cross - lingually  #TAUTHOR_TAG.', 'many vn']","['direct object position, are a common type of semantically - idiomatic mwe in english and cross - lingually  #TAUTHOR_TAG.', 'many vncs are ambiguous between mwes and literal combinations, as in']","['direct object position, are a common type of semantically - idiomatic mwe in english and cross - lingually  #TAUTHOR_TAG.', 'many vncs are ambiguous between mw']","['##word expressions ( mwes ) are combinations of multiple words that exhibit some degree of idiomaticity  #AUTHOR_TAG.', 'verb - noun combinations ( vncs ), consisting of a verb with a noun in its direct object position, are a common type of semantically - idiomatic mwe in english and cross - lingually  #TAUTHOR_TAG.', 'many vncs are ambiguous between mwes and literal combinations, as in the following examples of see stars, in which 1 is an idiomatic usage ( i. e., an mwe ), while 2 is a literal combination.', '1 1. hereford united were seeing stars at gillingham after letting in 2 early goals 2. look into the night sky to see the stars mwe identification is the task of automatically determining which word combinations at the token - level form mwes  #AUTHOR_TAG, and must be able to make such distinctions.', 'this is particularly important for applications such as machine translation  #AUTHOR_TAG, where the appropriate meaning of word combinations in context must be preserved for accurate translation.', 'in this paper, following prior work ( e. g.,  #AUTHOR_TAG, we frame token - level identification of vncs as a supervised binary classification problem, i. e., idiomatic vs. literal.', 'we consider a range of approaches to forming distributed representations of the context in which a vnc occurs, including word embeddings  #AUTHOR_TAG, word embeddings tailored to representing sentences  #AUTHOR_TAG, and skip - thoughts sentence embeddings  #AUTHOR_TAG.', '']",5
[' #TAUTHOR_TAG and  #AUTHOR_TAG - to train and'],[' #TAUTHOR_TAG and  #AUTHOR_TAG - to train and'],[' #TAUTHOR_TAG and  #AUTHOR_TAG - to train and'],"['use the vnc - tokens dataset  #AUTHOR_TAG - the same dataset used by  #TAUTHOR_TAG and  #AUTHOR_TAG - to train and evaluate our models.', 'this dataset consists of sentences containing vnc usages drawn from the british national corpus  #AUTHOR_TAG, 7 along with a label indicating whether the vnc is an idiomatic or literal usage ( or whether this cannot be determined, in which case it is labelled "" unknown "" ).', 'vnc - tokens is divided into dev and test sets that each include fourteen vnc types and a total of roughly six hundred instances of these types annotated as literal or idiomatic.', ' #AUTHOR_TAG, we use dev and test, and ignore all token instances annotated as "" unknown "".', ' #TAUTHOR_TAG and  #AUTHOR_TAG structured their experiments differently.', ' #TAUTHOR_TAG report results over dev and test separately.', 'in this setup test consists of expressions that were not seen during model development ( done on dev ).', 'salton et al., on the other hand, merge dev and test, and create new training and testing sets, such that each expression is present in the training and testing data, and the ratio of idiomatic to literal usages of each expression in the training data is roughly equal to that in the testing data.', 'we borrowed ideas from both of these approaches in structuring our experiments.', 'we retain we then divide each of these into training and testing sets, using the same ratios of idiomatic to literal usages for each expression as  #AUTHOR_TAG.', 'this allows us to develop and tune a model on dev, and then determine whether, when retrained on instances of unseen vncs in ( the training portion of ) test, that model is able to generalize to new vncs without further tuning to the specific expressions in test']",5
"['our models following  #TAUTHOR_TAG because the classes are roughly balanced.', 'we randomly']","['our models following  #TAUTHOR_TAG because the classes are roughly balanced.', 'we randomly']","['our models following  #TAUTHOR_TAG because the classes are roughly balanced.', 'we randomly']","['proportion of idiomatic usages in the testing portions of both dev and test is 63 %.', 'we therefore use accuracy to evaluate our models following  #TAUTHOR_TAG because the classes are roughly balanced.', 'we randomly divide both dev and test into training and testing portions ten times, following  #AUTHOR_TAG.', 'for each of the ten runs, we compute the accuracy for each expression, and then compute the average accuracy over the expressions.', 'we then report the average accuracy over the ten runs']",5
[' #TAUTHOR_TAG and  #AUTHOR_TAG - to train and'],[' #TAUTHOR_TAG and  #AUTHOR_TAG - to train and'],[' #TAUTHOR_TAG and  #AUTHOR_TAG - to train and'],"['use the vnc - tokens dataset  #AUTHOR_TAG - the same dataset used by  #TAUTHOR_TAG and  #AUTHOR_TAG - to train and evaluate our models.', 'this dataset consists of sentences containing vnc usages drawn from the british national corpus  #AUTHOR_TAG, 7 along with a label indicating whether the vnc is an idiomatic or literal usage ( or whether this cannot be determined, in which case it is labelled "" unknown "" ).', 'vnc - tokens is divided into dev and test sets that each include fourteen vnc types and a total of roughly six hundred instances of these types annotated as literal or idiomatic.', ' #AUTHOR_TAG, we use dev and test, and ignore all token instances annotated as "" unknown "".', ' #TAUTHOR_TAG and  #AUTHOR_TAG structured their experiments differently.', ' #TAUTHOR_TAG report results over dev and test separately.', 'in this setup test consists of expressions that were not seen during model development ( done on dev ).', 'salton et al., on the other hand, merge dev and test, and create new training and testing sets, such that each expression is present in the training and testing data, and the ratio of idiomatic to literal usages of each expression in the training data is roughly equal to that in the testing data.', 'we borrowed ideas from both of these approaches in structuring our experiments.', 'we retain we then divide each of these into training and testing sets, using the same ratios of idiomatic to literal usages for each expression as  #AUTHOR_TAG.', 'this allows us to develop and tune a model on dev, and then determine whether, when retrained on instances of unseen vncs in ( the training portion of ) test, that model is able to generalize to new vncs without further tuning to the specific expressions in test']",3
"['of  #TAUTHOR_TAG, cform']","['3. in line with the findings of  #TAUTHOR_TAG, cform']","['3. in line with the findings of  #TAUTHOR_TAG, cf']","['', 'and then the average over all ten runs. we do this using cform, and the word2vec model with and without the', 'canonical form feature. results are shown in table 3. in line with the findings of  #TAUTHOR_TAG, cform achieves higher precision and recall on idiomatic usages than literal ones. in particular, the relatively low recall for the literal class indicates that many literal usages occur in a canonical form', '']",3
"['our models following  #TAUTHOR_TAG because the classes are roughly balanced.', 'we randomly']","['our models following  #TAUTHOR_TAG because the classes are roughly balanced.', 'we randomly']","['our models following  #TAUTHOR_TAG because the classes are roughly balanced.', 'we randomly']","['proportion of idiomatic usages in the testing portions of both dev and test is 63 %.', 'we therefore use accuracy to evaluate our models following  #TAUTHOR_TAG because the classes are roughly balanced.', 'we randomly divide both dev and test into training and testing portions ten times, following  #AUTHOR_TAG.', 'for each of the ten runs, we compute the accuracy for each expression, and then compute the average accuracy over the expressions.', 'we then report the average accuracy over the ten runs']",1
"['of  #TAUTHOR_TAG, cform']","['3. in line with the findings of  #TAUTHOR_TAG, cform']","['3. in line with the findings of  #TAUTHOR_TAG, cf']","['', 'and then the average over all ten runs. we do this using cform, and the word2vec model with and without the', 'canonical form feature. results are shown in table 3. in line with the findings of  #TAUTHOR_TAG, cform achieves higher precision and recall on idiomatic usages than literal ones. in particular, the relatively low recall for the literal class indicates that many literal usages occur in a canonical form', '']",4
"['.', ' #TAUTHOR_TAG then showed that even better results']","['', ' #TAUTHOR_TAG then showed that even better results']","['', ' #TAUTHOR_TAG then showed that even better results']","['', 'in the first surface realization shared task  #AUTHOR_TAG henceforth sr - 11 ), which aimed to ameliorate these difficulties, attempts to use grammar - based realizers were unsuccessful, as converting shared task inputs to systemnative inputs turned out to be more difficult than anticipated.', ' #AUTHOR_TAG demonstrated that grammarbased systems can be substantially improved with error mining techniques, and  #AUTHOR_TAG showed that augmenting the ( shallow ) sr - 11 representation of coordination to include shared dependencies can benefit grammar - based realizers.', ' #TAUTHOR_TAG then showed that even better results can be achieved by inducing a grammar  #AUTHOR_TAG that is directly compatible with ( an enhanced version of ) the sr - 11 inputs.', 'however, as explained below, subsequent analysis revealed substantial remaining issues with the data, which this paper takes a step towards addressing.', 'a common thread in work on reversible, constraint - based grammars is emphasis on properly representing unbounded dependencies and coordination.', 'for parsing, this emphasis has been shown to pay off in improved recall of unbounded dependencies  #AUTHOR_TAG.', '']",0
"['.', ' #TAUTHOR_TAG then showed that even better results']","['', ' #TAUTHOR_TAG then showed that even better results']","['', ' #TAUTHOR_TAG then showed that even better results']","['', 'in the first surface realization shared task  #AUTHOR_TAG henceforth sr - 11 ), which aimed to ameliorate these difficulties, attempts to use grammar - based realizers were unsuccessful, as converting shared task inputs to systemnative inputs turned out to be more difficult than anticipated.', ' #AUTHOR_TAG demonstrated that grammarbased systems can be substantially improved with error mining techniques, and  #AUTHOR_TAG showed that augmenting the ( shallow ) sr - 11 representation of coordination to include shared dependencies can benefit grammar - based realizers.', ' #TAUTHOR_TAG then showed that even better results can be achieved by inducing a grammar  #AUTHOR_TAG that is directly compatible with ( an enhanced version of ) the sr - 11 inputs.', 'however, as explained below, subsequent analysis revealed substantial remaining issues with the data, which this paper takes a step towards addressing.', 'a common thread in work on reversible, constraint - based grammars is emphasis on properly representing unbounded dependencies and coordination.', 'for parsing, this emphasis has been shown to pay off in improved recall of unbounded dependencies  #AUTHOR_TAG.', '']",0
"['.', ' #TAUTHOR_TAG then showed that even better results']","['', ' #TAUTHOR_TAG then showed that even better results']","['', ' #TAUTHOR_TAG then showed that even better results']","['', 'in the first surface realization shared task  #AUTHOR_TAG henceforth sr - 11 ), which aimed to ameliorate these difficulties, attempts to use grammar - based realizers were unsuccessful, as converting shared task inputs to systemnative inputs turned out to be more difficult than anticipated.', ' #AUTHOR_TAG demonstrated that grammarbased systems can be substantially improved with error mining techniques, and  #AUTHOR_TAG showed that augmenting the ( shallow ) sr - 11 representation of coordination to include shared dependencies can benefit grammar - based realizers.', ' #TAUTHOR_TAG then showed that even better results can be achieved by inducing a grammar  #AUTHOR_TAG that is directly compatible with ( an enhanced version of ) the sr - 11 inputs.', 'however, as explained below, subsequent analysis revealed substantial remaining issues with the data, which this paper takes a step towards addressing.', 'a common thread in work on reversible, constraint - based grammars is emphasis on properly representing unbounded dependencies and coordination.', 'for parsing, this emphasis has been shown to pay off in improved recall of unbounded dependencies  #AUTHOR_TAG.', '']",1
['extended  #TAUTHOR_TAG cc'],['extended  #TAUTHOR_TAG ccg induction algorithm to work with'],['extended  #TAUTHOR_TAG ccg induction algorithm to work with the augmented uds that'],"['have adapted and extended  #TAUTHOR_TAG ccg induction algorithm to work with the augmented uds that our system produces.', ""white's algorithm assumed ccg phrases are only rarely projected from a dependent rather than a heade. g., where an np is projected from a determiner, which is a dependent of the head nounand thus could be easily handled by handcrafted lexical entries."", 'since such cases are very common in uds, the algorithm needed to be extended to induce such categories automatically.', 'once this was done, the algorithm yielded complete derivations in most cases ( approx. 94 % ).', 'in particular, derivations were induced that captured all but one of the extra dependencies in table 1 that appear in the ccgbank dev section, and realization experiments with the ud - based representations are underway.', 'with the augmented ud reported in this paper, we expect the resulting dependency graphs to serve as a promising basis for a second surface realization challenge ( with using just the basic dependency trees as an option ).', 'a remaining obstacle, however, are the dependent cluster and gapping cases in the ptb, for which the sdc produces rather degenerate output.', 'a promising avenue here would be to adapt  #AUTHOR_TAG method of enhancing the sr - 11 representations for these cases']",6
"['relations  #TAUTHOR_TAG.', 'matrix']","['relations  #TAUTHOR_TAG.', 'matrix']","['the relations  #TAUTHOR_TAG.', 'matrix factorization is']","['##ly - supervised relation extraction has gained prominence as it utilizes automatically aligned data to train accurate extractors.', 'universal schema, in particular, has found impressive accuracy gains by ( 1 ) treating the distant - supervision as a knowledge - base ( kb ) containing both structured relations such as bornin * first two authors contributed equally to the paper. and surface form relations such as "" was born in "" extracted from text, and ( 2 ) by completing the entries in such a kb using joint and compact encoding of the dependencies between the relations  #TAUTHOR_TAG.', '']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['universal schema is defined as the union of all openie - like surface form patterns found in text and fixed canonical relations that exist in a knowledge base  #TAUTHOR_TAG.', 'the task here is to complete this schema by jointly reasoning over surface form patterns and relations.', 'a successful approach to this joint reasoning is to embed both kinds of relations into the same low - dimensional embedding space, which can be achieved by matrix or tensor factorization methods.', 'we will study such representations for universal schema in this paper']",0
"['schema,  #TAUTHOR_TAG construct a sparse binary matrix of size | p | ×']","['schema,  #TAUTHOR_TAG construct a sparse binary matrix of size | p | × | r | whose rows are indexed by entity - pairs ( a, b ) ∈ p and columns by']","['universal schema,  #TAUTHOR_TAG construct a sparse binary matrix of size | p | × | r | whose rows are indexed by']","['matrix factorization for universal schema,  #TAUTHOR_TAG construct a sparse binary matrix of size | p | × | r | whose rows are indexed by entity - pairs ( a, b ) ∈ p and columns by surface form and freebase relations s ∈ r. subsequently, generalized pca  #AUTHOR_TAG is used to find a rank - k factorization, i. e., with relation factors r ∈ r | r | ×k and entity - pair factors p ∈ r | p | ×k, the probability of a relation s and two entities a and b is :', 'where σ is the sigmoid function.', 'using this factorization, similar entity - pairs and relations are embedded close to each other in a k - dimensional vector space.', 'since this model uses embeddings for pairs of entities, as opposed to per - entity embeddings, we refer to such models as pairwise models.', 'pairwise embeddings are especially suitable when working with universal schema data, since they can represent correlations between surface pattern relations and structured relations compactly.', 'furthermore, they combine multiple evidences specific to an entity - pair to predict a relation between them.', 'since the observed data matrix contains only true entries, the parameters are learned using bayesian personalized ranking  #AUTHOR_TAG that supports implicit feedback.', ' #AUTHOR_TAG explore a number of variants of this factorization, including a neighborhood model that learns local classifiers, and an entity model that includes entity representations ( we revisit this formulation in section 2. 3. 4 ).', 'in the rest of this paper we will only use the basic factorization model ( referred to as model f ) as the primary pairwise embedding model, however the ideas apply directly to these variants as well.', 'there are a few shortcomings of models that rely solely on pairwise embeddings.', 'to learn an appropriate representation of an entity - pair, the two entities need to be mentioned together frequently, which is not the case for many entity - pairs of interest.', 'since predicting relations often relies on the entity types, this lack of ample relational evidence for an entity pair can result in poor estimation of their types, and hence, of their relations.', 'further, a large number of pairwise relation instances ( relative to the number of entities ) results in a large number of model parameters, leading to scalability concerns']",0
"[', we isolate the entity factorization in  #TAUTHOR_TAG by viewing']","[', we isolate the entity factorization in  #TAUTHOR_TAG by viewing']","[', we isolate the entity factorization in  #TAUTHOR_TAG by viewing']","[', we isolate the entity factorization in  #TAUTHOR_TAG by viewing it as tensor factorization.', 'in this model, each relation is assigned an embedding for each of its two arguments, i. e.,', 'although not explored in isolation by  #TAUTHOR_TAG, model e can be used on its own to predict relations between entities, even if they have not been observed to be in a relation']",6
"[', we isolate the entity factorization in  #TAUTHOR_TAG by viewing']","[', we isolate the entity factorization in  #TAUTHOR_TAG by viewing']","[', we isolate the entity factorization in  #TAUTHOR_TAG by viewing']","[', we isolate the entity factorization in  #TAUTHOR_TAG by viewing it as tensor factorization.', 'in this model, each relation is assigned an embedding for each of its two arguments, i. e.,', 'although not explored in isolation by  #TAUTHOR_TAG, model e can be used on its own to predict relations between entities, even if they have not been observed to be in a relation']",6
"['model ( eq. 5 ), we consider the fe model from  #TAUTHOR_TAG, i. e., the additive combination of the two : p ( s ( a, b ) ) = σ']","['model ( eq. 5 ), we consider the fe model from  #TAUTHOR_TAG, i. e., the additive combination of the two : p ( s ( a, b ) ) =']","['( eq. 5 ), we consider the fe model from  #TAUTHOR_TAG, i. e., the additive combination of the two : p ( s ( a, b ) ) = σ']","['the direct combination of a pairwise model ( eq. 1 ) with an entity model ( eq. 5 ), we consider the fe model from  #TAUTHOR_TAG, i. e., the additive combination of the two : p ( s ( a, b ) ) = σ ( r s · e ab + r s, 1 · e a + r s, 2 · e b ) ( 6 ) both the matrix factorization model f and entity model e can de defined as special cases of this model, by setting r s, 1 / 2 or r s to zero, respectively']",6
"['for universal schema  #TAUTHOR_TAG,']","['for universal schema  #TAUTHOR_TAG,']","['for universal schema  #TAUTHOR_TAG, it is not']","['the previous section, we provided background on matrix factorization with pairwise factors, followed by a tensor factorization based formulation of universal schema.', 'although matrix factorization performs well for universal schema  #TAUTHOR_TAG, it is not robust to sparse data and does not capture latent entity types that can be crucial for accurate relation extraction.', 'on the other hand, although tensor factorization models are able to compactly represent entity types using unary embeddings, they are unable to adequately represent the pair - specific information that is necessary for modeling relations.', 'it is worth noting that tensor factorization for universal schema has been proposed by, who also observed that tensor factorization by itself performs poorly ( even with additional type constraints ), and the predictions need to be combined with matrix factorization to be accurate.', 'in this section we will present the fundamental differences between matrix and tensor factorization, and examine a few hybrid models that can address these concerns.', 'black is a sparsely observed relation between any pair of entities.', 'red relations correspond to each black edge, and a model that learns this implication can generalize to test instances ( red dotted edge ).', 'green relation exists between white and gray entities ( we omit many of these edges for clarity ), requiring the model to learn latent entity types.', 'finally, blue relations exist for pairs where both a black and green relation is observed']",1
"['model ( eq. 5 ), we consider the fe model from  #TAUTHOR_TAG, i. e., the additive combination of the two : p ( s ( a, b ) ) = σ']","['model ( eq. 5 ), we consider the fe model from  #TAUTHOR_TAG, i. e., the additive combination of the two : p ( s ( a, b ) ) =']","['( eq. 5 ), we consider the fe model from  #TAUTHOR_TAG, i. e., the additive combination of the two : p ( s ( a, b ) ) = σ']","['the direct combination of a pairwise model ( eq. 1 ) with an entity model ( eq. 5 ), we consider the fe model from  #TAUTHOR_TAG, i. e., the additive combination of the two : p ( s ( a, b ) ) = σ ( r s · e ab + r s, 1 · e a + r s, 2 · e b ) ( 6 ) both the matrix factorization model f and entity model e can de defined as special cases of this model, by setting r s, 1 / 2 or r s to zero, respectively']",1
"['model ( eq. 5 ), we consider the fe model from  #TAUTHOR_TAG, i. e., the additive combination of the two : p ( s ( a, b ) ) = σ']","['model ( eq. 5 ), we consider the fe model from  #TAUTHOR_TAG, i. e., the additive combination of the two : p ( s ( a, b ) ) =']","['( eq. 5 ), we consider the fe model from  #TAUTHOR_TAG, i. e., the additive combination of the two : p ( s ( a, b ) ) = σ']","['the direct combination of a pairwise model ( eq. 1 ) with an entity model ( eq. 5 ), we consider the fe model from  #TAUTHOR_TAG, i. e., the additive combination of the two : p ( s ( a, b ) ) = σ ( r s · e ab + r s, 1 · e a + r s, 2 · e b ) ( 6 ) both the matrix factorization model f and entity model e can de defined as special cases of this model, by setting r s, 1 / 2 or r s to zero, respectively']",5
"['by  #TAUTHOR_TAG, we use a bayesian personalized ranking objective  #AUTHOR_TAG to estimate parameters, i. e., for']","['by  #TAUTHOR_TAG, we use a bayesian personalized ranking objective  #AUTHOR_TAG to estimate parameters, i. e., for']","['by  #TAUTHOR_TAG, we use a bayesian personalized ranking objective  #AUTHOR_TAG to estimate parameters, i. e., for each observed training fact, we sample an unobserved fact']","['by  #TAUTHOR_TAG, we use a bayesian personalized ranking objective  #AUTHOR_TAG to estimate parameters, i. e., for each observed training fact, we sample an unobserved fact for the same relation, and maximize their relative ranking using adagrad.', 'for all models we use k = 100 as dimension of latent representations, an initial learning rate of 0. 1, and 2 - regularization of all parameters with a weight of 0. 01.', 'for candecomp / parafac and rescal we use the open - source scikit - tensor 2 package with default hyper - parameters']",5
"['.', 'following the experiment setup of  #TAUTHOR_TAG, we instanti']","['distantly - supervised relation extraction.', 'following the experiment setup of  #TAUTHOR_TAG, we instantiate']","['distantly - supervised relation extraction.', 'following the experiment setup of  #TAUTHOR_TAG, we instantiate the universal schema matrix over entity pairs and text']","['the promising results shown on synthetic data, we now turn to evaluation on real - world information extraction.', 'in particular, we evaluate the models on universal schema for distantly - supervised relation extraction.', 'following the experiment setup of  #TAUTHOR_TAG, we instantiate the universal schema matrix over entity pairs and text / freebase relations for new york times data, and compare the performance using average precision of the presented models.', 'table 1 summarizes the performance of our models, as compared to existing approaches ( see  #TAUTHOR_TAG for an overview ).', 'in particular, tr - r13 takes the output predictions of matrix factorization, and combines it with an entity - type aware rescal model.', '3 tensor factorization approaches perform poorly on this data.', 'we present results for model e, but other formulations such as parafac, transe, rescal, and tucker2 achieved even lower accuracy ; this is consistent with the results in.', 'models that use the matrix factorization ( f, fe, r13 - f and rfe ) are significantly better, but more importantly, the hybrid appraoch fe achieves the highest accuracy.', 'it is unclear why rfe fails to provide similar gains, in particular, performing slightly worse than matrix factorization.', 'note that we are not introducing a new state - of - art here, the neighborhood model ( nf ) that achieves a higher accuracy is omitted for clarity.', 'table 2 : nearest - neighbors for a few randomlyselected entities based on their embeddings, demonstrating that similar entities are close to each other']",5
"['.', 'following the experiment setup of  #TAUTHOR_TAG, we instanti']","['distantly - supervised relation extraction.', 'following the experiment setup of  #TAUTHOR_TAG, we instantiate']","['distantly - supervised relation extraction.', 'following the experiment setup of  #TAUTHOR_TAG, we instantiate the universal schema matrix over entity pairs and text']","['the promising results shown on synthetic data, we now turn to evaluation on real - world information extraction.', 'in particular, we evaluate the models on universal schema for distantly - supervised relation extraction.', 'following the experiment setup of  #TAUTHOR_TAG, we instantiate the universal schema matrix over entity pairs and text / freebase relations for new york times data, and compare the performance using average precision of the presented models.', 'table 1 summarizes the performance of our models, as compared to existing approaches ( see  #TAUTHOR_TAG for an overview ).', 'in particular, tr - r13 takes the output predictions of matrix factorization, and combines it with an entity - type aware rescal model.', '3 tensor factorization approaches perform poorly on this data.', 'we present results for model e, but other formulations such as parafac, transe, rescal, and tucker2 achieved even lower accuracy ; this is consistent with the results in.', 'models that use the matrix factorization ( f, fe, r13 - f and rfe ) are significantly better, but more importantly, the hybrid appraoch fe achieves the highest accuracy.', 'it is unclear why rfe fails to provide similar gains, in particular, performing slightly worse than matrix factorization.', 'note that we are not introducing a new state - of - art here, the neighborhood model ( nf ) that achieves a higher accuracy is omitted for clarity.', 'table 2 : nearest - neighbors for a few randomlyselected entities based on their embeddings, demonstrating that similar entities are close to each other']",5
"['', 'on the isnotes corpus  #TAUTHOR_TAG, our model with the']","['is classification.', 'on the isnotes corpus  #TAUTHOR_TAG, our model with the']","['', 'on the isnotes corpus  #TAUTHOR_TAG, our model with']","['', 'in this paper, we propose a discourse context - aware self - attention neural network model for fine - grained is classification.', 'on the isnotes corpus  #TAUTHOR_TAG, our model with the contextually - encoded word representations ( bert )  #AUTHOR_TAG achieves new state - of - the - art performances on fine - grained is classification, obtaining a 4. 1 % absolute overall accuracy improvement compared to  #AUTHOR_TAG a ).', 'more importantly, we also show an improvement of 3. 9 % f1 for bridging anaphora recognition without using any complex hand - crafted semantic features designed for capturing the bridging phenomenon']",5
"['', 'for instance, according to  #TAUTHOR_TAG, old mentions 1 refer']","[""hearer's knowledge and beliefs."", 'for instance, according to  #TAUTHOR_TAG, old mentions 1 refer']","['', 'for instance, according to  #TAUTHOR_TAG, old mentions 1 refer']","['structure  #AUTHOR_TAG prince,, 1992  #AUTHOR_TAG kruijff - korbayova and  #AUTHOR_TAG studies structural and semantic properties of a sentence according to its relation to the discourse context.', 'information structure affects how discourse entities are referred to in a text, which is known as information status  #AUTHOR_TAG.', ""specifically, information status ( is henceforth ) reflects the accessibility of a discourse entity based on the evolving discourse context and the speaker's assumption about the hearer's knowledge and beliefs."", 'for instance, according to  #TAUTHOR_TAG, old mentions 1 refer to entities that have been referred to previously ; mediated men - tions have not been mentioned before but are accessible to the hearer by reference to another old mention or to prior world knowledge ; and new mentions refer to entities that are introduced to the discourse for the first time and are not known to the hearer before.', 'in this paper, we follow the is scheme proposed by  #TAUTHOR_TAG and focus on learning finegrained is on written texts.', ""a mention's semantic and syntactic properties can signal its information status."", '']",5
"['', 'for instance, according to  #TAUTHOR_TAG, old mentions 1 refer']","[""hearer's knowledge and beliefs."", 'for instance, according to  #TAUTHOR_TAG, old mentions 1 refer']","['', 'for instance, according to  #TAUTHOR_TAG, old mentions 1 refer']","['structure  #AUTHOR_TAG prince,, 1992  #AUTHOR_TAG kruijff - korbayova and  #AUTHOR_TAG studies structural and semantic properties of a sentence according to its relation to the discourse context.', 'information structure affects how discourse entities are referred to in a text, which is known as information status  #AUTHOR_TAG.', ""specifically, information status ( is henceforth ) reflects the accessibility of a discourse entity based on the evolving discourse context and the speaker's assumption about the hearer's knowledge and beliefs."", 'for instance, according to  #TAUTHOR_TAG, old mentions 1 refer to entities that have been referred to previously ; mediated men - tions have not been mentioned before but are accessible to the hearer by reference to another old mention or to prior world knowledge ; and new mentions refer to entities that are introduced to the discourse for the first time and are not known to the hearer before.', 'in this paper, we follow the is scheme proposed by  #TAUTHOR_TAG and focus on learning finegrained is on written texts.', ""a mention's semantic and syntactic properties can signal its information status."", '']",5
"['experiments on the isnotes corpus  #TAUTHOR_TAG, which contains']","['experiments on the isnotes corpus  #TAUTHOR_TAG, which contains 10, 980 mentions annotated for information status in 50 news texts.', ' #AUTHOR_TAG a ),']","['perform experiments on the isnotes corpus  #TAUTHOR_TAG, which contains']","['perform experiments on the isnotes corpus  #TAUTHOR_TAG, which contains 10, 980 mentions annotated for information status in 50 news texts.', ' #AUTHOR_TAG a ), all experiments are performed via 10 - fold cross - validation on documents.', '']",5
"['', 'for instance, according to  #TAUTHOR_TAG, old mentions 1 refer']","[""hearer's knowledge and beliefs."", 'for instance, according to  #TAUTHOR_TAG, old mentions 1 refer']","['', 'for instance, according to  #TAUTHOR_TAG, old mentions 1 refer']","['structure  #AUTHOR_TAG prince,, 1992  #AUTHOR_TAG kruijff - korbayova and  #AUTHOR_TAG studies structural and semantic properties of a sentence according to its relation to the discourse context.', 'information structure affects how discourse entities are referred to in a text, which is known as information status  #AUTHOR_TAG.', ""specifically, information status ( is henceforth ) reflects the accessibility of a discourse entity based on the evolving discourse context and the speaker's assumption about the hearer's knowledge and beliefs."", 'for instance, according to  #TAUTHOR_TAG, old mentions 1 refer to entities that have been referred to previously ; mediated men - tions have not been mentioned before but are accessible to the hearer by reference to another old mention or to prior world knowledge ; and new mentions refer to entities that are introduced to the discourse for the first time and are not known to the hearer before.', 'in this paper, we follow the is scheme proposed by  #TAUTHOR_TAG and focus on learning finegrained is on written texts.', ""a mention's semantic and syntactic properties can signal its information status."", '']",0
"['as part of is classification problem.', ' #TAUTHOR_TAG et al']","['as part of is classification problem.', ' #TAUTHOR_TAG et al. ( 2013a ) regarding']","['as part of is classification problem.', ' #TAUTHOR_TAG et al']","['', 'bridging resolution  #AUTHOR_TAG contains two sub tasks : identifying bridging anaphors  #AUTHOR_TAG a ;  #AUTHOR_TAG and finding the correct antecedent among candidates  #AUTHOR_TAG b ;  #AUTHOR_TAG a, b ).', 'previous work handle bridging anaphora recognition as part of is classification problem.', ' #TAUTHOR_TAG et al. ( 2013a ) regarding the overall is classificiation accuracy but the result on bridging anaphora recognition is much worse than  #AUTHOR_TAG a ).', ' #AUTHOR_TAG incorporated carefully designed rules into an svm multiclass algorithm for is classification on the switchboard dialogue corpus  #AUTHOR_TAG.', ' #AUTHOR_TAG trained a crf model with syntactic and surface features for fine - grained is classification on the german dirndl radio news corpus  #AUTHOR_TAG 2.', 'different from the above mentioned work, we do not use any complicated hand - crafted features and our model improves the previous state - of - theart results on both overall is classification accuracy and bridging recognition by a large margin on the isnotes corpus.', 'self - attention.', 'recently, multi - head selfattention encoder  #AUTHOR_TAG has been shown to perform well in various nlp tasks, including semantic role labelling  #AUTHOR_TAG, question answering and natural language inference  #AUTHOR_TAG.', 'in our model, we create a "" pseudo sentence "" for each mention and apply the transformer encoder for our task.', ""the self - attention mechanism allows our model to attend to both the context and the mention itself for clues which are helpful for predicting the mention's is."", 'fine - tuning with contextual word embeddings.', 'recent work  #AUTHOR_TAG have shown that a range of downstream nlp tasks benefit from fine - tuning task - specific parameters with pre - trained contextual word representations.', 'our work belongs to this category and we fine - tune our model based on bert base representations  #AUTHOR_TAG']",0
"['is scheme proposed by  #TAUTHOR_TAG adopts three major is categories ( old, new and mediated ) from  #AUTHOR_TAG and distinguishes six subc']","['is scheme proposed by  #TAUTHOR_TAG adopts three major is categories ( old, new and mediated ) from  #AUTHOR_TAG and distinguishes six subcategories for mediated.', 'table 1 lists the definitions for these is categories and summarizes the main']","['is scheme proposed by  #TAUTHOR_TAG adopts three major is categories ( old, new and mediated ) from  #AUTHOR_TAG and distinguishes six subc']","['is scheme proposed by  #TAUTHOR_TAG adopts three major is categories ( old, new and mediated ) from  #AUTHOR_TAG and distinguishes six subcategories for mediated.', 'table 1 lists the definitions for these is categories and summarizes the main affecting factors for each is class.', ""as described in section 1, a mention's internal syntactic and semantic properties can signal its is."", 'for instance, a mention containing a possessive pronoun modifier is likely to be mediated / syntactic ( e. g., their father ) ; and a mediated / comparative mention often contains a premodifier indicating that this entity is compared to another preceding entity ( e. g., further attacks ).', 'in addition, for some is classes, the "" local context "" ( the sentence s which contains the target mention ) and "" previous context "" ( sentences from the discourse which precede s ) play an important role when assigning is to a mention.', 'example 1 and example 2 in section 1 demonstrate the role of the local context for is.', 'and sometimes we need to look at the previous context when deciding is for a mention.', 'in example 3, without looking at the previous context, we tend to think the is for "" poland "" in the second sentence is mediated / worldknowledge.', 'here the correct is for "" poland "" is old because it is mentioned before in the previous context']",0
"['of  #TAUTHOR_TAG, where delexicalized parsing models from a number of source languages', 'were']","['of  #TAUTHOR_TAG, where delexicalized parsing models from a number of source languages', 'were']","['- lingual study of  #TAUTHOR_TAG, where delexicalized parsing models from a number of source languages', 'were']","['monolingual parsers and evaluate their quality without reference to other languages, as in the original conll shared tasks, but there are many cases where heterogenous', 'treebanks are less than adequate. first, a homogeneous representation is critical for multilingual language technologies that require consistent cross - lingual analysis for downstream components. second,', 'consistent syntactic representations are desirable in the evaluation of unsupervised  #AUTHOR_TAG or cross - lingual syntactic parsers  #AUTHOR_TAG. in the', 'cross - lingual study of  #TAUTHOR_TAG, where delexicalized parsing models from a number of source languages', 'were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. in one stunn', '##ing example, danish was the worst source language when parsing swedish, solely due to greatly divergent annotation schemes. in order to overcome these difficulties,', 'some cross - lingual studies have resorted to heuristics to homogenize treebanks  #AUTHOR_TAG, but we are only aware of a few systematic attempts to', 'create homogenous syntactic dependency annotation in multiple languages. in terms of automatic construction,  #AUTHOR_TAG attempt to harmonize a large number of dependency tree', '##banks by mapping their annotation to a version of the prague dependency treebank scheme ( hajic et al., 2001 ; bohmova et al., 2003 ). additionally, there have been efforts to manually or semimanually construct resources with common syn', '- tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation  #AUTHOR_TAG. in order to facilitate research', 'on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages : german,', 'english, french, korean, spanish and swedish. this resource is freely available and we plan to extend it to include more data and languages. in the context of part - of - speech tagging, universal representations, such as that of  #AUTHOR_TAG, have already spur', '##red numerous examples of improved empirical cross - lingual systems  #AUTHOR_TAG tackstrom et al., 2013 ). we aim to do', 'the same for syntactic dependencies and present cross - lingual parsing experiments to highlight some of the benefits of cross - lingually consistent annotation. first, results largely conform to our expectations of which target languages should be useful for which source languages', ', unlike in the study of  #TAUTHOR_TAG. second, the evaluation scores in general are significantly higher than previous cross - lingual studies, suggesting that most of these studies underestimate true accuracy. finally, unlike all previous cross - lingual studies, we can report full labeled accuracies and', 'not just unlabeled structural accuracies']",0
"['of  #TAUTHOR_TAG, where delexicalized parsing models from a number of source languages', 'were']","['of  #TAUTHOR_TAG, where delexicalized parsing models from a number of source languages', 'were']","['- lingual study of  #TAUTHOR_TAG, where delexicalized parsing models from a number of source languages', 'were']","['monolingual parsers and evaluate their quality without reference to other languages, as in the original conll shared tasks, but there are many cases where heterogenous', 'treebanks are less than adequate. first, a homogeneous representation is critical for multilingual language technologies that require consistent cross - lingual analysis for downstream components. second,', 'consistent syntactic representations are desirable in the evaluation of unsupervised  #AUTHOR_TAG or cross - lingual syntactic parsers  #AUTHOR_TAG. in the', 'cross - lingual study of  #TAUTHOR_TAG, where delexicalized parsing models from a number of source languages', 'were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. in one stunn', '##ing example, danish was the worst source language when parsing swedish, solely due to greatly divergent annotation schemes. in order to overcome these difficulties,', 'some cross - lingual studies have resorted to heuristics to homogenize treebanks  #AUTHOR_TAG, but we are only aware of a few systematic attempts to', 'create homogenous syntactic dependency annotation in multiple languages. in terms of automatic construction,  #AUTHOR_TAG attempt to harmonize a large number of dependency tree', '##banks by mapping their annotation to a version of the prague dependency treebank scheme ( hajic et al., 2001 ; bohmova et al., 2003 ). additionally, there have been efforts to manually or semimanually construct resources with common syn', '- tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation  #AUTHOR_TAG. in order to facilitate research', 'on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages : german,', 'english, french, korean, spanish and swedish. this resource is freely available and we plan to extend it to include more data and languages. in the context of part - of - speech tagging, universal representations, such as that of  #AUTHOR_TAG, have already spur', '##red numerous examples of improved empirical cross - lingual systems  #AUTHOR_TAG tackstrom et al., 2013 ). we aim to do', 'the same for syntactic dependencies and present cross - lingual parsing experiments to highlight some of the benefits of cross - lingually consistent annotation. first, results largely conform to our expectations of which target languages should be useful for which source languages', ', unlike in the study of  #TAUTHOR_TAG. second, the evaluation scores in general are significantly higher than previous cross - lingual studies, suggesting that most of these studies underestimate true accuracy. finally, unlike all previous cross - lingual studies, we can report full labeled accuracies and', 'not just unlabeled structural accuracies']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG.', 'in particular, it is a perceptron - trained shift - reduce parser with a beam of size 8.', ""we use the features of  #AUTHOR_TAG, except that all lexical identities are dropped from the templates during training and testing, hence inducing a'delexicalized'model that employs only'universal'properties from source - side treebanks, such as part - ofspeech tags, labels, head - modifier distance, etc."", 'we ran a number of experiments, which can be seen in']","['of the motivating factors in creating such a data set was improved cross - lingual transfer evaluation.', 'to test this, we use a cross - lingual transfer parser similar to that of  #TAUTHOR_TAG.', 'in particular, it is a perceptron - trained shift - reduce parser with a beam of size 8.', ""we use the features of  #AUTHOR_TAG, except that all lexical identities are dropped from the templates during training and testing, hence inducing a'delexicalized'model that employs only'universal'properties from source - side treebanks, such as part - ofspeech tags, labels, head - modifier distance, etc."", 'we ran a number of experiments, which can be seen in table 3.', 'for these experiments we randomly split each data set into training, development and testing sets.', '5 the one exception is english, where we used the standard splits.', '']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG.', 'in particular, it is a perceptron - trained shift - reduce parser with a beam of size 8.', ""we use the features of  #AUTHOR_TAG, except that all lexical identities are dropped from the templates during training and testing, hence inducing a'delexicalized'model that employs only'universal'properties from source - side treebanks, such as part - ofspeech tags, labels, head - modifier distance, etc."", 'we ran a number of experiments, which can be seen in']","['of the motivating factors in creating such a data set was improved cross - lingual transfer evaluation.', 'to test this, we use a cross - lingual transfer parser similar to that of  #TAUTHOR_TAG.', 'in particular, it is a perceptron - trained shift - reduce parser with a beam of size 8.', ""we use the features of  #AUTHOR_TAG, except that all lexical identities are dropped from the templates during training and testing, hence inducing a'delexicalized'model that employs only'universal'properties from source - side treebanks, such as part - ofspeech tags, labels, head - modifier distance, etc."", 'we ran a number of experiments, which can be seen in table 3.', 'for these experiments we randomly split each data set into training, development and testing sets.', '5 the one exception is english, where we used the standard splits.', '']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG.', 'in particular, it is a perceptron - trained shift - reduce parser with a beam of size 8.', ""we use the features of  #AUTHOR_TAG, except that all lexical identities are dropped from the templates during training and testing, hence inducing a'delexicalized'model that employs only'universal'properties from source - side treebanks, such as part - ofspeech tags, labels, head - modifier distance, etc."", 'we ran a number of experiments, which can be seen in']","['of the motivating factors in creating such a data set was improved cross - lingual transfer evaluation.', 'to test this, we use a cross - lingual transfer parser similar to that of  #TAUTHOR_TAG.', 'in particular, it is a perceptron - trained shift - reduce parser with a beam of size 8.', ""we use the features of  #AUTHOR_TAG, except that all lexical identities are dropped from the templates during training and testing, hence inducing a'delexicalized'model that employs only'universal'properties from source - side treebanks, such as part - ofspeech tags, labels, head - modifier distance, etc."", 'we ran a number of experiments, which can be seen in table 3.', 'for these experiments we randomly split each data set into training, development and testing sets.', '5 the one exception is english, where we used the standard splits.', '']",0
"['of  #TAUTHOR_TAG, where delexicalized parsing models from a number of source languages', 'were']","['of  #TAUTHOR_TAG, where delexicalized parsing models from a number of source languages', 'were']","['- lingual study of  #TAUTHOR_TAG, where delexicalized parsing models from a number of source languages', 'were']","['monolingual parsers and evaluate their quality without reference to other languages, as in the original conll shared tasks, but there are many cases where heterogenous', 'treebanks are less than adequate. first, a homogeneous representation is critical for multilingual language technologies that require consistent cross - lingual analysis for downstream components. second,', 'consistent syntactic representations are desirable in the evaluation of unsupervised  #AUTHOR_TAG or cross - lingual syntactic parsers  #AUTHOR_TAG. in the', 'cross - lingual study of  #TAUTHOR_TAG, where delexicalized parsing models from a number of source languages', 'were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. in one stunn', '##ing example, danish was the worst source language when parsing swedish, solely due to greatly divergent annotation schemes. in order to overcome these difficulties,', 'some cross - lingual studies have resorted to heuristics to homogenize treebanks  #AUTHOR_TAG, but we are only aware of a few systematic attempts to', 'create homogenous syntactic dependency annotation in multiple languages. in terms of automatic construction,  #AUTHOR_TAG attempt to harmonize a large number of dependency tree', '##banks by mapping their annotation to a version of the prague dependency treebank scheme ( hajic et al., 2001 ; bohmova et al., 2003 ). additionally, there have been efforts to manually or semimanually construct resources with common syn', '- tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation  #AUTHOR_TAG. in order to facilitate research', 'on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages : german,', 'english, french, korean, spanish and swedish. this resource is freely available and we plan to extend it to include more data and languages. in the context of part - of - speech tagging, universal representations, such as that of  #AUTHOR_TAG, have already spur', '##red numerous examples of improved empirical cross - lingual systems  #AUTHOR_TAG tackstrom et al., 2013 ). we aim to do', 'the same for syntactic dependencies and present cross - lingual parsing experiments to highlight some of the benefits of cross - lingually consistent annotation. first, results largely conform to our expectations of which target languages should be useful for which source languages', ', unlike in the study of  #TAUTHOR_TAG. second, the evaluation scores in general are significantly higher than previous cross - lingual studies, suggesting that most of these studies underestimate true accuracy. finally, unlike all previous cross - lingual studies, we can report full labeled accuracies and', 'not just unlabeled structural accuracies']",1
"['of  #TAUTHOR_TAG, where delexicalized parsing models from a number of source languages', 'were']","['of  #TAUTHOR_TAG, where delexicalized parsing models from a number of source languages', 'were']","['- lingual study of  #TAUTHOR_TAG, where delexicalized parsing models from a number of source languages', 'were']","['monolingual parsers and evaluate their quality without reference to other languages, as in the original conll shared tasks, but there are many cases where heterogenous', 'treebanks are less than adequate. first, a homogeneous representation is critical for multilingual language technologies that require consistent cross - lingual analysis for downstream components. second,', 'consistent syntactic representations are desirable in the evaluation of unsupervised  #AUTHOR_TAG or cross - lingual syntactic parsers  #AUTHOR_TAG. in the', 'cross - lingual study of  #TAUTHOR_TAG, where delexicalized parsing models from a number of source languages', 'were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. in one stunn', '##ing example, danish was the worst source language when parsing swedish, solely due to greatly divergent annotation schemes. in order to overcome these difficulties,', 'some cross - lingual studies have resorted to heuristics to homogenize treebanks  #AUTHOR_TAG, but we are only aware of a few systematic attempts to', 'create homogenous syntactic dependency annotation in multiple languages. in terms of automatic construction,  #AUTHOR_TAG attempt to harmonize a large number of dependency tree', '##banks by mapping their annotation to a version of the prague dependency treebank scheme ( hajic et al., 2001 ; bohmova et al., 2003 ). additionally, there have been efforts to manually or semimanually construct resources with common syn', '- tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation  #AUTHOR_TAG. in order to facilitate research', 'on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages : german,', 'english, french, korean, spanish and swedish. this resource is freely available and we plan to extend it to include more data and languages. in the context of part - of - speech tagging, universal representations, such as that of  #AUTHOR_TAG, have already spur', '##red numerous examples of improved empirical cross - lingual systems  #AUTHOR_TAG tackstrom et al., 2013 ). we aim to do', 'the same for syntactic dependencies and present cross - lingual parsing experiments to highlight some of the benefits of cross - lingually consistent annotation. first, results largely conform to our expectations of which target languages should be useful for which source languages', ', unlike in the study of  #TAUTHOR_TAG. second, the evaluation scores in general are significantly higher than previous cross - lingual studies, suggesting that most of these studies underestimate true accuracy. finally, unlike all previous cross - lingual studies, we can report full labeled accuracies and', 'not just unlabeled structural accuracies']",4
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG.', 'in particular, it is a perceptron - trained shift - reduce parser with a beam of size 8.', ""we use the features of  #AUTHOR_TAG, except that all lexical identities are dropped from the templates during training and testing, hence inducing a'delexicalized'model that employs only'universal'properties from source - side treebanks, such as part - ofspeech tags, labels, head - modifier distance, etc."", 'we ran a number of experiments, which can be seen in']","['of the motivating factors in creating such a data set was improved cross - lingual transfer evaluation.', 'to test this, we use a cross - lingual transfer parser similar to that of  #TAUTHOR_TAG.', 'in particular, it is a perceptron - trained shift - reduce parser with a beam of size 8.', ""we use the features of  #AUTHOR_TAG, except that all lexical identities are dropped from the templates during training and testing, hence inducing a'delexicalized'model that employs only'universal'properties from source - side treebanks, such as part - ofspeech tags, labels, head - modifier distance, etc."", 'we ran a number of experiments, which can be seen in table 3.', 'for these experiments we randomly split each data set into training, development and testing sets.', '5 the one exception is english, where we used the standard splits.', '']",4
"[' #AUTHOR_TAG and parsers  #TAUTHOR_TAG.', 'the']","['shorter than three words.', 'the selected sentences were pre - processed using cross - lingual taggers  #AUTHOR_TAG and parsers  #TAUTHOR_TAG.', 'the annotators modified']","['- processed using cross - lingual taggers  #AUTHOR_TAG and parsers  #TAUTHOR_TAG.', 'the annotators modified the pre - parsed trees using the tred 2 tool.', 'at the beginning of the annotation process, double - blind annotation, followed by manual arbitration and consensus, was used iteratively for small batches of data until the guidelines were final']","['the remaining four languages, annotators were given three resources : 1 ) the english stanford guidelines ; 2 ) a set of english sentences with stanford dependencies and universal tags ( as above ) ; and 3 ) a large collection of unlabeled sentences randomly drawn from newswire, weblogs and / or consumer reviews, automatically tokenized with a rule - based system.', 'for german, french and spanish, contractions were split, except in the case of clitics.', 'for korean, tokenization was more coarse and included particles within token units.', 'annotators could correct this automatic tokenization.', 'the annotators were then tasked with producing language - specific annotation guidelines with the expressed goal of keeping the label and construction set as close as possible to the original english set, only adding labels for phenomena that do not exist in english.', 'making fine - grained label distinctions was discouraged.', 'once these guidelines were fixed, annotators selected roughly an equal amount of sentences to be annotated from each domain in the unlabeled data.', 'as the sentences were already randomly selected from a larger corpus, annotators were told to view the sentences in order and to discard a sentence only if it was 1 ) fragmented because of a sentence splitting error ; 2 ) not from the language of interest ; 3 ) incomprehensible to a native speaker ; or 4 ) shorter than three words.', 'the selected sentences were pre - processed using cross - lingual taggers  #AUTHOR_TAG and parsers  #TAUTHOR_TAG.', 'the annotators modified the pre - parsed trees using the tred 2 tool.', 'at the beginning of the annotation process, double - blind annotation, followed by manual arbitration and consensus, was used iteratively for small batches of data until the guidelines were finalized.', 'most of the data was annotated using single - annotation and full review : one annotator annotating the data and another reviewing it, making changes in close collaboration with the original annotator.', 'as a final step, all annotated data was semi - automatically checked for annotation consistency']",5
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG.', 'in particular, it is a perceptron - trained shift - reduce parser with a beam of size 8.', ""we use the features of  #AUTHOR_TAG, except that all lexical identities are dropped from the templates during training and testing, hence inducing a'delexicalized'model that employs only'universal'properties from source - side treebanks, such as part - ofspeech tags, labels, head - modifier distance, etc."", 'we ran a number of experiments, which can be seen in']","['of the motivating factors in creating such a data set was improved cross - lingual transfer evaluation.', 'to test this, we use a cross - lingual transfer parser similar to that of  #TAUTHOR_TAG.', 'in particular, it is a perceptron - trained shift - reduce parser with a beam of size 8.', ""we use the features of  #AUTHOR_TAG, except that all lexical identities are dropped from the templates during training and testing, hence inducing a'delexicalized'model that employs only'universal'properties from source - side treebanks, such as part - ofspeech tags, labels, head - modifier distance, etc."", 'we ran a number of experiments, which can be seen in table 3.', 'for these experiments we randomly split each data set into training, development and testing sets.', '5 the one exception is english, where we used the standard splits.', '']",6
['by  #TAUTHOR_TAG in using machine learning classifiers in'],['by  #TAUTHOR_TAG in using machine learning classifiers in'],['by  #TAUTHOR_TAG in using machine learning classifiers in'],[' #TAUTHOR_TAG'],0
['by  #TAUTHOR_TAG in using machine learning classifiers in'],['by  #TAUTHOR_TAG in using machine learning classifiers in'],['by  #TAUTHOR_TAG in using machine learning classifiers in'],[' #TAUTHOR_TAG'],0
['type hierarchy  #TAUTHOR_TAG we feel that the time is'],['type hierarchy  #TAUTHOR_TAG we feel that the time is'],[' #TAUTHOR_TAG we feel that the time is'],[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG uiuc ath is'],[' #TAUTHOR_TAG uiuc ath is'],"['.', 'in contrast, the  #TAUTHOR_TAG uiuc ath is designed especially for questions, but lacks the ability to extend the depth of the hierarchy']","['type hierarchies were first employed by  #AUTHOR_TAG using a heuristic classifier based on the wordnet  #AUTHOR_TAG ontology.', ' #AUTHOR_TAG explored the use of machine learning techniques to answer type detection and  #AUTHOR_TAG improved accuracy through the use of their informer span.', 'alternatively,  #AUTHOR_TAG use a probabilistic model with no pre - defined hierarchy in order to identify the type of information sought by a factoid question.', 'we know of no previous work which combines the ability to scale to large aths and provide the benefits of a machinelearning based system.', ""while the ath in  #AUTHOR_TAG could easily be scaled to include a potentially very large number of types ( e. g. see  #AUTHOR_TAG for an example of how this could be accomplished for a top - performing trec q / a system ), it is constrained in its aapproach to wordnet's hand - built hypernym relations, which does not coincide with common answer types in questions."", 'in contrast, the  #TAUTHOR_TAG uiuc ath is designed especially for questions, but lacks the ability to extend the depth of the hierarchy when q / a systems are capable of handling more detailed answer types.', '']",4
"['corpora  #TAUTHOR_TAG, ( 2 ) collections of']","['corpora  #TAUTHOR_TAG, ( 2 ) collections of']","[', 000 questions compiled from ( 1 ) existing annotated question corpora  #TAUTHOR_TAG, ( 2 ) collections of']","[""this section, we describe how we used the large ath introduced in section 3 in order to annotate a corpus drawn from more than 10, 000 questions compiled from ( 1 ) existing annotated question corpora  #TAUTHOR_TAG, ( 2 ) collections of questions mined from the web, and ( 3 ) questions submitted to lcc's ferret question - answering system  #AUTHOR_TAG a )."", '( a breakdown of the number of questions obtained from each of these three strategies is provided in table 3 table 3 : distribution of 10, 000 annotated questions by originating data set']",5
"['machine - learning based approaches  #TAUTHOR_TAG, we used a maximum entropy classifier to']","['machine - learning based approaches  #TAUTHOR_TAG, we used a maximum entropy classifier to']","['', 'in a departure from previous machine - learning based approaches  #TAUTHOR_TAG, we used a maximum entropy classifier to']","['', '3 classifiers are machine - learning based while the remainder are heuristic classifiers.', 'in a departure from previous machine - learning based approaches  #TAUTHOR_TAG, we used a maximum entropy classifier to learn our ath.', 'our classification process currently uses three machinelearned classifiers.', 'the first resolves all questions into one of 11 "" coarse "" types that are similar to the uiuc coarse types in table 1. if the first classifier\'s outcome is hu - man, then a machine classifier resolves between individ - ual, human - group, organization, and human ( not enough information ).', ""if the first classifier's outcome is lo - cation, then a machine classifier is used to resolve between physical - location, gpe, facility""]",5
"['corpora  #TAUTHOR_TAG, ( 2 ) collections of']","['corpora  #TAUTHOR_TAG, ( 2 ) collections of']","[', 000 questions compiled from ( 1 ) existing annotated question corpora  #TAUTHOR_TAG, ( 2 ) collections of']","[""this section, we describe how we used the large ath introduced in section 3 in order to annotate a corpus drawn from more than 10, 000 questions compiled from ( 1 ) existing annotated question corpora  #TAUTHOR_TAG, ( 2 ) collections of questions mined from the web, and ( 3 ) questions submitted to lcc's ferret question - answering system  #AUTHOR_TAG a )."", '( a breakdown of the number of questions obtained from each of these three strategies is provided in table 3 table 3 : distribution of 10, 000 annotated questions by originating data set']",3
"['machine - learning based approaches  #TAUTHOR_TAG, we used a maximum entropy classifier to']","['machine - learning based approaches  #TAUTHOR_TAG, we used a maximum entropy classifier to']","['', 'in a departure from previous machine - learning based approaches  #TAUTHOR_TAG, we used a maximum entropy classifier to']","['', '3 classifiers are machine - learning based while the remainder are heuristic classifiers.', 'in a departure from previous machine - learning based approaches  #TAUTHOR_TAG, we used a maximum entropy classifier to learn our ath.', 'our classification process currently uses three machinelearned classifiers.', 'the first resolves all questions into one of 11 "" coarse "" types that are similar to the uiuc coarse types in table 1. if the first classifier\'s outcome is hu - man, then a machine classifier resolves between individ - ual, human - group, organization, and human ( not enough information ).', ""if the first classifier's outcome is lo - cation, then a machine classifier is used to resolve between physical - location, gpe, facility""]",3
"[' #TAUTHOR_TAG, we have demonstrated how a large, multi - tier']","[' #TAUTHOR_TAG, we have demonstrated how a large, multi - tiered']","['in answer type detection  #TAUTHOR_TAG, we have demonstrated how a large, multi - tiered answer type hierarchy can be created which incorporates many of the entity types included in lc']","['paper described the creation of a new answer type detection system capable of recognizing more than 200 different expected answer types with greater 85 % precision.', ""in a departure from previous work in answer type detection  #TAUTHOR_TAG, we have demonstrated how a large, multi - tiered answer type hierarchy can be created which incorporates many of the entity types included in lcc's wide coverage named entity recognition system, cicerolite ; this hierarchy was then used in order to create a new corpus of more than 10, 000 questions which could be used to train an atd system."", 'we showed that the hierarchy we have developed can enhance a stateof - the - art question - answering system  #AUTHOR_TAG a ) by more than 7 % overall']",3
"['incorporating selectional preferences  #TAUTHOR_TAG.', 'few have attempted to use, or approximate, diathesis features directly for verb']","['incorporating selectional preferences  #TAUTHOR_TAG.', 'few have attempted to use, or approximate, diathesis features directly for verb']","[' #AUTHOR_TAG.', 'there has also been some success incorporating selectional preferences  #TAUTHOR_TAG.', 'few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related']","['', 'there has also been some success incorporating selectional preferences  #TAUTHOR_TAG.', 'few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the das themselves automatically using scf and semantic information  #AUTHOR_TAG mc  #AUTHOR_TAG mc  #AUTHOR_TAG.', 'exceptions to this include  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG, 2011 ).', ' #AUTHOR_TAG used cues such as passive voice, animacy and syntactic frames coupled with the overlap of lexical fillers between the alternating slots to predict a 3 - way classification ( unergative, unaccusative and object - drop ).', ' #AUTHOR_TAG used similar features to classify verbs on a much larger scale.', 'they classify up to 496 verbs using 11 different classifications each having between 2 and 14 classes.', ' #AUTHOR_TAG, 2011 ) used hierarchical bayesian models on slot frequency data obtained from childdirected speech parsed with a dependency parser to model acquisition of scf, alternations and ultimately verb classes which provided predictions for unseen syntactic behaviour of class members.', 'we are interested in the improvement that can be achieved to verb clustering using approximations for das, rather than the da per se.', 'as such we make the simple assumption that if a pair of scfs tends to occur with the same verbs, we']",0
"['used the spectral clustering', '( spec ) method and settings as in  #TAUTHOR_TAG but adopted the bhattach']","['the 17 classes set in  #AUTHOR_TAG. we used the spectral clustering', '( spec ) method and settings as in  #TAUTHOR_TAG but adopted the bhattach']","['used the spectral clustering', '( spec ) method and settings as in  #TAUTHOR_TAG but adopted the bhattachary']","['in f1. we extracted the scfs using the system of  #AUTHOR_TAG which classifies each corpus occurrence of', 'a verb as a member of one', 'of the 168 scfs on the basis of grammatical relations identified by the rasp  #AUTHOR_TAG parser. we experiment', '##ed with two datasets that have been used in prior work on verb clustering : the test sets 7', '- 11 ( 3 - 14 classes ) in  #AUTHOR_TAG, and the 17 classes set in  #AUTHOR_TAG. we used the spectral clustering', '( spec ) method and settings as in  #TAUTHOR_TAG but adopted the bhattacharyya kernel  #AUTHOR_TAG to improve', 'the computational efficiency of the approach given the high dimensionality of the quadratic feature space', '. the mean - filed bound of the bhattacharyya kernel is very similar to the kl divergence kernel  #AUTHOR_TAG which is frequently used in verb clustering experiments  #TAUTHOR_TAG. to further reduce computational complexity,', ""we restricted our scope to the more frequent features. in the experiment described in this section we used the 50 most frequent features for the 3 - 6 way classifications ( joanis et al.'s test set 7 -"", '9 ) and 100 features for the 7 - 17 way classifications. in the next section, we will demonstrate that f3 outperforms f1 regardless of the feature number setting. the features are normalized to sum', '1. the clustering results are evaluated using fmeasure as in  #TAUTHOR_TAG which provides the harmonic mean of precision ( p ) and recall ( r ) p is calculated using modified purity - a global measure which evaluates', 'the mean precision of clusters. each cluster ( k i ∈ k ) is associated with the gold - standard class to which the majority of its members belong. the number', 'of verbs in a cluster ( k i ) that take this class is denoted by', 'n prevalent ( k i ). | verbs | r is calculated using weighted class accuracy : the proportion of members of the dominant cluster dom - clust i within each of the gold - standard classes c i ∈ c. the results are', 'shown in table 3. the result of f2 is lower than that of f3, and even lower', 'than that of f1 for 3 -', '6 way classification. this indicates that the frame independence assumption is a poor assumption. f3 yields substantially better result than f2 and f1. the result of f3 is 6. 4 % higher than the result ( f =', '63. 28 ) reported in  #TAUTHOR_TAG using the f1 feature. this experiment shows, on two datasets, that da features are', 'clearly more effective than the frame features for verb clustering, even when relaxations are used']",0
"['used the spectral clustering', '( spec ) method and settings as in  #TAUTHOR_TAG but adopted the bhattach']","['the 17 classes set in  #AUTHOR_TAG. we used the spectral clustering', '( spec ) method and settings as in  #TAUTHOR_TAG but adopted the bhattach']","['used the spectral clustering', '( spec ) method and settings as in  #TAUTHOR_TAG but adopted the bhattachary']","['in f1. we extracted the scfs using the system of  #AUTHOR_TAG which classifies each corpus occurrence of', 'a verb as a member of one', 'of the 168 scfs on the basis of grammatical relations identified by the rasp  #AUTHOR_TAG parser. we experiment', '##ed with two datasets that have been used in prior work on verb clustering : the test sets 7', '- 11 ( 3 - 14 classes ) in  #AUTHOR_TAG, and the 17 classes set in  #AUTHOR_TAG. we used the spectral clustering', '( spec ) method and settings as in  #TAUTHOR_TAG but adopted the bhattacharyya kernel  #AUTHOR_TAG to improve', 'the computational efficiency of the approach given the high dimensionality of the quadratic feature space', '. the mean - filed bound of the bhattacharyya kernel is very similar to the kl divergence kernel  #AUTHOR_TAG which is frequently used in verb clustering experiments  #TAUTHOR_TAG. to further reduce computational complexity,', ""we restricted our scope to the more frequent features. in the experiment described in this section we used the 50 most frequent features for the 3 - 6 way classifications ( joanis et al.'s test set 7 -"", '9 ) and 100 features for the 7 - 17 way classifications. in the next section, we will demonstrate that f3 outperforms f1 regardless of the feature number setting. the features are normalized to sum', '1. the clustering results are evaluated using fmeasure as in  #TAUTHOR_TAG which provides the harmonic mean of precision ( p ) and recall ( r ) p is calculated using modified purity - a global measure which evaluates', 'the mean precision of clusters. each cluster ( k i ∈ k ) is associated with the gold - standard class to which the majority of its members belong. the number', 'of verbs in a cluster ( k i ) that take this class is denoted by', 'n prevalent ( k i ). | verbs | r is calculated using weighted class accuracy : the proportion of members of the dominant cluster dom - clust i within each of the gold - standard classes c i ∈ c. the results are', 'shown in table 3. the result of f2 is lower than that of f3, and even lower', 'than that of f1 for 3 -', '6 way classification. this indicates that the frame independence assumption is a poor assumption. f3 yields substantially better result than f2 and f1. the result of f3 is 6. 4 % higher than the result ( f =', '63. 28 ) reported in  #TAUTHOR_TAG using the f1 feature. this experiment shows, on two datasets, that da features are', 'clearly more effective than the frame features for verb clustering, even when relaxations are used']",5
"['used the spectral clustering', '( spec ) method and settings as in  #TAUTHOR_TAG but adopted the bhattach']","['the 17 classes set in  #AUTHOR_TAG. we used the spectral clustering', '( spec ) method and settings as in  #TAUTHOR_TAG but adopted the bhattach']","['used the spectral clustering', '( spec ) method and settings as in  #TAUTHOR_TAG but adopted the bhattachary']","['in f1. we extracted the scfs using the system of  #AUTHOR_TAG which classifies each corpus occurrence of', 'a verb as a member of one', 'of the 168 scfs on the basis of grammatical relations identified by the rasp  #AUTHOR_TAG parser. we experiment', '##ed with two datasets that have been used in prior work on verb clustering : the test sets 7', '- 11 ( 3 - 14 classes ) in  #AUTHOR_TAG, and the 17 classes set in  #AUTHOR_TAG. we used the spectral clustering', '( spec ) method and settings as in  #TAUTHOR_TAG but adopted the bhattacharyya kernel  #AUTHOR_TAG to improve', 'the computational efficiency of the approach given the high dimensionality of the quadratic feature space', '. the mean - filed bound of the bhattacharyya kernel is very similar to the kl divergence kernel  #AUTHOR_TAG which is frequently used in verb clustering experiments  #TAUTHOR_TAG. to further reduce computational complexity,', ""we restricted our scope to the more frequent features. in the experiment described in this section we used the 50 most frequent features for the 3 - 6 way classifications ( joanis et al.'s test set 7 -"", '9 ) and 100 features for the 7 - 17 way classifications. in the next section, we will demonstrate that f3 outperforms f1 regardless of the feature number setting. the features are normalized to sum', '1. the clustering results are evaluated using fmeasure as in  #TAUTHOR_TAG which provides the harmonic mean of precision ( p ) and recall ( r ) p is calculated using modified purity - a global measure which evaluates', 'the mean precision of clusters. each cluster ( k i ∈ k ) is associated with the gold - standard class to which the majority of its members belong. the number', 'of verbs in a cluster ( k i ) that take this class is denoted by', 'n prevalent ( k i ). | verbs | r is calculated using weighted class accuracy : the proportion of members of the dominant cluster dom - clust i within each of the gold - standard classes c i ∈ c. the results are', 'shown in table 3. the result of f2 is lower than that of f3, and even lower', 'than that of f1 for 3 -', '6 way classification. this indicates that the frame independence assumption is a poor assumption. f3 yields substantially better result than f2 and f1. the result of f3 is 6. 4 % higher than the result ( f =', '63. 28 ) reported in  #TAUTHOR_TAG using the f1 feature. this experiment shows, on two datasets, that da features are', 'clearly more effective than the frame features for verb clustering, even when relaxations are used']",5
"['will use selectional preferences to generalize the argument heads but will do so using preferences from distributional data  #TAUTHOR_TAG rather than wordnet, and use all argument head data in all frames.', 'we envisage using maximum average distributional similarity of the']","['will use selectional preferences to generalize the argument heads but will do so using preferences from distributional data  #TAUTHOR_TAG rather than wordnet, and use all argument head data in all frames.', 'we envisage using maximum average distributional similarity of the']","['we will use selectional preferences to generalize the argument heads but will do so using preferences from distributional data  #TAUTHOR_TAG rather than wordnet, and use all argument head data in all frames.', 'we envisage using maximum average distributional similarity of the argument heads in any potentially alternating slots in a pair of cooccurring frames as a feature, just as we currently use the frequency of the less frequent co - occurring frame']","['have demonstrated the merits of using das for verb clustering compared to the scf data from which they are derived on standard verb classification datasets and when integrated in a stateof - the - art verb clustering system.', 'we have also demonstrated that the performance of frame features is dominated by the high frequency frames.', 'in contrast, the da features enable the mid - range frequency frames to further improve the performance.', 'in the future, we plan to evaluate the performance of da features in a larger scale experiment.', 'due to the high dimensionality of the transformed feature space ( quadratic of the original feature space ), we will need to improve the computational efficiency further, e. g. via use of an unsupervised dimensionality reduction technique  #AUTHOR_TAG.', 'moreover, we plan to use bayesian inference as in  #AUTHOR_TAG ;  #AUTHOR_TAG, 2011 ) to infer the actual parameter values and avoid the relaxation.', 'finally, we plan to supplement the da feature with evidence from the slot fillers of the alternating slots, in the spirit of earlier work ( mc  #AUTHOR_TAG.', 'unlike these previous works, we will use selectional preferences to generalize the argument heads but will do so using preferences from distributional data  #TAUTHOR_TAG rather than wordnet, and use all argument head data in all frames.', 'we envisage using maximum average distributional similarity of the argument heads in any potentially alternating slots in a pair of cooccurring frames as a feature, just as we currently use the frequency of the less frequent co - occurring frame']",5
"['used the spectral clustering', '( spec ) method and settings as in  #TAUTHOR_TAG but adopted the bhattach']","['the 17 classes set in  #AUTHOR_TAG. we used the spectral clustering', '( spec ) method and settings as in  #TAUTHOR_TAG but adopted the bhattach']","['used the spectral clustering', '( spec ) method and settings as in  #TAUTHOR_TAG but adopted the bhattachary']","['in f1. we extracted the scfs using the system of  #AUTHOR_TAG which classifies each corpus occurrence of', 'a verb as a member of one', 'of the 168 scfs on the basis of grammatical relations identified by the rasp  #AUTHOR_TAG parser. we experiment', '##ed with two datasets that have been used in prior work on verb clustering : the test sets 7', '- 11 ( 3 - 14 classes ) in  #AUTHOR_TAG, and the 17 classes set in  #AUTHOR_TAG. we used the spectral clustering', '( spec ) method and settings as in  #TAUTHOR_TAG but adopted the bhattacharyya kernel  #AUTHOR_TAG to improve', 'the computational efficiency of the approach given the high dimensionality of the quadratic feature space', '. the mean - filed bound of the bhattacharyya kernel is very similar to the kl divergence kernel  #AUTHOR_TAG which is frequently used in verb clustering experiments  #TAUTHOR_TAG. to further reduce computational complexity,', ""we restricted our scope to the more frequent features. in the experiment described in this section we used the 50 most frequent features for the 3 - 6 way classifications ( joanis et al.'s test set 7 -"", '9 ) and 100 features for the 7 - 17 way classifications. in the next section, we will demonstrate that f3 outperforms f1 regardless of the feature number setting. the features are normalized to sum', '1. the clustering results are evaluated using fmeasure as in  #TAUTHOR_TAG which provides the harmonic mean of precision ( p ) and recall ( r ) p is calculated using modified purity - a global measure which evaluates', 'the mean precision of clusters. each cluster ( k i ∈ k ) is associated with the gold - standard class to which the majority of its members belong. the number', 'of verbs in a cluster ( k i ) that take this class is denoted by', 'n prevalent ( k i ). | verbs | r is calculated using weighted class accuracy : the proportion of members of the dominant cluster dom - clust i within each of the gold - standard classes c i ∈ c. the results are', 'shown in table 3. the result of f2 is lower than that of f3, and even lower', 'than that of f1 for 3 -', '6 way classification. this indicates that the frame independence assumption is a poor assumption. f3 yields substantially better result than f2 and f1. the result of f3 is 6. 4 % higher than the result ( f =', '63. 28 ) reported in  #TAUTHOR_TAG using the f1 feature. this experiment shows, on two datasets, that da features are', 'clearly more effective than the frame features for verb clustering, even when relaxations are used']",4
['- qa dataset  #TAUTHOR_TAG for vqa was created by parsing coc'],"['properly without augmentation.', 'although training using augmented text data is rare, generating new questions about images has been studied.', 'the coco - qa dataset  #TAUTHOR_TAG for vqa was created by parsing coco captions with a syntactic parser,']","['because large quantities of real data are available, models generalize properly without augmentation.', 'although training using augmented text data is rare, generating new questions about images has been studied.', 'the coco - qa dataset  #TAUTHOR_TAG for vqa was created by parsing coco captions with a syntactic parser,']","['supervised computer vision problems, e. g., image recognition, labels are scarcer than images.', 'this is especially a problem with deep convolutional neural networks ( cnns ) that have millions of parameters.', 'although more human labeled data would be ideal, it is easier to exploit the training dataset to generate new examples.', 'for image classification, common ways to exploit training images to create more labeled examples include mirror reflection, random crops etc.', 'many of these methods were used in training the seminal alexnet  #AUTHOR_TAG, which increased the training data by more than ten folds and produced relative improvement of over 4 % for image classification.', 'compared to vision, where augmentation is common, little work has been done on augmenting text for classification problems.', 'a notable exception is  #AUTHOR_TAG, where a thesaurus was used to replace synonymous words to create more training data for text classification.', 'however, this augmentation produced little improvement and sometimes even hurt performance.', ""the authors'argued that because large quantities of real data are available, models generalize properly without augmentation."", 'although training using augmented text data is rare, generating new questions about images has been studied.', 'the coco - qa dataset  #TAUTHOR_TAG for vqa was created by parsing coco captions with a syntactic parser, and then used this to create qa pairs for four kinds of questions using hand - crafted rules.', 'however, due to inability of the algorithm to cope with complex sentence structures, a significant portion of coco - qa questions have grammatical errors or are oddly phrased.', 'visual question generation was also studied in  #AUTHOR_TAG, with an emphasis on generating questions about images that are beyond the literal visual content of the image.', 'they endeavored to avoid simple questions such as counting and color, which were emphasized in coco - qa.', 'unlike our work, their objective was not data augmentation and they did not try to answer the generated questions']",0
"['- qa  #TAUTHOR_TAG.', ""'""]","[""dataset' #AUTHOR_TAG and coco - qa  #TAUTHOR_TAG."", ""' the vqa dataset'is currently the""]","['- qa  #TAUTHOR_TAG.', ""'""]","[""conduct experiments on two of the most popular vqa datasets :'the vqa dataset' #AUTHOR_TAG and coco - qa  #TAUTHOR_TAG."", ""' the vqa dataset'is currently the most popular vqa dataset and it contains both synthetic and real - world images."", 'the real - world images are from the coco dataset  #AUTHOR_TAG.', 'all questions were generated by human annotators.', 'we refer to this portion as coco - vqa, and use it for our experiments.', ""coco - qa  #TAUTHOR_TAG also uses images from coco, with the questions generated by an nlp algorithm that uses coco's captions."", 'all questions belong to four categories : object, number, color, and location.', 'many algorithms have been proposed for vqa.', 'some notable formulations include attention based methods  #AUTHOR_TAG, bayesian frameworks  #AUTHOR_TAG, and compositional approaches  #AUTHOR_TAG a, b ).', 'detailed reviews of existing methods can be found in  #AUTHOR_TAG and  #AUTHOR_TAG.', 'however, simpler models such as linear classifiers and multilayer perceptrons ( mlps ) perform only slightly worse on many vqa datasets.', 'these baseline methods predict the answer using a vector of image features concatenated to a vector of question features  #TAUTHOR_TAG.', 'we use the mlp model to conduct the bulk of the experiments, but we show that the proposed method is also effective on more sophisticated vqa systems like multimodal compact bilinear pooling ( mcb )  #AUTHOR_TAG']",0
"['- qa  #TAUTHOR_TAG.', ""'""]","[""dataset' #AUTHOR_TAG and coco - qa  #TAUTHOR_TAG."", ""' the vqa dataset'is currently the""]","['- qa  #TAUTHOR_TAG.', ""'""]","[""conduct experiments on two of the most popular vqa datasets :'the vqa dataset' #AUTHOR_TAG and coco - qa  #TAUTHOR_TAG."", ""' the vqa dataset'is currently the most popular vqa dataset and it contains both synthetic and real - world images."", 'the real - world images are from the coco dataset  #AUTHOR_TAG.', 'all questions were generated by human annotators.', 'we refer to this portion as coco - vqa, and use it for our experiments.', ""coco - qa  #TAUTHOR_TAG also uses images from coco, with the questions generated by an nlp algorithm that uses coco's captions."", 'all questions belong to four categories : object, number, color, and location.', 'many algorithms have been proposed for vqa.', 'some notable formulations include attention based methods  #AUTHOR_TAG, bayesian frameworks  #AUTHOR_TAG, and compositional approaches  #AUTHOR_TAG a, b ).', 'detailed reviews of existing methods can be found in  #AUTHOR_TAG and  #AUTHOR_TAG.', 'however, simpler models such as linear classifiers and multilayer perceptrons ( mlps ) perform only slightly worse on many vqa datasets.', 'these baseline methods predict the answer using a vector of image features concatenated to a vector of question features  #TAUTHOR_TAG.', 'we use the mlp model to conduct the bulk of the experiments, but we show that the proposed method is also effective on more sophisticated vqa systems like multimodal compact bilinear pooling ( mcb )  #AUTHOR_TAG']",0
['- qa dataset  #TAUTHOR_TAG for vqa was created by parsing coc'],"['properly without augmentation.', 'although training using augmented text data is rare, generating new questions about images has been studied.', 'the coco - qa dataset  #TAUTHOR_TAG for vqa was created by parsing coco captions with a syntactic parser,']","['because large quantities of real data are available, models generalize properly without augmentation.', 'although training using augmented text data is rare, generating new questions about images has been studied.', 'the coco - qa dataset  #TAUTHOR_TAG for vqa was created by parsing coco captions with a syntactic parser,']","['supervised computer vision problems, e. g., image recognition, labels are scarcer than images.', 'this is especially a problem with deep convolutional neural networks ( cnns ) that have millions of parameters.', 'although more human labeled data would be ideal, it is easier to exploit the training dataset to generate new examples.', 'for image classification, common ways to exploit training images to create more labeled examples include mirror reflection, random crops etc.', 'many of these methods were used in training the seminal alexnet  #AUTHOR_TAG, which increased the training data by more than ten folds and produced relative improvement of over 4 % for image classification.', 'compared to vision, where augmentation is common, little work has been done on augmenting text for classification problems.', 'a notable exception is  #AUTHOR_TAG, where a thesaurus was used to replace synonymous words to create more training data for text classification.', 'however, this augmentation produced little improvement and sometimes even hurt performance.', ""the authors'argued that because large quantities of real data are available, models generalize properly without augmentation."", 'although training using augmented text data is rare, generating new questions about images has been studied.', 'the coco - qa dataset  #TAUTHOR_TAG for vqa was created by parsing coco captions with a syntactic parser, and then used this to create qa pairs for four kinds of questions using hand - crafted rules.', 'however, due to inability of the algorithm to cope with complex sentence structures, a significant portion of coco - qa questions have grammatical errors or are oddly phrased.', 'visual question generation was also studied in  #AUTHOR_TAG, with an emphasis on generating questions about images that are beyond the literal visual content of the image.', 'they endeavored to avoid simple questions such as counting and color, which were emphasized in coco - qa.', 'unlike our work, their objective was not data augmentation and they did not try to answer the generated questions']",1
"['- qa  #TAUTHOR_TAG.', ""'""]","[""dataset' #AUTHOR_TAG and coco - qa  #TAUTHOR_TAG."", ""' the vqa dataset'is currently the""]","['- qa  #TAUTHOR_TAG.', ""'""]","[""conduct experiments on two of the most popular vqa datasets :'the vqa dataset' #AUTHOR_TAG and coco - qa  #TAUTHOR_TAG."", ""' the vqa dataset'is currently the most popular vqa dataset and it contains both synthetic and real - world images."", 'the real - world images are from the coco dataset  #AUTHOR_TAG.', 'all questions were generated by human annotators.', 'we refer to this portion as coco - vqa, and use it for our experiments.', ""coco - qa  #TAUTHOR_TAG also uses images from coco, with the questions generated by an nlp algorithm that uses coco's captions."", 'all questions belong to four categories : object, number, color, and location.', 'many algorithms have been proposed for vqa.', 'some notable formulations include attention based methods  #AUTHOR_TAG, bayesian frameworks  #AUTHOR_TAG, and compositional approaches  #AUTHOR_TAG a, b ).', 'detailed reviews of existing methods can be found in  #AUTHOR_TAG and  #AUTHOR_TAG.', 'however, simpler models such as linear classifiers and multilayer perceptrons ( mlps ) perform only slightly worse on many vqa datasets.', 'these baseline methods predict the answer using a vector of image features concatenated to a vector of question features  #TAUTHOR_TAG.', 'we use the mlp model to conduct the bulk of the experiments, but we show that the proposed method is also effective on more sophisticated vqa systems like multimodal compact bilinear pooling ( mcb )  #AUTHOR_TAG']",5
"['study  #TAUTHOR_TAG, in which a regression']","['study  #TAUTHOR_TAG, in which a regression']","[' #TAUTHOR_TAG, in which a regression model']",[' #TAUTHOR_TAG'],4
"['study  #TAUTHOR_TAG, in which a regression']","['study  #TAUTHOR_TAG, in which a regression']","[' #TAUTHOR_TAG, in which a regression model']",[' #TAUTHOR_TAG'],4
"['this paper, the 10 - k corpus  #TAUTHOR_TAG is used to']","['this paper, the 10 - k corpus  #TAUTHOR_TAG is used to']","['this paper, the 10 - k corpus  #TAUTHOR_TAG is used to']",[' #TAUTHOR_TAG'],4
"['study  #TAUTHOR_TAG, in which a regression']","['study  #TAUTHOR_TAG, in which a regression']","[' #TAUTHOR_TAG, in which a regression model']",[' #TAUTHOR_TAG'],0
"[' #TAUTHOR_TAG.', ' #AUTHOR_TAG use a weighting scheme']","[' #TAUTHOR_TAG.', ' #AUTHOR_TAG use a weighting scheme']","[' #TAUTHOR_TAG.', ' #AUTHOR_TAG use a weighting scheme']","['', 'considering the prevalence of learning - to - rank techniques, this paper attempts to use such techniques to deal with the ranking problem of financial risk.', 'in recent year, there have been some studies conducted on mining financial reports, such as  #TAUTHOR_TAG.', ' #AUTHOR_TAG use a weighting scheme to combine both qualitative and quantitative features of financial reports together, and propose a method to predict short - term stock price movements.', 'in the work, a hierarchical agglomerative clustering ( hac ) method with k - means updating is employed to improve the purity of the prototypes of financial reports, and then the generated prototypes are used to predict stock price movements.', '']",0
['bell shape  #TAUTHOR_TAG'],['bell shape  #TAUTHOR_TAG'],"['a bell shape  #TAUTHOR_TAG.', 'therefore, given']",[' #TAUTHOR_TAG'],0
"['this paper, the 10 - k corpus  #TAUTHOR_TAG is used to']","['this paper, the 10 - k corpus  #TAUTHOR_TAG is used to']","['this paper, the 10 - k corpus  #TAUTHOR_TAG is used to']",[' #TAUTHOR_TAG'],3
"['this paper, the 10 - k corpus  #TAUTHOR_TAG is used to']","['this paper, the 10 - k corpus  #TAUTHOR_TAG is used to']","['this paper, the 10 - k corpus  #TAUTHOR_TAG is used to']",[' #TAUTHOR_TAG'],3
"[') of different types include  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG', ';  #AUTHOR_TAG.  #TAUTHOR_TAG propose an approach that refrains from using any human annotation']","['multi - word expressions ( mwes ) of different types include  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG', ';  #AUTHOR_TAG.  #TAUTHOR_TAG propose an approach that refrains from using any human annotation']","['of multi - word expressions ( mwes ) of different types include  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG', ';  #AUTHOR_TAG.  #TAUTHOR_TAG propose an approach that refrains from using any human annotation']","['data is a drawback of supervised approaches. manual annotations are costly and time - consuming. to circumvent this need for annotated data, previous work has used cross - lingual supervision based on parallel corpora.  #AUTHOR_TAG made use of small', 'amounts of annotated data on the target side and complement this with bilingual features from unlabeled bitext in a co - trained classifier for coordination disambiguation in complex nps. previous work on using cross - lingual data for the analysis of multi - word expressions ( mwes ) of different types include  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG', ';  #AUTHOR_TAG.  #TAUTHOR_TAG propose an approach that refrains from using any human annotation. they use the fact, that languages differ in their preference for open or', 'closed compounding ( i. e., multiword vs. one -', 'word compounds ), for inducing the english bracketing of 3ncs. english open 3ncs like human rights abuses can be translated to partially closed phrases as in german verletzungen der', 'menschenrechte, ( abuses of human rights ), from', 'which we can induce the left - branching structure. although this approach achieves a solid accuracy, a crucial limitation is coverage, because restricting to six paraphrasing patterns ignores many other predictive cases. moreover, the system needs part of speech ( pos ) tags and splitting information for determining 2', '##ncs and is therefore rather language - dependent. in this paper, we present a precise, high - coverage and knowledge', '- lean method for bracketing kncs ( for k ≥ 3 ) occurring in parallel data. our method uses the distances of words', 'that are aligned to knc components in parallel languages. for example, the 3nc human rights violations can be bracketed using the positions', 'of aligned words in the italian fragment... che le violazioni gravi e sistematiche dei diritti', 'umani.... the fact, that the', 'alignment of the third noun, violations ( violazioni )', ', is separated from the rest, points us', 'in the direction of left - branching. using less restricted forms of cross - lingual supervision, we achieve a', 'much higher coverage than  #TAUTHOR_TAG. furthermore, our results are more accurate. in contrast to previous unsupervised methods, our system is applicable in both token - and type - based modes. token -', 'based bracketing is context - dependent and allows for a better treatment of structural ambiguity ( as in luxury cattle truck ). we generate large amounts of high - quality bracketed', 'kncs in a multilingual context that can be used to train', 'supervised learners']",0
"['in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48']","['% in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48']","['in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48. 1']","['', '. we refer to the harmonic mean of acc ω and coverage as harmonic ( ω ). as it turned out that', 'the adjacency model outperforms the dependency model, we only report results for', 'the first. table 1 presents the coverage of each system, based on the full dataset. our first result is that type -', 'based cross - lingual bracketing outperforms token - based and achieves up to 91. 2 % in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48. 1 %. the χ 2 method and', 'the back - off models cover all 3ncs in our dataset. the fact that awdb type misses 8. 8 % of the dataset is mainly due to', 'equal distances between aligned words ( e. g., crisis resolution mechanism is only aligned to closed compounds, such as the swedish krislos', '##ningsmekanism or to nouns separated by one preposition, such as the spanish mecanismo de resolucion de crisis )', '. in future work, we will add more languages in the hope to', 'find more variation and thus get an even higher coverage']",0
"['outperforms  #TAUTHOR_TAG significantly 7.', 'this']","['alignment.', 'awdb outperforms  #TAUTHOR_TAG significantly 7.', 'this']","['outperforms  #TAUTHOR_TAG significantly 7.', 'this']","['table 2 : direct comparison on common test sets ; † = significantly better than the systems in comparison table 2 directly compares the systems on common subsets ( com ), i. e., on 3ncs for which all systems in the set provide a result.', 'the main reason why cross - lingual systems make bracketing errors is the quality of automatic word alignment.', 'awdb outperforms  #TAUTHOR_TAG significantly 7.', 'this can be explained with the flexible structure of awdb, which can exploit more data and is thus more robust to word alignment errors.', 'awdb significantly outperforms χ 2 in accuracy but is inferior in harmonic ( com ).', '']",0
"[') of different types include  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG', ';  #AUTHOR_TAG.  #TAUTHOR_TAG propose an approach that refrains from using any human annotation']","['multi - word expressions ( mwes ) of different types include  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG', ';  #AUTHOR_TAG.  #TAUTHOR_TAG propose an approach that refrains from using any human annotation']","['of multi - word expressions ( mwes ) of different types include  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG', ';  #AUTHOR_TAG.  #TAUTHOR_TAG propose an approach that refrains from using any human annotation']","['data is a drawback of supervised approaches. manual annotations are costly and time - consuming. to circumvent this need for annotated data, previous work has used cross - lingual supervision based on parallel corpora.  #AUTHOR_TAG made use of small', 'amounts of annotated data on the target side and complement this with bilingual features from unlabeled bitext in a co - trained classifier for coordination disambiguation in complex nps. previous work on using cross - lingual data for the analysis of multi - word expressions ( mwes ) of different types include  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG', ';  #AUTHOR_TAG.  #TAUTHOR_TAG propose an approach that refrains from using any human annotation. they use the fact, that languages differ in their preference for open or', 'closed compounding ( i. e., multiword vs. one -', 'word compounds ), for inducing the english bracketing of 3ncs. english open 3ncs like human rights abuses can be translated to partially closed phrases as in german verletzungen der', 'menschenrechte, ( abuses of human rights ), from', 'which we can induce the left - branching structure. although this approach achieves a solid accuracy, a crucial limitation is coverage, because restricting to six paraphrasing patterns ignores many other predictive cases. moreover, the system needs part of speech ( pos ) tags and splitting information for determining 2', '##ncs and is therefore rather language - dependent. in this paper, we present a precise, high - coverage and knowledge', '- lean method for bracketing kncs ( for k ≥ 3 ) occurring in parallel data. our method uses the distances of words', 'that are aligned to knc components in parallel languages. for example, the 3nc human rights violations can be bracketed using the positions', 'of aligned words in the italian fragment... che le violazioni gravi e sistematiche dei diritti', 'umani.... the fact, that the', 'alignment of the third noun, violations ( violazioni )', ', is separated from the rest, points us', 'in the direction of left - branching. using less restricted forms of cross - lingual supervision, we achieve a', 'much higher coverage than  #TAUTHOR_TAG. furthermore, our results are more accurate. in contrast to previous unsupervised methods, our system is applicable in both token - and type - based modes. token -', 'based bracketing is context - dependent and allows for a better treatment of structural ambiguity ( as in luxury cattle truck ). we generate large amounts of high - quality bracketed', 'kncs in a multilingual context that can be used to train', 'supervised learners']",1
"['in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48']","['% in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48']","['in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48. 1']","['', '. we refer to the harmonic mean of acc ω and coverage as harmonic ( ω ). as it turned out that', 'the adjacency model outperforms the dependency model, we only report results for', 'the first. table 1 presents the coverage of each system, based on the full dataset. our first result is that type -', 'based cross - lingual bracketing outperforms token - based and achieves up to 91. 2 % in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48. 1 %. the χ 2 method and', 'the back - off models cover all 3ncs in our dataset. the fact that awdb type misses 8. 8 % of the dataset is mainly due to', 'equal distances between aligned words ( e. g., crisis resolution mechanism is only aligned to closed compounds, such as the swedish krislos', '##ningsmekanism or to nouns separated by one preposition, such as the spanish mecanismo de resolucion de crisis )', '. in future work, we will add more languages in the hope to', 'find more variation and thus get an even higher coverage']",1
"[') of different types include  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG', ';  #AUTHOR_TAG.  #TAUTHOR_TAG propose an approach that refrains from using any human annotation']","['multi - word expressions ( mwes ) of different types include  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG', ';  #AUTHOR_TAG.  #TAUTHOR_TAG propose an approach that refrains from using any human annotation']","['of multi - word expressions ( mwes ) of different types include  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG', ';  #AUTHOR_TAG.  #TAUTHOR_TAG propose an approach that refrains from using any human annotation']","['data is a drawback of supervised approaches. manual annotations are costly and time - consuming. to circumvent this need for annotated data, previous work has used cross - lingual supervision based on parallel corpora.  #AUTHOR_TAG made use of small', 'amounts of annotated data on the target side and complement this with bilingual features from unlabeled bitext in a co - trained classifier for coordination disambiguation in complex nps. previous work on using cross - lingual data for the analysis of multi - word expressions ( mwes ) of different types include  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #AUTHOR_TAG', ';  #AUTHOR_TAG.  #TAUTHOR_TAG propose an approach that refrains from using any human annotation. they use the fact, that languages differ in their preference for open or', 'closed compounding ( i. e., multiword vs. one -', 'word compounds ), for inducing the english bracketing of 3ncs. english open 3ncs like human rights abuses can be translated to partially closed phrases as in german verletzungen der', 'menschenrechte, ( abuses of human rights ), from', 'which we can induce the left - branching structure. although this approach achieves a solid accuracy, a crucial limitation is coverage, because restricting to six paraphrasing patterns ignores many other predictive cases. moreover, the system needs part of speech ( pos ) tags and splitting information for determining 2', '##ncs and is therefore rather language - dependent. in this paper, we present a precise, high - coverage and knowledge', '- lean method for bracketing kncs ( for k ≥ 3 ) occurring in parallel data. our method uses the distances of words', 'that are aligned to knc components in parallel languages. for example, the 3nc human rights violations can be bracketed using the positions', 'of aligned words in the italian fragment... che le violazioni gravi e sistematiche dei diritti', 'umani.... the fact, that the', 'alignment of the third noun, violations ( violazioni )', ', is separated from the rest, points us', 'in the direction of left - branching. using less restricted forms of cross - lingual supervision, we achieve a', 'much higher coverage than  #TAUTHOR_TAG. furthermore, our results are more accurate. in contrast to previous unsupervised methods, our system is applicable in both token - and type - based modes. token -', 'based bracketing is context - dependent and allows for a better treatment of structural ambiguity ( as in luxury cattle truck ). we generate large amounts of high - quality bracketed', 'kncs in a multilingual context that can be used to train', 'supervised learners']",4
"['in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48']","['% in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48']","['in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48. 1']","['', '. we refer to the harmonic mean of acc ω and coverage as harmonic ( ω ). as it turned out that', 'the adjacency model outperforms the dependency model, we only report results for', 'the first. table 1 presents the coverage of each system, based on the full dataset. our first result is that type -', 'based cross - lingual bracketing outperforms token - based and achieves up to 91. 2 % in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48. 1 %. the χ 2 method and', 'the back - off models cover all 3ncs in our dataset. the fact that awdb type misses 8. 8 % of the dataset is mainly due to', 'equal distances between aligned words ( e. g., crisis resolution mechanism is only aligned to closed compounds, such as the swedish krislos', '##ningsmekanism or to nouns separated by one preposition, such as the spanish mecanismo de resolucion de crisis )', '. in future work, we will add more languages in the hope to', 'find more variation and thus get an even higher coverage']",4
"['outperforms  #TAUTHOR_TAG significantly 7.', 'this']","['alignment.', 'awdb outperforms  #TAUTHOR_TAG significantly 7.', 'this']","['outperforms  #TAUTHOR_TAG significantly 7.', 'this']","['table 2 : direct comparison on common test sets ; † = significantly better than the systems in comparison table 2 directly compares the systems on common subsets ( com ), i. e., on 3ncs for which all systems in the set provide a result.', 'the main reason why cross - lingual systems make bracketing errors is the quality of automatic word alignment.', 'awdb outperforms  #TAUTHOR_TAG significantly 7.', 'this can be explained with the flexible structure of awdb, which can exploit more data and is thus more robust to word alignment errors.', 'awdb significantly outperforms χ 2 in accuracy but is inferior in harmonic ( com ).', '']",4
"['in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48']","['% in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48']","['in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48. 1']","['', '. we refer to the harmonic mean of acc ω and coverage as harmonic ( ω ). as it turned out that', 'the adjacency model outperforms the dependency model, we only report results for', 'the first. table 1 presents the coverage of each system, based on the full dataset. our first result is that type -', 'based cross - lingual bracketing outperforms token - based and achieves up to 91. 2 % in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48. 1 %. the χ 2 method and', 'the back - off models cover all 3ncs in our dataset. the fact that awdb type misses 8. 8 % of the dataset is mainly due to', 'equal distances between aligned words ( e. g., crisis resolution mechanism is only aligned to closed compounds, such as the swedish krislos', '##ningsmekanism or to nouns separated by one preposition, such as the spanish mecanismo de resolucion de crisis )', '. in future work, we will add more languages in the hope to', 'find more variation and thus get an even higher coverage']",5
"['in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48']","['% in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48']","['in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48. 1']","['', '. we refer to the harmonic mean of acc ω and coverage as harmonic ( ω ). as it turned out that', 'the adjacency model outperforms the dependency model, we only report results for', 'the first. table 1 presents the coverage of each system, based on the full dataset. our first result is that type -', 'based cross - lingual bracketing outperforms token - based and achieves up to 91. 2 % in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48. 1 %. the χ 2 method and', 'the back - off models cover all 3ncs in our dataset. the fact that awdb type misses 8. 8 % of the dataset is mainly due to', 'equal distances between aligned words ( e. g., crisis resolution mechanism is only aligned to closed compounds, such as the swedish krislos', '##ningsmekanism or to nouns separated by one preposition, such as the spanish mecanismo de resolucion de crisis )', '. in future work, we will add more languages in the hope to', 'find more variation and thus get an even higher coverage']",5
"['in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48']","['% in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48']","['in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48. 1']","['', '. we refer to the harmonic mean of acc ω and coverage as harmonic ( ω ). as it turned out that', 'the adjacency model outperforms the dependency model, we only report results for', 'the first. table 1 presents the coverage of each system, based on the full dataset. our first result is that type -', 'based cross - lingual bracketing outperforms token - based and achieves up to 91. 2 % in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48. 1 %. the χ 2 method and', 'the back - off models cover all 3ncs in our dataset. the fact that awdb type misses 8. 8 % of the dataset is mainly due to', 'equal distances between aligned words ( e. g., crisis resolution mechanism is only aligned to closed compounds, such as the swedish krislos', '##ningsmekanism or to nouns separated by one preposition, such as the spanish mecanismo de resolucion de crisis )', '. in future work, we will add more languages in the hope to', 'find more variation and thus get an even higher coverage']",5
"['in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48']","['% in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48']","['in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48. 1']","['', '. we refer to the harmonic mean of acc ω and coverage as harmonic ( ω ). as it turned out that', 'the adjacency model outperforms the dependency model, we only report results for', 'the first. table 1 presents the coverage of each system, based on the full dataset. our first result is that type -', 'based cross - lingual bracketing outperforms token - based and achieves up to 91. 2 % in coverage. as expected, the system of  #TAUTHOR_TAG', 'does not cover more than 48. 1 %. the χ 2 method and', 'the back - off models cover all 3ncs in our dataset. the fact that awdb type misses 8. 8 % of the dataset is mainly due to', 'equal distances between aligned words ( e. g., crisis resolution mechanism is only aligned to closed compounds, such as the swedish krislos', '##ningsmekanism or to nouns separated by one preposition, such as the spanish mecanismo de resolucion de crisis )', '. in future work, we will add more languages in the hope to', 'find more variation and thus get an even higher coverage']",5
"[')  #TAUTHOR_TAG trains domainindependent extractors.', 'this important characteristic']","['( oie )  #TAUTHOR_TAG trains domainindependent extractors.', 'this important characteristic']","[')  #TAUTHOR_TAG trains domainindependent extractors.', 'this important characteristic']","['the task of fact extraction from billions of web pages the method of open information extraction ( oie )  #TAUTHOR_TAG trains domainindependent extractors.', 'this important characteristic enables a potential application of oie for even very large corpora, such as the web.', 'existing approaches for oie, such as reverb  #TAUTHOR_TAG, woe  #AUTHOR_TAG or wander - lust  #AUTHOR_TAG focus on the extraction of binary facts, e. g. facts that consist of only two arguments, as well as a fact phrase which denotes the nature of the relationship between the arguments.', 'however, a recent analysis of oie based on semantic role labeling  #AUTHOR_TAG revealed that n - ary facts ( facts that connect more than two arguments ) were present in 40 % of surveyed english sentences.', 'worse, the analyses performed in  #TAUTHOR_TAG and  #AUTHOR_TAG show that incorrect handling of n - ary facts leads to extraction errors, such as incomplete, uninformative or erroneous facts.', 'our first example illustrates the case of a significant information loss : a ) in the 2002 film bubba ho - tep, elvis lives in a nursing home.', 'reverb : livesin ( elvis, nursing home )', 'in this case, the oie system ignores the significant contextual information in the argument the 2002 film bubba ho - tep, which denotes the domain in which the fact livesin ( elvis, nursing home ) is true.', 'as a result, and by itself, the extracted fact is false.', 'the next example shows a binary fact from a sentence that de - facto expresses an n - ary fact']",0
"[')  #TAUTHOR_TAG trains domainindependent extractors.', 'this important characteristic']","['( oie )  #TAUTHOR_TAG trains domainindependent extractors.', 'this important characteristic']","[')  #TAUTHOR_TAG trains domainindependent extractors.', 'this important characteristic']","['the task of fact extraction from billions of web pages the method of open information extraction ( oie )  #TAUTHOR_TAG trains domainindependent extractors.', 'this important characteristic enables a potential application of oie for even very large corpora, such as the web.', 'existing approaches for oie, such as reverb  #TAUTHOR_TAG, woe  #AUTHOR_TAG or wander - lust  #AUTHOR_TAG focus on the extraction of binary facts, e. g. facts that consist of only two arguments, as well as a fact phrase which denotes the nature of the relationship between the arguments.', 'however, a recent analysis of oie based on semantic role labeling  #AUTHOR_TAG revealed that n - ary facts ( facts that connect more than two arguments ) were present in 40 % of surveyed english sentences.', 'worse, the analyses performed in  #TAUTHOR_TAG and  #AUTHOR_TAG show that incorrect handling of n - ary facts leads to extraction errors, such as incomplete, uninformative or erroneous facts.', 'our first example illustrates the case of a significant information loss : a ) in the 2002 film bubba ho - tep, elvis lives in a nursing home.', 'reverb : livesin ( elvis, nursing home )', 'in this case, the oie system ignores the significant contextual information in the argument the 2002 film bubba ho - tep, which denotes the domain in which the fact livesin ( elvis, nursing home ) is true.', 'as a result, and by itself, the extracted fact is false.', 'the next example shows a binary fact from a sentence that de - facto expresses an n - ary fact']",0
"[')  #TAUTHOR_TAG trains domainindependent extractors.', 'this important characteristic']","['( oie )  #TAUTHOR_TAG trains domainindependent extractors.', 'this important characteristic']","[')  #TAUTHOR_TAG trains domainindependent extractors.', 'this important characteristic']","['the task of fact extraction from billions of web pages the method of open information extraction ( oie )  #TAUTHOR_TAG trains domainindependent extractors.', 'this important characteristic enables a potential application of oie for even very large corpora, such as the web.', 'existing approaches for oie, such as reverb  #TAUTHOR_TAG, woe  #AUTHOR_TAG or wander - lust  #AUTHOR_TAG focus on the extraction of binary facts, e. g. facts that consist of only two arguments, as well as a fact phrase which denotes the nature of the relationship between the arguments.', 'however, a recent analysis of oie based on semantic role labeling  #AUTHOR_TAG revealed that n - ary facts ( facts that connect more than two arguments ) were present in 40 % of surveyed english sentences.', 'worse, the analyses performed in  #TAUTHOR_TAG and  #AUTHOR_TAG show that incorrect handling of n - ary facts leads to extraction errors, such as incomplete, uninformative or erroneous facts.', 'our first example illustrates the case of a significant information loss : a ) in the 2002 film bubba ho - tep, elvis lives in a nursing home.', 'reverb : livesin ( elvis, nursing home )', 'in this case, the oie system ignores the significant contextual information in the argument the 2002 film bubba ho - tep, which denotes the domain in which the fact livesin ( elvis, nursing home ) is true.', 'as a result, and by itself, the extracted fact is false.', 'the next example shows a binary fact from a sentence that de - facto expresses an n - ary fact']",0
['system reverb  #TAUTHOR_TAG by contrast uses a fast'],"['speed.', 'the oie system reverb  #TAUTHOR_TAG by contrast uses a fast']","[', however at a high cost in extraction speed.', 'the oie system reverb  #TAUTHOR_TAG by contrast uses a fast shallow syntax parser']","['- oie : our previous system wanderlust  #AUTHOR_TAG operates using a typed dependency - style grammar representation called link grammar.', 'the system traverses paths of typed dependencies ( referred to as linkpaths ) to find pairs of arguments connected by a valid grammatical relationship.', 'we identified a set of 46 common linkpaths that can be used for fact extraction.', 'later, the authors  #AUTHOR_TAG trained extractors in a system called woe, one using only shallow syntactic features and one ( called woeparse ) that also uses typed dependencies as features.', 'the latter system learned more than 15. 000 patterns over typed dependencies.', 'in their evaluation they showed that using deep syntactic parsing improves the precision of their system, however at a high cost in extraction speed.', 'the oie system reverb  #TAUTHOR_TAG by contrast uses a fast shallow syntax parser for labeling sentences and applies syntactic and a lexical constraints for identifying binary facts.', 'however, the shallow syntactic analysis limits the capability of reverb of extracting higher order n - ary facts.', 'higher order fact extraction for wikipedia : in previous work on higher order fact extraction, the focus was placed on specific types of arguments.', 'the authors of  #AUTHOR_TAG for example extract temporal, spatial and category information from wikipedia info boxes. and  #AUTHOR_TAG focused on n - ary fact types from english sentences that contain at least one temporal argument.', 'in contrast, kraken extracts n - ary facts with arbitrary argument types']",0
"[')  #TAUTHOR_TAG trains domainindependent extractors.', 'this important characteristic']","['( oie )  #TAUTHOR_TAG trains domainindependent extractors.', 'this important characteristic']","[')  #TAUTHOR_TAG trains domainindependent extractors.', 'this important characteristic']","['the task of fact extraction from billions of web pages the method of open information extraction ( oie )  #TAUTHOR_TAG trains domainindependent extractors.', 'this important characteristic enables a potential application of oie for even very large corpora, such as the web.', 'existing approaches for oie, such as reverb  #TAUTHOR_TAG, woe  #AUTHOR_TAG or wander - lust  #AUTHOR_TAG focus on the extraction of binary facts, e. g. facts that consist of only two arguments, as well as a fact phrase which denotes the nature of the relationship between the arguments.', 'however, a recent analysis of oie based on semantic role labeling  #AUTHOR_TAG revealed that n - ary facts ( facts that connect more than two arguments ) were present in 40 % of surveyed english sentences.', 'worse, the analyses performed in  #TAUTHOR_TAG and  #AUTHOR_TAG show that incorrect handling of n - ary facts leads to extraction errors, such as incomplete, uninformative or erroneous facts.', 'our first example illustrates the case of a significant information loss : a ) in the 2002 film bubba ho - tep, elvis lives in a nursing home.', 'reverb : livesin ( elvis, nursing home )', 'in this case, the oie system ignores the significant contextual information in the argument the 2002 film bubba ho - tep, which denotes the domain in which the fact livesin ( elvis, nursing home ) is true.', 'as a result, and by itself, the extracted fact is false.', 'the next example shows a binary fact from a sentence that de - facto expresses an n - ary fact']",1
"['intra sentence fact correctness ( true / false ) and fact completeness for kraken and reverb on the corpus of  #TAUTHOR_TAG.', 'in the']","['intra sentence fact correctness ( true / false ) and fact completeness for kraken and reverb on the corpus of  #TAUTHOR_TAG.', 'in the']","['##ing unary, binary and higher order n - ary facts.', '2.', 'we examine intra sentence fact correctness ( true / false ) and fact completeness for kraken and reverb on the corpus of  #TAUTHOR_TAG.', 'in the rest of the paper we review earlier work and outline kraken, our method']","['##b : movedto ( elvis, memphis ) wanderlust : movedin  #AUTHOR_TAG contrary to the previous example, the oie systems extracted two binary facts that are not false, but incomplete, as the interaction between all three entities in this sentence can only be adequately modeled using an ternary fact.', 'the fact movedin  #AUTHOR_TAG for example misses an important aspect, namely the location elvis moved to in 1948.', 'therefore, each of these two facts is an example of important, but not crucial information loss.', 'unfortunately, current oie systems are not designed to capture the complete set of arguments for 52 each fact phrase within a sentence and to link arguments into an n - ary fact.', 'we view intra - sentence fact completeness as a major measure of data quality.', 'following existing work from  #AUTHOR_TAG complete factual data is a key for advanced data cleansing tasks, such as fact de - duplication, object resolution across n - ary facts, semantic fact interpretation and corpus wide fact aggregation.', 'therefore we argue that complete facts may serve a human reader or an advanced data cleansing approach as additional clue for interpreting and validating the fact.', 'in order to investigate the need and feasibility for n - ary oie we have performed the following, the results of which we present in this paper : 1. we introduce the oie system kraken, which has been built specifically for capturing complete facts from sentences and is capable of extracing unary, binary and higher order n - ary facts.', '2.', 'we examine intra sentence fact correctness ( true / false ) and fact completeness for kraken and reverb on the corpus of  #TAUTHOR_TAG.', 'in the rest of the paper we review earlier work and outline kraken, our method for extracting n - ary facts and contextual information.', 'next, we describe our experiments and end with conclusions']",5
['set from  #TAUTHOR_TAG which consists of 500 sentences sampled from the web using ya'],"[""set from  #TAUTHOR_TAG which consists of 500 sentences sampled from the web using yahoo's random link service."", '2 the sentences were labeled both with facts found with kraken and']",['set : we use the data set from  #TAUTHOR_TAG which consists of 500 sentences sampled from the web using ya'],"[""set : we use the data set from  #TAUTHOR_TAG which consists of 500 sentences sampled from the web using yahoo's random link service."", '2 the sentences were labeled both with facts found with kraken and the current version of reverb.', '3 we then paired facts for the same sentence that overlap in at least one of the fact phrase words, in order to present to the judges two different versions of the same fact - often one binary ( reverb ) and one nary ( kraken ).', 'measurements / instructions : given a sentence and a fact ( or fact - pair ), we asked two human judges to label each fact as either 1 ) true and complete, 2 ) true and incomplete, or 3 ) false.', 'true and incomplete facts either lack contextual information in the form of arguments that were present in the sentence, or contain underspecified arguments, but are nevertheless valid statements in themselves ( see our examples in section 1 ).', 'in previous evaluations, such 2 facts have been counted as true.', 'we distinguish them from true and complete facts that capture all relevant arguments as given by the sentence they were extracted from.', 'we measured an inter - annotator agreement of 87 %, differently evaluated facts were discussed by the judges and resolved.', 'most disagreement was caused by facts with underspecified arguments, labeled as false by one judge and as true and incomplete by the other']",5
"[' #TAUTHOR_TAG.', 'we propose novel methods']","[' #TAUTHOR_TAG.', 'we propose novel methods']","[' #TAUTHOR_TAG.', 'we propose novel methods']","[""models of lexical semantics, which assume that aspects of a word's meaning can be related to the contexts in which that word is typically used, have a long history in natural language processing ( sparck  #AUTHOR_TAG."", 'such models still constitute one of the most popular approaches to lexical semantics, with many proven applications.', 'much work in distributional semantics treats words as non - contextualised units ; the models that are constructed can answer questions such as "" how similar are the words body and corpse? "" but do not capture the way the syntactic context in which a word appears can affect its interpretation.', 'recent developments  #AUTHOR_TAG erk and pado, 2008 ;  #AUTHOR_TAG have aimed to address compositionality of meaning in terms of distributional semantics, leading to new kinds of questions such as "" how similar are the usages of the words body and corpse in the phrase the body / corpse deliberated the motion...? "" and "" how similar are the phrases the body deliberated the motion and the corpse rotted? "".', 'in this paper we focus on answering questions of the former type and investigate models that describe the effect of syntactic context on the meaning of a single word.', 'the work described in this paper uses probabilistic latent variable models to describe patterns of syntactic interaction, building on the selectional preference models ofo seaghdha ( 2010 ) and  #AUTHOR_TAG and the lexical substitution models of  #TAUTHOR_TAG.', 'we propose novel methods for incorporating information about syntactic context in models of lexical choice, yielding a probabilistic analogue to dependency - based models of contextual similarity.', 'our models attain state - of - the - art performance on two evaluation datasets : a set of sentence similarity judgements collected by  #AUTHOR_TAG and the dataset of the english lexical substitution task ( mc  #AUTHOR_TAG.', 'in view of the well - established effectiveness of dependency - based distributional semantics and of probabilistic frameworks for semantic inference, we expect that our approach will prove to be of value in a wide range of application settings']",5
"['semantic similarity is well - known in document modelling and was proposed in the context of lexical substitution by  #TAUTHOR_TAG. in terms', 'of the equations presented above, we could compare the distributions']","['semantic similarity is well - known in document modelling and was proposed in the context of lexical substitution by  #TAUTHOR_TAG. in terms', 'of the equations presented above, we could compare the distributions']","['semantic similarity is well - known in document modelling and was proposed in the context of lexical substitution by  #TAUTHOR_TAG. in terms', 'of the equations presented above, we could compare the distributions p ( z | o, c ) with p ( z | n,', 'c ) using equations ( 5 ) or ( 16 ).  #AUTHOR_TAG and  #TAUTHOR_TAG both observe that contextualising both o and n can degrade performance ; in']","['n | o, c ) is a fully generative model of lexical substitution. a non - generative alternative is one that estimates', 'the similarity of the latent variable distributions associated with seeing n and o in context c. the principle that similarity between topic distributions corresponds to semantic similarity is well - known in document modelling and was proposed in the context of lexical substitution by  #TAUTHOR_TAG. in terms', 'of the equations presented above, we could compare the distributions p ( z | o, c ) with p ( z | n,', 'c ) using equations ( 5 ) or ( 16 ).  #AUTHOR_TAG and  #TAUTHOR_TAG both observe that contextualising both o and n can degrade performance ; in view of this we actually compare p ( z | o, c ) with p ( z | n ) and make the further', 'simplifying assumption that p ( z | n ) ∝ p ( n | z ). the similarity measure we adopt is the bhattacharyya coefficient, which', 'is a natural measure of similarity between probability distributions and is closely related to the hellinger distance used in previous work on topic modelling  #AUTHOR_TAG : this measure takes values between 0 and 1. in this paper we train lda models of p ( w | c ) and p ( c | w ). in the former case, the analogy to document modelling is that each', 'context type plays the role of a "" document "" consisting of all the words observed in that context in a corpus ; for p ( c | w ) the roles are reversed. the models are trained by gibbs sampling using the', 'efficient procedure of  #AUTHOR_TAG. the empirical estimates for distributions over words and latent variables are derived from', 'the assignment 1049 of topics over the training corpus in a single sampling state. for example, to model p ( w | c ) we calculate : where f zw is the number of words of type w assigned topic', 'z, f zc is the number of times z is associated with context c, f z · and', 'f · c are the marginal topic and context counts respectively, n is the number of word types and α and', 'β parameterise the dirichlet prior distributions over p ( z | c ) and p ( w | z ). following the recommendations of  #AUTHOR_TAG', 'we use asymmetric α and symmetric β ; rather than using fixed values for these hyperparameters we estimate them from data in', 'the course of lda training using an em - like method. 2 we use standard settings for the number of training iterations ( 1000 ), the length of the burnin period before hyperparameter estimation begins ( 200 iterations ) and the frequency of hyperparameter estimation ( 50 iterations )']",5
"['by  #AUTHOR_TAG and  #TAUTHOR_TAG, respectively.', 'generalised averaged']","['by  #AUTHOR_TAG and  #TAUTHOR_TAG, respectively.', 'generalised averaged']","['s τ b rank correlation coefficient, which were used for this task by  #AUTHOR_TAG and  #TAUTHOR_TAG, respectively.', 'generalised averaged precision ( gap ) is a precision - like measure']","['english lexical substitution task, run as part of the semeval - 1 competition, required participants to propose good substitutes for a set of target words in various sentential contexts ( mc  #AUTHOR_TAG.', 'table 2 shows two example sentences and the substitutes appearing in the gold standard, ranked by the number of human annotators who proposed each substitute.', 'the dataset contains a total of 2, 010 annotated sentences with 205 distinct target words across four parts of speech ( noun, verb, adjective, adverb ).', 'in line with previous work on contextual disambiguation, we focus here on the subtask of ranking attested substitutes rather than proposing them from an unrestricted vocabulary.', 'to this end, a candidate set is constructed for each target word from all the substitutes proposed for that word in all sentences in the dataset.', 'the data contains a number of multiword paraphrases such as rush at ; as our models ( like most 1051 realizing immediately that strangers have come, attack ( 5 ), rush at ( 1 ) the animals charge them and the horses began to fight.', 'commission is the amount charged to execute a trade. levy ( 2 ), impose ( 1 ), take ( 1 ), demand ( 1 ) table 2 : examples for the verb charge from the english lexical substitution task current models of distributional semantics ) do not represent multiword expressions, we remove such paraphrases and discard the 17 sentences which have only multiword substitutes in the gold standard.', '4 there are also 7 sentences for which the gold standard contains no substitutes.', 'this leaves a total of 1986 sentences.', 'these sentences were lemmatised and parsed with rasp.', 'previous authors have partitioned the dataset in various ways.', 'erk and pado ( 2008 ) use only a subset of the data where the target is a noun headed by a verb or a verb heading a noun.', ' #AUTHOR_TAG discard sentences which their parser cannot parse and paraphrases absent from their training corpus and then optimise the parameters of their model through four - fold cross - validation.', 'here we aim for complete coverage on the dataset and do not perform any parameter tuning.', ""we use two measures to evaluate performance : generalised averaged precision  #AUTHOR_TAG and kendall's τ b rank correlation coefficient, which were used for this task by  #AUTHOR_TAG and  #TAUTHOR_TAG, respectively."", 'generalised averaged precision ( gap ) is a precision - like measure for evaluating ranked predictions against a gold standard.', ""τ b is a variant of kendall's τ that is appropriate for data containing tied ranks."", 'we do not use the "" precision out of ten ""']",5
"['3.', 'for the window - based context model we follow  #TAUTHOR_TAG in treating']","['3.', 'for the window - based context model we follow  #TAUTHOR_TAG in treating']","['the transformations shown in table 3.', 'for the window - based context model we follow  #TAUTHOR_TAG in']","['apply the models developed in section 3. 1 to the lexical substitution task dataset using dependencyand window - based context information.', 'here we only use the sim predictor type.', 'para did not give satisfactory results ; in particular, it tended to rank common words highly in most contexts.', '6 as before we compiled training data by extracting target - context cooccurrences from a text corpus.', 'in addition to the parsed bnc described above we used a corpus of wikipedia text consisting of over 45 million sentences ( almost 1 billion words ) parsed using the fast combinatory categorial grammar ( ccg ) parser described by  #AUTHOR_TAG.', 'the depen - 5 we use the software package available at http : / / www.', 'nlpado. de / [UNK] / sigf. html.', '6 favouring more general words may indeed make sense in some paraphrasing tasks  #AUTHOR_TAG.', 'dency representation produced by this parser is interoperable with the rasp dependency format.', 'in order to focus our models on semantically discriminative information and make inference more tractable we ignored all parts of speech other than nouns, verbs, adjectives, prepositions and adverbs.', 'stopwords and words of fewer than three characters were removed.', 'we also removed the very frequent but semantically weak lemmas be and have.', 'we compare two classes of context models : models learned from window - based contexts and models learned from syntactic dependency contexts.', 'for the syntactic models we extracted all dependencies and inverse dependencies between lemmas of the aforementioned pos types ; in order to maximise the extraction yield, the dependency graph for each sentence was preprocessed using the transformations shown in table 3.', 'for the window - based context model we follow  #TAUTHOR_TAG in treating each word within five words of a target as a member of its context set.', 'it proved necessary to subsample the corpora in order to make lda training tractable, especially for the window - based model where the training set of context - target counts is extremely dense ( each instance of a word in the corpus contributes up to 10 context instances ).', 'for the window - based data, we divided each context - target count by a factor of 5 and a factor of 70 for the bnc and wikipedia corpora respectively, rounding fractional counts to the closest integer.', 'the choice of 70 for scaling wikipedia counts is adopted from  #TAUTHOR_TAG, who used the same factor for the comparably sized english gigaword corpus.', 'as the dependency data is an order of magnitude smaller we downsampled the wikipedia counts by 5 and left the bnc counts untouched.', 'finally, we created a larger corpus by']",5
"['3.', 'for the window - based context model we follow  #TAUTHOR_TAG in treating']","['3.', 'for the window - based context model we follow  #TAUTHOR_TAG in treating']","['the transformations shown in table 3.', 'for the window - based context model we follow  #TAUTHOR_TAG in']","['apply the models developed in section 3. 1 to the lexical substitution task dataset using dependencyand window - based context information.', 'here we only use the sim predictor type.', 'para did not give satisfactory results ; in particular, it tended to rank common words highly in most contexts.', '6 as before we compiled training data by extracting target - context cooccurrences from a text corpus.', 'in addition to the parsed bnc described above we used a corpus of wikipedia text consisting of over 45 million sentences ( almost 1 billion words ) parsed using the fast combinatory categorial grammar ( ccg ) parser described by  #AUTHOR_TAG.', 'the depen - 5 we use the software package available at http : / / www.', 'nlpado. de / [UNK] / sigf. html.', '6 favouring more general words may indeed make sense in some paraphrasing tasks  #AUTHOR_TAG.', 'dency representation produced by this parser is interoperable with the rasp dependency format.', 'in order to focus our models on semantically discriminative information and make inference more tractable we ignored all parts of speech other than nouns, verbs, adjectives, prepositions and adverbs.', 'stopwords and words of fewer than three characters were removed.', 'we also removed the very frequent but semantically weak lemmas be and have.', 'we compare two classes of context models : models learned from window - based contexts and models learned from syntactic dependency contexts.', 'for the syntactic models we extracted all dependencies and inverse dependencies between lemmas of the aforementioned pos types ; in order to maximise the extraction yield, the dependency graph for each sentence was preprocessed using the transformations shown in table 3.', 'for the window - based context model we follow  #TAUTHOR_TAG in treating each word within five words of a target as a member of its context set.', 'it proved necessary to subsample the corpora in order to make lda training tractable, especially for the window - based model where the training set of context - target counts is extremely dense ( each instance of a word in the corpus contributes up to 10 context instances ).', 'for the window - based data, we divided each context - target count by a factor of 5 and a factor of 70 for the bnc and wikipedia corpora respectively, rounding fractional counts to the closest integer.', 'the choice of 70 for scaling wikipedia counts is adopted from  #TAUTHOR_TAG, who used the same factor for the comparably sized english gigaword corpus.', 'as the dependency data is an order of magnitude smaller we downsampled the wikipedia counts by 5 and left the bnc counts untouched.', 'finally, we created a larger corpus by']",5
"[' #TAUTHOR_TAG.', 'we propose novel methods']","[' #TAUTHOR_TAG.', 'we propose novel methods']","[' #TAUTHOR_TAG.', 'we propose novel methods']","[""models of lexical semantics, which assume that aspects of a word's meaning can be related to the contexts in which that word is typically used, have a long history in natural language processing ( sparck  #AUTHOR_TAG."", 'such models still constitute one of the most popular approaches to lexical semantics, with many proven applications.', 'much work in distributional semantics treats words as non - contextualised units ; the models that are constructed can answer questions such as "" how similar are the words body and corpse? "" but do not capture the way the syntactic context in which a word appears can affect its interpretation.', 'recent developments  #AUTHOR_TAG erk and pado, 2008 ;  #AUTHOR_TAG have aimed to address compositionality of meaning in terms of distributional semantics, leading to new kinds of questions such as "" how similar are the usages of the words body and corpse in the phrase the body / corpse deliberated the motion...? "" and "" how similar are the phrases the body deliberated the motion and the corpse rotted? "".', 'in this paper we focus on answering questions of the former type and investigate models that describe the effect of syntactic context on the meaning of a single word.', 'the work described in this paper uses probabilistic latent variable models to describe patterns of syntactic interaction, building on the selectional preference models ofo seaghdha ( 2010 ) and  #AUTHOR_TAG and the lexical substitution models of  #TAUTHOR_TAG.', 'we propose novel methods for incorporating information about syntactic context in models of lexical choice, yielding a probabilistic analogue to dependency - based models of contextual similarity.', 'our models attain state - of - the - art performance on two evaluation datasets : a set of sentence similarity judgements collected by  #AUTHOR_TAG and the dataset of the english lexical substitution task ( mc  #AUTHOR_TAG.', 'in view of the well - established effectiveness of dependency - based distributional semantics and of probabilistic frameworks for semantic inference, we expect that our approach will prove to be of value in a wide range of application settings']",6
"['3 below,  #TAUTHOR_TAG propose an lda - based model for lexical substitution ; the techniques presented in this paper can be viewed as a generalisation of theirs.', 'topic models have also been']","['3 below,  #TAUTHOR_TAG propose an lda - based model for lexical substitution ; the techniques presented in this paper can be viewed as a generalisation of theirs.', 'topic models have also been']","['teasing apart the context distributions of polysemous words.', 'as described in section 3 below,  #TAUTHOR_TAG propose an lda - based model for lexical substitution ; the techniques presented in this paper can be viewed as a generalisation of theirs.', 'topic models have also been applied to other classes of semantic task,']","['literature on distributional semantics is vast ; in this section we focus on outlining the research that is most directly related to capturing effects of context and compositionality.', '1  #AUTHOR_TAG follow  #AUTHOR_TAG in observing that most distributional approaches to meaning at the phrase or sentence level assume that the contribution of syntactic structure can be ignored and the meaning of a phrase is simply the commutative sum of the meanings of its constituent words.', 'as mitchell and lapata argue, this assumption clearly leads to an impoverished model of semantics.', 'mitchell and lapata investigate a number of simple methods for combining distributional word vectors, concluding that pointwise multiplication best corresponds to the effects of syntactic interaction.', 'erk and pado ( 2008 ) introduce the concept of a structured vector space in which each word is associated with a set of selectional preference vectors corresponding to different syntactic dependencies.', ' #AUTHOR_TAG develop this geometric approach further using a space of second - order distributional vectors that represent the words typically co - occurring with the contexts in which a word typically appears.', 'the primary concern of these authors is to model the effect of context on word meaning ; the work we present in this paper uses similar intuitions in a probabilistic modelling framework.', 'a parallel strand of research seeks to represent the meaning of larger compositional structures using matrix and tensor algebra  #AUTHOR_TAG.', 'this nascent approach holds the promise of providing a much richer notion of context than is currently exploited in semantic applications.', 'probabilistic latent variable frameworks for generalising about contextual behaviour ( in the form of verb - noun selectional preferences ) were proposed by  #AUTHOR_TAG and  #AUTHOR_TAG.', 'latent variable models are also conceptually similar to non - probabilistic dimensionality reduction techniques such as latent semantic analysis  #AUTHOR_TAG.', 'more recently, o seaghdha ( 2010 ) and  #AUTHOR_TAG reformulated rooth et al.\'s approach in a bayesian framework using models related to latent dirichlet allocation  #AUTHOR_TAG, demonstrating that this "" topic modelling "" architecture is a very good fit for capturing selectional preferences.', ' #AUTHOR_TAG investigate nonparametric bayesian models for teasing apart the context distributions of polysemous words.', 'as described in section 3 below,  #TAUTHOR_TAG propose an lda - based model for lexical substitution ; the techniques presented in this paper can be viewed as a generalisation of theirs.', 'topic models have also been applied to other classes of semantic task, for example word sense disambiguation  #AUTHOR_TAG, word sense induction  #AUTHOR_TAG and modelling human judgements of semantic association  #AUTHOR_TAG']",0
"['semantic similarity is well - known in document modelling and was proposed in the context of lexical substitution by  #TAUTHOR_TAG. in terms', 'of the equations presented above, we could compare the distributions']","['semantic similarity is well - known in document modelling and was proposed in the context of lexical substitution by  #TAUTHOR_TAG. in terms', 'of the equations presented above, we could compare the distributions']","['semantic similarity is well - known in document modelling and was proposed in the context of lexical substitution by  #TAUTHOR_TAG. in terms', 'of the equations presented above, we could compare the distributions p ( z | o, c ) with p ( z | n,', 'c ) using equations ( 5 ) or ( 16 ).  #AUTHOR_TAG and  #TAUTHOR_TAG both observe that contextualising both o and n can degrade performance ; in']","['n | o, c ) is a fully generative model of lexical substitution. a non - generative alternative is one that estimates', 'the similarity of the latent variable distributions associated with seeing n and o in context c. the principle that similarity between topic distributions corresponds to semantic similarity is well - known in document modelling and was proposed in the context of lexical substitution by  #TAUTHOR_TAG. in terms', 'of the equations presented above, we could compare the distributions p ( z | o, c ) with p ( z | n,', 'c ) using equations ( 5 ) or ( 16 ).  #AUTHOR_TAG and  #TAUTHOR_TAG both observe that contextualising both o and n can degrade performance ; in view of this we actually compare p ( z | o, c ) with p ( z | n ) and make the further', 'simplifying assumption that p ( z | n ) ∝ p ( n | z ). the similarity measure we adopt is the bhattacharyya coefficient, which', 'is a natural measure of similarity between probability distributions and is closely related to the hellinger distance used in previous work on topic modelling  #AUTHOR_TAG : this measure takes values between 0 and 1. in this paper we train lda models of p ( w | c ) and p ( c | w ). in the former case, the analogy to document modelling is that each', 'context type plays the role of a "" document "" consisting of all the words observed in that context in a corpus ; for p ( c | w ) the roles are reversed. the models are trained by gibbs sampling using the', 'efficient procedure of  #AUTHOR_TAG. the empirical estimates for distributions over words and latent variables are derived from', 'the assignment 1049 of topics over the training corpus in a single sampling state. for example, to model p ( w | c ) we calculate : where f zw is the number of words of type w assigned topic', 'z, f zc is the number of times z is associated with context c, f z · and', 'f · c are the marginal topic and context counts respectively, n is the number of word types and α and', 'β parameterise the dirichlet prior distributions over p ( z | c ) and p ( w | z ). following the recommendations of  #AUTHOR_TAG', 'we use asymmetric α and symmetric β ; rather than using fixed values for these hyperparameters we estimate them from data in', 'the course of lda training using an em - like method. 2 we use standard settings for the number of training iterations ( 1000 ), the length of the burnin period before hyperparameter estimation begins ( 200 iterations ) and the frequency of hyperparameter estimation ( 50 iterations )']",0
"['semantic similarity is well - known in document modelling and was proposed in the context of lexical substitution by  #TAUTHOR_TAG. in terms', 'of the equations presented above, we could compare the distributions']","['semantic similarity is well - known in document modelling and was proposed in the context of lexical substitution by  #TAUTHOR_TAG. in terms', 'of the equations presented above, we could compare the distributions']","['semantic similarity is well - known in document modelling and was proposed in the context of lexical substitution by  #TAUTHOR_TAG. in terms', 'of the equations presented above, we could compare the distributions p ( z | o, c ) with p ( z | n,', 'c ) using equations ( 5 ) or ( 16 ).  #AUTHOR_TAG and  #TAUTHOR_TAG both observe that contextualising both o and n can degrade performance ; in']","['n | o, c ) is a fully generative model of lexical substitution. a non - generative alternative is one that estimates', 'the similarity of the latent variable distributions associated with seeing n and o in context c. the principle that similarity between topic distributions corresponds to semantic similarity is well - known in document modelling and was proposed in the context of lexical substitution by  #TAUTHOR_TAG. in terms', 'of the equations presented above, we could compare the distributions p ( z | o, c ) with p ( z | n,', 'c ) using equations ( 5 ) or ( 16 ).  #AUTHOR_TAG and  #TAUTHOR_TAG both observe that contextualising both o and n can degrade performance ; in view of this we actually compare p ( z | o, c ) with p ( z | n ) and make the further', 'simplifying assumption that p ( z | n ) ∝ p ( n | z ). the similarity measure we adopt is the bhattacharyya coefficient, which', 'is a natural measure of similarity between probability distributions and is closely related to the hellinger distance used in previous work on topic modelling  #AUTHOR_TAG : this measure takes values between 0 and 1. in this paper we train lda models of p ( w | c ) and p ( c | w ). in the former case, the analogy to document modelling is that each', 'context type plays the role of a "" document "" consisting of all the words observed in that context in a corpus ; for p ( c | w ) the roles are reversed. the models are trained by gibbs sampling using the', 'efficient procedure of  #AUTHOR_TAG. the empirical estimates for distributions over words and latent variables are derived from', 'the assignment 1049 of topics over the training corpus in a single sampling state. for example, to model p ( w | c ) we calculate : where f zw is the number of words of type w assigned topic', 'z, f zc is the number of times z is associated with context c, f z · and', 'f · c are the marginal topic and context counts respectively, n is the number of word types and α and', 'β parameterise the dirichlet prior distributions over p ( z | c ) and p ( w | z ). following the recommendations of  #AUTHOR_TAG', 'we use asymmetric α and symmetric β ; rather than using fixed values for these hyperparameters we estimate them from data in', 'the course of lda training using an em - like method. 2 we use standard settings for the number of training iterations ( 1000 ), the length of the burnin period before hyperparameter estimation begins ( 200 iterations ) and the frequency of hyperparameter estimation ( 50 iterations )']",0
"['semantic similarity is well - known in document modelling and was proposed in the context of lexical substitution by  #TAUTHOR_TAG. in terms', 'of the equations presented above, we could compare the distributions']","['semantic similarity is well - known in document modelling and was proposed in the context of lexical substitution by  #TAUTHOR_TAG. in terms', 'of the equations presented above, we could compare the distributions']","['semantic similarity is well - known in document modelling and was proposed in the context of lexical substitution by  #TAUTHOR_TAG. in terms', 'of the equations presented above, we could compare the distributions p ( z | o, c ) with p ( z | n,', 'c ) using equations ( 5 ) or ( 16 ).  #AUTHOR_TAG and  #TAUTHOR_TAG both observe that contextualising both o and n can degrade performance ; in']","['n | o, c ) is a fully generative model of lexical substitution. a non - generative alternative is one that estimates', 'the similarity of the latent variable distributions associated with seeing n and o in context c. the principle that similarity between topic distributions corresponds to semantic similarity is well - known in document modelling and was proposed in the context of lexical substitution by  #TAUTHOR_TAG. in terms', 'of the equations presented above, we could compare the distributions p ( z | o, c ) with p ( z | n,', 'c ) using equations ( 5 ) or ( 16 ).  #AUTHOR_TAG and  #TAUTHOR_TAG both observe that contextualising both o and n can degrade performance ; in view of this we actually compare p ( z | o, c ) with p ( z | n ) and make the further', 'simplifying assumption that p ( z | n ) ∝ p ( n | z ). the similarity measure we adopt is the bhattacharyya coefficient, which', 'is a natural measure of similarity between probability distributions and is closely related to the hellinger distance used in previous work on topic modelling  #AUTHOR_TAG : this measure takes values between 0 and 1. in this paper we train lda models of p ( w | c ) and p ( c | w ). in the former case, the analogy to document modelling is that each', 'context type plays the role of a "" document "" consisting of all the words observed in that context in a corpus ; for p ( c | w ) the roles are reversed. the models are trained by gibbs sampling using the', 'efficient procedure of  #AUTHOR_TAG. the empirical estimates for distributions over words and latent variables are derived from', 'the assignment 1049 of topics over the training corpus in a single sampling state. for example, to model p ( w | c ) we calculate : where f zw is the number of words of type w assigned topic', 'z, f zc is the number of times z is associated with context c, f z · and', 'f · c are the marginal topic and context counts respectively, n is the number of word types and α and', 'β parameterise the dirichlet prior distributions over p ( z | c ) and p ( w | z ). following the recommendations of  #AUTHOR_TAG', 'we use asymmetric α and symmetric β ; rather than using fixed values for these hyperparameters we estimate them from data in', 'the course of lda training using an em - like method. 2 we use standard settings for the number of training iterations ( 1000 ), the length of the burnin period before hyperparameter estimation begins ( 200 iterations ) and the frequency of hyperparameter estimation ( 50 iterations )']",1
"['remarked in section 3. 1,  #TAUTHOR_TAG use a slightly different formulation of p ( z | c, o ).', 'using the window - based context model our formulation ( 5 ) outperforms ( 7 ) for both training corpora ; the  #TAUTHOR_TAG']","['remarked in section 3. 1,  #TAUTHOR_TAG use a slightly different formulation of p ( z | c, o ).', 'using the window - based context model our formulation ( 5 ) outperforms ( 7 ) for both training corpora ; the  #TAUTHOR_TAG']","['remarked in section 3. 1,  #TAUTHOR_TAG use a slightly different formulation of p ( z | c, o ).', 'using the window - based context model our formulation ( 5 ) outperforms ( 7 ) for both training corpora ; the  #TAUTHOR_TAG table 6 :']","['remarked in section 3. 1,  #TAUTHOR_TAG use a slightly different formulation of p ( z | c, o ).', 'using the window - based context model our formulation ( 5 ) outperforms ( 7 ) for both training corpora ; the  #TAUTHOR_TAG table 6 : performance by part of speech table 6 gives a breakdown of performance by target part of speech for the bnc + wikipedia - trained w5 and w5 + t ↔ c models, as well as figures provided by previous researchers.', '7 w5 + t ↔ c outperforms w5 on all parts of speech using both evaluation metrics.', 'as remarked above, previous researchers have used the corpus in slightly different ways ; we believe that the results of  #TAUTHOR_TAG are fully comparable, while those of  #AUTHOR_TAG were attained on a slightly smaller dataset with parameters set through cross - validation.', ""the results for w5 + t ↔ c outperform all of dinu and lapata's per - pos and overall results except for a slightly superior score on adverbs attained by their nmf model ( τ b = 0. 26 compared to 0. 24 )."", 'turning to thater et al., we report higher scores for every pos with the exception of the verbs where their model 1 achieves 45. 9 gap compared to 45. 1 ; the overall average for w5 + t ↔ c is substantially higher at 49. 5 compared to 44. 6.', 'on balance, we suggest that our models do have an advantage over the current state of the art for lexical substitution']",4
"['remarked in section 3. 1,  #TAUTHOR_TAG use a slightly different formulation of p ( z | c, o ).', 'using the window - based context model our formulation ( 5 ) outperforms ( 7 ) for both training corpora ; the  #TAUTHOR_TAG']","['remarked in section 3. 1,  #TAUTHOR_TAG use a slightly different formulation of p ( z | c, o ).', 'using the window - based context model our formulation ( 5 ) outperforms ( 7 ) for both training corpora ; the  #TAUTHOR_TAG']","['remarked in section 3. 1,  #TAUTHOR_TAG use a slightly different formulation of p ( z | c, o ).', 'using the window - based context model our formulation ( 5 ) outperforms ( 7 ) for both training corpora ; the  #TAUTHOR_TAG table 6 :']","['remarked in section 3. 1,  #TAUTHOR_TAG use a slightly different formulation of p ( z | c, o ).', 'using the window - based context model our formulation ( 5 ) outperforms ( 7 ) for both training corpora ; the  #TAUTHOR_TAG table 6 : performance by part of speech table 6 gives a breakdown of performance by target part of speech for the bnc + wikipedia - trained w5 and w5 + t ↔ c models, as well as figures provided by previous researchers.', '7 w5 + t ↔ c outperforms w5 on all parts of speech using both evaluation metrics.', 'as remarked above, previous researchers have used the corpus in slightly different ways ; we believe that the results of  #TAUTHOR_TAG are fully comparable, while those of  #AUTHOR_TAG were attained on a slightly smaller dataset with parameters set through cross - validation.', ""the results for w5 + t ↔ c outperform all of dinu and lapata's per - pos and overall results except for a slightly superior score on adverbs attained by their nmf model ( τ b = 0. 26 compared to 0. 24 )."", 'turning to thater et al., we report higher scores for every pos with the exception of the verbs where their model 1 achieves 45. 9 gap compared to 45. 1 ; the overall average for w5 + t ↔ c is substantially higher at 49. 5 compared to 44. 6.', 'on balance, we suggest that our models do have an advantage over the current state of the art for lexical substitution']",4
"['remarked in section 3. 1,  #TAUTHOR_TAG use a slightly different formulation of p ( z | c, o ).', 'using the window - based context model our formulation ( 5 ) outperforms ( 7 ) for both training corpora ; the  #TAUTHOR_TAG']","['remarked in section 3. 1,  #TAUTHOR_TAG use a slightly different formulation of p ( z | c, o ).', 'using the window - based context model our formulation ( 5 ) outperforms ( 7 ) for both training corpora ; the  #TAUTHOR_TAG']","['remarked in section 3. 1,  #TAUTHOR_TAG use a slightly different formulation of p ( z | c, o ).', 'using the window - based context model our formulation ( 5 ) outperforms ( 7 ) for both training corpora ; the  #TAUTHOR_TAG table 6 :']","['remarked in section 3. 1,  #TAUTHOR_TAG use a slightly different formulation of p ( z | c, o ).', 'using the window - based context model our formulation ( 5 ) outperforms ( 7 ) for both training corpora ; the  #TAUTHOR_TAG table 6 : performance by part of speech table 6 gives a breakdown of performance by target part of speech for the bnc + wikipedia - trained w5 and w5 + t ↔ c models, as well as figures provided by previous researchers.', '7 w5 + t ↔ c outperforms w5 on all parts of speech using both evaluation metrics.', 'as remarked above, previous researchers have used the corpus in slightly different ways ; we believe that the results of  #TAUTHOR_TAG are fully comparable, while those of  #AUTHOR_TAG were attained on a slightly smaller dataset with parameters set through cross - validation.', ""the results for w5 + t ↔ c outperform all of dinu and lapata's per - pos and overall results except for a slightly superior score on adverbs attained by their nmf model ( τ b = 0. 26 compared to 0. 24 )."", 'turning to thater et al., we report higher scores for every pos with the exception of the verbs where their model 1 achieves 45. 9 gap compared to 45. 1 ; the overall average for w5 + t ↔ c is substantially higher at 49. 5 compared to 44. 6.', 'on balance, we suggest that our models do have an advantage over the current state of the art for lexical substitution']",3
"['##s in wikipedia  #TAUTHOR_TAG.', '']","['from hierachical layouts in wikipedia  #TAUTHOR_TAG.', '']","['s method of acquiring hyponymy relations from hierachical layouts in wikipedia  #TAUTHOR_TAG.', 'we extract hypony']","[""paper proposes an extension of sumida and torisawa's method of acquiring hyponymy relations from hierachical layouts in wikipedia  #TAUTHOR_TAG."", '']",6
"['by  #TAUTHOR_TAG, but differs in']","['by  #TAUTHOR_TAG, but differs in']","['by  #TAUTHOR_TAG, but differs in the way of enumerating hyponymy relation candidates ( hereafter, hrcs ) from the hierarchical layouts, and in the features of machine learning.', 'our method consists of the following two steps :', 'step 1 : we first']","['method of acquiring hyponymy relations is an extension of the supervised method proposed by  #TAUTHOR_TAG, but differs in the way of enumerating hyponymy relation candidates ( hereafter, hrcs ) from the hierarchical layouts, and in the features of machine learning.', 'our method consists of the following two steps :', 'step 1 : we first extract hrcs from hierarchical layouts in wikipedia articles.', 'step 2 : we then select proper hyponymy relations from the hrcs extracted in step 1 by using support vector machines ( svms ) as a classifier  #AUTHOR_TAG.', 'x ( partial list of x ), * x ( details of x ), * x ( typical x ), * x ( basic x ), * x ( notable x ), * x ( partial list of x )', 'figure 3 : patterns for finding plausible hypernym x ; patterns with * are newly introduced in this study ( japanese terms used in our experiments are followed by english translations ).', 'in what follows, we describe each step in detail']",6
"['##s in wikipedia  #TAUTHOR_TAG.', '']","['from hierachical layouts in wikipedia  #TAUTHOR_TAG.', '']","['s method of acquiring hyponymy relations from hierachical layouts in wikipedia  #TAUTHOR_TAG.', 'we extract hypony']","[""paper proposes an extension of sumida and torisawa's method of acquiring hyponymy relations from hierachical layouts in wikipedia  #TAUTHOR_TAG."", '']",4
"['by  #TAUTHOR_TAG, but differs in']","['by  #TAUTHOR_TAG, but differs in']","['by  #TAUTHOR_TAG, but differs in the way of enumerating hyponymy relation candidates ( hereafter, hrcs ) from the hierarchical layouts, and in the features of machine learning.', 'our method consists of the following two steps :', 'step 1 : we first']","['method of acquiring hyponymy relations is an extension of the supervised method proposed by  #TAUTHOR_TAG, but differs in the way of enumerating hyponymy relation candidates ( hereafter, hrcs ) from the hierarchical layouts, and in the features of machine learning.', 'our method consists of the following two steps :', 'step 1 : we first extract hrcs from hierarchical layouts in wikipedia articles.', 'step 2 : we then select proper hyponymy relations from the hrcs extracted in step 1 by using support vector machines ( svms ) as a classifier  #AUTHOR_TAG.', 'x ( partial list of x ), * x ( details of x ), * x ( typical x ), * x ( basic x ), * x ( notable x ), * x ( partial list of x )', 'figure 3 : patterns for finding plausible hypernym x ; patterns with * are newly introduced in this study ( japanese terms used in our experiments are followed by english translations ).', 'in what follows, we describe each step in detail']",4
['1. note that  #TAUTHOR_TAG extracted hr'],['1. note that  #TAUTHOR_TAG extracted hrcs by'],['in figure 1. note that  #TAUTHOR_TAG extracted hr'],"[""obtain hrcs by considering the title of each marked - up item as a hypernym candidate, and titles of its all subordinate marked - up items as its hyponym candidates ; for example, we extract'england ','france ','wedgwood ','lipton ', and'fauchon'as hyponym candidates of'common tea brands'from the hierarchical structure in figure 1. note that  #TAUTHOR_TAG extracted hrcs by regarding the title of each marked - up item as a hypernym candidate and titles of its direct subordinate marked - up items as its hyponyms ; for example, they extracted only'england'and'france'as hyponym candidates of'common tea brands'from the hierarchical structure in figure 1."", 'they also employed patterns shown in figure 3 ( e. g., "" x "" ( list of x ) ) to find plausible hypernyms denoted by x in the pattern.', '']",4
,,,,4
"['researchers have attempted to automatically acquire hyponymy relations from texts  #TAUTHOR_TAG.', '']","['', 'many nlp researchers have attempted to automatically acquire hyponymy relations from texts  #TAUTHOR_TAG.', '']","['', 'many nlp researchers have attempted to automatically acquire hyponymy relations from texts  #TAUTHOR_TAG.', '']","['', 'in this paper, a hyponymy relation is defined as a relation between a hypernym and a hyponym when "" the hyponym is a ( kind of ) hypernym. ""', '1 we acquired more than 1. 34 million hyponymy relations in japanese with a precision of 90. 1 %.', 'many nlp researchers have attempted to automatically acquire hyponymy relations from texts  #TAUTHOR_TAG.', '']",0
"['researchers have attempted to automatically acquire hyponymy relations from texts  #TAUTHOR_TAG.', '']","['', 'many nlp researchers have attempted to automatically acquire hyponymy relations from texts  #TAUTHOR_TAG.', '']","['', 'many nlp researchers have attempted to automatically acquire hyponymy relations from texts  #TAUTHOR_TAG.', '']","['', 'in this paper, a hyponymy relation is defined as a relation between a hypernym and a hyponym when "" the hyponym is a ( kind of ) hypernym. ""', '1 we acquired more than 1. 34 million hyponymy relations in japanese with a precision of 90. 1 %.', 'many nlp researchers have attempted to automatically acquire hyponymy relations from texts  #TAUTHOR_TAG.', '']",0
"['english version of wikipedia,  #TAUTHOR_TAG extracted hyponymy']","['english version of wikipedia,  #TAUTHOR_TAG extracted hyponymy']","['.', 'although the above studies extracted hyponymy relations from the english version of wikipedia,  #TAUTHOR_TAG extracted hyponymy relations from definition sentences, category labels, and hierarchical structures in wikipedia articles.', 'they reported that the number of hyponymy relations acquired from']","['', 'a languagedependent heuristics then selected correct hypernyms from the hypernym candidates.', 'they acquired more than 2. 04 millions of hyponymy relations ( relations subclassof and type in their paper ) from 1. 6 millions of wikipedia articles with a precision of about 95 %.', 'although the above studies extracted hyponymy relations from the english version of wikipedia,  #TAUTHOR_TAG extracted hyponymy relations from definition sentences, category labels, and hierarchical structures in wikipedia articles.', 'they reported that the number of hyponymy relations acquired from the hierarchical structures was larger than the number of hyponymy relations acquired from the other resources.', 'we thus focus on the hierarchical structures to acquire more hyponymy relations']",0
"['the features proposed by  #TAUTHOR_TAG,']","['the features proposed by  #TAUTHOR_TAG,']","['the features proposed by  #TAUTHOR_TAG,']","['select proper hyponymy relations from the hrcs obtained in step 1 by using svms  #AUTHOR_TAG as a classifier.', 'in what follows, we briefly review the features proposed by  #TAUTHOR_TAG, and then explain the novel features introduced in this study.', 'we expect that the readers will refer to the literature  #TAUTHOR_TAG to see the effect of the features proposed by sumida and torisawa.', 'in the following explanation, we refer to the hypernym candidate or the hyponym candidate of each hrc as hypernym or hyponym']",0
"['english version of wikipedia,  #TAUTHOR_TAG extracted hyponymy']","['english version of wikipedia,  #TAUTHOR_TAG extracted hyponymy']","['.', 'although the above studies extracted hyponymy relations from the english version of wikipedia,  #TAUTHOR_TAG extracted hyponymy relations from definition sentences, category labels, and hierarchical structures in wikipedia articles.', 'they reported that the number of hyponymy relations acquired from']","['', 'a languagedependent heuristics then selected correct hypernyms from the hypernym candidates.', 'they acquired more than 2. 04 millions of hyponymy relations ( relations subclassof and type in their paper ) from 1. 6 millions of wikipedia articles with a precision of about 95 %.', 'although the above studies extracted hyponymy relations from the english version of wikipedia,  #TAUTHOR_TAG extracted hyponymy relations from definition sentences, category labels, and hierarchical structures in wikipedia articles.', 'they reported that the number of hyponymy relations acquired from the hierarchical structures was larger than the number of hyponymy relations acquired from the other resources.', 'we thus focus on the hierarchical structures to acquire more hyponymy relations']",5
"['the features proposed by  #TAUTHOR_TAG,']","['the features proposed by  #TAUTHOR_TAG,']","['the features proposed by  #TAUTHOR_TAG,']","['select proper hyponymy relations from the hrcs obtained in step 1 by using svms  #AUTHOR_TAG as a classifier.', 'in what follows, we briefly review the features proposed by  #TAUTHOR_TAG, and then explain the novel features introduced in this study.', 'we expect that the readers will refer to the literature  #TAUTHOR_TAG to see the effect of the features proposed by sumida and torisawa.', 'in the following explanation, we refer to the hypernym candidate or the hyponym candidate of each hrc as hypernym or hyponym']",5
"['created by  #TAUTHOR_TAG, when a hypernym / hyponym is included as an element of the attribute']","['created by  #TAUTHOR_TAG, when a hypernym / hyponym is included as an element of the attribute set,']","['an element in a feature vector, and the corresponding element is set to 1.', 'attr using the attribute set created by  #TAUTHOR_TAG, when a hypernym / hyponym is included as an element of the attribute set, we set a feature corresponding to the element to 1']","['', 'the last morpheme is mapped to the dimension that is different from that of the other morphemes.', 'exp the expression of a hypernym / hyponym itself is mapped to an element in a feature vector, and the corresponding element is set to 1.', 'attr using the attribute set created by  #TAUTHOR_TAG, when a hypernym / hyponym is included as an element of the attribute set, we set a feature corresponding to the element to 1.', 'layer each type of the marking items from which the hypernym / hyponym is extracted ( namely, headings, bulleted lists, ordered lists, or definition lists ) is mapped to an element of a feature vector, and the feature corresponding to the marking type for the hypernym / hyponym is set to 1.', 'in this study, we introduce the following three new features to improve the performance of the classifier.', '']",5
,,,,5
"['created by  #TAUTHOR_TAG, when a hypernym / hyponym is included as an element of the attribute']","['created by  #TAUTHOR_TAG, when a hypernym / hyponym is included as an element of the attribute set,']","['an element in a feature vector, and the corresponding element is set to 1.', 'attr using the attribute set created by  #TAUTHOR_TAG, when a hypernym / hyponym is included as an element of the attribute set, we set a feature corresponding to the element to 1']","['', 'the last morpheme is mapped to the dimension that is different from that of the other morphemes.', 'exp the expression of a hypernym / hyponym itself is mapped to an element in a feature vector, and the corresponding element is set to 1.', 'attr using the attribute set created by  #TAUTHOR_TAG, when a hypernym / hyponym is included as an element of the attribute set, we set a feature corresponding to the element to 1.', 'layer each type of the marking items from which the hypernym / hyponym is extracted ( namely, headings, bulleted lists, ordered lists, or definition lists ) is mapped to an element of a feature vector, and the feature corresponding to the marking type for the hypernym / hyponym is set to 1.', 'in this study, we introduce the following three new features to improve the performance of the classifier.', '']",3
"['4 words based on the experiments in  #TAUTHOR_TAG.', 'dps need']","['4 words based on the experiments in  #TAUTHOR_TAG.', 'dps need']","['4 words based on the experiments in  #TAUTHOR_TAG.', 'dps need an association measure a ( ·, · )']","['distributional profile ( dp ) of a word or phrase was first proposed in  #AUTHOR_TAG for smt.', 'given a word f, its distributional profile is :', 'v is the vocabulary and the surrounding words w i are taken from a monolingual corpus using a fixed window size.', 'we use a window size of 4 words based on the experiments in  #TAUTHOR_TAG.', 'dps need an association measure a ( ·, · ) to compute distances between potential paraphrases.', 'a comparison of different association measures appears in  #TAUTHOR_TAG and our preliminary experiments validated the choice of the same association measure as in these papers, namely pointwise mutual information  #AUTHOR_TAG ( pmi ).', 'for each potential context word w i :', 'to evaluate the similarity between two phrases we use cosine similarity.', 'the cosine coefficient of two phrases f 1 and f 2 is :', 'where v is the vocabulary.', 'note that in eqn.', ""( 2 ) w i's are the words that appear in the context of f 1 or f 2, otherwise the pmi values would be zero."", 'considering all possible candidate paraphrases is very expensive.', 'thus, we use the heuristic applied in previous works  #TAUTHOR_TAG to reduce the search space.', 'for each phrase we keep candidate paraphrases which appear in one of the surrounding context ( e. g. left right ) among all occurrences of the phrase']",5
"['4 words based on the experiments in  #TAUTHOR_TAG.', 'dps need']","['4 words based on the experiments in  #TAUTHOR_TAG.', 'dps need']","['4 words based on the experiments in  #TAUTHOR_TAG.', 'dps need an association measure a ( ·, · )']","['distributional profile ( dp ) of a word or phrase was first proposed in  #AUTHOR_TAG for smt.', 'given a word f, its distributional profile is :', 'v is the vocabulary and the surrounding words w i are taken from a monolingual corpus using a fixed window size.', 'we use a window size of 4 words based on the experiments in  #TAUTHOR_TAG.', 'dps need an association measure a ( ·, · ) to compute distances between potential paraphrases.', 'a comparison of different association measures appears in  #TAUTHOR_TAG and our preliminary experiments validated the choice of the same association measure as in these papers, namely pointwise mutual information  #AUTHOR_TAG ( pmi ).', 'for each potential context word w i :', 'to evaluate the similarity between two phrases we use cosine similarity.', 'the cosine coefficient of two phrases f 1 and f 2 is :', 'where v is the vocabulary.', 'note that in eqn.', ""( 2 ) w i's are the words that appear in the context of f 1 or f 2, otherwise the pmi values would be zero."", 'considering all possible candidate paraphrases is very expensive.', 'thus, we use the heuristic applied in previous works  #TAUTHOR_TAG to reduce the search space.', 'for each phrase we keep candidate paraphrases which appear in one of the surrounding context ( e. g. left right ) among all occurrences of the phrase']",5
['as in  #TAUTHOR_TAG'],['as in  #TAUTHOR_TAG'],"['oov phrases using the steps in algo.', '( 1 ) : 1 ) a graph of source phrases is constructed as in  #TAUTHOR_TAG']","['paraphrase extraction we have paraphrase pairs, ( f 1, f 2 ) and a score s ( f 1, f 2 ) we can induce new translation rules for oov phrases using the steps in algo.', '( 1 ) : 1 ) a graph of source phrases is constructed as in  #TAUTHOR_TAG ; 2 ) translations are propagated as labels through the graph as explained in fig. 2 ; and 3 ) new translation rules obtained from graph - propagation are integrated with the original phrase table']",5
"['used in  #TAUTHOR_TAG.', 'the structured label propagation algorithm ( slp ) was used in  #AUTHOR_TAG which uses a graph structure on the target side phrases as well.', 'however, we have found that in our diverse experimental settings ( see']","['used in  #TAUTHOR_TAG.', 'the structured label propagation algorithm ( slp ) was used in  #AUTHOR_TAG which uses a graph structure on the target side phrases as well.', 'however, we have found that in our diverse experimental settings ( see sec. 5 )']","['choosing mad over other graph propagation algorithms.', 'the mad graph propagation generalizes the approach used in  #TAUTHOR_TAG.', 'the structured label propagation algorithm ( slp ) was used in  #AUTHOR_TAG which uses a graph structure on the target side phrases as well.', 'however, we have found that in our diverse experimental settings ( see']","['', 'we describe in sec. 4. 2. 2 the reasons for choosing mad over other graph propagation algorithms.', 'the mad graph propagation generalizes the approach used in  #TAUTHOR_TAG.', 'the structured label propagation algorithm ( slp ) was used in  #AUTHOR_TAG which uses a graph structure on the target side phrases as well.', 'however, we have found that in our diverse experimental settings ( see sec. 5 ) mad had two properties we needed compared to slp : one was the use of graph random walks which allowed us to control translation candidates and mad also has the ability to penalize nodes with a large number of edges ( also see sec. 4. 2. 2 )']",5
"['tripartite results to  #TAUTHOR_TAG.', 'in the']","['tripartite results to  #TAUTHOR_TAG.', 'in the']","['tripartite results to  #TAUTHOR_TAG.', 'in the rest of the experiments we use the tripartite approach']","['avoid a fully connected graph structure.', 'they pre - structure the graph into bipartite graphs ( only connections between phrases with known translation and oov phrases ) and tripartite graphs ( connections can also go from a known phrasal node to an oov phrasal node through one node that is a paraphrase of both but does not have translations, i. e. it is an unlabeled node ).', 'in these pre - structured graphs there are no connections between nodes of the same type ( known, oov or unlabeled ).', 'we apply this method in our low resource setting experiments ( sec. 5. 3 ) to compare our bipartite and tripartite results to  #TAUTHOR_TAG.', 'in the rest of the experiments we use the tripartite approach since it outperforms the bipartite approach']",5
"['to fairly compare our approach with  #TAUTHOR_TAG on low resource setting, we follow their setup in']","['to fairly compare our approach with  #TAUTHOR_TAG on low resource setting, we follow their setup in sec. 5. 3 : moses  #AUTHOR_TAG as smt pipeline, giza + +  #AUTHOR_TAG']","['. 5 ).', 'but as we wish to fairly compare our approach with  #TAUTHOR_TAG on low resource setting, we follow their setup in']","['use cdec 1  #AUTHOR_TAG as an endto - end smt pipeline with its standard features 2. fast align  #AUTHOR_TAG is used for word alignment, and weights are tuned by minimizing bleu loss on the dev set using mira  #AUTHOR_TAG.', 'this setup is used for most of our experiments : oracle ( sec. 5. 2 ), domain adaptation ( sec. 5. 4 ) and morphologically complex languages ( sec. 5. 5 ).', 'but as we wish to fairly compare our approach with  #TAUTHOR_TAG on low resource setting, we follow their setup in sec. 5. 3 : moses  #AUTHOR_TAG as smt pipeline, giza + +  #AUTHOR_TAG for word alignment and mert  #AUTHOR_TAG for tuning.', 'we add our own feature to the smt log - linear model as described in sec. kenlm  #AUTHOR_TAG is used to train a 5 - gram language model on english gigaword ( v5 : ldc2011t07 ).', 'for scalable graph propagation we use the junto framework 3.', 'we use maximum phrase length 10.', 'for our experiments we use the hadoop distributed computing framework executed on a cluster with 12 nodes ( each node has 8 cores and 16gb of ram ).', 'each graph propagation iteration takes about 3 minutes.', 'for french, we apply a simple heuristic to detect named entities : words that are capitalized in the original dev / test set that do not appear at the beginning of a sentence are named entities.', 'based on eyeballing the results, this works very well in our data.', 'for arabic, aqmar is used to exclude named - entities  #AUTHOR_TAG.', 'for each of the experimental settings below we show the oov statistics in table 2']",5
"['to train translation system, as reported in  #TAUTHOR_TAG.', 'acl / wmt']","['to train translation system, as reported in  #TAUTHOR_TAG.', 'acl / wmt']","['to train translation system, as reported in  #TAUTHOR_TAG.', 'acl / wmt 2005 4 is used for dev and test data.', 'we re - implement their paraphr']","['this experiment we use a setup similar to  #AUTHOR_TAG we use 10k french - english parallel sentences, randomly chosen from europarl to train translation system, as reported in  #TAUTHOR_TAG.', 'acl / wmt 2005 4 is used for dev and test data.', 'we re - implement their paraphrase extraction method ( dp ) to extract paraphrases from french side of europarl ( 2m sentences ).', 'we use unigram nodes to construct graphs for both dp and ppdb.', 'in bipartite graphs, each node is connected to at most 20 nodes.', 'for tripartite graphs, each node is connected to 15 labeled and 5 unlabeled nodes.', 'for intrinsic evaluation, we use meanreciprocal - rank ( mrr ) and recall.', 'mrr is the mean of reciprocal rank of the candidate list compared to the gold list ( eqn. 5 ).', 'recall shows percentage of gold list covered by the candidate list ( eqn. 6 ).', 'gold translations for oovs are given by concatenating the test data to training and running a word aligner.', 'to show how well our ppdb approach does compared to the dp approach in terms of mrr and recall ; and 3 ) to show applicability of our approach for a low - resource language.', 'however we used french instead of a language which is truly resource - poor due to the lack of available paraphrases for a true resource poor language, e. g. malagasy']",5
"['4 words based on the experiments in  #TAUTHOR_TAG.', 'dps need']","['4 words based on the experiments in  #TAUTHOR_TAG.', 'dps need']","['4 words based on the experiments in  #TAUTHOR_TAG.', 'dps need an association measure a ( ·, · )']","['distributional profile ( dp ) of a word or phrase was first proposed in  #AUTHOR_TAG for smt.', 'given a word f, its distributional profile is :', 'v is the vocabulary and the surrounding words w i are taken from a monolingual corpus using a fixed window size.', 'we use a window size of 4 words based on the experiments in  #TAUTHOR_TAG.', 'dps need an association measure a ( ·, · ) to compute distances between potential paraphrases.', 'a comparison of different association measures appears in  #TAUTHOR_TAG and our preliminary experiments validated the choice of the same association measure as in these papers, namely pointwise mutual information  #AUTHOR_TAG ( pmi ).', 'for each potential context word w i :', 'to evaluate the similarity between two phrases we use cosine similarity.', 'the cosine coefficient of two phrases f 1 and f 2 is :', 'where v is the vocabulary.', 'note that in eqn.', ""( 2 ) w i's are the words that appear in the context of f 1 or f 2, otherwise the pmi values would be zero."", 'considering all possible candidate paraphrases is very expensive.', 'thus, we use the heuristic applied in previous works  #TAUTHOR_TAG to reduce the search space.', 'for each phrase we keep candidate paraphrases which appear in one of the surrounding context ( e. g. left right ) among all occurrences of the phrase']",3
"['', 'recently,  #TAUTHOR_TAG introduced an embedding alignment approach']","['without substantial retraining.', 'recently,  #TAUTHOR_TAG introduced an embedding alignment approach']","['without substantial retraining.', 'recently,  #TAUTHOR_TAG introduced an embedding alignment approach']","['', 'recently,  #TAUTHOR_TAG introduced an embedding alignment approach to enable continual learning for relation extraction models.', '']",6
"['', 'recently,  #TAUTHOR_TAG introduced an embedding alignment approach']","['without substantial retraining.', 'recently,  #TAUTHOR_TAG introduced an embedding alignment approach']","['without substantial retraining.', 'recently,  #TAUTHOR_TAG introduced an embedding alignment approach']","['', 'recently,  #TAUTHOR_TAG introduced an embedding alignment approach to enable continual learning for relation extraction models.', '']",6
"['principle the learner model f θ could be any gradient - optimized relation extraction model.', 'in order to use the same number of parameters and ensure fair comparison to  #TAUTHOR_TAG for their experiments.', 'the hr - bilstm is a relation']","['principle the learner model f θ could be any gradient - optimized relation extraction model.', 'in order to use the same number of parameters and ensure fair comparison to  #TAUTHOR_TAG for their experiments.', 'the hr - bilstm is a relation']","['principle the learner model f θ could be any gradient - optimized relation extraction model.', 'in order to use the same number of parameters and ensure fair comparison to  #TAUTHOR_TAG for their experiments.', 'the hr - bilstm is a relation classifier']","['principle the learner model f θ could be any gradient - optimized relation extraction model.', 'in order to use the same number of parameters and ensure fair comparison to  #TAUTHOR_TAG for their experiments.', 'the hr - bilstm is a relation classifier which accepts as input a sentence and a candidate relation, then utilizes two bidirectional long short - term memory  #AUTHOR_TAG ( bilstm ) units with shared parameters to process the glove  #AUTHOR_TAG embeddings of words in the sentence and relation names, then selects the relation with the maximum cosine similarity to the sentence as its response.', 'hyperparameters apart from the hyperparameters specific to meta - learning ( such as the step size ), all other hyperparameters we use for the learner model are the same as used by  #TAUTHOR_TAG.', 'we also use the same buffer memory size ( 50 ) for each task.', 'note that the meta - learning algorithm uses sgd as the update rule ( u ), and does not add any additional trainable parameters to the learner model']",5
"['principle the learner model f θ could be any gradient - optimized relation extraction model.', 'in order to use the same number of parameters and ensure fair comparison to  #TAUTHOR_TAG for their experiments.', 'the hr - bilstm is a relation']","['principle the learner model f θ could be any gradient - optimized relation extraction model.', 'in order to use the same number of parameters and ensure fair comparison to  #TAUTHOR_TAG for their experiments.', 'the hr - bilstm is a relation']","['principle the learner model f θ could be any gradient - optimized relation extraction model.', 'in order to use the same number of parameters and ensure fair comparison to  #TAUTHOR_TAG for their experiments.', 'the hr - bilstm is a relation classifier']","['principle the learner model f θ could be any gradient - optimized relation extraction model.', 'in order to use the same number of parameters and ensure fair comparison to  #TAUTHOR_TAG for their experiments.', 'the hr - bilstm is a relation classifier which accepts as input a sentence and a candidate relation, then utilizes two bidirectional long short - term memory  #AUTHOR_TAG ( bilstm ) units with shared parameters to process the glove  #AUTHOR_TAG embeddings of words in the sentence and relation names, then selects the relation with the maximum cosine similarity to the sentence as its response.', 'hyperparameters apart from the hyperparameters specific to meta - learning ( such as the step size ), all other hyperparameters we use for the learner model are the same as used by  #TAUTHOR_TAG.', 'we also use the same buffer memory size ( 50 ) for each task.', 'note that the meta - learning algorithm uses sgd as the update rule ( u ), and does not add any additional trainable parameters to the learner model']",5
"['conduct experiments on  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is derived from the fewrel  #AUTHOR_TAG dataset, by partitioning its 80 relations into']","['conduct experiments on  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is derived from the fewrel  #AUTHOR_TAG dataset, by partitioning its 80 relations into']","['conduct experiments on  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is derived from the fewrel  #AUTHOR_TAG dataset, by partitioning its 80 relations into 10 distinct clusters']","['conduct experiments on  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is derived from the fewrel  #AUTHOR_TAG dataset, by partitioning its 80 relations into 10 distinct clusters made up of 8 relations each, with each cluster serving as a task where a sentence must be labeled with the correct relation.', 'the 8 relations in each cluster were obtained by clustering the averaged glove word embeddings of the relation names in the fewrel dataset.', 'each instance of the dataset contains a sentence, the relation it expresses and a set of randomly sampled negative relations.', 'lifelong simplequestions was similarly obtained from the simplequestions  #AUTHOR_TAG dataset, and is made up of 20 clusters of relations, with each cluster serving as a task']",5
"['1 gives both the  #TAUTHOR_TAG.', 'across all metrics, our']","['1 gives both the  #TAUTHOR_TAG.', 'across all metrics, our']","['supervision results table 1 gives both the  #TAUTHOR_TAG.', 'across all metrics, our']","['supervision results table 1 gives both the  #TAUTHOR_TAG.', 'across all metrics, our approach outperforms the previous approaches, demonstrating its effectiveness in this setting.', 'this result is likely because our approach is able to efficiently learn new relations by exploiting knowledge from previously observed relations']",5
['by  #TAUTHOR_TAG to using'],['by  #TAUTHOR_TAG to using'],['by  #TAUTHOR_TAG to using'],"['aim of our limited supervision experiments is to compare the use of an alignment module as proposed by  #TAUTHOR_TAG to using our approach when only limited supervision is available for all tasks.', 'we compare three approaches, full ea - emr ( which uses their alignment module ), its variant without the alignment module ( ea - emr noalign ) and our approach ( mllre ).', 'figures 1 ( a ) and 1 ( b ) show results obtained using 100 supervision instances for each task on lifelong fewrel and lifelong simplequestions.', 'figures 2 ( a ) and 2 ( b ) show the corresponding plots using 200 supervision instances for each task.', 'from the figures, we observe that the use of a separate alignment model results in only minor gains when supervision for the tasks is limited, whereas the use of our approach leads to wide gains on both datasets.', 'in summary, because our approach explicitly encourages the model to learn to share and transfer knowledge between relations ( by means of the meta - learning objective ), the model is able to learn to exploit common structures across relations in different tasks to efficiently learn new relations over time.', 'this leads to the performance improvements obtained by our approach']",5
['performs better on predicting a coarser level of topic segmentation'],['performs better on predicting a coarser level of topic segmentation'],"['has explored the eect of lexical cohesion and conversational features on characterizing topic boundaries, following  #AUTHOR_TAG.', 'in previous work, we have also studied the problem of predicting topic boundaries at dierent levels of granularity and showed that a supervised classication approach performs better on predicting a coarser level of topic segmentation']","['dialogue act has its constituent words scored using tf. idf, and as the user compresses the meeting to a greater degree the browser gradually removes the less important words from each dialogue act, leaving only the most informative material of the meeting.', 'previous work has explored the eect of lexical cohesion and conversational features on characterizing topic boundaries, following  #AUTHOR_TAG.', 'in previous work, we have also studied the problem of predicting topic boundaries at dierent levels of granularity and showed that a supervised classication approach performs better on predicting a coarser level of topic segmentation']",0
['association of spoken words with sensorimotor experience  #TAUTHOR_TAG 11 ] or sensorimotor representations'],"['association of spoken words with sensorimotor experience  #TAUTHOR_TAG 11 ] or sensorimotor representations [ 12 ], learning tool use capabilities [ 13, 14 ],']",['of spoken words with sensorimotor experience  #TAUTHOR_TAG 11 ] or sensorimotor representations'],"['', 'in addition, several works have demonstrated how combining robot affordance learning with language grounding can provide cognitive robots with new and useful skills, such as learning the association of spoken words with sensorimotor experience  #TAUTHOR_TAG 11 ] or sensorimotor representations [ 12 ], learning tool use capabilities [ 13, 14 ], and carrying out complex manipulation tasks expressed in natural language instructions which require planning and reasoning [ 15 ].', '']",0
['association of spoken words with sensorimotor experience  #TAUTHOR_TAG 11 ] or sensorimotor representations'],"['association of spoken words with sensorimotor experience  #TAUTHOR_TAG 11 ] or sensorimotor representations [ 12 ], learning tool use capabilities [ 13, 14 ],']",['of spoken words with sensorimotor experience  #TAUTHOR_TAG 11 ] or sensorimotor representations'],"['', 'in addition, several works have demonstrated how combining robot affordance learning with language grounding can provide cognitive robots with new and useful skills, such as learning the association of spoken words with sensorimotor experience  #TAUTHOR_TAG 11 ] or sensorimotor representations [ 12 ], learning tool use capabilities [ 13, 14 ], and carrying out complex manipulation tasks expressed in natural language instructions which require planning and reasoning [ 15 ].', '']",0
"['in  #TAUTHOR_TAG, we use a bayesian probabilistic']","['in  #TAUTHOR_TAG, we use a bayesian probabilistic']","['in  #TAUTHOR_TAG, we use a bayesian probabilistic framework to allow a robot to ground the basic world behavior and verbal descriptions']","['the method adopted in  #TAUTHOR_TAG, we use a bayesian probabilistic framework to allow a robot to ground the basic world behavior and verbal descriptions associated to it.', 'the world behavior is defined by random variables describing : the actions a, defined over the set a = { ai }, object properties f, over f = { fi }, and effects e, over e = { ei }. we denote x = { a, f, e } the state of the world as experienced by the robot.', 'the verbal descriptions are denoted by the set of words w = { wi }. consequently, the relationships between words and concepts are expressed by the joint probability distribution p ( x, w ) of actions, object features, effects, and words in the spoken utterance.', 'the symbolic variables and their discrete values are listed in table 1.', 'in addition to the symbolic variables, the model also includes word variables, describing figure 3 : structure of the hmms used for human gesture recognition, adapted from [ 4 ].', 'in this work, we consider three independent, multiple - state hmms, each of them trained to recognize one of the considered manipulation gestures.', 'the probability of each word co - occurring in the verbal description associated to a robot experiment in the environment.', 'this joint probability distribution, that is illustrated by the part of fig. 2 enclosed in the dashed box, is estimated by the robot in an ego - centric way through interaction with the environment, as in  #TAUTHOR_TAG.', 'as a consequence, during learning, the robot knows what action it is performing with certainty, and the variable a assumes a deterministic value.', 'this assumption is relaxed in the present study, by extending the model to the observation of external ( human ) agents as explained below']",0
"['this study we wish to generalize the model of  #TAUTHOR_TAG by observing external ( human ) agents, as shown in fig. 1.', '']","['this study we wish to generalize the model of  #TAUTHOR_TAG by observing external ( human ) agents, as shown in fig. 1.', '']","['this study we wish to generalize the model of  #TAUTHOR_TAG by observing external ( human ) agents, as shown in fig. 1.', '']","['this study we wish to generalize the model of  #TAUTHOR_TAG by observing external ( human ) agents, as shown in fig. 1.', 'for this reason, the full model is now extended with a perception module capable of inferring the action of the agent from visual inputs.', 'this corresponds to the gesture hmms block in fig. 2.', 'the affordance - words bayesian network ( bn ) model and the gestures hmms may be combined in different ways [ 19 ] :', '1. the gesture hmms may provide a hard decision on the action performed by the human ( i. e., considering only the top result ) to the bn, 2.', 'the gesture hmms may provide a posterior distribution ( i. e., soft decision ) to the bn, 3. if the task is to infer the action, the posterior from the gesture hmms and the one from the bn may be combined as follows, assuming that they provide independent information :', 'in the experimental section, we will show that what the robot has learned subjectively or alone ( by self - exploration, knowing the action identity as a prior  #TAUTHOR_TAG, can subsequently be used when observing a new agent ( human ), provided that the actions can be estimated with gesture hmms as in [ 4 ]']",0
"['model of  #TAUTHOR_TAG,']","['robot affordance model of  #TAUTHOR_TAG,']","['this paper, we combine ( 1 ) the robot affordance model of  #TAUTHOR_TAG,']","['this paper, we combine ( 1 ) the robot affordance model of  #TAUTHOR_TAG, which associates verbal descriptions to the physical interactions of an agent with the environment, with ( 2 ) the gesture recognition system of [ 4 ], which infers the type of action from human user movements.', 'we consider three manipulative gestures corresponding to physical actions performed by agent ( s ) onto objects on a table ( see fig. 1 ) : grasp, tap, and touch.', 'we reason on the effects of these actions onto the objects of the world, and on the co - occurring verbal description of the experiments.', 'in the complete framework, we will use bayesian networks ( bns ), which are a probabilistic model that represents random variables and conditional dependencies on a graph, such as in fig. 2.', 'one of the advantages of using bns is that their expressive power allows the marginalization over any set of variables given any other set of variables.', 'our main contribution is that of extending  #TAUTHOR_TAG by relaxing the assumption that the action is known during the learning phase.', 'this assumption is acceptable when the robot learns through self - exploration and interaction with the environment, but must be relaxed if the robot needs to generalize the acquired knowledge through the observation of another ( human ) agent.', 'we estimate the action performed by a human user during a human - robot collaborative task, by employing statistical inference methods and hidden markov models ( hmms ).', 'this provides two advantages.', 'first, we can infer the executed action during training.', 'secondly, at testing time we can merge the action information obtained from gesture recognition with the information about affordances']",4
"['model of  #TAUTHOR_TAG,']","['robot affordance model of  #TAUTHOR_TAG,']","['this paper, we combine ( 1 ) the robot affordance model of  #TAUTHOR_TAG,']","['this paper, we combine ( 1 ) the robot affordance model of  #TAUTHOR_TAG, which associates verbal descriptions to the physical interactions of an agent with the environment, with ( 2 ) the gesture recognition system of [ 4 ], which infers the type of action from human user movements.', 'we consider three manipulative gestures corresponding to physical actions performed by agent ( s ) onto objects on a table ( see fig. 1 ) : grasp, tap, and touch.', 'we reason on the effects of these actions onto the objects of the world, and on the co - occurring verbal description of the experiments.', 'in the complete framework, we will use bayesian networks ( bns ), which are a probabilistic model that represents random variables and conditional dependencies on a graph, such as in fig. 2.', 'one of the advantages of using bns is that their expressive power allows the marginalization over any set of variables given any other set of variables.', 'our main contribution is that of extending  #TAUTHOR_TAG by relaxing the assumption that the action is known during the learning phase.', 'this assumption is acceptable when the robot learns through self - exploration and interaction with the environment, but must be relaxed if the robot needs to generalize the acquired knowledge through the observation of another ( human ) agent.', 'we estimate the action performed by a human user during a human - robot collaborative task, by employing statistical inference methods and hidden markov models ( hmms ).', 'this provides two advantages.', 'first, we can infer the executed action during training.', 'secondly, at testing time we can merge the action information obtained from gesture recognition with the information about affordances']",4
"['model of  #TAUTHOR_TAG,']","['robot affordance model of  #TAUTHOR_TAG,']","['this paper, we combine ( 1 ) the robot affordance model of  #TAUTHOR_TAG,']","['this paper, we combine ( 1 ) the robot affordance model of  #TAUTHOR_TAG, which associates verbal descriptions to the physical interactions of an agent with the environment, with ( 2 ) the gesture recognition system of [ 4 ], which infers the type of action from human user movements.', 'we consider three manipulative gestures corresponding to physical actions performed by agent ( s ) onto objects on a table ( see fig. 1 ) : grasp, tap, and touch.', 'we reason on the effects of these actions onto the objects of the world, and on the co - occurring verbal description of the experiments.', 'in the complete framework, we will use bayesian networks ( bns ), which are a probabilistic model that represents random variables and conditional dependencies on a graph, such as in fig. 2.', 'one of the advantages of using bns is that their expressive power allows the marginalization over any set of variables given any other set of variables.', 'our main contribution is that of extending  #TAUTHOR_TAG by relaxing the assumption that the action is known during the learning phase.', 'this assumption is acceptable when the robot learns through self - exploration and interaction with the environment, but must be relaxed if the robot needs to generalize the acquired knowledge through the observation of another ( human ) agent.', 'we estimate the action performed by a human user during a human - robot collaborative task, by employing statistical inference methods and hidden markov models ( hmms ).', 'this provides two advantages.', 'first, we can infer the executed action during training.', 'secondly, at testing time we can merge the action information obtained from gesture recognition with the information about affordances']",6
"['model of  #TAUTHOR_TAG,']","['robot affordance model of  #TAUTHOR_TAG,']","['this paper, we combine ( 1 ) the robot affordance model of  #TAUTHOR_TAG,']","['this paper, we combine ( 1 ) the robot affordance model of  #TAUTHOR_TAG, which associates verbal descriptions to the physical interactions of an agent with the environment, with ( 2 ) the gesture recognition system of [ 4 ], which infers the type of action from human user movements.', 'we consider three manipulative gestures corresponding to physical actions performed by agent ( s ) onto objects on a table ( see fig. 1 ) : grasp, tap, and touch.', 'we reason on the effects of these actions onto the objects of the world, and on the co - occurring verbal description of the experiments.', 'in the complete framework, we will use bayesian networks ( bns ), which are a probabilistic model that represents random variables and conditional dependencies on a graph, such as in fig. 2.', 'one of the advantages of using bns is that their expressive power allows the marginalization over any set of variables given any other set of variables.', 'our main contribution is that of extending  #TAUTHOR_TAG by relaxing the assumption that the action is known during the learning phase.', 'this assumption is acceptable when the robot learns through self - exploration and interaction with the environment, but must be relaxed if the robot needs to generalize the acquired knowledge through the observation of another ( human ) agent.', 'we estimate the action performed by a human user during a human - robot collaborative task, by employing statistical inference methods and hidden markov models ( hmms ).', 'this provides two advantages.', 'first, we can infer the executed action during training.', 'secondly, at testing time we can merge the action information obtained from gesture recognition with the information about affordances']",6
"['this study we wish to generalize the model of  #TAUTHOR_TAG by observing external ( human ) agents, as shown in fig. 1.', '']","['this study we wish to generalize the model of  #TAUTHOR_TAG by observing external ( human ) agents, as shown in fig. 1.', '']","['this study we wish to generalize the model of  #TAUTHOR_TAG by observing external ( human ) agents, as shown in fig. 1.', '']","['this study we wish to generalize the model of  #TAUTHOR_TAG by observing external ( human ) agents, as shown in fig. 1.', 'for this reason, the full model is now extended with a perception module capable of inferring the action of the agent from visual inputs.', 'this corresponds to the gesture hmms block in fig. 2.', 'the affordance - words bayesian network ( bn ) model and the gestures hmms may be combined in different ways [ 19 ] :', '1. the gesture hmms may provide a hard decision on the action performed by the human ( i. e., considering only the top result ) to the bn, 2.', 'the gesture hmms may provide a posterior distribution ( i. e., soft decision ) to the bn, 3. if the task is to infer the action, the posterior from the gesture hmms and the one from the bn may be combined as follows, assuming that they provide independent information :', 'in the experimental section, we will show that what the robot has learned subjectively or alone ( by self - exploration, knowing the action identity as a prior  #TAUTHOR_TAG, can subsequently be used when observing a new agent ( human ), provided that the actions can be estimated with gesture hmms as in [ 4 ]']",6
"['in  #TAUTHOR_TAG, we use a bayesian probabilistic']","['in  #TAUTHOR_TAG, we use a bayesian probabilistic']","['in  #TAUTHOR_TAG, we use a bayesian probabilistic framework to allow a robot to ground the basic world behavior and verbal descriptions']","['the method adopted in  #TAUTHOR_TAG, we use a bayesian probabilistic framework to allow a robot to ground the basic world behavior and verbal descriptions associated to it.', 'the world behavior is defined by random variables describing : the actions a, defined over the set a = { ai }, object properties f, over f = { fi }, and effects e, over e = { ei }. we denote x = { a, f, e } the state of the world as experienced by the robot.', 'the verbal descriptions are denoted by the set of words w = { wi }. consequently, the relationships between words and concepts are expressed by the joint probability distribution p ( x, w ) of actions, object features, effects, and words in the spoken utterance.', 'the symbolic variables and their discrete values are listed in table 1.', 'in addition to the symbolic variables, the model also includes word variables, describing figure 3 : structure of the hmms used for human gesture recognition, adapted from [ 4 ].', 'in this work, we consider three independent, multiple - state hmms, each of them trained to recognize one of the considered manipulation gestures.', 'the probability of each word co - occurring in the verbal description associated to a robot experiment in the environment.', 'this joint probability distribution, that is illustrated by the part of fig. 2 enclosed in the dashed box, is estimated by the robot in an ego - centric way through interaction with the environment, as in  #TAUTHOR_TAG.', 'as a consequence, during learning, the robot knows what action it is performing with certainty, and the variable a assumes a deterministic value.', 'this assumption is relaxed in the present study, by extending the model to the observation of external ( human ) agents as explained below']",3
"['in  #TAUTHOR_TAG, we use a bayesian probabilistic']","['in  #TAUTHOR_TAG, we use a bayesian probabilistic']","['in  #TAUTHOR_TAG, we use a bayesian probabilistic framework to allow a robot to ground the basic world behavior and verbal descriptions']","['the method adopted in  #TAUTHOR_TAG, we use a bayesian probabilistic framework to allow a robot to ground the basic world behavior and verbal descriptions associated to it.', 'the world behavior is defined by random variables describing : the actions a, defined over the set a = { ai }, object properties f, over f = { fi }, and effects e, over e = { ei }. we denote x = { a, f, e } the state of the world as experienced by the robot.', 'the verbal descriptions are denoted by the set of words w = { wi }. consequently, the relationships between words and concepts are expressed by the joint probability distribution p ( x, w ) of actions, object features, effects, and words in the spoken utterance.', 'the symbolic variables and their discrete values are listed in table 1.', 'in addition to the symbolic variables, the model also includes word variables, describing figure 3 : structure of the hmms used for human gesture recognition, adapted from [ 4 ].', 'in this work, we consider three independent, multiple - state hmms, each of them trained to recognize one of the considered manipulation gestures.', 'the probability of each word co - occurring in the verbal description associated to a robot experiment in the environment.', 'this joint probability distribution, that is illustrated by the part of fig. 2 enclosed in the dashed box, is estimated by the robot in an ego - centric way through interaction with the environment, as in  #TAUTHOR_TAG.', 'as a consequence, during learning, the robot knows what action it is performing with certainty, and the variable a assumes a deterministic value.', 'this assumption is relaxed in the present study, by extending the model to the observation of external ( human ) agents as explained below']",5
[' #TAUTHOR_TAG contain'],['on distant supervision  #TAUTHOR_TAG contain'],"['on distant supervision  #TAUTHOR_TAG contain about 2m documents, we run', 'experiments on a 100m - document ( 50x more ) corpus drawn']","['with input data sets that are orders of magnitude larger than those in prior work. while the largest corpus ( wikipedia and new york times ) employed', 'by recent work on distant supervision  #TAUTHOR_TAG contain about 2m documents, we run', 'experiments on a 100m - document ( 50x more ) corpus drawn from clueweb. 1 while prior work  #AUTHOR_TAG on crowdsourcing for distant supervision', 'used thousands of human feedback units, we acquire tens of thousands of human - provided labels. despite the', 'large scale, we follow state - of - the - art distant supervision approaches and use deep linguistic features, e. g., part - of - speech tags and dependency parsing. 2 our experiments shed insight on the following two questions : 1. how does increasing the corpus', '']",1
[' #TAUTHOR_TAG contain'],['on distant supervision  #TAUTHOR_TAG contain'],"['on distant supervision  #TAUTHOR_TAG contain about 2m documents, we run', 'experiments on a 100m - document ( 50x more ) corpus drawn']","['with input data sets that are orders of magnitude larger than those in prior work. while the largest corpus ( wikipedia and new york times ) employed', 'by recent work on distant supervision  #TAUTHOR_TAG contain about 2m documents, we run', 'experiments on a 100m - document ( 50x more ) corpus drawn from clueweb. 1 while prior work  #AUTHOR_TAG on crowdsourcing for distant supervision', 'used thousands of human feedback units, we acquire tens of thousands of human - provided labels. despite the', 'large scale, we follow state - of - the - art distant supervision approaches and use deep linguistic features, e. g., part - of - speech tags and dependency parsing. 2 our experiments shed insight on the following two questions : 1. how does increasing the corpus', '']",0
[' #TAUTHOR_TAG'],['as in a question. since  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG'],['as in a question. since  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],3
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['', 'following recent work  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],3
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['', 'following recent work  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],3
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['', 'following recent work  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],3
"['on distant supervision  #TAUTHOR_TAG,']","['on distant supervision  #TAUTHOR_TAG,']","['string features.', 'following recent work on distant supervision  #TAUTHOR_TAG, we use both lexical and syntactic features.', 'after this stage, we have a well - defined machine learning problem that is solvable using standard supervised techniques']","['we have constructed the set of possible mention pairs, the state - of - the - art technique to generate feature vectors uses linguistic tools such as partof - speech taggers, named - entity recognizers, dependency parsers, and string features.', 'following recent work on distant supervision  #TAUTHOR_TAG, we use both lexical and syntactic features.', 'after this stage, we have a well - defined machine learning problem that is solvable using standard supervised techniques.', 'we use sparse logistic regression ( 1 regularized )  #AUTHOR_TAG, which is used in previous studies.', 'our feature extraction process consists of three steps :', '']",3
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['', 'following recent work  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],5
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['', 'following recent work  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],5
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['', 'following recent work  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],5
"['on distant supervision  #TAUTHOR_TAG,']","['on distant supervision  #TAUTHOR_TAG,']","['string features.', 'following recent work on distant supervision  #TAUTHOR_TAG, we use both lexical and syntactic features.', 'after this stage, we have a well - defined machine learning problem that is solvable using standard supervised techniques']","['we have constructed the set of possible mention pairs, the state - of - the - art technique to generate feature vectors uses linguistic tools such as partof - speech taggers, named - entity recognizers, dependency parsers, and string features.', 'following recent work on distant supervision  #TAUTHOR_TAG, we use both lexical and syntactic features.', 'after this stage, we have a well - defined machine learning problem that is solvable using standard supervised techniques.', 'we use sparse logistic regression ( 1 regularized )  #AUTHOR_TAG, which is used in previous studies.', 'our feature extraction process consists of three steps :', '']",5
"['', 'interestingly, the freebase held - out metric  #TAUTHOR_TAG turns out to be']","['set of 100 entities.', 'interestingly, the freebase held - out metric  #TAUTHOR_TAG turns out to be']","['set of 100 entities.', 'interestingly, the freebase held - out metric  #TAUTHOR_TAG turns out to be heavily']","['as direct training data are scarce, ground truth for relation extraction is scarce as well.', 'as a result, prior work mainly considers two types of evaluation methods : ( 1 ) randomly sample a small portion of predictions ( e. g., top - k ) and manually evaluate precision / recall ; and ( 2 ) use a held - out portion of seed facts ( usually freebase ) as a kind of "" distant "" ground truth.', 'we replace manual evaluation with a standardized relation - extraction benchmark : tac - kbp 2010.', 'tac - kbp asks for extractions of 46 relations on a given set of 100 entities.', 'interestingly, the freebase held - out metric  #TAUTHOR_TAG turns out to be heavily biased toward distantly labeled data ( e. g., increasing human feedback hurts precision ; see section 4. 6 )']",4
"['addition to the tac - kbp benchmark, we also follow prior work  #TAUTHOR_TAG and measure the quality using held - out data']","['addition to the tac - kbp benchmark, we also follow prior work  #TAUTHOR_TAG and measure the quality using held - out data']","['addition to the tac - kbp benchmark, we also follow prior work  #TAUTHOR_TAG and measure the quality using held - out data']","['addition to the tac - kbp benchmark, we also follow prior work  #TAUTHOR_TAG and measure the quality using held - out data from freebase.', 'we randomly partition both freebase and the corpus into two halves.', 'one database - corpus pair is used for training and the other pair for testing.', 'we evaluate the precision over the 10 3 highest - probability predictions on the test set.', 'in figure 5, we vary the size of the corpus in the train pair and the number of human labels ; the precision reaches a dramatic peak when we the corpus size is above 10 5 and uses little human feedback.', 'this suggests that this freebase held - out metric is biased toward solely relying on distant labels alone']",4
"[')  #TAUTHOR_TAG.', 'as in topic']","['( abae )  #TAUTHOR_TAG.', 'as in topic']","[')  #TAUTHOR_TAG.', 'as in topic modeling']",[' #TAUTHOR_TAG'],5
['model  #TAUTHOR_TAG.'],['model  #TAUTHOR_TAG.'],"['of', 'the basic abae model  #TAUTHOR_TAG.']","['##ley del prolific', 'ti festivals 3 brisk dialouge manipulation snappy plotlines dialogues taunt camerawork muddled 4 sock vegans', ""peanut stifling bats buh ammonium trollstench vegetables pepsi 5 the a and to is of joe's enters that fatal table 4 : sample aspects from instant videos discovered by aspera ( glove ). # aspect words 1 protein diagnose cell genes brain membrane interacts interact oxygen spinal"", '2 boost monetary raise introduce measures credit expects increase push demand 3 towel soaked greasy towels cloth dripping tucked crisp coat buckets 4 offbeat comic parody spoof comedic qui', ""##rky cinematic campy parodies animated 5 sheesh wham whew hurrah oops yikes c'mon shhh oooh"", 'och amazon instant videos 5 - core reviews with the ratio 80 : 10 : 10. we also used the', 'results of narre model [ 4 ], obtained in the same setup as [ 5 ] but with a different random', 'seed. note that while aspera with generic glove word embeddings still works better than any other model, adding custom word embeddings trained on the same type of texts improves the results greatly. topic quality', 'we compared the performance of aspera with onlinelda [ 10 ] trained with the gensim library [ 31 ], with', 'the same vocabulary and number of topics, and abae with 10 aspects and 18 epochs, initialized with the same word2vec vectors ( sgns ) as aspera and having', 'the same ortho - regularization coefficient as the best aspera model, evaluating the results in terms of topic coherence metrics, npmi [ 2 ] and pmi [ 25, 26 ] computed with companion software for [ 15 ]. figure 2 shows that', 'the quality is', 'generally lower for larger number of representative words per aspect ( horizontal axis ), and that aspera achieves scores comparable to lda and abae, although abae remains ahead. tables 3 and 4 present several sample aspects discovered by aspera. qualitative analysis shows that some aspects describe what could be called a topic ( a set of words diverse by part of speech and', 'function describing a certain domain ), some encode sentiment ( top words are adjectives showing attitude to certain objects', 'discussed in the text ), and some encode names ( actors, directors, etc. ). we also found similar patterns in the output of', 'the basic abae model  #TAUTHOR_TAG. thus, most aspects are clearly coherent, but there is room for improvement']",5
['model  #TAUTHOR_TAG.'],['model  #TAUTHOR_TAG.'],"['of', 'the basic abae model  #TAUTHOR_TAG.']","['##ley del prolific', 'ti festivals 3 brisk dialouge manipulation snappy plotlines dialogues taunt camerawork muddled 4 sock vegans', ""peanut stifling bats buh ammonium trollstench vegetables pepsi 5 the a and to is of joe's enters that fatal table 4 : sample aspects from instant videos discovered by aspera ( glove ). # aspect words 1 protein diagnose cell genes brain membrane interacts interact oxygen spinal"", '2 boost monetary raise introduce measures credit expects increase push demand 3 towel soaked greasy towels cloth dripping tucked crisp coat buckets 4 offbeat comic parody spoof comedic qui', ""##rky cinematic campy parodies animated 5 sheesh wham whew hurrah oops yikes c'mon shhh oooh"", 'och amazon instant videos 5 - core reviews with the ratio 80 : 10 : 10. we also used the', 'results of narre model [ 4 ], obtained in the same setup as [ 5 ] but with a different random', 'seed. note that while aspera with generic glove word embeddings still works better than any other model, adding custom word embeddings trained on the same type of texts improves the results greatly. topic quality', 'we compared the performance of aspera with onlinelda [ 10 ] trained with the gensim library [ 31 ], with', 'the same vocabulary and number of topics, and abae with 10 aspects and 18 epochs, initialized with the same word2vec vectors ( sgns ) as aspera and having', 'the same ortho - regularization coefficient as the best aspera model, evaluating the results in terms of topic coherence metrics, npmi [ 2 ] and pmi [ 25, 26 ] computed with companion software for [ 15 ]. figure 2 shows that', 'the quality is', 'generally lower for larger number of representative words per aspect ( horizontal axis ), and that aspera achieves scores comparable to lda and abae, although abae remains ahead. tables 3 and 4 present several sample aspects discovered by aspera. qualitative analysis shows that some aspects describe what could be called a topic ( a set of words diverse by part of speech and', 'function describing a certain domain ), some encode sentiment ( top words are adjectives showing attitude to certain objects', 'discussed in the text ), and some encode names ( actors, directors, etc. ). we also found similar patterns in the output of', 'the basic abae model  #TAUTHOR_TAG. thus, most aspects are clearly coherent, but there is room for improvement']",3
['model  #TAUTHOR_TAG.'],['model  #TAUTHOR_TAG.'],"['of', 'the basic abae model  #TAUTHOR_TAG.']","['##ley del prolific', 'ti festivals 3 brisk dialouge manipulation snappy plotlines dialogues taunt camerawork muddled 4 sock vegans', ""peanut stifling bats buh ammonium trollstench vegetables pepsi 5 the a and to is of joe's enters that fatal table 4 : sample aspects from instant videos discovered by aspera ( glove ). # aspect words 1 protein diagnose cell genes brain membrane interacts interact oxygen spinal"", '2 boost monetary raise introduce measures credit expects increase push demand 3 towel soaked greasy towels cloth dripping tucked crisp coat buckets 4 offbeat comic parody spoof comedic qui', ""##rky cinematic campy parodies animated 5 sheesh wham whew hurrah oops yikes c'mon shhh oooh"", 'och amazon instant videos 5 - core reviews with the ratio 80 : 10 : 10. we also used the', 'results of narre model [ 4 ], obtained in the same setup as [ 5 ] but with a different random', 'seed. note that while aspera with generic glove word embeddings still works better than any other model, adding custom word embeddings trained on the same type of texts improves the results greatly. topic quality', 'we compared the performance of aspera with onlinelda [ 10 ] trained with the gensim library [ 31 ], with', 'the same vocabulary and number of topics, and abae with 10 aspects and 18 epochs, initialized with the same word2vec vectors ( sgns ) as aspera and having', 'the same ortho - regularization coefficient as the best aspera model, evaluating the results in terms of topic coherence metrics, npmi [ 2 ] and pmi [ 25, 26 ] computed with companion software for [ 15 ]. figure 2 shows that', 'the quality is', 'generally lower for larger number of representative words per aspect ( horizontal axis ), and that aspera achieves scores comparable to lda and abae, although abae remains ahead. tables 3 and 4 present several sample aspects discovered by aspera. qualitative analysis shows that some aspects describe what could be called a topic ( a set of words diverse by part of speech and', 'function describing a certain domain ), some encode sentiment ( top words are adjectives showing attitude to certain objects', 'discussed in the text ), and some encode names ( actors, directors, etc. ). we also found similar patterns in the output of', 'the basic abae model  #TAUTHOR_TAG. thus, most aspects are clearly coherent, but there is room for improvement']",3
['.  #TAUTHOR_TAG proposed an unsupervised neural'],['al.  #TAUTHOR_TAG proposed an unsupervised neural'],['.  #TAUTHOR_TAG proposed'],[' #TAUTHOR_TAG'],0
"['times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression']","['times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression']","['times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression']","['neural networks ( rnns ) yield high - quality results in many applications [ 1, 4, 18, 21 ] but often overfit due to overparametrization.', 'in many practical problems, rnns can be compressed orders of times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression can be divided into three groups : based on matrix factorization [ 6, 19 ], quantization [ 7 ] or sparsification  #TAUTHOR_TAG 15, 20 ].', 'we focus on rnns sparsification.', 'two main groups of approaches for sparsification are pruning and bayesian sparsification.', '']",0
"['times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression']","['times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression']","['times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression']","['neural networks ( rnns ) yield high - quality results in many applications [ 1, 4, 18, 21 ] but often overfit due to overparametrization.', 'in many practical problems, rnns can be compressed orders of times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression can be divided into three groups : based on matrix factorization [ 6, 19 ], quantization [ 7 ] or sparsification  #TAUTHOR_TAG 15, 20 ].', 'we focus on rnns sparsification.', 'two main groups of approaches for sparsification are pruning and bayesian sparsification.', '']",0
"['times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression']","['times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression']","['times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression']","['neural networks ( rnns ) yield high - quality results in many applications [ 1, 4, 18, 21 ] but often overfit due to overparametrization.', 'in many practical problems, rnns can be compressed orders of times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression can be divided into three groups : based on matrix factorization [ 6, 19 ], quantization [ 7 ] or sparsification  #TAUTHOR_TAG 15, 20 ].', 'we focus on rnns sparsification.', 'two main groups of approaches for sparsification are pruning and bayesian sparsification.', '']",0
"[""not affect network's output."", 'in  #TAUTHOR_TAG sparse']","[""not affect network's output."", 'in  #TAUTHOR_TAG sparsevd is adapted to rnns.', 'in [ 8 ] the authors propose to multiply']","[""these weights do not affect network's output."", 'in  #TAUTHOR_TAG sparsevd is adapted to rnns.', 'in']","['a dataset of n objects ( x i, y i ) and a model p ( y | x, w ) parametrized by a neural network with weights w.', 'in [ 14 ], the authors propose a bayesian technique called sparse variational dropout ( sparsevd ) for neural networks sparsification.', 'this model comprises log - uniform prior over weights : p ( | w ij | ) ∝ 1 | wij | and fully factorized normal approximate posterior : q ( w ij ) = n ( w ij | m ij, σ 2 ij ).', 'to find parameters of the approximate posterior distribution, evidence lower bound ( elbo ) is optimized :', ""because of the log - uniform prior, for the majority of weights signal - to - noise ratio m 2 ij / σ 2 ij → 0 and these weights do not affect network's output."", 'in  #TAUTHOR_TAG sparsevd is adapted to rnns.', 'in [ 8 ] the authors propose to multiply activations of neurons on group variables z and to learn and sparsify group variables along with w.', 'they put standard normal prior on w and log - uniform prior on z. the first prior moves mean values of w to 0, and it helps to set to zero z and to remove neurons from the model.', 'this model is equivalent to multiplying rows of weight matrices on group variables']",0
"['times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression']","['times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression']","['times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression']","['neural networks ( rnns ) yield high - quality results in many applications [ 1, 4, 18, 21 ] but often overfit due to overparametrization.', 'in many practical problems, rnns can be compressed orders of times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression can be divided into three groups : based on matrix factorization [ 6, 19 ], quantization [ 7 ] or sparsification  #TAUTHOR_TAG 15, 20 ].', 'we focus on rnns sparsification.', 'two main groups of approaches for sparsification are pruning and bayesian sparsification.', '']",5
"['times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression']","['times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression']","['times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression']","['neural networks ( rnns ) yield high - quality results in many applications [ 1, 4, 18, 21 ] but often overfit due to overparametrization.', 'in many practical problems, rnns can be compressed orders of times with only slight quality drop or even with quality improvement  #TAUTHOR_TAG 15, 20 ].', 'methods for rnn compression can be divided into three groups : based on matrix factorization [ 6, 19 ], quantization [ 7 ] or sparsification  #TAUTHOR_TAG 15, 20 ].', 'we focus on rnns sparsification.', 'two main groups of approaches for sparsification are pruning and bayesian sparsification.', '']",5
"['weights of the rnn, taking into account recurrent specifics underlined in  #TAUTHOR_TAG.', 'to compress layers']","['weights of the rnn, taking into account recurrent specifics underlined in  #TAUTHOR_TAG.', 'to compress layers']","['sparsevd [ 14 ] to all weights of the rnn, taking into account recurrent specifics underlined in  #TAUTHOR_TAG.', 'to compress layers']","['sparsify individual weights, we apply sparsevd [ 14 ] to all weights of the rnn, taking into account recurrent specifics underlined in  #TAUTHOR_TAG.', 'to compress layers and remove neurons, we follow [ 8 ] and introduce group variables for the neurons of all layers ( excluding output predictions ), and specifically, z', 'x and z h for input and hidden neurons of lstm.', 'the key component of our model is introducing groups variables z i, z f, z g, z o on preactivations of gates and information flow.', 'the resulting lstm layer looks as follows :', 'described model is equivalent to multiplying rows and columns of weight matrices on group variables :', '{ same for i, o and g }', 'we learn group variables z in the same way as weights w : approximate posterior with fully factorized normal distribution given fully factorized log - uniform prior distribution 2.', 'to find approximate posterior distribution, we maximize elbo ( 1 ).', 'after learning, we set all weights and group variables with signal - to - noise ratio less than 0. 05 to 0']",5
"['+ g + n ) following  #TAUTHOR_TAG.', 'on the contrary, in language']","['group sparsification ( w + n, w + g + n ) following  #TAUTHOR_TAG.', 'on the contrary, in language']","['+ n, w + g + n ) following  #TAUTHOR_TAG.', 'on the contrary, in language modeling tasks']","['', 'for language modeling, we use networks with one we compare four models in terms of quality and sparsity : baseline model without any regularization, standard sparsevd model for weights sparsification only ( w ), sparsevd model with group variables for neurons sparsification ( w + n ) and sparsevd model with group variables for gates and neurons sparsification ( w + g + n ).', 'in all sparsevd models, we sparsify weights matrices of all layers.', 'since in text classification tasks usually only a small number of input words are important, we use additional multiplicative weights to sparsify the input vocabulary in case of group sparsification ( w + n, w + g + n ) following  #TAUTHOR_TAG.', 'on the contrary, in language modeling tasks all input characters or words are usually important, therefore we do not use z x for this task.', '']",5
"['assign multiple senses to a single usage ( veronis, 1998 ;  #TAUTHOR_TAG b ).', 'therefore, we focus on annotation methodologies']","['assign multiple senses to a single usage ( veronis, 1998 ;  #TAUTHOR_TAG b ).', 'therefore, we focus on annotation methodologies']","['assign multiple senses to a single usage ( veronis, 1998 ;  #TAUTHOR_TAG b ).', 'therefore, we focus on annotation methodologies']","['sense annotation is regarded as one of the most difficult annotation tasks  #AUTHOR_TAG and building manually - annotated corpora with highquality sense labels is often a time - and resourceconsuming task.', 'as a result, nearly all sense - tagged corpora in wide - spread use are created using trained annotators  #AUTHOR_TAG, which results in a knowledge acquisition bottleneck for training systems that require sense labels  #AUTHOR_TAG.', 'in other nlp areas, this bottleneck has been addressed through gathering annotations using many untrained workers on platforms such as amazon mechanical turk ( mturk ), a task commonly referred to as crowdsourcing.', 'recently, several works have proposed gathering sense annotations using crowdsourcing  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'however, these methods produce sense labels that are different from the commonly used sense inventories such as wordnet  #AUTHOR_TAG or ontonotes  #AUTHOR_TAG.', 'furthermore, while  #AUTHOR_TAG b ) did use wordnet sense labels, they found the quality was well below that of trained experts.', 'we revisit the task of crowdsourcing word sense annotations, focusing on two key aspects : ( 1 ) the annotation methodology itself, and ( 2 ) the restriction to single sense assignment.', 'first, the choice in sense inventory plays an important role in gathering high - quality annotations ; fine - grained inventories such as wordnet often contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context  #AUTHOR_TAG mc  #AUTHOR_TAG.', 'however, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter - annotator agreement ( iaa ) in the presence of ambiguous or polysemous usages ; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage ( veronis, 1998 ;  #TAUTHOR_TAG b ).', 'therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the question : if allowed to make labeling ambiguity explicit, will annotators agree?', 'furthermore, we adopt the goal of  #TAUTHOR_TAG, which enabled annotators to weight each sense by its applicability to the given context, thereby quantifying the ambiguity.', 'this paper provides the following contributions.', 'first, we demonstrate that the choice in annotation setup can significantly improve iaa and that the labels of untrained workers follow consistent patterns that enable creating high quality labeling from their aggregate.', 'second, we find that the sense labeling from crowdsourcing matches performance with annotators in a controlled setting']",1
"['assign multiple senses to a single usage ( veronis, 1998 ;  #TAUTHOR_TAG b ).', 'therefore, we focus on annotation methodologies']","['assign multiple senses to a single usage ( veronis, 1998 ;  #TAUTHOR_TAG b ).', 'therefore, we focus on annotation methodologies']","['assign multiple senses to a single usage ( veronis, 1998 ;  #TAUTHOR_TAG b ).', 'therefore, we focus on annotation methodologies']","['sense annotation is regarded as one of the most difficult annotation tasks  #AUTHOR_TAG and building manually - annotated corpora with highquality sense labels is often a time - and resourceconsuming task.', 'as a result, nearly all sense - tagged corpora in wide - spread use are created using trained annotators  #AUTHOR_TAG, which results in a knowledge acquisition bottleneck for training systems that require sense labels  #AUTHOR_TAG.', 'in other nlp areas, this bottleneck has been addressed through gathering annotations using many untrained workers on platforms such as amazon mechanical turk ( mturk ), a task commonly referred to as crowdsourcing.', 'recently, several works have proposed gathering sense annotations using crowdsourcing  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'however, these methods produce sense labels that are different from the commonly used sense inventories such as wordnet  #AUTHOR_TAG or ontonotes  #AUTHOR_TAG.', 'furthermore, while  #AUTHOR_TAG b ) did use wordnet sense labels, they found the quality was well below that of trained experts.', 'we revisit the task of crowdsourcing word sense annotations, focusing on two key aspects : ( 1 ) the annotation methodology itself, and ( 2 ) the restriction to single sense assignment.', 'first, the choice in sense inventory plays an important role in gathering high - quality annotations ; fine - grained inventories such as wordnet often contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context  #AUTHOR_TAG mc  #AUTHOR_TAG.', 'however, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter - annotator agreement ( iaa ) in the presence of ambiguous or polysemous usages ; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage ( veronis, 1998 ;  #TAUTHOR_TAG b ).', 'therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the question : if allowed to make labeling ambiguity explicit, will annotators agree?', 'furthermore, we adopt the goal of  #TAUTHOR_TAG, which enabled annotators to weight each sense by its applicability to the given context, thereby quantifying the ambiguity.', 'this paper provides the following contributions.', 'first, we demonstrate that the choice in annotation setup can significantly improve iaa and that the labels of untrained workers follow consistent patterns that enable creating high quality labeling from their aggregate.', 'second, we find that the sense labeling from crowdsourcing matches performance with annotators in a controlled setting']",0
"['assign multiple senses to a single usage ( veronis, 1998 ;  #TAUTHOR_TAG b ).', 'therefore, we focus on annotation methodologies']","['assign multiple senses to a single usage ( veronis, 1998 ;  #TAUTHOR_TAG b ).', 'therefore, we focus on annotation methodologies']","['assign multiple senses to a single usage ( veronis, 1998 ;  #TAUTHOR_TAG b ).', 'therefore, we focus on annotation methodologies']","['sense annotation is regarded as one of the most difficult annotation tasks  #AUTHOR_TAG and building manually - annotated corpora with highquality sense labels is often a time - and resourceconsuming task.', 'as a result, nearly all sense - tagged corpora in wide - spread use are created using trained annotators  #AUTHOR_TAG, which results in a knowledge acquisition bottleneck for training systems that require sense labels  #AUTHOR_TAG.', 'in other nlp areas, this bottleneck has been addressed through gathering annotations using many untrained workers on platforms such as amazon mechanical turk ( mturk ), a task commonly referred to as crowdsourcing.', 'recently, several works have proposed gathering sense annotations using crowdsourcing  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'however, these methods produce sense labels that are different from the commonly used sense inventories such as wordnet  #AUTHOR_TAG or ontonotes  #AUTHOR_TAG.', 'furthermore, while  #AUTHOR_TAG b ) did use wordnet sense labels, they found the quality was well below that of trained experts.', 'we revisit the task of crowdsourcing word sense annotations, focusing on two key aspects : ( 1 ) the annotation methodology itself, and ( 2 ) the restriction to single sense assignment.', 'first, the choice in sense inventory plays an important role in gathering high - quality annotations ; fine - grained inventories such as wordnet often contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context  #AUTHOR_TAG mc  #AUTHOR_TAG.', 'however, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter - annotator agreement ( iaa ) in the presence of ambiguous or polysemous usages ; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage ( veronis, 1998 ;  #TAUTHOR_TAG b ).', 'therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the question : if allowed to make labeling ambiguity explicit, will annotators agree?', 'furthermore, we adopt the goal of  #TAUTHOR_TAG, which enabled annotators to weight each sense by its applicability to the given context, thereby quantifying the ambiguity.', 'this paper provides the following contributions.', 'first, we demonstrate that the choice in annotation setup can significantly improve iaa and that the labels of untrained workers follow consistent patterns that enable creating high quality labeling from their aggregate.', 'second, we find that the sense labeling from crowdsourcing matches performance with annotators in a controlled setting']",3
[') the methodology of  #TAUTHOR_TAG'],['gathering sense labels : ( 1 ) the methodology of  #TAUTHOR_TAG'],[') the methodology of  #TAUTHOR_TAG'],"['consider three methodologies for gathering sense labels : ( 1 ) the methodology of  #TAUTHOR_TAG for gathering weighted labels, ( 2 ) a multistage strategy that uses both binary and likert ratings, and ( 3 ) maxdiff, a paired choice format.', 'likert ratings likert rating scales provide the most direct way of gathering weighted sense labels ; turkers are presented with all senses of a word and then asked to rate each on a numeric scale.', 'we adopt the annotation guidelines of  #TAUTHOR_TAG which used a five - point scale, ranging from 1 to 5, indicating the sense does not apply or that it matches the contextual usage exactly, respectively.', 'select and rate recent efforts in crowdsourcing have proposed multi - stage processes for accomplishing complex tasks, where efforts by one group of workers are used to create new subtasks for other workers to complete  #AUTHOR_TAG.', 'we propose a two - stage strategy that aims to reduce the complexity of the annotation task, referred to as select and rate ( s + r ).', 'first, turkers are presented with all the senses and asked to make a binary choice of which senses apply.', 'second, a likert rating task is created for only those senses whose selection frequency is above a threshold, thereby concentrating worker focus on a potentially smaller set of senses.', 'our motivation for s + r is two - fold.', '']",7
[') the methodology of  #TAUTHOR_TAG'],['gathering sense labels : ( 1 ) the methodology of  #TAUTHOR_TAG'],[') the methodology of  #TAUTHOR_TAG'],"['consider three methodologies for gathering sense labels : ( 1 ) the methodology of  #TAUTHOR_TAG for gathering weighted labels, ( 2 ) a multistage strategy that uses both binary and likert ratings, and ( 3 ) maxdiff, a paired choice format.', 'likert ratings likert rating scales provide the most direct way of gathering weighted sense labels ; turkers are presented with all senses of a word and then asked to rate each on a numeric scale.', 'we adopt the annotation guidelines of  #TAUTHOR_TAG which used a five - point scale, ranging from 1 to 5, indicating the sense does not apply or that it matches the contextual usage exactly, respectively.', 'select and rate recent efforts in crowdsourcing have proposed multi - stage processes for accomplishing complex tasks, where efforts by one group of workers are used to create new subtasks for other workers to complete  #AUTHOR_TAG.', 'we propose a two - stage strategy that aims to reduce the complexity of the annotation task, referred to as select and rate ( s + r ).', 'first, turkers are presented with all the senses and asked to make a binary choice of which senses apply.', 'second, a likert rating task is created for only those senses whose selection frequency is above a threshold, thereby concentrating worker focus on a potentially smaller set of senses.', 'our motivation for s + r is two - fold.', '']",5
[') the methodology of  #TAUTHOR_TAG'],['gathering sense labels : ( 1 ) the methodology of  #TAUTHOR_TAG'],[') the methodology of  #TAUTHOR_TAG'],"['consider three methodologies for gathering sense labels : ( 1 ) the methodology of  #TAUTHOR_TAG for gathering weighted labels, ( 2 ) a multistage strategy that uses both binary and likert ratings, and ( 3 ) maxdiff, a paired choice format.', 'likert ratings likert rating scales provide the most direct way of gathering weighted sense labels ; turkers are presented with all senses of a word and then asked to rate each on a numeric scale.', 'we adopt the annotation guidelines of  #TAUTHOR_TAG which used a five - point scale, ranging from 1 to 5, indicating the sense does not apply or that it matches the contextual usage exactly, respectively.', 'select and rate recent efforts in crowdsourcing have proposed multi - stage processes for accomplishing complex tasks, where efforts by one group of workers are used to create new subtasks for other workers to complete  #AUTHOR_TAG.', 'we propose a two - stage strategy that aims to reduce the complexity of the annotation task, referred to as select and rate ( s + r ).', 'first, turkers are presented with all the senses and asked to make a binary choice of which senses apply.', 'second, a likert rating task is created for only those senses whose selection frequency is above a threshold, thereby concentrating worker focus on a potentially smaller set of senses.', 'our motivation for s + r is two - fold.', '']",5
"['of  #TAUTHOR_TAG, where']","['of  #TAUTHOR_TAG, where']","['the gws dataset of  #TAUTHOR_TAG, where']","['', 'this sampling is repeated 50 times and we report the mean iaa as a measure of the expected degree of replicability when annotating using different groups of turkers.', 'for the reference sense labeling, we use a subset of the gws dataset of  #TAUTHOR_TAG, where three annotators rated 50 instances each for eight words.', '']",5
"['important aspects emerge.', 'first, the word itself plays a significant role in iaa.', 'though  #TAUTHOR_TAG reported a pair']","['important aspects emerge.', 'first, the word itself plays a significant role in iaa.', 'though  #TAUTHOR_TAG reported a pair - wise iaa of the gws annotators between']","['', 'two important aspects emerge.', 'first, the word itself plays a significant role in iaa.', 'though  #TAUTHOR_TAG reported a pair - wise iaa of the gws annotators between']","['', 'two important aspects emerge.', 'first, the word itself plays a significant role in iaa.', ""though  #TAUTHOR_TAG reported a pair - wise iaa of the gws annotators between 0. 466 and 0. 506 using spearman's ρ, the iaa varies considerably between words for both turkers and gws annotators when measured using krippendorff's α."", 'second, the choice of annotation methodology significantly impacts iaa.', '']",4
['transformer neural sequence model  #TAUTHOR_TAG has emerged as a popular alternative to recurrent sequence models'],"['transformer neural sequence model  #TAUTHOR_TAG has emerged as a popular alternative to recurrent sequence models.', 'transformer relies on attention layers to communicate information between and across sequences.', 'one major challenge with transformer is the speed of incremental']","['transformer neural sequence model  #TAUTHOR_TAG has emerged as a popular alternative to recurrent sequence models.', 'transformer relies on attention layers to communicate information between and across sequences']","['transformer neural sequence model  #TAUTHOR_TAG has emerged as a popular alternative to recurrent sequence models.', 'transformer relies on attention layers to communicate information between and across sequences.', 'one major challenge with transformer is the speed of incremental inference.', 'as we will discuss, the speed of incremental transformer inference on modern computing hardware is limited by the memory bandwidth necessary to reload the large "" keys "" and "" values "" tensors which encode the state of the attention layers.', 'in the following sections, we will review the multi - head - attention layers used by transformer, provide a performance analysis, and propose an architectural variation ( multi - query attention ) which greatly improves inference speed with only minor quality degradation.', 'd e f do tp r o ductattentio n ( q, k, v ) :', '"" "" "" dot−product a t t e n t i o n on one quer y.', '']",0
"['transformer "" seuqence - to - sequence model  #TAUTHOR_TAG uses h different attention layers ( heads']","['"" transformer "" seuqence - to - sequence model  #TAUTHOR_TAG uses h different attention layers ( heads ) in parallel, which the authors refer to as "" multi - head']","['"" transformer "" seuqence - to - sequence model  #TAUTHOR_TAG uses h different attention layers ( heads']","['"" transformer "" seuqence - to - sequence model  #TAUTHOR_TAG uses h different attention layers ( heads ) in parallel, which the authors refer to as "" multi - head attention "".', '']",0
"['transformer "" seuqence - to - sequence model  #TAUTHOR_TAG uses h different attention layers ( heads']","['"" transformer "" seuqence - to - sequence model  #TAUTHOR_TAG uses h different attention layers ( heads ) in parallel, which the authors refer to as "" multi - head']","['"" transformer "" seuqence - to - sequence model  #TAUTHOR_TAG uses h different attention layers ( heads']","['"" transformer "" seuqence - to - sequence model  #TAUTHOR_TAG uses h different attention layers ( heads ) in parallel, which the authors refer to as "" multi - head attention "".', '']",4
"['transformer "" seuqence - to - sequence model  #TAUTHOR_TAG uses h different attention layers ( heads']","['"" transformer "" seuqence - to - sequence model  #TAUTHOR_TAG uses h different attention layers ( heads ) in parallel, which the authors refer to as "" multi - head']","['"" transformer "" seuqence - to - sequence model  #TAUTHOR_TAG uses h different attention layers ( heads']","['"" transformer "" seuqence - to - sequence model  #TAUTHOR_TAG uses h different attention layers ( heads ) in parallel, which the authors refer to as "" multi - head attention "".', '']",6
"['process a batch of b different non - interacting sequences at once.', 'following  #TAUTHOR_TAG, in an autoregressive model, we can prevent backward - information - flow by adding a "" mask "" to the logits containing the value −∞ in the illegal positions']","['process a batch of b different non - interacting sequences at once.', 'following  #TAUTHOR_TAG, in an autoregressive model, we can prevent backward - information - flow by adding a "" mask "" to the logits containing the value −∞ in the illegal positions']","['', 'first, we generate queries from n different positions in a sequence.', 'these queries all interact with the same keys and values.', 'in addition, we process a batch of b different non - interacting sequences at once.', 'following  #TAUTHOR_TAG, in an autoregressive model, we can prevent backward - information - flow by adding a "" mask "" to the logits containing the value −∞ in the illegal positions']","['practice, it is far more efficient to batch together multiple queries.', 'the code below adds two types of batching.', 'first, we generate queries from n different positions in a sequence.', 'these queries all interact with the same keys and values.', 'in addition, we process a batch of b different non - interacting sequences at once.', 'following  #TAUTHOR_TAG, in an autoregressive model, we can prevent backward - information - flow by adding a "" mask "" to the logits containing the value −∞ in the illegal positions']",5
"['several simplifying assumptions :', 'h, as suggested by  #TAUTHOR_TAG', 'the total']","['several simplifying assumptions :', 'h, as suggested by  #TAUTHOR_TAG', 'the total']","['simplify the performance analysis, we will make several simplifying assumptions :', 'h, as suggested by  #TAUTHOR_TAG', 'the total number of arithmetic operations is θ ( bnd 2 ).', '( since the complexity of']","['simplify the performance analysis, we will make several simplifying assumptions :', 'h, as suggested by  #TAUTHOR_TAG', 'the total number of arithmetic operations is θ ( bnd 2 ).', '( since the complexity of each of the tf. einsum operations above is o ( bnd 2 ) given the simplifying assumptions.', 'the total size of memory to be accessed is equal to the sum of the sizes of all the tensors involved : o ( bnd + bhn 2 + d 2 ).', 'the first term is due to x, m, q, k, v, o and y, the second term due to the logits and weights, and the third term due to the projection tensors p q, p k, p v and p o.', 'dividing the two, we find that the ratio of memory access to arithmetic operations is o ( 1 k + 1 bn ).', 'this low ratio is necessary for good performance on modern gpu / tpu hardware, where the computational capacity can be two orders of magnitude higher than the memory bandwidth']",5
"['attention as a variation of multi - head attention as described in  #TAUTHOR_TAG.', 'multi - head attention consists of multiple attention layers ( heads']","['introduce multi - query attention as a variation of multi - head attention as described in  #TAUTHOR_TAG.', 'multi - head attention consists of multiple attention layers ( heads ) in parallel with different linear transformations on']","['introduce multi - query attention as a variation of multi - head attention as described in  #TAUTHOR_TAG.', 'multi - head attention consists of multiple attention layers ( heads']","['introduce multi - query attention as a variation of multi - head attention as described in  #TAUTHOR_TAG.', 'multi - head attention consists of multiple attention layers ( heads ) in parallel with different linear transformations on the queries, keys, values and outputs.', 'multi - query attention is identical except that the different heads share a single set of keys and values.', 'the code for ( incremental ) multi - query ( self ) attention is identical to the code listed above for multi - head attention, except that we remove the letter "" h "" from the tf. einsum equations where it represents the "" heads "" dimension of k, v, p k, or p v']",5
"[' #TAUTHOR_TAG, we evaluate on the wmt 2014']","[' #TAUTHOR_TAG, we evaluate on the wmt 2014']","[' #TAUTHOR_TAG, we evaluate on the wmt 2014']","[' #TAUTHOR_TAG, we evaluate on the wmt 2014 english - german translation task.', 'as a baseline, we use an encoder - decoder transformer model with 6 layers, using d model = 1024 d f f = 4096, h = 8, d k = d v = 128, learned positional embeddings, and weight - sharing between the token - embedding and output layers.', 'the baseline model and all variations have 211 million parameters.', 'all models were trained for 100, 000 steps ( 20 epochs ).', 'each training batch consisted of 128 examples, each of which consisted of a 256 - token input sequence and a 256 - token target sequence ( multiple training sentences were concatenated together to reach this length ).', 'models were trained on a 32 - core tpuv3 cluster, with each model taking about 2 hours to train.', 'we used an implementation from the tensor2tensor and mesh - tensorflow libraries.', 'the configurations used can be found at [ to be added before publication ], including details about learning rates, dropout, label smoothing, etc.', 'in our "" multi - query "" model, we replace all of the attention layers in the model to multi - query attention.', 'this includes the encoder - self - attention, decoder - self - attention and encoder - decoder - attention layers.', '']",5
"['several simplifying assumptions :', 'h, as suggested by  #TAUTHOR_TAG', 'the total']","['several simplifying assumptions :', 'h, as suggested by  #TAUTHOR_TAG', 'the total']","['simplify the performance analysis, we will make several simplifying assumptions :', 'h, as suggested by  #TAUTHOR_TAG', 'the total number of arithmetic operations is θ ( bnd 2 ).', '( since the complexity of']","['simplify the performance analysis, we will make several simplifying assumptions :', 'h, as suggested by  #TAUTHOR_TAG', 'the total number of arithmetic operations is θ ( bnd 2 ).', '( since the complexity of each of the tf. einsum operations above is o ( bnd 2 ) given the simplifying assumptions.', 'the total size of memory to be accessed is equal to the sum of the sizes of all the tensors involved : o ( bnd + bhn 2 + d 2 ).', 'the first term is due to x, m, q, k, v, o and y, the second term due to the logits and weights, and the third term due to the projection tensors p q, p k, p v and p o.', 'dividing the two, we find that the ratio of memory access to arithmetic operations is o ( 1 k + 1 bn ).', 'this low ratio is necessary for good performance on modern gpu / tpu hardware, where the computational capacity can be two orders of magnitude higher than the memory bandwidth']",3
"['transformer  #TAUTHOR_TAG.', 'the']","['transformer  #TAUTHOR_TAG.', 'the']","['transformer  #TAUTHOR_TAG.', 'the queries produced at each position attend to']","['some settings, data dependencies make it is impossible to process queries from multiple positions in parallel.', 'an example is a self - attention layer in an autoregressive language model such as transformer  #TAUTHOR_TAG.', 'the queries produced at each position attend to key - value pairs produced at all positions up to and including that position.', 'during training, the ground - truth target sequence is known, and we can use an efficient parallel implementation similar to that in section 2. 3.', '']",3
"['attention as a variation of multi - head attention as described in  #TAUTHOR_TAG.', 'multi - head attention consists of multiple attention layers ( heads']","['introduce multi - query attention as a variation of multi - head attention as described in  #TAUTHOR_TAG.', 'multi - head attention consists of multiple attention layers ( heads ) in parallel with different linear transformations on']","['introduce multi - query attention as a variation of multi - head attention as described in  #TAUTHOR_TAG.', 'multi - head attention consists of multiple attention layers ( heads']","['introduce multi - query attention as a variation of multi - head attention as described in  #TAUTHOR_TAG.', 'multi - head attention consists of multiple attention layers ( heads ) in parallel with different linear transformations on the queries, keys, values and outputs.', 'multi - query attention is identical except that the different heads share a single set of keys and values.', 'the code for ( incremental ) multi - query ( self ) attention is identical to the code listed above for multi - head attention, except that we remove the letter "" h "" from the tf. einsum equations where it represents the "" heads "" dimension of k, v, p k, or p v']",3
"[' #TAUTHOR_TAG, we evaluate on the wmt 2014']","[' #TAUTHOR_TAG, we evaluate on the wmt 2014']","[' #TAUTHOR_TAG, we evaluate on the wmt 2014']","[' #TAUTHOR_TAG, we evaluate on the wmt 2014 english - german translation task.', 'as a baseline, we use an encoder - decoder transformer model with 6 layers, using d model = 1024 d f f = 4096, h = 8, d k = d v = 128, learned positional embeddings, and weight - sharing between the token - embedding and output layers.', 'the baseline model and all variations have 211 million parameters.', 'all models were trained for 100, 000 steps ( 20 epochs ).', 'each training batch consisted of 128 examples, each of which consisted of a 256 - token input sequence and a 256 - token target sequence ( multiple training sentences were concatenated together to reach this length ).', 'models were trained on a 32 - core tpuv3 cluster, with each model taking about 2 hours to train.', 'we used an implementation from the tensor2tensor and mesh - tensorflow libraries.', 'the configurations used can be found at [ to be added before publication ], including details about learning rates, dropout, label smoothing, etc.', 'in our "" multi - query "" model, we replace all of the attention layers in the model to multi - query attention.', 'this includes the encoder - self - attention, decoder - self - attention and encoder - decoder - attention layers.', '']",3
"['fifth sentence. recently,  #TAUTHOR_TAG have introduced an approach to']","['one cp', 'in every fifth sentence. recently,  #TAUTHOR_TAG have introduced an approach to']","['', 'in every fifth sentence. recently,  #TAUTHOR_TAG have introduced an approach to']","[""treated language as if it were purely compositional, and has therefore lumped the majority of mwes in with lexical verb usages '. for example the predicates in the"", 'cps take a hard line, take time and many others are all annotated with a', 'sense of take, meaning acquire, come to have, chose, bring with you from somewhere. this results in', 'a loss of semantic information in the pb annotations. this is especially critical because cps are a frequent phenomenon. the wiki50', 'corpus  #AUTHOR_TAG, which provides a full coverage mwe annotation, counts 814 occurrences of lvcs and vpcs in 4350 sentences. this makes for one cp', 'in every fifth sentence. recently,  #TAUTHOR_TAG have introduced an approach to improve the handling of mwes in pb while keeping annotation costs low.', 'the process is called aliasing. instead of creating new frames for cps, human annotators map them to existing pb rolesets which encompass the same semantic and argument structure. for example,', 'the cp give ( a ) talk could be mapped to the alias lecture. 01. while this method significantly reduces the effort to create new rolesets, the time consuming manual mapping is still required.', '']",0
"['fifth sentence. recently,  #TAUTHOR_TAG have introduced an approach to']","['one cp', 'in every fifth sentence. recently,  #TAUTHOR_TAG have introduced an approach to']","['', 'in every fifth sentence. recently,  #TAUTHOR_TAG have introduced an approach to']","[""treated language as if it were purely compositional, and has therefore lumped the majority of mwes in with lexical verb usages '. for example the predicates in the"", 'cps take a hard line, take time and many others are all annotated with a', 'sense of take, meaning acquire, come to have, chose, bring with you from somewhere. this results in', 'a loss of semantic information in the pb annotations. this is especially critical because cps are a frequent phenomenon. the wiki50', 'corpus  #AUTHOR_TAG, which provides a full coverage mwe annotation, counts 814 occurrences of lvcs and vpcs in 4350 sentences. this makes for one cp', 'in every fifth sentence. recently,  #TAUTHOR_TAG have introduced an approach to improve the handling of mwes in pb while keeping annotation costs low.', 'the process is called aliasing. instead of creating new frames for cps, human annotators map them to existing pb rolesets which encompass the same semantic and argument structure. for example,', 'the cp give ( a ) talk could be mapped to the alias lecture. 01. while this method significantly reduces the effort to create new rolesets, the time consuming manual mapping is still required.', '']",0
['counterpart separately. the  #TAUTHOR_TAG tries to extend'],"['counterpart separately. the  #TAUTHOR_TAG tries to extend the coverage', 'of pb for cps while keeping']","['counterpart separately. the  #TAUTHOR_TAG tries to extend the coverage', 'of pb for cps while keeping']","['', '. first, the arguments of the light verb and true predicate are annotated with roles regarding their relationship to the combination of the light verb', 'and true predicate. then, the light verb and predicate lemmas are joined into a single', 'predicate. the result of this process is shown in figure 2.  #AUTHOR_TAG discuss the analysis of brazilian', 'portuguese cps. similarly to  #AUTHOR_TAG they argue that cps should be treated as single predicates, not only for lvcs but for all cps.', 'they automatically extract cp candidates from a corpus and represent, if possible, the meaning of the cps with one or more single -', 'verb paraphrases.  #AUTHOR_TAG describe a way in which lvcs can be annotated in framenet  #AUTHOR_TAG, a framework that describes the semantic argument structure of predicates with semantic roles specific to the meaning of the predicate. in contrast to', 'the proposals for pb by  #AUTHOR_TAG and  #AUTHOR_TAG, they suggest to annotate the light verb and its counterpart separately. the  #TAUTHOR_TAG tries to extend the coverage', 'of pb for cps while keeping the number of rolesets that should be newly', 'created to a minimum.  #TAUTHOR_TAG conducted a pilot study re', '- annotating 138 cps involving the verb take', '']",0
['counterpart separately. the  #TAUTHOR_TAG tries to extend'],"['counterpart separately. the  #TAUTHOR_TAG tries to extend the coverage', 'of pb for cps while keeping']","['counterpart separately. the  #TAUTHOR_TAG tries to extend the coverage', 'of pb for cps while keeping']","['', '. first, the arguments of the light verb and true predicate are annotated with roles regarding their relationship to the combination of the light verb', 'and true predicate. then, the light verb and predicate lemmas are joined into a single', 'predicate. the result of this process is shown in figure 2.  #AUTHOR_TAG discuss the analysis of brazilian', 'portuguese cps. similarly to  #AUTHOR_TAG they argue that cps should be treated as single predicates, not only for lvcs but for all cps.', 'they automatically extract cp candidates from a corpus and represent, if possible, the meaning of the cps with one or more single -', 'verb paraphrases.  #AUTHOR_TAG describe a way in which lvcs can be annotated in framenet  #AUTHOR_TAG, a framework that describes the semantic argument structure of predicates with semantic roles specific to the meaning of the predicate. in contrast to', 'the proposals for pb by  #AUTHOR_TAG and  #AUTHOR_TAG, they suggest to annotate the light verb and its counterpart separately. the  #TAUTHOR_TAG tries to extend the coverage', 'of pb for cps while keeping the number of rolesets that should be newly', 'created to a minimum.  #TAUTHOR_TAG conducted a pilot study re', '- annotating 138 cps involving the verb take', '']",0
"['fifth sentence. recently,  #TAUTHOR_TAG have introduced an approach to']","['one cp', 'in every fifth sentence. recently,  #TAUTHOR_TAG have introduced an approach to']","['', 'in every fifth sentence. recently,  #TAUTHOR_TAG have introduced an approach to']","[""treated language as if it were purely compositional, and has therefore lumped the majority of mwes in with lexical verb usages '. for example the predicates in the"", 'cps take a hard line, take time and many others are all annotated with a', 'sense of take, meaning acquire, come to have, chose, bring with you from somewhere. this results in', 'a loss of semantic information in the pb annotations. this is especially critical because cps are a frequent phenomenon. the wiki50', 'corpus  #AUTHOR_TAG, which provides a full coverage mwe annotation, counts 814 occurrences of lvcs and vpcs in 4350 sentences. this makes for one cp', 'in every fifth sentence. recently,  #TAUTHOR_TAG have introduced an approach to improve the handling of mwes in pb while keeping annotation costs low.', 'the process is called aliasing. instead of creating new frames for cps, human annotators map them to existing pb rolesets which encompass the same semantic and argument structure. for example,', 'the cp give ( a ) talk could be mapped to the alias lecture. 01. while this method significantly reduces the effort to create new rolesets, the time consuming manual mapping is still required.', '']",4
"['system on the dataset', 'from  #TAUTHOR_TAG, restricted to']","['on the dataset', 'from  #TAUTHOR_TAG, restricted to']","['system on the dataset', 'from  #TAUTHOR_TAG, restricted to the type of cp our system handles ( lv']","['', 'even if an adequate pb roleset exists, this roleset was not selected by the srl system. we hope to also improve these cases with our method. all cps were labeled with one to four appropriate pb alias rolesets. in addition, we evaluated our system on the dataset', 'from  #TAUTHOR_TAG, restricted to the type of cp our system handles ( lvcs and vpcs ) and verb ali', '##ases ( as opposed to aliases being a noun or adjective roles', '##et ). we used 70 of the 100 mw', ""##es from their annotations. evaluation measures and baseline. we report the accuracy of our system's predictions as compared to the gold standard. for the strict ac - cur"", '##acy, an alias is counted as correct if it corresponds exactly to one of the gold aliases. this', 'evaluation is very rigid and regards synonymous rolesets as incorrect. thus, we also compute a more le - nient accuracy, which counts an alias as correct if', '']",4
['counterpart separately. the  #TAUTHOR_TAG tries to extend'],"['counterpart separately. the  #TAUTHOR_TAG tries to extend the coverage', 'of pb for cps while keeping']","['counterpart separately. the  #TAUTHOR_TAG tries to extend the coverage', 'of pb for cps while keeping']","['', '. first, the arguments of the light verb and true predicate are annotated with roles regarding their relationship to the combination of the light verb', 'and true predicate. then, the light verb and predicate lemmas are joined into a single', 'predicate. the result of this process is shown in figure 2.  #AUTHOR_TAG discuss the analysis of brazilian', 'portuguese cps. similarly to  #AUTHOR_TAG they argue that cps should be treated as single predicates, not only for lvcs but for all cps.', 'they automatically extract cp candidates from a corpus and represent, if possible, the meaning of the cps with one or more single -', 'verb paraphrases.  #AUTHOR_TAG describe a way in which lvcs can be annotated in framenet  #AUTHOR_TAG, a framework that describes the semantic argument structure of predicates with semantic roles specific to the meaning of the predicate. in contrast to', 'the proposals for pb by  #AUTHOR_TAG and  #AUTHOR_TAG, they suggest to annotate the light verb and its counterpart separately. the  #TAUTHOR_TAG tries to extend the coverage', 'of pb for cps while keeping the number of rolesets that should be newly', 'created to a minimum.  #TAUTHOR_TAG conducted a pilot study re', '- annotating 138 cps involving the verb take', '']",1
"['system on the dataset', 'from  #TAUTHOR_TAG, restricted to']","['on the dataset', 'from  #TAUTHOR_TAG, restricted to']","['system on the dataset', 'from  #TAUTHOR_TAG, restricted to the type of cp our system handles ( lv']","['', 'even if an adequate pb roleset exists, this roleset was not selected by the srl system. we hope to also improve these cases with our method. all cps were labeled with one to four appropriate pb alias rolesets. in addition, we evaluated our system on the dataset', 'from  #TAUTHOR_TAG, restricted to the type of cp our system handles ( lvcs and vpcs ) and verb ali', '##ases ( as opposed to aliases being a noun or adjective roles', '##et ). we used 70 of the 100 mw', ""##es from their annotations. evaluation measures and baseline. we report the accuracy of our system's predictions as compared to the gold standard. for the strict ac - cur"", '##acy, an alias is counted as correct if it corresponds exactly to one of the gold aliases. this', 'evaluation is very rigid and regards synonymous rolesets as incorrect. thus, we also compute a more le - nient accuracy, which counts an alias as correct if', '']",6
"['that extends on work from  #TAUTHOR_TAG.', 'we']","['have presented an approach to handle cps in srl that extends on work from  #TAUTHOR_TAG.', 'we']","['that extends on work from  #TAUTHOR_TAG.', 'we']","['have presented an approach to handle cps in srl that extends on work from  #TAUTHOR_TAG.', 'we automatically link vpcs and lvcs to the pb roleset that best describes their meaning, by relying on word alignments in parallel corpora and distributional methods.', 'we set up an annotation effort to gather a frequency - balanced, contextualized evaluation set that is more natural, varied and larger than the pilot annotations provided by  #TAUTHOR_TAG.', 'our method can be used to alleviate the manual annotation effort by providing a correct alias in 44 % of the cases ( up to 72 % for the more frequent test items when taking synonymous rolesets into account ).', 'these results are not too far from the upper bounds we calculate from human annotations.', 'in future work, we would like to improve our method by incorporating the methods discussed in the error analysis section.', 'additionally, we plan to evaluate the impact of the new cp representation on downstream applications by retraining an srl system on the new annotations']",6
"['that extends on work from  #TAUTHOR_TAG.', 'we']","['have presented an approach to handle cps in srl that extends on work from  #TAUTHOR_TAG.', 'we']","['that extends on work from  #TAUTHOR_TAG.', 'we']","['have presented an approach to handle cps in srl that extends on work from  #TAUTHOR_TAG.', 'we automatically link vpcs and lvcs to the pb roleset that best describes their meaning, by relying on word alignments in parallel corpora and distributional methods.', 'we set up an annotation effort to gather a frequency - balanced, contextualized evaluation set that is more natural, varied and larger than the pilot annotations provided by  #TAUTHOR_TAG.', 'our method can be used to alleviate the manual annotation effort by providing a correct alias in 44 % of the cases ( up to 72 % for the more frequent test items when taking synonymous rolesets into account ).', 'these results are not too far from the upper bounds we calculate from human annotations.', 'in future work, we would like to improve our method by incorporating the methods discussed in the error analysis section.', 'additionally, we plan to evaluate the impact of the new cp representation on downstream applications by retraining an srl system on the new annotations']",6
"['system on the dataset', 'from  #TAUTHOR_TAG, restricted to']","['on the dataset', 'from  #TAUTHOR_TAG, restricted to']","['system on the dataset', 'from  #TAUTHOR_TAG, restricted to the type of cp our system handles ( lv']","['', 'even if an adequate pb roleset exists, this roleset was not selected by the srl system. we hope to also improve these cases with our method. all cps were labeled with one to four appropriate pb alias rolesets. in addition, we evaluated our system on the dataset', 'from  #TAUTHOR_TAG, restricted to the type of cp our system handles ( lvcs and vpcs ) and verb ali', '##ases ( as opposed to aliases being a noun or adjective roles', '##et ). we used 70 of the 100 mw', ""##es from their annotations. evaluation measures and baseline. we report the accuracy of our system's predictions as compared to the gold standard. for the strict ac - cur"", '##acy, an alias is counted as correct if it corresponds exactly to one of the gold aliases. this', 'evaluation is very rigid and regards synonymous rolesets as incorrect. thus, we also compute a more le - nient accuracy, which counts an alias as correct if', '']",3
"['system on the dataset', 'from  #TAUTHOR_TAG, restricted to']","['on the dataset', 'from  #TAUTHOR_TAG, restricted to']","['system on the dataset', 'from  #TAUTHOR_TAG, restricted to the type of cp our system handles ( lv']","['', 'even if an adequate pb roleset exists, this roleset was not selected by the srl system. we hope to also improve these cases with our method. all cps were labeled with one to four appropriate pb alias rolesets. in addition, we evaluated our system on the dataset', 'from  #TAUTHOR_TAG, restricted to the type of cp our system handles ( lvcs and vpcs ) and verb ali', '##ases ( as opposed to aliases being a noun or adjective roles', '##et ). we used 70 of the 100 mw', ""##es from their annotations. evaluation measures and baseline. we report the accuracy of our system's predictions as compared to the gold standard. for the strict ac - cur"", '##acy, an alias is counted as correct if it corresponds exactly to one of the gold aliases. this', 'evaluation is very rigid and regards synonymous rolesets as incorrect. thus, we also compute a more le - nient accuracy, which counts an alias as correct if', '']",5
['cps from  #TAUTHOR_TAG ('],"['cps from  #TAUTHOR_TAG ( take set ) and compare our results to the baseline.', '']","['cps from  #TAUTHOR_TAG ( take set ) and compare our results to the baseline.', 'table 2 shows percentage coverage, accuracy']","['evaluated our approach on the 160 cps annotated in the course of this work ( wiki50 set ), as well as on the 70 take cps from  #TAUTHOR_TAG ( take set ) and compare our results to the baseline.', 'table 2 shows percentage coverage, accuracy and the harmonic mean of coverage and accuracy for our system and the baseline.', 'we report results on the two evaluation sets in the strict and lenient evaluation.', 'the first five rows of table 2 show the results for the wiki50 set and its subsets.', '']",5
"['discourse structure, and compositional semantics.', 'following  #TAUTHOR_TAG,']","['of syntax, discourse structure, and compositional semantics.', 'following  #TAUTHOR_TAG,']","['of syntax, discourse structure, and compositional semantics.', 'following  #TAUTHOR_TAG,']","['paper presents a corpus - based study of the discourse connective in contrast.', 'the corpus data are drawn from the british national corpus ( bnc ) and are analyzed at the levels of syntax, discourse structure, and compositional semantics.', 'following  #TAUTHOR_TAG, the paper argues that in contrast crucially involves discourse anaphora and, thus, resembles other discourse adverbials such as then, otherwise, and nevertheless.', 'the compositional semantics proposed for other discourse connectives, however, does not straightforwardly generalize to in contrast, for which the notions of contrast pairs and contrast properties are essential']",4
['between coordinating and subordinating conjunctions and discourse adverbials that has been put forth by  #TAUTHOR_TAG'],['between coordinating and subordinating conjunctions and discourse adverbials that has been put forth by  #TAUTHOR_TAG'],['between coordinating and subordinating conjunctions and discourse adverbials that has been put forth by  #TAUTHOR_TAG'],"['- more - than - 130 - per - cent - over - the - last - three - years ] ), ( the - price - of - average - properties, - those - on - which - the - halifax - has - lent - mortgages, x [ x has - risen - by - morethan - 130 - per - cent - over - the - last - three - years ] ) ). b. in - contrast ( ( the - holsteins, x [ x also - tend - tohave - much - more - white - in - the - coat - so - thatthe - white - areas - predominate - and - they - couldalmost - be - described - as - white - and - blacks ] ),', '( the - black - and - white - friesian - type, x ¡ [ x alsotend - to - have - much - more - white - in - the - coat - sothat - the - white - areas - predominate - and - they - couldalmost - be - described - as - white - and - blacks ] ) ). in ( the account of in contrast which has been illustrated by the formulas in ( 19 ) and ( 20b ) has two attractive properties : ( i ) from a theoretical perspective, it provides a unified analysis of the in contrast construction with and without a postmodifying prepositional phrase ; ( ii ) by separating out the contrast pairs (', 'as the first members of each argument pair ) from their contrasting properties, it provides a transparent representation for applications such as information extraction and text summarization, which require tracking discourse entities and their relevant properties. finally, it is worth reviewing the proposed analysis in light of the generalization put forth by  #AUTHOR_TAG and by  #AUTHOR_TAG, namely that discourse connectives always denote two - place relations. the semantics of in contrast proposed in this section is consistent with this hypothesis since it assumes a two - place relation. however, notice that each of the two arguments is further', 'structured into a contrast item and a contrast property. it is this highly structured character of the in - contrast relation that distinguishes this discourse connective from the much simpler two - place', 'relations denoted by coordinating and subordinating conjunctions. the latter simply denote relations between events and / or states of', 'affairs, namely those denoted by the two conjunct clauses. the semantics proposed for in - contrast, thus', ', provides further evidence for the distinction between coordinating and subordinating conjunctions and discourse adverbials that has been put forth by  #TAUTHOR_TAG']",4
"['discourse structure, and compositional semantics.', 'following  #TAUTHOR_TAG,']","['levels of syntax, discourse structure, and compositional semantics.', 'following  #TAUTHOR_TAG,']","['the british national corpus ( bnc ) and were analyzed at the levels of syntax, discourse structure, and compositional semantics.', 'following  #TAUTHOR_TAG,']","['paper has presented a corpus - based study of the discourse connective in contrast.', 'the corpus data were drawn from the british national corpus ( bnc ) and were analyzed at the levels of syntax, discourse structure, and compositional semantics.', 'following  #TAUTHOR_TAG, the paper argues that in contrast crucially involves discourse anaphora and, thus, resembles other discourse adverbials such as then, otherwise, and nevertheless.', 'the compositional semantics proposed for other discourse connectives, however, does not straightforwardly generalize to in contrast, for which the notions of contrast pairs and contrast properties are essential.', 'in future work we plan to consider a wider range of contrast relations in discourse such as by comparison, contrary to and on the other hand in order to ascertain whether the properties of the discourse connective in contrast will generalize to these cases as well.', 'a second line of research will investigate ways of automatically detecting comparison patterns and contrast pairs, which figure prominently in the compositional semantics of in contrast, by means of machine learning techniques.', 'here we expect that elliptical expressions, other - anaphora, and syntactic parallelism will provide important cues']",4
"['is further elaborated by  #TAUTHOR_TAG who distinguish between coordinating conjunctions such as and, or, so, and but and subordinating conjunctions such as although, whereas, and when on the one hand, and']","['is further elaborated by  #TAUTHOR_TAG who distinguish between coordinating conjunctions such as and, or, so, and but and subordinating conjunctions such as although, whereas, and when on the one hand, and']","['is further elaborated by  #TAUTHOR_TAG who distinguish between coordinating conjunctions such as and, or, so, and but and subordinating conjunctions such as although, whereas, and when on the one hand, and discourse adverbials such as']","['', '2. while syntactic dependencies can be quite complex and may involve highly nested or even crossing dependencies of various kinds, dependencies expressed by discourse connectives tend to be much more limited, typically involving tree - like structures and not introducing structural ambiguities of scope or attachment.', '3. more complex cases of discourse connectives that prima facie seem to involve crossing or partially overlapping arguments can be reduced to the independent discourse mechanisms of anaphora and attribution and thus do not introduce any added complexities.', 'the third generalization is further elaborated by  #TAUTHOR_TAG who distinguish between coordinating conjunctions such as and, or, so, and but and subordinating conjunctions such as although, whereas, and when on the one hand, and discourse adverbials such as then, otherwise, nevertheless, and instead on the other hand.', 'it is the latter group, namely discourse adverbials, that, according to  #TAUTHOR_TAG, should be considered as anaphors in very much the same way as other anaphoric expressions']",0
"['of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as']","['hours. b0h ( 0476 ) another property', 'that distinguishes anaphoric discourse adverbials from structural connectives in the sense of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as their arguments', ', discourse adverbials']","['hours. b0h ( 0476 ) another property', 'that distinguishes anaphoric discourse adverbials from structural connectives in the sense of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as']","['and death had to be cushioned away, made to look as if', ""they didn't exist. bpd ( 0200 ) the referent of outside in ( 13 ) is never explicitly mentioned. rather, outside refers back to the entire scene described"", 'before. another type of inference that is sometimes necessitated by the in contrast connective concerns the operation of complementarity of reference as in ( 14 ). ( 14 ) other speed - reducing devices may be', 'added, such as regular shifts in the axis of the road, together with changes in the profile in the form of ramps and speed humps ( figure 4. 3 ). narrowings that', 'allow a cycle to pass but not two cars are frequently added, often reinforced by the placement of trees, planters and street furniture. in contrast to the flowing design of fast roads, design elements are angular and of pedestrian', '##scale, typified by low - level lamp posts which avoid the "" sea of light "" provided by high poles in traffic streets ( figure 4. 4 ). c8f ( 0297 ) in this text, which is', 'on the topic of child safety, roads are never explicitly mentioned. rather the concept', 'of slow neighborhood roads can only be inferred', 'from the description. the first explicit mention of the term road then refers to the opposite term fast roads. comparison of in contrast with personal pronoun', '##s yields yet another similarity with other anaphoric expressions. like with personal pronouns, the antecedent of in contrast can either occur across sentences, as in all of the examples considered so far, or it can occur intras', '##ententially, as in example ( 15 ). ( 15 ) in contrast to his predec', '##essors who worked at all hours of the day macmillan tended to keep office hours. b0h ( 0476 ) another property', 'that distinguishes anaphoric discourse adverbials from structural connectives in the sense of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as their arguments', ', discourse adverbials may involve crossing dependencies among non - adjacent material - just like other anaphoric expressions. e. g. pronouns and definite descriptions. figure 1 and figure 2 show this type of crossing dependency', 'for in contrast both intrasententially and across sentence boundaries. while figure 2 involves', 'material in adjacent clauses, there are plenty of examples', 'where such dependencies extend over an entire paragraph or over even larger amounts of text']",0
"['of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as']","['hours. b0h ( 0476 ) another property', 'that distinguishes anaphoric discourse adverbials from structural connectives in the sense of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as their arguments', ', discourse adverbials']","['hours. b0h ( 0476 ) another property', 'that distinguishes anaphoric discourse adverbials from structural connectives in the sense of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as']","['and death had to be cushioned away, made to look as if', ""they didn't exist. bpd ( 0200 ) the referent of outside in ( 13 ) is never explicitly mentioned. rather, outside refers back to the entire scene described"", 'before. another type of inference that is sometimes necessitated by the in contrast connective concerns the operation of complementarity of reference as in ( 14 ). ( 14 ) other speed - reducing devices may be', 'added, such as regular shifts in the axis of the road, together with changes in the profile in the form of ramps and speed humps ( figure 4. 3 ). narrowings that', 'allow a cycle to pass but not two cars are frequently added, often reinforced by the placement of trees, planters and street furniture. in contrast to the flowing design of fast roads, design elements are angular and of pedestrian', '##scale, typified by low - level lamp posts which avoid the "" sea of light "" provided by high poles in traffic streets ( figure 4. 4 ). c8f ( 0297 ) in this text, which is', 'on the topic of child safety, roads are never explicitly mentioned. rather the concept', 'of slow neighborhood roads can only be inferred', 'from the description. the first explicit mention of the term road then refers to the opposite term fast roads. comparison of in contrast with personal pronoun', '##s yields yet another similarity with other anaphoric expressions. like with personal pronouns, the antecedent of in contrast can either occur across sentences, as in all of the examples considered so far, or it can occur intras', '##ententially, as in example ( 15 ). ( 15 ) in contrast to his predec', '##essors who worked at all hours of the day macmillan tended to keep office hours. b0h ( 0476 ) another property', 'that distinguishes anaphoric discourse adverbials from structural connectives in the sense of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as their arguments', ', discourse adverbials may involve crossing dependencies among non - adjacent material - just like other anaphoric expressions. e. g. pronouns and definite descriptions. figure 1 and figure 2 show this type of crossing dependency', 'for in contrast both intrasententially and across sentence boundaries. while figure 2 involves', 'material in adjacent clauses, there are plenty of examples', 'where such dependencies extend over an entire paragraph or over even larger amounts of text']",0
"['of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as']","['hours. b0h ( 0476 ) another property', 'that distinguishes anaphoric discourse adverbials from structural connectives in the sense of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as their arguments', ', discourse adverbials']","['hours. b0h ( 0476 ) another property', 'that distinguishes anaphoric discourse adverbials from structural connectives in the sense of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as']","['and death had to be cushioned away, made to look as if', ""they didn't exist. bpd ( 0200 ) the referent of outside in ( 13 ) is never explicitly mentioned. rather, outside refers back to the entire scene described"", 'before. another type of inference that is sometimes necessitated by the in contrast connective concerns the operation of complementarity of reference as in ( 14 ). ( 14 ) other speed - reducing devices may be', 'added, such as regular shifts in the axis of the road, together with changes in the profile in the form of ramps and speed humps ( figure 4. 3 ). narrowings that', 'allow a cycle to pass but not two cars are frequently added, often reinforced by the placement of trees, planters and street furniture. in contrast to the flowing design of fast roads, design elements are angular and of pedestrian', '##scale, typified by low - level lamp posts which avoid the "" sea of light "" provided by high poles in traffic streets ( figure 4. 4 ). c8f ( 0297 ) in this text, which is', 'on the topic of child safety, roads are never explicitly mentioned. rather the concept', 'of slow neighborhood roads can only be inferred', 'from the description. the first explicit mention of the term road then refers to the opposite term fast roads. comparison of in contrast with personal pronoun', '##s yields yet another similarity with other anaphoric expressions. like with personal pronouns, the antecedent of in contrast can either occur across sentences, as in all of the examples considered so far, or it can occur intras', '##ententially, as in example ( 15 ). ( 15 ) in contrast to his predec', '##essors who worked at all hours of the day macmillan tended to keep office hours. b0h ( 0476 ) another property', 'that distinguishes anaphoric discourse adverbials from structural connectives in the sense of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as their arguments', ', discourse adverbials may involve crossing dependencies among non - adjacent material - just like other anaphoric expressions. e. g. pronouns and definite descriptions. figure 1 and figure 2 show this type of crossing dependency', 'for in contrast both intrasententially and across sentence boundaries. while figure 2 involves', 'material in adjacent clauses, there are plenty of examples', 'where such dependencies extend over an entire paragraph or over even larger amounts of text']",0
"['of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as']","['hours. b0h ( 0476 ) another property', 'that distinguishes anaphoric discourse adverbials from structural connectives in the sense of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as their arguments', ', discourse adverbials']","['hours. b0h ( 0476 ) another property', 'that distinguishes anaphoric discourse adverbials from structural connectives in the sense of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as']","['and death had to be cushioned away, made to look as if', ""they didn't exist. bpd ( 0200 ) the referent of outside in ( 13 ) is never explicitly mentioned. rather, outside refers back to the entire scene described"", 'before. another type of inference that is sometimes necessitated by the in contrast connective concerns the operation of complementarity of reference as in ( 14 ). ( 14 ) other speed - reducing devices may be', 'added, such as regular shifts in the axis of the road, together with changes in the profile in the form of ramps and speed humps ( figure 4. 3 ). narrowings that', 'allow a cycle to pass but not two cars are frequently added, often reinforced by the placement of trees, planters and street furniture. in contrast to the flowing design of fast roads, design elements are angular and of pedestrian', '##scale, typified by low - level lamp posts which avoid the "" sea of light "" provided by high poles in traffic streets ( figure 4. 4 ). c8f ( 0297 ) in this text, which is', 'on the topic of child safety, roads are never explicitly mentioned. rather the concept', 'of slow neighborhood roads can only be inferred', 'from the description. the first explicit mention of the term road then refers to the opposite term fast roads. comparison of in contrast with personal pronoun', '##s yields yet another similarity with other anaphoric expressions. like with personal pronouns, the antecedent of in contrast can either occur across sentences, as in all of the examples considered so far, or it can occur intras', '##ententially, as in example ( 15 ). ( 15 ) in contrast to his predec', '##essors who worked at all hours of the day macmillan tended to keep office hours. b0h ( 0476 ) another property', 'that distinguishes anaphoric discourse adverbials from structural connectives in the sense of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as their arguments', ', discourse adverbials may involve crossing dependencies among non - adjacent material - just like other anaphoric expressions. e. g. pronouns and definite descriptions. figure 1 and figure 2 show this type of crossing dependency', 'for in contrast both intrasententially and across sentence boundaries. while figure 2 involves', 'material in adjacent clauses, there are plenty of examples', 'where such dependencies extend over an entire paragraph or over even larger amounts of text']",0
"[' #AUTHOR_TAG and  #AUTHOR_TAG,  #TAUTHOR_TAG assume']","[' #AUTHOR_TAG and  #AUTHOR_TAG,  #TAUTHOR_TAG assume']","[' #AUTHOR_TAG and  #AUTHOR_TAG,  #TAUTHOR_TAG assume']","['', 'following earlier proposals by  #AUTHOR_TAG and  #AUTHOR_TAG,  #TAUTHOR_TAG assume that the semantics of discourse adverbials such as then involves an anaphoric relation between two events.', 'for example, the two clauses in ( 16 ) refer to individual events, which are put in the sequence - relation by the adverbial then.', '']",0
"['is further elaborated by  #TAUTHOR_TAG who distinguish between coordinating conjunctions such as and, or, so, and but and subordinating conjunctions such as although, whereas, and when on the one hand, and']","['is further elaborated by  #TAUTHOR_TAG who distinguish between coordinating conjunctions such as and, or, so, and but and subordinating conjunctions such as although, whereas, and when on the one hand, and']","['is further elaborated by  #TAUTHOR_TAG who distinguish between coordinating conjunctions such as and, or, so, and but and subordinating conjunctions such as although, whereas, and when on the one hand, and discourse adverbials such as']","['', '2. while syntactic dependencies can be quite complex and may involve highly nested or even crossing dependencies of various kinds, dependencies expressed by discourse connectives tend to be much more limited, typically involving tree - like structures and not introducing structural ambiguities of scope or attachment.', '3. more complex cases of discourse connectives that prima facie seem to involve crossing or partially overlapping arguments can be reduced to the independent discourse mechanisms of anaphora and attribution and thus do not introduce any added complexities.', 'the third generalization is further elaborated by  #TAUTHOR_TAG who distinguish between coordinating conjunctions such as and, or, so, and but and subordinating conjunctions such as although, whereas, and when on the one hand, and discourse adverbials such as then, otherwise, nevertheless, and instead on the other hand.', 'it is the latter group, namely discourse adverbials, that, according to  #TAUTHOR_TAG, should be considered as anaphors in very much the same way as other anaphoric expressions']",3
"['of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as']","['hours. b0h ( 0476 ) another property', 'that distinguishes anaphoric discourse adverbials from structural connectives in the sense of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as their arguments', ', discourse adverbials']","['hours. b0h ( 0476 ) another property', 'that distinguishes anaphoric discourse adverbials from structural connectives in the sense of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as']","['and death had to be cushioned away, made to look as if', ""they didn't exist. bpd ( 0200 ) the referent of outside in ( 13 ) is never explicitly mentioned. rather, outside refers back to the entire scene described"", 'before. another type of inference that is sometimes necessitated by the in contrast connective concerns the operation of complementarity of reference as in ( 14 ). ( 14 ) other speed - reducing devices may be', 'added, such as regular shifts in the axis of the road, together with changes in the profile in the form of ramps and speed humps ( figure 4. 3 ). narrowings that', 'allow a cycle to pass but not two cars are frequently added, often reinforced by the placement of trees, planters and street furniture. in contrast to the flowing design of fast roads, design elements are angular and of pedestrian', '##scale, typified by low - level lamp posts which avoid the "" sea of light "" provided by high poles in traffic streets ( figure 4. 4 ). c8f ( 0297 ) in this text, which is', 'on the topic of child safety, roads are never explicitly mentioned. rather the concept', 'of slow neighborhood roads can only be inferred', 'from the description. the first explicit mention of the term road then refers to the opposite term fast roads. comparison of in contrast with personal pronoun', '##s yields yet another similarity with other anaphoric expressions. like with personal pronouns, the antecedent of in contrast can either occur across sentences, as in all of the examples considered so far, or it can occur intras', '##ententially, as in example ( 15 ). ( 15 ) in contrast to his predec', '##essors who worked at all hours of the day macmillan tended to keep office hours. b0h ( 0476 ) another property', 'that distinguishes anaphoric discourse adverbials from structural connectives in the sense of  #TAUTHOR_TAG, i. e. coordinating and subordinating conjunctions, concerns', 'the type of dependencies that the arguments of the types of connectives can enter into. while structural connectives', 'only allow non - crossing adjacent material as their arguments', ', discourse adverbials may involve crossing dependencies among non - adjacent material - just like other anaphoric expressions. e. g. pronouns and definite descriptions. figure 1 and figure 2 show this type of crossing dependency', 'for in contrast both intrasententially and across sentence boundaries. while figure 2 involves', 'material in adjacent clauses, there are plenty of examples', 'where such dependencies extend over an entire paragraph or over even larger amounts of text']",3
"['of 192, 800 word occurrences reported in  #TAUTHOR_TAG, i examine the']","['of 192, 800 word occurrences reported in  #TAUTHOR_TAG, i examine the']","['', 'using the sense - tagged corpus of 192, 800 word occurrences reported in  #TAUTHOR_TAG, i examine the effect of the number of training examples on']","['advances in large - scale, broad coverage part - of - speech tagging and syntactic parsing have been achieved in no small part due to the availability of large amounts of online, human - annotated corpora.', 'in this paper, i argue that a large, human sensetagged corpus is also critical as well as necessary to achieve broad coverage, high accuracy word sense disambiguation, where the sense distinction is at the level of a good desk - top dictionary such as word - net.', 'using the sense - tagged corpus of 192, 800 word occurrences reported in  #TAUTHOR_TAG, i examine the effect of the number of training examples on the accuracy of an exemplar - based classifier versus the base - line, most - frequent - sense classitier.', 'i also estimate the amount of human sense - tagged corpus and the manual annotation effort needed to build a largescale, broad coverage word sense disambiguation program which can significantly outperform the most - frequent - sense classifier.', 'finally, i suggest that intelligent example selection techniques may significantly reduce the amount of sense - tagged corpus needed and offer this research problem as a fruitful area for word sense disambiguation research']",5
['##net senses  #TAUTHOR_TAG'],"['of wsd, using a corpus of 192, 800 occurrences of 191 words hand tagged with wordnet senses  #TAUTHOR_TAG']","['##net senses  #TAUTHOR_TAG.', 'in section 4, i estimate the amount of human sense - tagged corpus and the manual annotation effort needed to build a broad coverage,']","['', 'in section 3, i examine the size of the training corpus on the accuracy of wsd, using a corpus of 192, 800 occurrences of 191 words hand tagged with wordnet senses  #TAUTHOR_TAG.', 'in section 4, i estimate the amount of human sense - tagged corpus and the manual annotation effort needed to build a broad coverage, high accuracy wsd program.', 'finally, in section 5, i suggest that intelligent example selection techniques may significantly reduce the amount of sense - tagged corpus needed and offer this research problem as a fruitful area for wsd research']",5
['are described in  #TAUTHOR_TAG'],['are described in  #TAUTHOR_TAG'],['curves for the 191 words ( details of lexas are described in  #TAUTHOR_TAG'],"['number of training examples to obtain learning curves for the 191 words ( details of lexas are described in  #TAUTHOR_TAG. for each word, 10 random trials were', 'conducted and the accuracy figures were averaged over the i0 trials. in', 'each trial, i00 examples were randomly selected to form the test set, while the remaining examples ( randomly shuffled ) were used for training. lexas', 'was given training examples in multiple s of i00, starting with i00, 200, 300,... training examples, up to the maximum number of training examples ( in a multiple of 100 ) available in the corpus. note that each word w (', 'of the 191 words ) can have a different number of sense - tagged occurrences in our corpus. from the combination of brown corpus ( 1 million words ) and wall street journal corpus ( 2. 5 million words ), up', 'to 1, 500 sentences each containing an occurrence of the word w are extracted from the combined corpus, with each sentence containing a sense -', 'tagged', 'occurrence of w. when the combined corpus has less than 1, 500 occurrences of w, the max = imum number of available occurrences of w is used. for instance, while 137 words have at least 600 occurrences in the combined corpus, only a subset of 43', 'words has at least 1400 occurrences. figure 1 and 2 show the learning curves averaged over these 43 words and', '']",5
['are described in  #TAUTHOR_TAG'],['are described in  #TAUTHOR_TAG'],['curves for the 191 words ( details of lexas are described in  #TAUTHOR_TAG'],"['number of training examples to obtain learning curves for the 191 words ( details of lexas are described in  #TAUTHOR_TAG. for each word, 10 random trials were', 'conducted and the accuracy figures were averaged over the i0 trials. in', 'each trial, i00 examples were randomly selected to form the test set, while the remaining examples ( randomly shuffled ) were used for training. lexas', 'was given training examples in multiple s of i00, starting with i00, 200, 300,... training examples, up to the maximum number of training examples ( in a multiple of 100 ) available in the corpus. note that each word w (', 'of the 191 words ) can have a different number of sense - tagged occurrences in our corpus. from the combination of brown corpus ( 1 million words ) and wall street journal corpus ( 2. 5 million words ), up', 'to 1, 500 sentences each containing an occurrence of the word w are extracted from the combined corpus, with each sentence containing a sense -', 'tagged', 'occurrence of w. when the combined corpus has less than 1, 500 occurrences of w, the max = imum number of available occurrences of w is used. for instance, while 137 words have at least 600 occurrences in the combined corpus, only a subset of 43', 'words has at least 1400 occurrences. figure 1 and 2 show the learning curves averaged over these 43 words and', '']",5
"['are generally not very high  #TAUTHOR_TAG.', 'this indicates that ws']","['are generally not very high  #TAUTHOR_TAG.', 'this indicates that wsd is a challenging task and much']","['- net senses, the accuracy figures achieved are generally not very high  #TAUTHOR_TAG.', 'this indicates that ws']","['', 'these examples clearly demonstrate the utility of wsd in practical nlp applications.', 'in this paper, by word sense disambiguation, i mean identifying the correct sense of a word in context such that the sense distinction is at the level of a good desk - top dictionary like wordnet  #AUTHOR_TAG.', 'i only focus on content word disambiguation ( i. e., words in the part of speech noun t, verb, adjective and adverb ).', 'this is also the task addressed by other wsd research such as  #AUTHOR_TAG.', 'when the task is to resolve word senses to the fine - grain distinction of word - net senses, the accuracy figures achieved are generally not very high  #TAUTHOR_TAG.', 'this indicates that wsd is a challenging task and much improvement is still needed.', 'however, if one were to resolve word sense to the level of homograph, or coarse sense distinction, then quite high accuracy can be achieved ( in excess of 90 % ), as reported in  #AUTHOR_TAG.', 'similarly, if the task is to distinguish between binary, coarse sense distinction, then current wsd techniques can achieve very high accuracy ( in excess of 96 % when tested on a dozen words in  #AUTHOR_TAG ).', 'this is to be expected, since homograph contexts are quite distinct and hence it is a much simpler task to disambiguate among a small number of coarse sense classes.', 'this is in contrast to disambiguating word senses to the refined senses of wordnet, where for instance, the average number of senses per noun is 7. 8 and the average number of senses per verb is 12. 0 for the set of 191 most ambiguous words investigated in  #TAUTHOR_TAG.', 'we can readily collapse the refined senses of wordnet into a smaller set if only a coarse ( hot i will only focus on common noun in this paper and ignore proper noun.', '2 mographic ) sense distinction is needed, say for']",0
"['are generally not very high  #TAUTHOR_TAG.', 'this indicates that ws']","['are generally not very high  #TAUTHOR_TAG.', 'this indicates that wsd is a challenging task and much']","['- net senses, the accuracy figures achieved are generally not very high  #TAUTHOR_TAG.', 'this indicates that ws']","['', 'these examples clearly demonstrate the utility of wsd in practical nlp applications.', 'in this paper, by word sense disambiguation, i mean identifying the correct sense of a word in context such that the sense distinction is at the level of a good desk - top dictionary like wordnet  #AUTHOR_TAG.', 'i only focus on content word disambiguation ( i. e., words in the part of speech noun t, verb, adjective and adverb ).', 'this is also the task addressed by other wsd research such as  #AUTHOR_TAG.', 'when the task is to resolve word senses to the fine - grain distinction of word - net senses, the accuracy figures achieved are generally not very high  #TAUTHOR_TAG.', 'this indicates that wsd is a challenging task and much improvement is still needed.', 'however, if one were to resolve word sense to the level of homograph, or coarse sense distinction, then quite high accuracy can be achieved ( in excess of 90 % ), as reported in  #AUTHOR_TAG.', 'similarly, if the task is to distinguish between binary, coarse sense distinction, then current wsd techniques can achieve very high accuracy ( in excess of 96 % when tested on a dozen words in  #AUTHOR_TAG ).', 'this is to be expected, since homograph contexts are quite distinct and hence it is a much simpler task to disambiguate among a small number of coarse sense classes.', 'this is in contrast to disambiguating word senses to the refined senses of wordnet, where for instance, the average number of senses per noun is 7. 8 and the average number of senses per verb is 12. 0 for the set of 191 most ambiguous words investigated in  #TAUTHOR_TAG.', 'we can readily collapse the refined senses of wordnet into a smaller set if only a coarse ( hot i will only focus on common noun in this paper and ignore proper noun.', '2 mographic ) sense distinction is needed, say for']",0
"['are generally not very high  #TAUTHOR_TAG.', 'this indicates that ws']","['are generally not very high  #TAUTHOR_TAG.', 'this indicates that wsd is a challenging task and much']","['- net senses, the accuracy figures achieved are generally not very high  #TAUTHOR_TAG.', 'this indicates that ws']","['', 'these examples clearly demonstrate the utility of wsd in practical nlp applications.', 'in this paper, by word sense disambiguation, i mean identifying the correct sense of a word in context such that the sense distinction is at the level of a good desk - top dictionary like wordnet  #AUTHOR_TAG.', 'i only focus on content word disambiguation ( i. e., words in the part of speech noun t, verb, adjective and adverb ).', 'this is also the task addressed by other wsd research such as  #AUTHOR_TAG.', 'when the task is to resolve word senses to the fine - grain distinction of word - net senses, the accuracy figures achieved are generally not very high  #TAUTHOR_TAG.', 'this indicates that wsd is a challenging task and much improvement is still needed.', 'however, if one were to resolve word sense to the level of homograph, or coarse sense distinction, then quite high accuracy can be achieved ( in excess of 90 % ), as reported in  #AUTHOR_TAG.', 'similarly, if the task is to distinguish between binary, coarse sense distinction, then current wsd techniques can achieve very high accuracy ( in excess of 96 % when tested on a dozen words in  #AUTHOR_TAG ).', 'this is to be expected, since homograph contexts are quite distinct and hence it is a much simpler task to disambiguate among a small number of coarse sense classes.', 'this is in contrast to disambiguating word senses to the refined senses of wordnet, where for instance, the average number of senses per noun is 7. 8 and the average number of senses per verb is 12. 0 for the set of 191 most ambiguous words investigated in  #TAUTHOR_TAG.', 'we can readily collapse the refined senses of wordnet into a smaller set if only a coarse ( hot i will only focus on common noun in this paper and ignore proper noun.', '2 mographic ) sense distinction is needed, say for']",1
"['are generally not very high  #TAUTHOR_TAG.', 'this indicates that ws']","['are generally not very high  #TAUTHOR_TAG.', 'this indicates that wsd is a challenging task and much']","['- net senses, the accuracy figures achieved are generally not very high  #TAUTHOR_TAG.', 'this indicates that ws']","['', 'these examples clearly demonstrate the utility of wsd in practical nlp applications.', 'in this paper, by word sense disambiguation, i mean identifying the correct sense of a word in context such that the sense distinction is at the level of a good desk - top dictionary like wordnet  #AUTHOR_TAG.', 'i only focus on content word disambiguation ( i. e., words in the part of speech noun t, verb, adjective and adverb ).', 'this is also the task addressed by other wsd research such as  #AUTHOR_TAG.', 'when the task is to resolve word senses to the fine - grain distinction of word - net senses, the accuracy figures achieved are generally not very high  #TAUTHOR_TAG.', 'this indicates that wsd is a challenging task and much improvement is still needed.', 'however, if one were to resolve word sense to the level of homograph, or coarse sense distinction, then quite high accuracy can be achieved ( in excess of 90 % ), as reported in  #AUTHOR_TAG.', 'similarly, if the task is to distinguish between binary, coarse sense distinction, then current wsd techniques can achieve very high accuracy ( in excess of 96 % when tested on a dozen words in  #AUTHOR_TAG ).', 'this is to be expected, since homograph contexts are quite distinct and hence it is a much simpler task to disambiguate among a small number of coarse sense classes.', 'this is in contrast to disambiguating word senses to the refined senses of wordnet, where for instance, the average number of senses per noun is 7. 8 and the average number of senses per verb is 12. 0 for the set of 191 most ambiguous words investigated in  #TAUTHOR_TAG.', 'we can readily collapse the refined senses of wordnet into a smaller set if only a coarse ( hot i will only focus on common noun in this paper and ignore proper noun.', '2 mographic ) sense distinction is needed, say for']",6
['are described in  #TAUTHOR_TAG'],['are described in  #TAUTHOR_TAG'],['curves for the 191 words ( details of lexas are described in  #TAUTHOR_TAG'],"['number of training examples to obtain learning curves for the 191 words ( details of lexas are described in  #TAUTHOR_TAG. for each word, 10 random trials were', 'conducted and the accuracy figures were averaged over the i0 trials. in', 'each trial, i00 examples were randomly selected to form the test set, while the remaining examples ( randomly shuffled ) were used for training. lexas', 'was given training examples in multiple s of i00, starting with i00, 200, 300,... training examples, up to the maximum number of training examples ( in a multiple of 100 ) available in the corpus. note that each word w (', 'of the 191 words ) can have a different number of sense - tagged occurrences in our corpus. from the combination of brown corpus ( 1 million words ) and wall street journal corpus ( 2. 5 million words ), up', 'to 1, 500 sentences each containing an occurrence of the word w are extracted from the combined corpus, with each sentence containing a sense -', 'tagged', 'occurrence of w. when the combined corpus has less than 1, 500 occurrences of w, the max = imum number of available occurrences of w is used. for instance, while 137 words have at least 600 occurrences in the combined corpus, only a subset of 43', 'words has at least 1400 occurrences. figure 1 and 2 show the learning curves averaged over these 43 words and', '']",4
['methodologies  #TAUTHOR_TAG'],['methodologies  #TAUTHOR_TAG'],['different approaches have been proposed for sentence compression : purely statistical methodologies  #TAUTHOR_TAG'],"['different approaches have been proposed for sentence compression : purely statistical methodologies  #TAUTHOR_TAG and hybrid linguistic / statistic methodologies  #AUTHOR_TAG.', 'as our work is based on the first paradigm, we will focus on the works proposed by  #TAUTHOR_TAG and  #AUTHOR_TAG.', ' #AUTHOR_TAG present a knowledge - lean algorithm that uses multiple - sequence alignment to learn generate sentence - level paraphrases essentially from unannotated corpus data alone.', 'in contrast to  #AUTHOR_TAG, they need neither parallel data nor explicit information about sentence semantics.', 'rather, they use two comparable corpora.', '']",4
"[', as  #TAUTHOR_TAG evidence, clusters of paraphr']","['one hand, as  #TAUTHOR_TAG evidence, clusters of paraphrases']","['one hand, as  #TAUTHOR_TAG evidence, clusters of paraphr']","['shows that there are two main reasons to apply clustering for paraphrase extraction.', 'on one hand, as  #TAUTHOR_TAG evidence, clusters of paraphrases can lead to better learning of text - totext rewriting rules compared to just pairs of paraphrases.', 'on the other hand, clustering algorithms may lead to better performance than stand - alone similarity measures as they may take advantage of the different structures of sentences in the cluster to detect a new similar sentence.', 'however, as  #TAUTHOR_TAG do not propose any evaluation of which clustering algorithm should be used, we experiment a set of clustering algorithms and present the comparative results.', 'contrarily to what expected, we will see that clustering is not a worthy effort.', 'instead of extracting only sentence pairs from corpora 3, one may consider the extraction of paraphrase sentence clusters.', 'there are many well - known clustering algorithms, which may be applied to a corpus sentence set s = { s 1,..., s n }.', 'clustering implies the definition of a similarity or ( distance ) matrix a n×n, where each each element a ij is the similarity ( distance ) between sentences s i and s j.', '3 a pair may be seen as a cluster with only two elements']",4
[' #TAUTHOR_TAG suggest'],[' #TAUTHOR_TAG suggest'],[' #TAUTHOR_TAG suggest'],"['', 'then, according to the clustering method used, several types of clusters can be expected : very small clusters which contain "" satellite "" data ( pretty relevant ) or large clusters with part of the main central class ( pretty irrelevant ).', 'these results confirm the observed figures in the previous subsection and reinforce the sight that clustering is a worthless effort for automatic paraphrase corpora construction, contrarily to what  #TAUTHOR_TAG suggest']",4
"['literature claims  #TAUTHOR_TAG.', 'therefore simple parap']","['literature claims  #TAUTHOR_TAG.', 'therefore simple']","['set of important steps toward automatic construction of aligned paraphrase corpora are presented and inherent relevant issues discussed, like clustering and alignment.', 'experiments, by using 4 algorithms and through visualization techniques, revealed that clustering is a worthless effort for paraphrase corpora construction, contrary to the literature claims  #TAUTHOR_TAG.', 'therefore simple paraphrase pair extraction']","['set of important steps toward automatic construction of aligned paraphrase corpora are presented and inherent relevant issues discussed, like clustering and alignment.', 'experiments, by using 4 algorithms and through visualization techniques, revealed that clustering is a worthless effort for paraphrase corpora construction, contrary to the literature claims  #TAUTHOR_TAG.', 'therefore simple paraphrase pair extraction is suggested and by using a recent and more reliable metric ( sumo - metric )  #AUTHOR_TAG designed for asymmetrical entailed pairs.', 'we also propose a dynamic choosing of the alignment algorithm and a word scoring function for the alignment algorithms.', 'in the future we intend to clean the automatic constructed corpus by introducing syntactical constraints to filter the wrong alignments.', 'our next step will be to employ machine learning techniques for rewriting rule induction, by using this automatically constructed aligned paraphrase corpus']",4
['to question the results presented by  #TAUTHOR_TAG who only keep the clusters that contain more than'],['to question the results presented by  #TAUTHOR_TAG who only keep the clusters that contain more than'],"['to question the results presented by  #TAUTHOR_TAG who only keep the clusters that contain more than 10 sentences.', 'in fact, the first conclusion is that']","['', 'one main conclusion, from table 1 is that clustering tends to achieve worst results than simple paraphrase pair extraction.', 'only the qt achieves better results, but if we take the average of the four clustering algorithms it is equal to 0. 568, smaller than the 0. 618 baseline.', 'moreover, these results with the qt algorithm were applied with a very restrictive value for cluster attribution as it is shown in table 2 with an average of almost two sentences per cluster.', 'in fact, table 2 shows that most of the clusters have less than 6 sentences which leads to question the results presented by  #TAUTHOR_TAG who only keep the clusters that contain more than 10 sentences.', 'in fact, the first conclusion is that the number of experimented clusters is very low, and more important, all clusters with more than 10 sentences showed to be of very bad quality.', 'the next subsection will reinforce the sight that clustering is a worthless effort for automatic paraphrase corpora construction']",7
"['training', 'step. sgns is preferred in general, yet sghs showed slight benefits in some reliability scenarios in our prior', 'investigations  #TAUTHOR_TAG. there are']","['each training', 'step. sgns is preferred in general, yet sghs showed slight benefits in some reliability scenarios in our prior', 'investigations  #TAUTHOR_TAG. there are']","['training', 'step. sgns is preferred in general, yet sghs showed slight benefits in some reliability scenarios in our prior', 'investigations  #TAUTHOR_TAG. there are two']","['is generally reported to be superior ( see e. g.,  #AUTHOR_TAG ). there are two strategies for managing the huge number of potential contexts a word can appear in. skip - gram hierarchical softmax ( sghs ) uses a binary', 'tree to more efficiently represent the vocabulary, whereas skip - gram negative sampling ( sgns ) updates only a limited number of word vectors during each training', 'step. sgns is preferred in general, yet sghs showed slight benefits in some reliability scenarios in our prior', 'investigations  #TAUTHOR_TAG. there are two sources of randomness involved in the training of neural word embeddings : first, the random initialization of all word vectors before any examples are processed. second,', 'the order in which these examples are processed. both can be replaced by deterministic alternatives, 1 yet this would simply replace a random distortion with a fixed one, thus providing faux', 'reliability only useful for testing purposes. a range of other word embedding algorithms was inspired by word2vec, either trying to avoid the opaqueness stemming from its neural network heritage ( glove ;', 'still using random initialization, see  #AUTHOR_TAG ) or adding capabilities, like using syntactic information during training  #AUTHOR_TAG or modeling', 'multiple word senses  #AUTHOR_TAG.  #AUTHOR_TAG created svd ppmi, a variant of the classical pointwise mutual information co - occurrence metric ( see e. g., manning', 'and schutze ( 1999, pp. 178 - 183 ) ), by transferring pre - processing steps and hyper - parameters uncovered by the development of these algorithms, and reported similar or slightly better performance', 'than sgns on evaluation tasks. it is conceptually not affected by reliability problems, as there is no random initialization or relevant processing order. word embeddings capture both syntactic and semantic information ( and arguably also social biases, see  #AUTHOR_TAG', ') in vector form and can thus be evaluated by their ability to calculate the similarity of two words and perform analogy - based reasoning ; there exist several other evaluation methods and more test sets than discussed', 'here,', 'see e. g.,  #AUTHOR_TAG.  #AUTHOR_TAG provide an analogy test set for measuring performance as the percentage of correctly calculated analogies for test cases such as', ""the frequently cited'king'-'queen'example ( see section 3 ). word similarity is evaluated by calculating spearman's rank coefficient between embedding - derived predictions and a gold standard of human word similarity judgments.  #AUTHOR_TAG developed a widely used test set with 353"", 'english word pairs, 2 a similar resource for german with 350 word pairs was provided by  #AUTHOR_TAG', '. 3 recent work cautions that performance on such tasks is not always predictive for performance in down - stream applications  #AUTHOR_TAG']",0
"['training', 'step. sgns is preferred in general, yet sghs showed slight benefits in some reliability scenarios in our prior', 'investigations  #TAUTHOR_TAG. there are']","['each training', 'step. sgns is preferred in general, yet sghs showed slight benefits in some reliability scenarios in our prior', 'investigations  #TAUTHOR_TAG. there are']","['training', 'step. sgns is preferred in general, yet sghs showed slight benefits in some reliability scenarios in our prior', 'investigations  #TAUTHOR_TAG. there are two']","['is generally reported to be superior ( see e. g.,  #AUTHOR_TAG ). there are two strategies for managing the huge number of potential contexts a word can appear in. skip - gram hierarchical softmax ( sghs ) uses a binary', 'tree to more efficiently represent the vocabulary, whereas skip - gram negative sampling ( sgns ) updates only a limited number of word vectors during each training', 'step. sgns is preferred in general, yet sghs showed slight benefits in some reliability scenarios in our prior', 'investigations  #TAUTHOR_TAG. there are two sources of randomness involved in the training of neural word embeddings : first, the random initialization of all word vectors before any examples are processed. second,', 'the order in which these examples are processed. both can be replaced by deterministic alternatives, 1 yet this would simply replace a random distortion with a fixed one, thus providing faux', 'reliability only useful for testing purposes. a range of other word embedding algorithms was inspired by word2vec, either trying to avoid the opaqueness stemming from its neural network heritage ( glove ;', 'still using random initialization, see  #AUTHOR_TAG ) or adding capabilities, like using syntactic information during training  #AUTHOR_TAG or modeling', 'multiple word senses  #AUTHOR_TAG.  #AUTHOR_TAG created svd ppmi, a variant of the classical pointwise mutual information co - occurrence metric ( see e. g., manning', 'and schutze ( 1999, pp. 178 - 183 ) ), by transferring pre - processing steps and hyper - parameters uncovered by the development of these algorithms, and reported similar or slightly better performance', 'than sgns on evaluation tasks. it is conceptually not affected by reliability problems, as there is no random initialization or relevant processing order. word embeddings capture both syntactic and semantic information ( and arguably also social biases, see  #AUTHOR_TAG', ') in vector form and can thus be evaluated by their ability to calculate the similarity of two words and perform analogy - based reasoning ; there exist several other evaluation methods and more test sets than discussed', 'here,', 'see e. g.,  #AUTHOR_TAG.  #AUTHOR_TAG provide an analogy test set for measuring performance as the percentage of correctly calculated analogies for test cases such as', ""the frequently cited'king'-'queen'example ( see section 3 ). word similarity is evaluated by calculating spearman's rank coefficient between embedding - derived predictions and a gold standard of human word similarity judgments.  #AUTHOR_TAG developed a widely used test set with 353"", 'english word pairs, 2 a similar resource for german with 350 word pairs was provided by  #AUTHOR_TAG', '. 3 recent work cautions that performance on such tasks is not always predictive for performance in down - stream applications  #AUTHOR_TAG']",0
"['samples  #TAUTHOR_TAG.', 'word embeddings']","['samples  #TAUTHOR_TAG.', 'word embeddings']","['different samples  #TAUTHOR_TAG.', 'word embeddings']","['embeddings can be used rather directly for tracking semantic changes, namely by measuring the similarity of word representations generated for one word at different points in time - words which underwent semantic shifts will be dissimilar with themselves.', 'these models must either be trained in a continuous manner where the model for each time span is initialized with its predecessor  #AUTHOR_TAG b ), or a mapping between models for different points in time must be calculated  #AUTHOR_TAG.', 'the first approach cannot be performed in parallel and is thus rather time - consuming, if texts are not subsampled.', 'we nevertheless discourage using samples instead of full corpora, as we observed extremely low reliability values between different samples  #TAUTHOR_TAG.', 'word embeddings can also be used in diachronic studies without any kind of mapping to track clusters of similar words over time and, thus, model the evolution of topics  #AUTHOR_TAG or compare neighborhoods in embedding space for preselected words  #AUTHOR_TAG.', 'besides temporal variations, word embeddings can also used to analyze geographic ones, e. g., the distinction between us american and british english variants  #AUTHOR_TAG.', 'most of these studies were performed with algorithms from the word2vec family, respectively glove in  #AUTHOR_TAG, and are thus likely to be affected by the same systematic reliability problems on which we focus here.', ' #AUTHOR_TAG used svd ppmi in some of their very recent experiments and showed it to be adequate for exploring historical semantics.', 'the google books ngram corpus ( gbn ;  #AUTHOR_TAG,  #AUTHOR_TAG is used in most of the studies we already mentioned, including our current study and its predecessor  #TAUTHOR_TAG.', 'it contains about 6 % of all books published between 1500 and 2009 in the form of n - grams ( up to pentagrams ), together with their frequency for each year.', 'this corpus has often been criticized for its opaque sampling strategy, as its constituent books remain unknown and can be shown to form an unbalanced collection  #AUTHOR_TAG.', 'gbn is multilingual, with its english part being subdivided into regional segments ( british, us ) and topic categories ( general language and fiction texts ).', 'diachronic research focuses on the english fiction part, with the exception of some work relating to german data  #AUTHOR_TAG b )']",0
"['samples  #TAUTHOR_TAG.', 'word embeddings']","['samples  #TAUTHOR_TAG.', 'word embeddings']","['different samples  #TAUTHOR_TAG.', 'word embeddings']","['embeddings can be used rather directly for tracking semantic changes, namely by measuring the similarity of word representations generated for one word at different points in time - words which underwent semantic shifts will be dissimilar with themselves.', 'these models must either be trained in a continuous manner where the model for each time span is initialized with its predecessor  #AUTHOR_TAG b ), or a mapping between models for different points in time must be calculated  #AUTHOR_TAG.', 'the first approach cannot be performed in parallel and is thus rather time - consuming, if texts are not subsampled.', 'we nevertheless discourage using samples instead of full corpora, as we observed extremely low reliability values between different samples  #TAUTHOR_TAG.', 'word embeddings can also be used in diachronic studies without any kind of mapping to track clusters of similar words over time and, thus, model the evolution of topics  #AUTHOR_TAG or compare neighborhoods in embedding space for preselected words  #AUTHOR_TAG.', 'besides temporal variations, word embeddings can also used to analyze geographic ones, e. g., the distinction between us american and british english variants  #AUTHOR_TAG.', 'most of these studies were performed with algorithms from the word2vec family, respectively glove in  #AUTHOR_TAG, and are thus likely to be affected by the same systematic reliability problems on which we focus here.', ' #AUTHOR_TAG used svd ppmi in some of their very recent experiments and showed it to be adequate for exploring historical semantics.', 'the google books ngram corpus ( gbn ;  #AUTHOR_TAG,  #AUTHOR_TAG is used in most of the studies we already mentioned, including our current study and its predecessor  #TAUTHOR_TAG.', 'it contains about 6 % of all books published between 1500 and 2009 in the form of n - grams ( up to pentagrams ), together with their frequency for each year.', 'this corpus has often been criticized for its opaque sampling strategy, as its constituent books remain unknown and can be shown to form an unbalanced collection  #AUTHOR_TAG.', 'gbn is multilingual, with its english part being subdivided into regional segments ( british, us ) and topic categories ( general language and fiction texts ).', 'diachronic research focuses on the english fiction part, with the exception of some work relating to german data  #AUTHOR_TAG b )']",0
"['samples  #TAUTHOR_TAG.', 'word embeddings']","['samples  #TAUTHOR_TAG.', 'word embeddings']","['different samples  #TAUTHOR_TAG.', 'word embeddings']","['embeddings can be used rather directly for tracking semantic changes, namely by measuring the similarity of word representations generated for one word at different points in time - words which underwent semantic shifts will be dissimilar with themselves.', 'these models must either be trained in a continuous manner where the model for each time span is initialized with its predecessor  #AUTHOR_TAG b ), or a mapping between models for different points in time must be calculated  #AUTHOR_TAG.', 'the first approach cannot be performed in parallel and is thus rather time - consuming, if texts are not subsampled.', 'we nevertheless discourage using samples instead of full corpora, as we observed extremely low reliability values between different samples  #TAUTHOR_TAG.', 'word embeddings can also be used in diachronic studies without any kind of mapping to track clusters of similar words over time and, thus, model the evolution of topics  #AUTHOR_TAG or compare neighborhoods in embedding space for preselected words  #AUTHOR_TAG.', 'besides temporal variations, word embeddings can also used to analyze geographic ones, e. g., the distinction between us american and british english variants  #AUTHOR_TAG.', 'most of these studies were performed with algorithms from the word2vec family, respectively glove in  #AUTHOR_TAG, and are thus likely to be affected by the same systematic reliability problems on which we focus here.', ' #AUTHOR_TAG used svd ppmi in some of their very recent experiments and showed it to be adequate for exploring historical semantics.', 'the google books ngram corpus ( gbn ;  #AUTHOR_TAG,  #AUTHOR_TAG is used in most of the studies we already mentioned, including our current study and its predecessor  #TAUTHOR_TAG.', 'it contains about 6 % of all books published between 1500 and 2009 in the form of n - grams ( up to pentagrams ), together with their frequency for each year.', 'this corpus has often been criticized for its opaque sampling strategy, as its constituent books remain unknown and can be shown to form an unbalanced collection  #AUTHOR_TAG.', 'gbn is multilingual, with its english part being subdivided into regional segments ( british, us ) and topic categories ( general language and fiction texts ).', 'diachronic research focuses on the english fiction part, with the exception of some work relating to german data  #AUTHOR_TAG b )']",1
"['observed in previous investigations  #TAUTHOR_TAG.', 'we tested']","['observed in previous investigations  #TAUTHOR_TAG.', 'we tested']","['to the extremely low reliability values between samples we observed in previous investigations  #TAUTHOR_TAG.', 'we tested both sgns with 5 noise words and sghs training strategies and trained for 10 iterations, saving the resulting embeddings']","['used the python - based gensim 8 implementation of word2vec to independently train word embeddings for each time span with 200 dimensions, a context window of 4 ( limited by the 5 - gram size ), a minimum frequency of 10, and 10 −5 as the threshold for downsampling frequent words.', 'we processed the full subcorpora for each time span, due to the extremely low reliability values between samples we observed in previous investigations  #TAUTHOR_TAG.', 'we tested both sgns with 5 noise words and sghs training strategies and trained for 10 iterations, saving the resulting embeddings after each epoch.', '']",1
"['samples  #TAUTHOR_TAG.', 'word embeddings']","['samples  #TAUTHOR_TAG.', 'word embeddings']","['different samples  #TAUTHOR_TAG.', 'word embeddings']","['embeddings can be used rather directly for tracking semantic changes, namely by measuring the similarity of word representations generated for one word at different points in time - words which underwent semantic shifts will be dissimilar with themselves.', 'these models must either be trained in a continuous manner where the model for each time span is initialized with its predecessor  #AUTHOR_TAG b ), or a mapping between models for different points in time must be calculated  #AUTHOR_TAG.', 'the first approach cannot be performed in parallel and is thus rather time - consuming, if texts are not subsampled.', 'we nevertheless discourage using samples instead of full corpora, as we observed extremely low reliability values between different samples  #TAUTHOR_TAG.', 'word embeddings can also be used in diachronic studies without any kind of mapping to track clusters of similar words over time and, thus, model the evolution of topics  #AUTHOR_TAG or compare neighborhoods in embedding space for preselected words  #AUTHOR_TAG.', 'besides temporal variations, word embeddings can also used to analyze geographic ones, e. g., the distinction between us american and british english variants  #AUTHOR_TAG.', 'most of these studies were performed with algorithms from the word2vec family, respectively glove in  #AUTHOR_TAG, and are thus likely to be affected by the same systematic reliability problems on which we focus here.', ' #AUTHOR_TAG used svd ppmi in some of their very recent experiments and showed it to be adequate for exploring historical semantics.', 'the google books ngram corpus ( gbn ;  #AUTHOR_TAG,  #AUTHOR_TAG is used in most of the studies we already mentioned, including our current study and its predecessor  #TAUTHOR_TAG.', 'it contains about 6 % of all books published between 1500 and 2009 in the form of n - grams ( up to pentagrams ), together with their frequency for each year.', 'this corpus has often been criticized for its opaque sampling strategy, as its constituent books remain unknown and can be shown to form an unbalanced collection  #AUTHOR_TAG.', 'gbn is multilingual, with its english part being subdivided into regional segments ( british, us ) and topic categories ( general language and fiction texts ).', 'diachronic research focuses on the english fiction part, with the exception of some work relating to german data  #AUTHOR_TAG b )']",5
"['samples  #TAUTHOR_TAG.', 'word embeddings']","['samples  #TAUTHOR_TAG.', 'word embeddings']","['different samples  #TAUTHOR_TAG.', 'word embeddings']","['embeddings can be used rather directly for tracking semantic changes, namely by measuring the similarity of word representations generated for one word at different points in time - words which underwent semantic shifts will be dissimilar with themselves.', 'these models must either be trained in a continuous manner where the model for each time span is initialized with its predecessor  #AUTHOR_TAG b ), or a mapping between models for different points in time must be calculated  #AUTHOR_TAG.', 'the first approach cannot be performed in parallel and is thus rather time - consuming, if texts are not subsampled.', 'we nevertheless discourage using samples instead of full corpora, as we observed extremely low reliability values between different samples  #TAUTHOR_TAG.', 'word embeddings can also be used in diachronic studies without any kind of mapping to track clusters of similar words over time and, thus, model the evolution of topics  #AUTHOR_TAG or compare neighborhoods in embedding space for preselected words  #AUTHOR_TAG.', 'besides temporal variations, word embeddings can also used to analyze geographic ones, e. g., the distinction between us american and british english variants  #AUTHOR_TAG.', 'most of these studies were performed with algorithms from the word2vec family, respectively glove in  #AUTHOR_TAG, and are thus likely to be affected by the same systematic reliability problems on which we focus here.', ' #AUTHOR_TAG used svd ppmi in some of their very recent experiments and showed it to be adequate for exploring historical semantics.', 'the google books ngram corpus ( gbn ;  #AUTHOR_TAG,  #AUTHOR_TAG is used in most of the studies we already mentioned, including our current study and its predecessor  #TAUTHOR_TAG.', 'it contains about 6 % of all books published between 1500 and 2009 in the form of n - grams ( up to pentagrams ), together with their frequency for each year.', 'this corpus has often been criticized for its opaque sampling strategy, as its constituent books remain unknown and can be shown to form an unbalanced collection  #AUTHOR_TAG.', 'gbn is multilingual, with its english part being subdivided into regional segments ( british, us ) and topic categories ( general language and fiction texts ).', 'diachronic research focuses on the english fiction part, with the exception of some work relating to german data  #AUTHOR_TAG b )']",7
"['observed in previous investigations  #TAUTHOR_TAG.', 'we tested']","['observed in previous investigations  #TAUTHOR_TAG.', 'we tested']","['to the extremely low reliability values between samples we observed in previous investigations  #TAUTHOR_TAG.', 'we tested both sgns with 5 noise words and sghs training strategies and trained for 10 iterations, saving the resulting embeddings']","['used the python - based gensim 8 implementation of word2vec to independently train word embeddings for each time span with 200 dimensions, a context window of 4 ( limited by the 5 - gram size ), a minimum frequency of 10, and 10 −5 as the threshold for downsampling frequent words.', 'we processed the full subcorpora for each time span, due to the extremely low reliability values between samples we observed in previous investigations  #TAUTHOR_TAG.', 'we tested both sgns with 5 noise words and sghs training strategies and trained for 10 iterations, saving the resulting embeddings after each epoch.', '']",3
"['.', 'with proper averaging in place, we notice that the distillation model described in  #TAUTHOR_TAG, which incorporates explicit logic rules']","['reported.', 'with proper averaging in place, we notice that the distillation model described in  #TAUTHOR_TAG, which incorporates explicit logic rules']","['.', 'with proper averaging in place, we notice that the distillation model described in  #TAUTHOR_TAG, which incorporates explicit logic rules']","['analyze the performance of different sentiment classification models on syntacticallycomplex inputs like a - but - b sentences.', 'the first contribution of this analysis addresses reproducible research : to meaningfully compare different models, their accuracies must be averaged over far more random seeds than what has traditionally been reported.', 'with proper averaging in place, we notice that the distillation model described in  #TAUTHOR_TAG, which incorporates explicit logic rules for sentiment classification, is ineffective.', 'in contrast, using contextualized elmo embeddings  #AUTHOR_TAG a ) instead of logic rules yields significantly better performance.', ""additionally, we provide analysis and visualizations that demonstrate elmo's ability to implicitly learn logic rules."", 'finally, a crowdsourced analysis reveals how elmo outperforms baseline models even on sentences with ambiguous sentiment labels']",4
"['2 ) models in  #TAUTHOR_TAG ( with and', 'without distillation ) benefit considerably']","[') models in  #TAUTHOR_TAG ( with and', 'without distillation ) benefit considerably']","['2 ) models in  #TAUTHOR_TAG ( with and', 'without distillation ) benefit considerably']","['', '), sentences in [ 0, 1 − x ) marked as negative, and sentences in [ 1 − x, x ] are marked as neutral. for instance, "" flat, but with a reve', '##latory performance by michelle williams "" ( score = 0. 56 ) is neutral when x = 0. 6. 9 we present statistics of our dataset 10 in table 3.', 'inter - annotator agree - 7 trained on sentences and', 'not phrase - level labels for a fair comparison with baseline and elmo, unlike section 3. 2. 8 https : / / www. figure - eight. com / 9 more examples of neutral sentences have been provided in the appendix in table a1, as well as a few ""', 'flipped "" sentences receiving an average score opposite to their sst2 label ( table a2 ). 10 the dataset along with source code can be found in ment was computed', ""using fleiss'kappa ( κ ). as expected,"", 'inter - annotator agreement is higher for higher thresholds ( less ambiguous sentences ). according to  #AUTHOR_TAG, κ ∈ ( 0. 2, 0. 4 ] corresponds', 'to "" fair agreement "", whereas κ ∈ ( 0. 4, 0. 6 ] corresponds to "" moderate agreement "". we next compute the accuracy of our model for each threshold by removing the corresponding neutral sentences. higher thresholds correspond to sets of less ambiguous sentences. table 3 shows', ""that elmo's performance gains in table 2 extends across all thresholds. in figure 4 we compare all the models"", 'on the a - but - b sentences in this set. across all thresholds, we notice trends', 'similar to previous sections : 1 ) elmo performs the best among all models on a - but - b style sentences, and projection results in only a slight improvement ; 2 ) models in  #TAUTHOR_TAG ( with and', 'without distillation ) benefit considerably from projection ; but 3 ) distillation offers little', 'improvement ( with or without projection ). also,', 'as the ambiguity threshold increases, we see decreasing gains from projection on all models. in fact', ', beyond the 0. 85 threshold, projection degrades the average performance, indicating that projection is useful for', 'more ambiguous sentences']",4
"['2 ) models in  #TAUTHOR_TAG ( with and', 'without distillation ) benefit considerably']","[') models in  #TAUTHOR_TAG ( with and', 'without distillation ) benefit considerably']","['2 ) models in  #TAUTHOR_TAG ( with and', 'without distillation ) benefit considerably']","['', '), sentences in [ 0, 1 − x ) marked as negative, and sentences in [ 1 − x, x ] are marked as neutral. for instance, "" flat, but with a reve', '##latory performance by michelle williams "" ( score = 0. 56 ) is neutral when x = 0. 6. 9 we present statistics of our dataset 10 in table 3.', 'inter - annotator agree - 7 trained on sentences and', 'not phrase - level labels for a fair comparison with baseline and elmo, unlike section 3. 2. 8 https : / / www. figure - eight. com / 9 more examples of neutral sentences have been provided in the appendix in table a1, as well as a few ""', 'flipped "" sentences receiving an average score opposite to their sst2 label ( table a2 ). 10 the dataset along with source code can be found in ment was computed', ""using fleiss'kappa ( κ ). as expected,"", 'inter - annotator agreement is higher for higher thresholds ( less ambiguous sentences ). according to  #AUTHOR_TAG, κ ∈ ( 0. 2, 0. 4 ] corresponds', 'to "" fair agreement "", whereas κ ∈ ( 0. 4, 0. 6 ] corresponds to "" moderate agreement "". we next compute the accuracy of our model for each threshold by removing the corresponding neutral sentences. higher thresholds correspond to sets of less ambiguous sentences. table 3 shows', ""that elmo's performance gains in table 2 extends across all thresholds. in figure 4 we compare all the models"", 'on the a - but - b sentences in this set. across all thresholds, we notice trends', 'similar to previous sections : 1 ) elmo performs the best among all models on a - but - b style sentences, and projection results in only a slight improvement ; 2 ) models in  #TAUTHOR_TAG ( with and', 'without distillation ) benefit considerably from projection ; but 3 ) distillation offers little', 'improvement ( with or without projection ). also,', 'as the ambiguity threshold increases, we see decreasing gains from projection on all models. in fact', ', beyond the 0. 85 threshold, projection degrades the average performance, indicating that projection is useful for', 'more ambiguous sentences']",4
"['.', 'with proper averaging in place, we notice that the distillation model described in  #TAUTHOR_TAG, which incorporates explicit logic rules']","['reported.', 'with proper averaging in place, we notice that the distillation model described in  #TAUTHOR_TAG, which incorporates explicit logic rules']","['.', 'with proper averaging in place, we notice that the distillation model described in  #TAUTHOR_TAG, which incorporates explicit logic rules']","['analyze the performance of different sentiment classification models on syntacticallycomplex inputs like a - but - b sentences.', 'the first contribution of this analysis addresses reproducible research : to meaningfully compare different models, their accuracies must be averaged over far more random seeds than what has traditionally been reported.', 'with proper averaging in place, we notice that the distillation model described in  #TAUTHOR_TAG, which incorporates explicit logic rules for sentiment classification, is ineffective.', 'in contrast, using contextualized elmo embeddings  #AUTHOR_TAG a ) instead of logic rules yields significantly better performance.', ""additionally, we provide analysis and visualizations that demonstrate elmo's ability to implicitly learn logic rules."", 'finally, a crowdsourced analysis reveals how elmo outperforms baseline models even on sentences with ambiguous sentiment labels']",1
"['on these more complicated sentences.', 'recently,  #TAUTHOR_TAG incorporate logical rules into a neural model']","['on these more complicated sentences.', 'recently,  #TAUTHOR_TAG incorporate logical rules into a neural model']","['explicitly encode word order  #AUTHOR_TAG, syntax  #AUTHOR_TAG and semantic features  #AUTHOR_TAG have been proposed with the aim of improving performance on these more complicated sentences.', 'recently,  #TAUTHOR_TAG incorporate logical rules into a neural model']","['this paper, we explore the effectiveness of methods designed to improve sentiment classification ( positive vs. negative ) of sentences that contain complex syntactic structures.', 'while simple bag - of - words or lexicon - based methods  #AUTHOR_TAG achieve good performance on this task, they are unequipped to deal with syntactic structures that affect sentiment, such as contrastive conjunctions ( i. e., sentences of the form "" a - but - b "" ) or negations.', 'neural models that explicitly encode word order  #AUTHOR_TAG, syntax  #AUTHOR_TAG and semantic features  #AUTHOR_TAG have been proposed with the aim of improving performance on these more complicated sentences.', ""recently,  #TAUTHOR_TAG incorporate logical rules into a neural model and show that these rules increase the model's accuracy on sentences containing contrastive conjunctions, while  #AUTHOR_TAG a ) demonstrate increased overall accuracy on sentiment analysis by initializing a model with representations from a language model trained on millions of sentences."", 'in this work, we carry out an in - depth study of the effectiveness of the techniques in  #TAUTHOR_TAG and  #AUTHOR_TAG a ) for sentiment classification of complex sentences.', 'part of our contribution is to identify an important gap in the methodology used in  #TAUTHOR_TAG for performance measurement, which is addressed by averaging the experiments over several executions.', 'with the averaging in place, we obtain three key findings : ( 1 ) the improvements in  #TAUTHOR_TAG can almost entirely be attributed to just one of their two proposed mechanisms and are also less pronounced than previously reported ; ( 2 ) contextualized word embeddings  #AUTHOR_TAG a ) incorporate the "" a - but - b "" rules more effectively without explicitly programming for them ; and ( 3 ) an analysis using crowdsourcing reveals a bigger picture where the errors in the automated systems have a striking correlation with the inherent sentiment - ambiguity in the data']",1
"['on these more complicated sentences.', 'recently,  #TAUTHOR_TAG incorporate logical rules into a neural model']","['on these more complicated sentences.', 'recently,  #TAUTHOR_TAG incorporate logical rules into a neural model']","['explicitly encode word order  #AUTHOR_TAG, syntax  #AUTHOR_TAG and semantic features  #AUTHOR_TAG have been proposed with the aim of improving performance on these more complicated sentences.', 'recently,  #TAUTHOR_TAG incorporate logical rules into a neural model']","['this paper, we explore the effectiveness of methods designed to improve sentiment classification ( positive vs. negative ) of sentences that contain complex syntactic structures.', 'while simple bag - of - words or lexicon - based methods  #AUTHOR_TAG achieve good performance on this task, they are unequipped to deal with syntactic structures that affect sentiment, such as contrastive conjunctions ( i. e., sentences of the form "" a - but - b "" ) or negations.', 'neural models that explicitly encode word order  #AUTHOR_TAG, syntax  #AUTHOR_TAG and semantic features  #AUTHOR_TAG have been proposed with the aim of improving performance on these more complicated sentences.', ""recently,  #TAUTHOR_TAG incorporate logical rules into a neural model and show that these rules increase the model's accuracy on sentences containing contrastive conjunctions, while  #AUTHOR_TAG a ) demonstrate increased overall accuracy on sentiment analysis by initializing a model with representations from a language model trained on millions of sentences."", 'in this work, we carry out an in - depth study of the effectiveness of the techniques in  #TAUTHOR_TAG and  #AUTHOR_TAG a ) for sentiment classification of complex sentences.', 'part of our contribution is to identify an important gap in the methodology used in  #TAUTHOR_TAG for performance measurement, which is addressed by averaging the experiments over several executions.', 'with the averaging in place, we obtain three key findings : ( 1 ) the improvements in  #TAUTHOR_TAG can almost entirely be attributed to just one of their two proposed mechanisms and are also less pronounced than previously reported ; ( 2 ) contextualized word embeddings  #AUTHOR_TAG a ) incorporate the "" a - but - b "" rules more effectively without explicitly programming for them ; and ( 3 ) an analysis using crowdsourcing reveals a bigger picture where the errors in the automated systems have a striking correlation with the inherent sentiment - ambiguity in the data']",0
"['otherwise ). next, we discuss the two techniques from  #TAUTHOR_TAG']","['a - but - b ( and 1 otherwise ). next, we discuss the two techniques from  #TAUTHOR_TAG for incorporating rules into models : projection,']","['otherwise ). next, we discuss the two techniques from  #TAUTHOR_TAG for incorporating rules into models : projection,']","['+, − } for an input x by the baseline model using parameters θ. a logic rule is', '( softly ) encoded as a variable r θ ( x, y ) ∈ [ 0, 1 ] indicating how well labeling x with y satisfies the', 'rule. for the case of a - but - b sentences, r θ ( x, y ) = p θ ( y | b ) if x has', 'the structure a - but - b ( and 1 otherwise ). next, we discuss the two techniques from  #TAUTHOR_TAG for incorporating rules into models : projection, which directly alters a trained model, and', 'distillation, which progressively adjusts the loss function during training. projection. the first technique is to project a trained model into a rule - regularized subspace, in a fashion', 'similar to  #AUTHOR_TAG. more precisely, a given model p θ is projected to a model q θ defined by the optimum value of q in the following optimization problem : 2 min q, ξ≥0 here q ( x', ', y ) denotes the distribution of ( x, y ) when x is drawn uniformly from the set x and y is drawn according to q ( · | x ). iterative rule knowledge distilla', '##tion.', '']",0
"['otherwise ). next, we discuss the two techniques from  #TAUTHOR_TAG']","['a - but - b ( and 1 otherwise ). next, we discuss the two techniques from  #TAUTHOR_TAG for incorporating rules into models : projection,']","['otherwise ). next, we discuss the two techniques from  #TAUTHOR_TAG for incorporating rules into models : projection,']","['+, − } for an input x by the baseline model using parameters θ. a logic rule is', '( softly ) encoded as a variable r θ ( x, y ) ∈ [ 0, 1 ] indicating how well labeling x with y satisfies the', 'rule. for the case of a - but - b sentences, r θ ( x, y ) = p θ ( y | b ) if x has', 'the structure a - but - b ( and 1 otherwise ). next, we discuss the two techniques from  #TAUTHOR_TAG for incorporating rules into models : projection, which directly alters a trained model, and', 'distillation, which progressively adjusts the loss function during training. projection. the first technique is to project a trained model into a rule - regularized subspace, in a fashion', 'similar to  #AUTHOR_TAG. more precisely, a given model p θ is projected to a model q θ defined by the optimum value of q in the following optimization problem : 2 min q, ξ≥0 here q ( x', ', y ) denotes the distribution of ( x, y ) when x is drawn uniformly from the set x and y is drawn according to q ( · | x ). iterative rule knowledge distilla', '##tion.', '']",0
"['on these more complicated sentences.', 'recently,  #TAUTHOR_TAG incorporate logical rules into a neural model']","['on these more complicated sentences.', 'recently,  #TAUTHOR_TAG incorporate logical rules into a neural model']","['explicitly encode word order  #AUTHOR_TAG, syntax  #AUTHOR_TAG and semantic features  #AUTHOR_TAG have been proposed with the aim of improving performance on these more complicated sentences.', 'recently,  #TAUTHOR_TAG incorporate logical rules into a neural model']","['this paper, we explore the effectiveness of methods designed to improve sentiment classification ( positive vs. negative ) of sentences that contain complex syntactic structures.', 'while simple bag - of - words or lexicon - based methods  #AUTHOR_TAG achieve good performance on this task, they are unequipped to deal with syntactic structures that affect sentiment, such as contrastive conjunctions ( i. e., sentences of the form "" a - but - b "" ) or negations.', 'neural models that explicitly encode word order  #AUTHOR_TAG, syntax  #AUTHOR_TAG and semantic features  #AUTHOR_TAG have been proposed with the aim of improving performance on these more complicated sentences.', ""recently,  #TAUTHOR_TAG incorporate logical rules into a neural model and show that these rules increase the model's accuracy on sentences containing contrastive conjunctions, while  #AUTHOR_TAG a ) demonstrate increased overall accuracy on sentiment analysis by initializing a model with representations from a language model trained on millions of sentences."", 'in this work, we carry out an in - depth study of the effectiveness of the techniques in  #TAUTHOR_TAG and  #AUTHOR_TAG a ) for sentiment classification of complex sentences.', 'part of our contribution is to identify an important gap in the methodology used in  #TAUTHOR_TAG for performance measurement, which is addressed by averaging the experiments over several executions.', 'with the averaging in place, we obtain three key findings : ( 1 ) the improvements in  #TAUTHOR_TAG can almost entirely be attributed to just one of their two proposed mechanisms and are also less pronounced than previously reported ; ( 2 ) contextualized word embeddings  #AUTHOR_TAG a ) incorporate the "" a - but - b "" rules more effectively without explicitly programming for them ; and ( 3 ) an analysis using crowdsourcing reveals a bigger picture where the errors in the automated systems have a striking correlation with the inherent sentiment - ambiguity in the data']",6
"['on these more complicated sentences.', 'recently,  #TAUTHOR_TAG incorporate logical rules into a neural model']","['on these more complicated sentences.', 'recently,  #TAUTHOR_TAG incorporate logical rules into a neural model']","['explicitly encode word order  #AUTHOR_TAG, syntax  #AUTHOR_TAG and semantic features  #AUTHOR_TAG have been proposed with the aim of improving performance on these more complicated sentences.', 'recently,  #TAUTHOR_TAG incorporate logical rules into a neural model']","['this paper, we explore the effectiveness of methods designed to improve sentiment classification ( positive vs. negative ) of sentences that contain complex syntactic structures.', 'while simple bag - of - words or lexicon - based methods  #AUTHOR_TAG achieve good performance on this task, they are unequipped to deal with syntactic structures that affect sentiment, such as contrastive conjunctions ( i. e., sentences of the form "" a - but - b "" ) or negations.', 'neural models that explicitly encode word order  #AUTHOR_TAG, syntax  #AUTHOR_TAG and semantic features  #AUTHOR_TAG have been proposed with the aim of improving performance on these more complicated sentences.', ""recently,  #TAUTHOR_TAG incorporate logical rules into a neural model and show that these rules increase the model's accuracy on sentences containing contrastive conjunctions, while  #AUTHOR_TAG a ) demonstrate increased overall accuracy on sentiment analysis by initializing a model with representations from a language model trained on millions of sentences."", 'in this work, we carry out an in - depth study of the effectiveness of the techniques in  #TAUTHOR_TAG and  #AUTHOR_TAG a ) for sentiment classification of complex sentences.', 'part of our contribution is to identify an important gap in the methodology used in  #TAUTHOR_TAG for performance measurement, which is addressed by averaging the experiments over several executions.', 'with the averaging in place, we obtain three key findings : ( 1 ) the improvements in  #TAUTHOR_TAG can almost entirely be attributed to just one of their two proposed mechanisms and are also less pronounced than previously reported ; ( 2 ) contextualized word embeddings  #AUTHOR_TAG a ) incorporate the "" a - but - b "" rules more effectively without explicitly programming for them ; and ( 3 ) an analysis using crowdsourcing reveals a bigger picture where the errors in the automated systems have a striking correlation with the inherent sentiment - ambiguity in the data']",6
['##nalyze the effectiveness of the techniques of  #TAUTHOR_TAG and find that'],['this section we reanalyze the effectiveness of the techniques of  #TAUTHOR_TAG and find that'],['this section we reanalyze the effectiveness of the techniques of  #TAUTHOR_TAG and find that'],"['this section we reanalyze the effectiveness of the techniques of  #TAUTHOR_TAG and find that most of the performance gain is due to projection and not knowledge distillation.', 'the discrepancy with the original analysis can be attributed to the relatively small dataset and the resulting variance across random initializations.', 'we start by analyzing the baseline cnn by  #AUTHOR_TAG to point out the need for an averaged analysis']",6
"['2 ) models in  #TAUTHOR_TAG ( with and', 'without distillation ) benefit considerably']","[') models in  #TAUTHOR_TAG ( with and', 'without distillation ) benefit considerably']","['2 ) models in  #TAUTHOR_TAG ( with and', 'without distillation ) benefit considerably']","['', '), sentences in [ 0, 1 − x ) marked as negative, and sentences in [ 1 − x, x ] are marked as neutral. for instance, "" flat, but with a reve', '##latory performance by michelle williams "" ( score = 0. 56 ) is neutral when x = 0. 6. 9 we present statistics of our dataset 10 in table 3.', 'inter - annotator agree - 7 trained on sentences and', 'not phrase - level labels for a fair comparison with baseline and elmo, unlike section 3. 2. 8 https : / / www. figure - eight. com / 9 more examples of neutral sentences have been provided in the appendix in table a1, as well as a few ""', 'flipped "" sentences receiving an average score opposite to their sst2 label ( table a2 ). 10 the dataset along with source code can be found in ment was computed', ""using fleiss'kappa ( κ ). as expected,"", 'inter - annotator agreement is higher for higher thresholds ( less ambiguous sentences ). according to  #AUTHOR_TAG, κ ∈ ( 0. 2, 0. 4 ] corresponds', 'to "" fair agreement "", whereas κ ∈ ( 0. 4, 0. 6 ] corresponds to "" moderate agreement "". we next compute the accuracy of our model for each threshold by removing the corresponding neutral sentences. higher thresholds correspond to sets of less ambiguous sentences. table 3 shows', ""that elmo's performance gains in table 2 extends across all thresholds. in figure 4 we compare all the models"", 'on the a - but - b sentences in this set. across all thresholds, we notice trends', 'similar to previous sections : 1 ) elmo performs the best among all models on a - but - b style sentences, and projection results in only a slight improvement ; 2 ) models in  #TAUTHOR_TAG ( with and', 'without distillation ) benefit considerably from projection ; but 3 ) distillation offers little', 'improvement ( with or without projection ). also,', 'as the ambiguity threshold increases, we see decreasing gains from projection on all models. in fact', ', beyond the 0. 85 threshold, projection degrades the average performance, indicating that projection is useful for', 'more ambiguous sentences']",6
"['2 ) models in  #TAUTHOR_TAG ( with and', 'without distillation ) benefit considerably']","[') models in  #TAUTHOR_TAG ( with and', 'without distillation ) benefit considerably']","['2 ) models in  #TAUTHOR_TAG ( with and', 'without distillation ) benefit considerably']","['', '), sentences in [ 0, 1 − x ) marked as negative, and sentences in [ 1 − x, x ] are marked as neutral. for instance, "" flat, but with a reve', '##latory performance by michelle williams "" ( score = 0. 56 ) is neutral when x = 0. 6. 9 we present statistics of our dataset 10 in table 3.', 'inter - annotator agree - 7 trained on sentences and', 'not phrase - level labels for a fair comparison with baseline and elmo, unlike section 3. 2. 8 https : / / www. figure - eight. com / 9 more examples of neutral sentences have been provided in the appendix in table a1, as well as a few ""', 'flipped "" sentences receiving an average score opposite to their sst2 label ( table a2 ). 10 the dataset along with source code can be found in ment was computed', ""using fleiss'kappa ( κ ). as expected,"", 'inter - annotator agreement is higher for higher thresholds ( less ambiguous sentences ). according to  #AUTHOR_TAG, κ ∈ ( 0. 2, 0. 4 ] corresponds', 'to "" fair agreement "", whereas κ ∈ ( 0. 4, 0. 6 ] corresponds to "" moderate agreement "". we next compute the accuracy of our model for each threshold by removing the corresponding neutral sentences. higher thresholds correspond to sets of less ambiguous sentences. table 3 shows', ""that elmo's performance gains in table 2 extends across all thresholds. in figure 4 we compare all the models"", 'on the a - but - b sentences in this set. across all thresholds, we notice trends', 'similar to previous sections : 1 ) elmo performs the best among all models on a - but - b style sentences, and projection results in only a slight improvement ; 2 ) models in  #TAUTHOR_TAG ( with and', 'without distillation ) benefit considerably from projection ; but 3 ) distillation offers little', 'improvement ( with or without projection ). also,', 'as the ambiguity threshold increases, we see decreasing gains from projection on all models. in fact', ', beyond the 0. 85 threshold, projection degrades the average performance, indicating that projection is useful for', 'more ambiguous sentences']",6
"['7,  #TAUTHOR_TAG 9 ]']","['of nlp tasks [ 7,  #TAUTHOR_TAG 9 ].', 'the word2vec [ 10 ] is among the']","['useful information from unannotated large scale corpus has been a research issue.', 'word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of nlp tasks [ 7,  #TAUTHOR_TAG 9 ].', 'the word2vec [ 10 ] is among the']","['', 'learning knowledge from analyzing large - scaled unlabeled data is compulsory and proved useful in the previous works [ 4, 5, 6 ].', 'how to extract useful information from unannotated large scale corpus has been a research issue.', 'word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of nlp tasks [ 7,  #TAUTHOR_TAG 9 ].', 'the word2vec [ 10 ] is among the most widely used word embedding models today.', 'their success is largely due to an efficient and user - friendly implementation that learns high quality word embeddings from very large corpora.', 'the word2vec learns low dimensional continuous vector representations for words by considering window - based contexts, i. e., context words within some fixed distance of each side of the target words.', 'another different context type is dependency - based word embedding [ 11, 12, 13 ], which considers syntactic contexts']",1
"['7,  #TAUTHOR_TAG, 9 ]']","['[ 7,  #TAUTHOR_TAG, 9 ]']","['nlp tasks [ 7,  #TAUTHOR_TAG, 9 ].', 'the word2vec [ 10 ] is among the']","['from analyzing large - scaled unlabeled data is compulsory and proved useful in the previous works [ 4, 5, 6 ].', 'how to extract useful information from unannotated large scale corpus has been a research issue.', 'word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of nlp tasks [ 7,  #TAUTHOR_TAG, 9 ].', 'the word2vec [ 10 ] is among the most widely used word embedding models today.', 'their success is largely due to an efficient and user - friendly implementation that learns high quality word embeddings from very large corpora.', 'the word2vec learns low dimensional continuous vector representations for words by considering window - based contexts, i. e., context words within some fixed distance of each side of the target words.', 'another different context type is dependency - based word embedding [ 11, 12, 13 ], which considers syntactic contexts rather', 'the 2016 conference on computational linguistics and speech processing rocling 2016, pp.', '100 - 102 the association for computational linguistics and chinese language processing 100 than window contexts in word2vec.', '']",1
"['7,  #TAUTHOR_TAG, 9 ]']","['[ 7,  #TAUTHOR_TAG, 9 ]']","['nlp tasks [ 7,  #TAUTHOR_TAG, 9 ].', 'the word2vec [ 10 ] is among the']","['from analyzing large - scaled unlabeled data is compulsory and proved useful in the previous works [ 4, 5, 6 ].', 'how to extract useful information from unannotated large scale corpus has been a research issue.', 'word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of nlp tasks [ 7,  #TAUTHOR_TAG, 9 ].', 'the word2vec [ 10 ] is among the most widely used word embedding models today.', 'their success is largely due to an efficient and user - friendly implementation that learns high quality word embeddings from very large corpora.', 'the word2vec learns low dimensional continuous vector representations for words by considering window - based contexts, i. e., context words within some fixed distance of each side of the target words.', 'another different context type is dependency - based word embedding [ 11, 12, 13 ], which considers syntactic contexts rather', 'the 2016 conference on computational linguistics and speech processing rocling 2016, pp.', '100 - 102 the association for computational linguistics and chinese language processing 100 than window contexts in word2vec.', '']",0
['as discriminative ones  #TAUTHOR_TAG. as multiple factors'],['as discriminative ones  #TAUTHOR_TAG. as multiple factors'],['as discriminative ones  #TAUTHOR_TAG. as multiple factors'],"['latent semantic analysis ( lsa )  #AUTHOR_TAG, and generative machine learning models  #AUTHOR_TAG as well as discriminative ones  #TAUTHOR_TAG. as multiple factors influence the linguistic quality of', 'texts, such systems exploit features that correspond to different properties of texts, such', 'as grammar, style, vocabulary usage, topic similarity, and discourse coherence and cohesion. cohesion refers to the use of explicit linguistic cohesive devices ( e. g., anaphora, lexical semantic relatedness, discourse markers, etc.', ') within a text that can signal primarily suprasentential discourse relations between textual units  #AUTHOR_TAG. cohesion is not the only mechanism of discourse coherence, which may also be inferred from meaning without presence of explicit', 'linguistic cues. coherence can be assessed locally in terms of transitions between adjacent clauses, parentheticals, and other textual units capable of standing in discourse relations, or more globally in terms of the overall topical coherence of text passages. there is a', 'large body of work that has investigated a number of different coherence models on news texts ( e. g.,  #AUTHOR_TAG,', ' #AUTHOR_TAG, and  #AUTHOR_TAG ).  #AUTHOR_TAG presented a detailed survey of current techniques in coherence analysis of extractive summaries. to date, however, few attempts have been made to develop new methods and validate existing ones for automatic', 'evaluation of discourse coherence and cohesion in the noisy domain of learner texts, where spelling and grammatical errors are common. coherence quality is typically present in marking criteria for', 'evaluating learner texts, and it is iden - tified by examiners as a', 'determinant of the overall score. thus we expect that adding a coherence metric to the feature set of an aa system would better reflect the evaluation performed by examiners and improve performance. the goal of the experiments presented in this paper', 'is to measure the effect a number of ( previously - developed and new ) coherence models have on performance when combined with an aa system that achieves competitive results,', 'but does not use discourse coherence features. our contribution is threefold : 1 ) we present the first systematic analysis of several methods for assessing discourse coherence in', 'the framework of aa of learner free - text responses, 2 ) we identify new discourse features that serve as proxies for the level of ( in ) coherence in texts and outperform previously developed techniques, and 3 ) we improve the best results reported by', "" #TAUTHOR_TAG on the publically available'english as a second or other language'( esol ) corpus of learner texts ( to date, this is the only public - domain corpus that contains grades ). finally, we explore the utility of our best model for assessing the incoherent'outlier'texts used in  #TAUTHOR_TAG""]",0
['aa system described in  #TAUTHOR_TAG exploited features'],['aa system described in  #TAUTHOR_TAG exploited features'],['aa system described in  #TAUTHOR_TAG exploited features'],"['aa system described in  #TAUTHOR_TAG exploited features based on pos tag sequences, but did not consider the distribution of pos types across grades.', 'in coherent texts, constituent clauses and sentences are related and depend on each other for their interpretation.', 'anaphors such as pronouns link the current sentence to those where the entities were previously mentioned.', 'pronouns can be directly related to ( lack of ) coherence and make intuitive sense as cohesive devices.', 'we compute the number of pronouns in a text and use it as a shallow feature for capturing coherence']",0
['as discriminative ones  #TAUTHOR_TAG. as multiple factors'],['as discriminative ones  #TAUTHOR_TAG. as multiple factors'],['as discriminative ones  #TAUTHOR_TAG. as multiple factors'],"['latent semantic analysis ( lsa )  #AUTHOR_TAG, and generative machine learning models  #AUTHOR_TAG as well as discriminative ones  #TAUTHOR_TAG. as multiple factors influence the linguistic quality of', 'texts, such systems exploit features that correspond to different properties of texts, such', 'as grammar, style, vocabulary usage, topic similarity, and discourse coherence and cohesion. cohesion refers to the use of explicit linguistic cohesive devices ( e. g., anaphora, lexical semantic relatedness, discourse markers, etc.', ') within a text that can signal primarily suprasentential discourse relations between textual units  #AUTHOR_TAG. cohesion is not the only mechanism of discourse coherence, which may also be inferred from meaning without presence of explicit', 'linguistic cues. coherence can be assessed locally in terms of transitions between adjacent clauses, parentheticals, and other textual units capable of standing in discourse relations, or more globally in terms of the overall topical coherence of text passages. there is a', 'large body of work that has investigated a number of different coherence models on news texts ( e. g.,  #AUTHOR_TAG,', ' #AUTHOR_TAG, and  #AUTHOR_TAG ).  #AUTHOR_TAG presented a detailed survey of current techniques in coherence analysis of extractive summaries. to date, however, few attempts have been made to develop new methods and validate existing ones for automatic', 'evaluation of discourse coherence and cohesion in the noisy domain of learner texts, where spelling and grammatical errors are common. coherence quality is typically present in marking criteria for', 'evaluating learner texts, and it is iden - tified by examiners as a', 'determinant of the overall score. thus we expect that adding a coherence metric to the feature set of an aa system would better reflect the evaluation performed by examiners and improve performance. the goal of the experiments presented in this paper', 'is to measure the effect a number of ( previously - developed and new ) coherence models have on performance when combined with an aa system that achieves competitive results,', 'but does not use discourse coherence features. our contribution is threefold : 1 ) we present the first systematic analysis of several methods for assessing discourse coherence in', 'the framework of aa of learner free - text responses, 2 ) we identify new discourse features that serve as proxies for the level of ( in ) coherence in texts and outperform previously developed techniques, and 3 ) we improve the best results reported by', "" #TAUTHOR_TAG on the publically available'english as a second or other language'( esol ) corpus of learner texts ( to date, this is the only public - domain corpus that contains grades ). finally, we explore the utility of our best model for assessing the incoherent'outlier'texts used in  #TAUTHOR_TAG""]",7
['aa system presented in  #TAUTHOR_TAG ; aa is'],['aa system presented in  #TAUTHOR_TAG ; aa is'],"['- art results, but does not use discourse coherence features.', 'specifically, we describe a number of different experiments improving on the aa system presented in  #TAUTHOR_TAG ; aa is treated as a rank preference supervised learning problem and ranking support vector machines ( svms )  #AUTHOR_TAG']","['examine the predictive power of a number of different coherence models by measuring the effect on performance when combined with an aa system that achieves state - of - the - art results, but does not use discourse coherence features.', 'specifically, we describe a number of different experiments improving on the aa system presented in  #TAUTHOR_TAG ; aa is treated as a rank preference supervised learning problem and ranking support vector machines ( svms )  #AUTHOR_TAG are used to explicitly model the grade relationships between scripts.', 'this system uses a number of different linguistic features that achieve good performance on the aa task.', ""however, these features only focus on lexical and grammatical properties, as well as errors within individual sentences, ignoring discourse coherence, which is also present in marking criteria for evaluating learner texts, as well as a strong indicator of a writer's understanding of a language."", ""also, in  #TAUTHOR_TAG, experiments are presented that test the validity of the system using a number of automatically - created'outlier'texts."", 'the results showed that the model is vulnerable to input where individually high - scoring sentences are randomly ordered within a text.', 'failing to identify such pathological cases makes aa systems vulnerable to subversion by writers who understand something of its workings, thus posing a threat to their validity.', 'for example, an examinee might learn by rote a set of well - formed sentences and reproduce these in an exam in the knowledge that an aa system is not checking for prompt relevance or coherence 1']",7
"['assessment ) described in detail in  #TAUTHOR_TAG,']","['assessment ) described in detail in  #TAUTHOR_TAG,']","['intermediate level assessment ) described in detail in  #TAUTHOR_TAG,']","['use the first certificate in english ( fce ) esol examination scripts 2 ( upper - intermediate level assessment ) described in detail in  #TAUTHOR_TAG, extracted from the cambridge learner corpus 3 ( clc ).', 'the dataset consists of 1, 238 texts between 200 and 400 words produced by 1, 238 distinct learners in response to two different prompts.', 'an overall mark has been assigned in the range 1 - 40.', 'for all experiments, we use a series of 5 - fold cross - validation runs on 1, 141 texts from the examination year 2000 to evaluate performance as well as generalization of numerous models.', 'moreover, we identify the best model on year 2000 and we also test it on 97 texts from the examination year 2001, previously used in  #TAUTHOR_TAG to report the best published results.', 'validating the results on a different examination year tests generalization to some prompts not used in 2000, and also allows us to test correlation between examiners and the aa system.', 'again, we treat aa as a rank preference learning problem and use svms, utilizing the svm light package  #AUTHOR_TAG, to facilitate comparison with  #TAUTHOR_TAG']",7
"['word length dependent correlations  #AUTHOR_TAG.', '20 see  #TAUTHOR_TAG']","['word length dependent correlations  #AUTHOR_TAG.', '20 see  #TAUTHOR_TAG']","['word length dependent correlations  #AUTHOR_TAG.', '20 see  #TAUTHOR_TAG']","['the previous section, we evaluated various cohesion and coherence features on learner data, and found different patterns of performance compared to those previously reported on news texts ( see section 7 for more details ).', 'although most of the models examined gave a minimal effect on aa performance, isa, lowbow lex, ibm model pos f and word length dependent correlations  #AUTHOR_TAG.', '20 see  #TAUTHOR_TAG for details.', '']",7
['as discriminative ones  #TAUTHOR_TAG. as multiple factors'],['as discriminative ones  #TAUTHOR_TAG. as multiple factors'],['as discriminative ones  #TAUTHOR_TAG. as multiple factors'],"['latent semantic analysis ( lsa )  #AUTHOR_TAG, and generative machine learning models  #AUTHOR_TAG as well as discriminative ones  #TAUTHOR_TAG. as multiple factors influence the linguistic quality of', 'texts, such systems exploit features that correspond to different properties of texts, such', 'as grammar, style, vocabulary usage, topic similarity, and discourse coherence and cohesion. cohesion refers to the use of explicit linguistic cohesive devices ( e. g., anaphora, lexical semantic relatedness, discourse markers, etc.', ') within a text that can signal primarily suprasentential discourse relations between textual units  #AUTHOR_TAG. cohesion is not the only mechanism of discourse coherence, which may also be inferred from meaning without presence of explicit', 'linguistic cues. coherence can be assessed locally in terms of transitions between adjacent clauses, parentheticals, and other textual units capable of standing in discourse relations, or more globally in terms of the overall topical coherence of text passages. there is a', 'large body of work that has investigated a number of different coherence models on news texts ( e. g.,  #AUTHOR_TAG,', ' #AUTHOR_TAG, and  #AUTHOR_TAG ).  #AUTHOR_TAG presented a detailed survey of current techniques in coherence analysis of extractive summaries. to date, however, few attempts have been made to develop new methods and validate existing ones for automatic', 'evaluation of discourse coherence and cohesion in the noisy domain of learner texts, where spelling and grammatical errors are common. coherence quality is typically present in marking criteria for', 'evaluating learner texts, and it is iden - tified by examiners as a', 'determinant of the overall score. thus we expect that adding a coherence metric to the feature set of an aa system would better reflect the evaluation performed by examiners and improve performance. the goal of the experiments presented in this paper', 'is to measure the effect a number of ( previously - developed and new ) coherence models have on performance when combined with an aa system that achieves competitive results,', 'but does not use discourse coherence features. our contribution is threefold : 1 ) we present the first systematic analysis of several methods for assessing discourse coherence in', 'the framework of aa of learner free - text responses, 2 ) we identify new discourse features that serve as proxies for the level of ( in ) coherence in texts and outperform previously developed techniques, and 3 ) we improve the best results reported by', "" #TAUTHOR_TAG on the publically available'english as a second or other language'( esol ) corpus of learner texts ( to date, this is the only public - domain corpus that contains grades ). finally, we explore the utility of our best model for assessing the incoherent'outlier'texts used in  #TAUTHOR_TAG""]",5
"['assessment ) described in detail in  #TAUTHOR_TAG,']","['assessment ) described in detail in  #TAUTHOR_TAG,']","['intermediate level assessment ) described in detail in  #TAUTHOR_TAG,']","['use the first certificate in english ( fce ) esol examination scripts 2 ( upper - intermediate level assessment ) described in detail in  #TAUTHOR_TAG, extracted from the cambridge learner corpus 3 ( clc ).', 'the dataset consists of 1, 238 texts between 200 and 400 words produced by 1, 238 distinct learners in response to two different prompts.', 'an overall mark has been assigned in the range 1 - 40.', 'for all experiments, we use a series of 5 - fold cross - validation runs on 1, 141 texts from the examination year 2000 to evaluate performance as well as generalization of numerous models.', 'moreover, we identify the best model on year 2000 and we also test it on 97 texts from the examination year 2001, previously used in  #TAUTHOR_TAG to report the best published results.', 'validating the results on a different examination year tests generalization to some prompts not used in 2000, and also allows us to test correlation between examiners and the aa system.', 'again, we treat aa as a rank preference learning problem and use svms, utilizing the svm light package  #AUTHOR_TAG, to facilitate comparison with  #TAUTHOR_TAG']",5
"['assessment ) described in detail in  #TAUTHOR_TAG,']","['assessment ) described in detail in  #TAUTHOR_TAG,']","['intermediate level assessment ) described in detail in  #TAUTHOR_TAG,']","['use the first certificate in english ( fce ) esol examination scripts 2 ( upper - intermediate level assessment ) described in detail in  #TAUTHOR_TAG, extracted from the cambridge learner corpus 3 ( clc ).', 'the dataset consists of 1, 238 texts between 200 and 400 words produced by 1, 238 distinct learners in response to two different prompts.', 'an overall mark has been assigned in the range 1 - 40.', 'for all experiments, we use a series of 5 - fold cross - validation runs on 1, 141 texts from the examination year 2000 to evaluate performance as well as generalization of numerous models.', 'moreover, we identify the best model on year 2000 and we also test it on 97 texts from the examination year 2001, previously used in  #TAUTHOR_TAG to report the best published results.', 'validating the results on a different examination year tests generalization to some prompts not used in 2000, and also allows us to test correlation between examiners and the aa system.', 'again, we treat aa as a rank preference learning problem and use svms, utilizing the svm light package  #AUTHOR_TAG, to facilitate comparison with  #TAUTHOR_TAG']",5
"['.', 'as in  #TAUTHOR_TAG, we']","['of the methods we investigate require syntactic analysis.', 'as in  #TAUTHOR_TAG, we']","['of the methods we investigate require syntactic analysis.', 'as in  #TAUTHOR_TAG, we analyze all texts using the rasp toolkit  #AUTHOR_TAG 4']","['focus on the development and evaluation of ( automated ) methods for assessing coherence in learner texts under the framework of aa.', 'most of the methods we investigate require syntactic analysis.', 'as in  #TAUTHOR_TAG, we analyze all texts using the rasp toolkit  #AUTHOR_TAG 4']",5
"['2001, previously used in  #TAUTHOR_TAG to report']","[', previously used in  #TAUTHOR_TAG to report results of']",['previously used in  #TAUTHOR_TAG to report'],"['pos cosine similarity between adjacent sentences are also 17 we compute mean values of correlation coefficients by first applying the r - to - z fisher transformation, and then using the fisher weighted mean correlation coefficient  #AUTHOR_TAG. 18 significance tests in averaged correlations are omitted as variable estimates are', 'produced, whose variance is hard to be estimated unbiasedly. among the weakest', 'predictors.  #AUTHOR_TAG b ) have shown that combining the entity - grid with the pronoun, discourse - new and lexicalized ibm models gives state', '- of - the - art results for discriminating news documents and their random permutations. we also combine these models and assess their performance under the aa framework. row 16 of table 1 shows that the combination does not give an improvement over the individual models. moreover, combining all feature classes together in row 17 does not yield higher results', 'than those obtained with isa, while ρ is no better than the baseline. in the following experiments,', 'we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in  #TAUTHOR_TAG to report results of the final best', 'system. validating the model on a different exam year also shows', '']",5
"['2001, previously used in  #TAUTHOR_TAG to report']","[', previously used in  #TAUTHOR_TAG to report results of']",['previously used in  #TAUTHOR_TAG to report'],"['pos cosine similarity between adjacent sentences are also 17 we compute mean values of correlation coefficients by first applying the r - to - z fisher transformation, and then using the fisher weighted mean correlation coefficient  #AUTHOR_TAG. 18 significance tests in averaged correlations are omitted as variable estimates are', 'produced, whose variance is hard to be estimated unbiasedly. among the weakest', 'predictors.  #AUTHOR_TAG b ) have shown that combining the entity - grid with the pronoun, discourse - new and lexicalized ibm models gives state', '- of - the - art results for discriminating news documents and their random permutations. we also combine these models and assess their performance under the aa framework. row 16 of table 1 shows that the combination does not give an improvement over the individual models. moreover, combining all feature classes together in row 17 does not yield higher results', 'than those obtained with isa, while ρ is no better than the baseline. in the following experiments,', 'we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in  #TAUTHOR_TAG to report results of the final best', 'system. validating the model on a different exam year also shows', '']",5
"['2001, previously used in  #TAUTHOR_TAG to report']","[', previously used in  #TAUTHOR_TAG to report results of']",['previously used in  #TAUTHOR_TAG to report'],"['pos cosine similarity between adjacent sentences are also 17 we compute mean values of correlation coefficients by first applying the r - to - z fisher transformation, and then using the fisher weighted mean correlation coefficient  #AUTHOR_TAG. 18 significance tests in averaged correlations are omitted as variable estimates are', 'produced, whose variance is hard to be estimated unbiasedly. among the weakest', 'predictors.  #AUTHOR_TAG b ) have shown that combining the entity - grid with the pronoun, discourse - new and lexicalized ibm models gives state', '- of - the - art results for discriminating news documents and their random permutations. we also combine these models and assess their performance under the aa framework. row 16 of table 1 shows that the combination does not give an improvement over the individual models. moreover, combining all feature classes together in row 17 does not yield higher results', 'than those obtained with isa, while ρ is no better than the baseline. in the following experiments,', 'we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in  #TAUTHOR_TAG to report results of the final best', 'system. validating the model on a different exam year also shows', '']",5
['aa system presented in  #TAUTHOR_TAG ; aa is'],['aa system presented in  #TAUTHOR_TAG ; aa is'],"['- art results, but does not use discourse coherence features.', 'specifically, we describe a number of different experiments improving on the aa system presented in  #TAUTHOR_TAG ; aa is treated as a rank preference supervised learning problem and ranking support vector machines ( svms )  #AUTHOR_TAG']","['examine the predictive power of a number of different coherence models by measuring the effect on performance when combined with an aa system that achieves state - of - the - art results, but does not use discourse coherence features.', 'specifically, we describe a number of different experiments improving on the aa system presented in  #TAUTHOR_TAG ; aa is treated as a rank preference supervised learning problem and ranking support vector machines ( svms )  #AUTHOR_TAG are used to explicitly model the grade relationships between scripts.', 'this system uses a number of different linguistic features that achieve good performance on the aa task.', ""however, these features only focus on lexical and grammatical properties, as well as errors within individual sentences, ignoring discourse coherence, which is also present in marking criteria for evaluating learner texts, as well as a strong indicator of a writer's understanding of a language."", ""also, in  #TAUTHOR_TAG, experiments are presented that test the validity of the system using a number of automatically - created'outlier'texts."", 'the results showed that the model is vulnerable to input where individually high - scoring sentences are randomly ordered within a text.', 'failing to identify such pathological cases makes aa systems vulnerable to subversion by writers who understand something of its workings, thus posing a threat to their validity.', 'for example, an examinee might learn by rote a set of well - formed sentences and reproduce these in an exam in the knowledge that an aa system is not checking for prompt relevance or coherence 1']",6
"['.', 'as in  #TAUTHOR_TAG, we']","['of the methods we investigate require syntactic analysis.', 'as in  #TAUTHOR_TAG, we']","['of the methods we investigate require syntactic analysis.', 'as in  #TAUTHOR_TAG, we analyze all texts using the rasp toolkit  #AUTHOR_TAG 4']","['focus on the development and evaluation of ( automated ) methods for assessing coherence in learner texts under the framework of aa.', 'most of the methods we investigate require syntactic analysis.', 'as in  #TAUTHOR_TAG, we analyze all texts using the rasp toolkit  #AUTHOR_TAG 4']",3
"['assessing discourse coherence.', 'among the features used in  #TAUTHOR_TAG, none explicitly captures coherence and none models inters']","['assessing discourse coherence.', 'among the features used in  #TAUTHOR_TAG, none explicitly captures coherence and none models intersentential relationships.', 'incremental']","['assessing discourse coherence.', 'among the features used in  #TAUTHOR_TAG, none explicitly captures coherence and none models inters']","['explore the utility of inter - sentential feature types for assessing discourse coherence.', 'among the features used in  #TAUTHOR_TAG, none explicitly captures coherence and none models intersentential relationships.', 'incremental semantic analysis ( isa )  #AUTHOR_TAG is a word - level distributional model that induces a semantic space from input texts.', 'isa is a fully - incremental variation of random indexing ( ri )  #AUTHOR_TAG, which can efficiently capture second - order effects in common with other dimensionality - reduction methods based on singular value decomposition, but does not rely on stoplists or global statistics for weighting purposes.', 'utilizing the s - space package  #AUTHOR_TAG, we trained an isa model 5 using a subset of ukwac  #AUTHOR_TAG, a large corpus of english containing more than 2 billion tokens.', 'we used the pos tagger lexicon provided with the rasp system to discard documents whose proportion of valid english words to total words is less than 0. 4 ; 78, 000 documents were extracted in total and were then preprocessed replacing urls, email addresses, ip addresses, numbers and emoticons with special markers.', 'to measure local coherence we define the similarity between two sentences s i and s i + 1 as the maximum cosine similarity between the history vectors of the words they contain.', 'the overall coherence of a text t is then measured by taking the mean of all sentence - pair scores :', ') is the cosine similarity between the history vectors of the k th word in s i and the j th word in s i + 1, and n is the total number of sentences 6.', 'we investigate the efficacy of isa by adding this coherence score, as well as the maximum sim value found over the entire text, to the vectors of features associated with a text.', 'the hypothesis is that the degree of semantic relatedness between adjoining sentences serves as a proxy for local discourse coherence ; that is, coherent text units contain semantically - related words.', ' #AUTHOR_TAG and  #AUTHOR_TAG use ri to determine the semantic similarity between sentences of same / different discourse segments ( e. g., from the essay thesis and conclusion, or between sentences and the essay prompt ), and assess the percentage of sentences that are correctly classified as related or unrelated.', 'the main differences from our approach are that we assess the utility of semantic space models for predicting the overall grade for a text, in contrast to binary classification at the sentence - level, and we use isa rather than ri 7']",4
['presented in  #TAUTHOR_TAG'],['presented in  #TAUTHOR_TAG'],['presented in  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],4
"[' #TAUTHOR_TAG ; thus,']","[' #TAUTHOR_TAG ; thus,']","[' #TAUTHOR_TAG ; thus, the']",[' #TAUTHOR_TAG'],6
"[' #TAUTHOR_TAG ; thus,']","[' #TAUTHOR_TAG ; thus,']","[' #TAUTHOR_TAG ; thus, the']",[' #TAUTHOR_TAG'],5
"[' #TAUTHOR_TAG.', 'scholar  #AUTHOR_TAG collection of']","[' #TAUTHOR_TAG.', 'scholar  #AUTHOR_TAG collection of']","[' #TAUTHOR_TAG.', 'scholar  #AUTHOR_TAG collection of']",[' #TAUTHOR_TAG'],5
"[' #TAUTHOR_TAG.', 'scholar  #AUTHOR_TAG collection of']","[' #TAUTHOR_TAG.', 'scholar  #AUTHOR_TAG collection of']","[' #TAUTHOR_TAG.', 'scholar  #AUTHOR_TAG collection of']",[' #TAUTHOR_TAG'],5
"[' #TAUTHOR_TAG.', 'scholar  #AUTHOR_TAG collection of']","[' #TAUTHOR_TAG.', 'scholar  #AUTHOR_TAG collection of']","[' #TAUTHOR_TAG.', 'scholar  #AUTHOR_TAG collection of']",[' #TAUTHOR_TAG'],5
"['based split  #TAUTHOR_TAG.', 'in']","['split  #TAUTHOR_TAG.', 'in']","['based split  #TAUTHOR_TAG.', 'in']","['evaluated the query generation accuracy for both the question - based split and query - based split  #TAUTHOR_TAG.', '']",5
"['from  #TAUTHOR_TAG.', ' #AUTHOR_TAG proposed an encoder - decoder']","['from  #TAUTHOR_TAG.', ' #AUTHOR_TAG proposed an encoder - decoder']","['from  #TAUTHOR_TAG.', ' #AUTHOR_TAG proposed an encoder - decoder model with global attention  #AUTHOR_TAG to directly generate a sequence of sql tokens from']",[' #TAUTHOR_TAG'],5
['of  #TAUTHOR_TAG'],['of  #TAUTHOR_TAG'],"['', 'this architecture is based on an idea similar to the template - based model of  #TAUTHOR_TAG.', 'however, the previous model requires a number of examples for each template']","['', 'this architecture is based on an idea similar to the template - based model of  #TAUTHOR_TAG.', 'however, the previous model requires a number of examples for each template and needs retraining to support new templates of sql.', 'conversely, we applied one - shot learning so that our model could learn a template with just a single example.', 'moreover, our model does not require any additional training to support new sql templates']",3
[' #TAUTHOR_TAG proposed a way to generate onthe - fly'],[' #TAUTHOR_TAG proposed a way to generate onthe - fly'],['. 3 alignment with logical clues  #TAUTHOR_TAG proposed a way to generate onthe - fly'],"[', which caused significant damage, was a tropical storm when she entered virginia. note the coreference between', 'hurricane isabel and she, suggesting us to copy the subtree of hur', '##ricane isabel to she, in a tree edit approach. this is not enough yet, because the head storm in t is', 'not placed at the subject of cause. the issue is indeed very logical : from', '"" hurricane isabel = she "", "" hurricane isabel = storm "", "" she = subject of enter "" and "" hurricane isabel', '= subject of cause "", we can imply that "" storm = subject of enter = subject of cause "". 3 alignment with logical clues  #TAUTHOR_TAG proposed a way to generate onthe - fly knowledge to fill knowledge gaps : if h is not proven, compare dcs trees of t and h to generate', '']",5
"[""running time of  #TAUTHOR_TAG's""]","[""running time of  #TAUTHOR_TAG's""]","['can be proven with on - the - fly knowledge.', ""we plot the running time of  #TAUTHOR_TAG's""]",[' #TAUTHOR_TAG'],5
"[""running time of  #TAUTHOR_TAG's""]","[""running time of  #TAUTHOR_TAG's""]","['can be proven with on - the - fly knowledge.', ""we plot the running time of  #TAUTHOR_TAG's""]",[' #TAUTHOR_TAG'],5
[' #TAUTHOR_TAG proposed a way to generate onthe - fly'],[' #TAUTHOR_TAG proposed a way to generate onthe - fly'],['. 3 alignment with logical clues  #TAUTHOR_TAG proposed a way to generate onthe - fly'],"[', which caused significant damage, was a tropical storm when she entered virginia. note the coreference between', 'hurricane isabel and she, suggesting us to copy the subtree of hur', '##ricane isabel to she, in a tree edit approach. this is not enough yet, because the head storm in t is', 'not placed at the subject of cause. the issue is indeed very logical : from', '"" hurricane isabel = she "", "" hurricane isabel = storm "", "" she = subject of enter "" and "" hurricane isabel', '= subject of cause "", we can imply that "" storm = subject of enter = subject of cause "". 3 alignment with logical clues  #TAUTHOR_TAG proposed a way to generate onthe - fly knowledge to fill knowledge gaps : if h is not proven, compare dcs trees of t and h to generate', '']",0
[' #TAUTHOR_TAG proposed a way to generate onthe - fly'],[' #TAUTHOR_TAG proposed a way to generate onthe - fly'],['. 3 alignment with logical clues  #TAUTHOR_TAG proposed a way to generate onthe - fly'],"[', which caused significant damage, was a tropical storm when she entered virginia. note the coreference between', 'hurricane isabel and she, suggesting us to copy the subtree of hur', '##ricane isabel to she, in a tree edit approach. this is not enough yet, because the head storm in t is', 'not placed at the subject of cause. the issue is indeed very logical : from', '"" hurricane isabel = she "", "" hurricane isabel = storm "", "" she = subject of enter "" and "" hurricane isabel', '= subject of cause "", we can imply that "" storm = subject of enter = subject of cause "". 3 alignment with logical clues  #TAUTHOR_TAG proposed a way to generate onthe - fly knowledge to fill knowledge gaps : if h is not proven, compare dcs trees of t and h to generate', '']",0
[' #TAUTHOR_TAG proposed a way to generate onthe - fly'],[' #TAUTHOR_TAG proposed a way to generate onthe - fly'],['. 3 alignment with logical clues  #TAUTHOR_TAG proposed a way to generate onthe - fly'],"[', which caused significant damage, was a tropical storm when she entered virginia. note the coreference between', 'hurricane isabel and she, suggesting us to copy the subtree of hur', '##ricane isabel to she, in a tree edit approach. this is not enough yet, because the head storm in t is', 'not placed at the subject of cause. the issue is indeed very logical : from', '"" hurricane isabel = she "", "" hurricane isabel = storm "", "" she = subject of enter "" and "" hurricane isabel', '= subject of cause "", we can imply that "" storm = subject of enter = subject of cause "". 3 alignment with logical clues  #TAUTHOR_TAG proposed a way to generate onthe - fly knowledge to fill knowledge gaps : if h is not proven, compare dcs trees of t and h to generate', '']",0
[' #TAUTHOR_TAG proposed a way to generate onthe - fly'],[' #TAUTHOR_TAG proposed a way to generate onthe - fly'],['. 3 alignment with logical clues  #TAUTHOR_TAG proposed a way to generate onthe - fly'],"[', which caused significant damage, was a tropical storm when she entered virginia. note the coreference between', 'hurricane isabel and she, suggesting us to copy the subtree of hur', '##ricane isabel to she, in a tree edit approach. this is not enough yet, because the head storm in t is', 'not placed at the subject of cause. the issue is indeed very logical : from', '"" hurricane isabel = she "", "" hurricane isabel = storm "", "" she = subject of enter "" and "" hurricane isabel', '= subject of cause "", we can imply that "" storm = subject of enter = subject of cause "". 3 alignment with logical clues  #TAUTHOR_TAG proposed a way to generate onthe - fly knowledge to fill knowledge gaps : if h is not proven, compare dcs trees of t and h to generate', '']",0
"[', as  #TAUTHOR_TAG demonstrates a system with competitive results.', 'however, despite the expectation that']","['inference is shown to be useful for rte, as  #TAUTHOR_TAG demonstrates a system with competitive results.', 'however, despite the expectation that']","[', as  #TAUTHOR_TAG demonstrates a system with competitive results.', 'however, despite the expectation that']","['inference is shown to be useful for rte, as  #TAUTHOR_TAG demonstrates a system with competitive results.', 'however, despite the expectation that all entailment matters can be explained logically, our observation is that currently logical inference only fills very limited short gaps from t to h. the logical phenomena easily addressed by  #TAUTHOR_TAG table 2 : proportion ( % ) of exit status of prover9', 'the system of  #TAUTHOR_TAG generated onthe - fly knowledge to join several fragments in t and wrongly proved h. in examples of such complexity, distributional similarity is no longer reliable.', 'however, it may be possible to build a priori logical models at the meta level, such as on epistemic, intentional and reportive attitudes.', 'the models then can provide signals for semantic parsing to connect the logic to natural language, such as the words "" grant "", "" decertify "", and "" accuse "" in the above example.', 'we hope this approach can bring new progress to rte and other semantic processing tasks']",0
"[', as  #TAUTHOR_TAG demonstrates a system with competitive results.', 'however, despite the expectation that']","['inference is shown to be useful for rte, as  #TAUTHOR_TAG demonstrates a system with competitive results.', 'however, despite the expectation that']","[', as  #TAUTHOR_TAG demonstrates a system with competitive results.', 'however, despite the expectation that']","['inference is shown to be useful for rte, as  #TAUTHOR_TAG demonstrates a system with competitive results.', 'however, despite the expectation that all entailment matters can be explained logically, our observation is that currently logical inference only fills very limited short gaps from t to h. the logical phenomena easily addressed by  #TAUTHOR_TAG table 2 : proportion ( % ) of exit status of prover9', 'the system of  #TAUTHOR_TAG generated onthe - fly knowledge to join several fragments in t and wrongly proved h. in examples of such complexity, distributional similarity is no longer reliable.', 'however, it may be possible to build a priori logical models at the meta level, such as on epistemic, intentional and reportive attitudes.', 'the models then can provide signals for semantic parsing to connect the logic to natural language, such as the words "" grant "", "" decertify "", and "" accuse "" in the above example.', 'we hope this approach can bring new progress to rte and other semantic processing tasks']",0
"[', as  #TAUTHOR_TAG demonstrates a system with competitive results.', 'however, despite the expectation that']","['inference is shown to be useful for rte, as  #TAUTHOR_TAG demonstrates a system with competitive results.', 'however, despite the expectation that']","[', as  #TAUTHOR_TAG demonstrates a system with competitive results.', 'however, despite the expectation that']","['inference is shown to be useful for rte, as  #TAUTHOR_TAG demonstrates a system with competitive results.', 'however, despite the expectation that all entailment matters can be explained logically, our observation is that currently logical inference only fills very limited short gaps from t to h. the logical phenomena easily addressed by  #TAUTHOR_TAG table 2 : proportion ( % ) of exit status of prover9', 'the system of  #TAUTHOR_TAG generated onthe - fly knowledge to join several fragments in t and wrongly proved h. in examples of such complexity, distributional similarity is no longer reliable.', 'however, it may be possible to build a priori logical models at the meta level, such as on epistemic, intentional and reportive attitudes.', 'the models then can provide signals for semantic parsing to connect the logic to natural language, such as the words "" grant "", "" decertify "", and "" accuse "" in the above example.', 'we hope this approach can bring new progress to rte and other semantic processing tasks']",0
"['calculate phrase similarities,  #TAUTHOR_TAG use']","['calculate phrase similarities,  #TAUTHOR_TAG use']","['calculate phrase similarities,  #TAUTHOR_TAG use']","['', 'improvement of similarity score to calculate phrase similarities,  #TAUTHOR_TAG use the cosine similarity of sums of word vectors, which ignores syntactic information.', 'we plan to add syntactic information to words by some supertags, and learn a vector space embedding for this structure.', '']",0
"[""running time of  #TAUTHOR_TAG's""]","[""running time of  #TAUTHOR_TAG's""]","['can be proven with on - the - fly knowledge.', ""we plot the running time of  #TAUTHOR_TAG's""]",[' #TAUTHOR_TAG'],7
"[""running time of  #TAUTHOR_TAG's""]","[""running time of  #TAUTHOR_TAG's""]","['can be proven with on - the - fly knowledge.', ""we plot the running time of  #TAUTHOR_TAG's""]",[' #TAUTHOR_TAG'],3
"['calculate phrase similarities,  #TAUTHOR_TAG use']","['calculate phrase similarities,  #TAUTHOR_TAG use']","['calculate phrase similarities,  #TAUTHOR_TAG use']","['', 'improvement of similarity score to calculate phrase similarities,  #TAUTHOR_TAG use the cosine similarity of sums of word vectors, which ignores syntactic information.', 'we plan to add syntactic information to words by some supertags, and learn a vector space embedding for this structure.', '']",6
['structure  #TAUTHOR_TAG'],['structure  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
['structure  #TAUTHOR_TAG'],['structure  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],5
['by  #TAUTHOR_TAG'],"['using top100', '), much higher than reported by  #TAUTHOR_TAG. we obtain similar results']","['using top100', '), much higher than reported by  #TAUTHOR_TAG. we obtain similar results']","['the "" top100 "" setup. table 5 shows the results for relation identification in the first setting ( out - of - domain ). the f1 score of identifying support relations is 84. 3', '% ( or 89 % using top100', '), much higher than reported by  #TAUTHOR_TAG. we obtain similar results when training and testing on t oef l arg. we observe that two specific feature groups, structural and lexical, individually achieve high', '']",5
['by  #TAUTHOR_TAG'],"['using top100', '), much higher than reported by  #TAUTHOR_TAG. we obtain similar results']","['using top100', '), much higher than reported by  #TAUTHOR_TAG. we obtain similar results']","['the "" top100 "" setup. table 5 shows the results for relation identification in the first setting ( out - of - domain ). the f1 score of identifying support relations is 84. 3', '% ( or 89 % using top100', '), much higher than reported by  #TAUTHOR_TAG. we obtain similar results when training and testing on t oef l arg. we observe that two specific feature groups, structural and lexical, individually achieve high', '']",5
['by  #TAUTHOR_TAG'],"['using top100', '), much higher than reported by  #TAUTHOR_TAG. we obtain similar results']","['using top100', '), much higher than reported by  #TAUTHOR_TAG. we obtain similar results']","['the "" top100 "" setup. table 5 shows the results for relation identification in the first setting ( out - of - domain ). the f1 score of identifying support relations is 84. 3', '% ( or 89 % using top100', '), much higher than reported by  #TAUTHOR_TAG. we obtain similar results when training and testing on t oef l arg. we observe that two specific feature groups, structural and lexical, individually achieve high', '']",5
['in  #TAUTHOR_TAG dataset and thus the coefficients'],"['ar features. in addition, we notice that attack relations are sparse, as was the case in  #TAUTHOR_TAG dataset and thus the coefficients']","['in  #TAUTHOR_TAG dataset and thus the coefficients for attack relations features ( # 10,', '']","['', '1 structures "" and "" number of t ree h > 1 structures "" ( ts features ) have the highest correlation with high scoring essays. for example,', 'in a good persuasive essay, test takers are inclined to use multiple', 'premises ( e. g., reasons or examples ) to support a claim, which is captured by the ts and ar features. in addition, we notice that attack relations are sparse, as was the case in  #TAUTHOR_TAG dataset and thus the coefficients for attack relations features ( # 10,', '# 11 in table 1 ) are negligible. in summary, our findings contribute to research on essay scoring, showing that argumentation features are good predictors of', 'essay scores, besides spelling, grammar, and stylistic properties of text', '']",3
['by  #TAUTHOR_TAG'],"['using top100', '), much higher than reported by  #TAUTHOR_TAG. we obtain similar results']","['using top100', '), much higher than reported by  #TAUTHOR_TAG. we obtain similar results']","['the "" top100 "" setup. table 5 shows the results for relation identification in the first setting ( out - of - domain ). the f1 score of identifying support relations is 84. 3', '% ( or 89 % using top100', '), much higher than reported by  #TAUTHOR_TAG. we obtain similar results when training and testing on t oef l arg. we observe that two specific feature groups, structural and lexical, individually achieve high', '']",3
"[', 6,  #TAUTHOR_TAG 8, 9, 10 ] is effective in extracting high - level properties from speech.', 'slp downstream tasks can be improved through speech representations']","['speech representation learning [ 2, 3, 4, 5, 6,  #TAUTHOR_TAG 8, 9, 10 ] is effective in extracting high - level properties from speech.', 'slp downstream tasks can be improved through speech representations']","['speech representation learning [ 2, 3, 4, 5, 6,  #TAUTHOR_TAG 8, 9, 10 ] is effective in extracting high - level properties from speech.', 'slp downstream tasks can be improved through speech representations']","['speech representation learning [ 2, 3, 4, 5, 6,  #TAUTHOR_TAG 8, 9, 10 ] is effective in extracting high - level properties from speech.', 'slp downstream tasks can be improved through speech representations because surface features such as log mel - spectrograms or waveform can poorly reveal the abundant information within speech.', 'contrastive predictive coding ( cpc ) [ 5 ] and wav2vec  #TAUTHOR_TAG use a multi - layer cnn to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task.', 'autoregressive predictive coding ( apc ) [ 6 ] uses autoregressive models to encode temporal information of past acoustic sequences ; the model predicts future frames like an rnn - based language model [ 11 ], optimized with reconstruction loss.', 'unidirectional models are commonly used in the previous approaches [ 2, 3, 4, 5, 6,  #TAUTHOR_TAG.', 'however, this constraint on model architectures limits the potential of speech representation learning.', 'the recently proposed vq - wav2vec [ 8 ] approach attempts to apply the well - performing natural language processing ( nlp ) algorithm bert [ 12 ] on continuous speech.', 'input speech is discretized to a k - way quantized embedding space, so continuous speech could act like discrete units similar to word tokens in nlp tasks.', 'in vq - wav2vec [ 8 ], an exhaustive two - stage training pipeline with massive computing resources are required to adapt speech to nlp algorithm, as the quantization process is against the continuous nature of speech.', 'unlike [ 8 ] that adapts speech to bert [ 12 ] through quantization, the proposed approach can be seen as a modified version of bert [ 12 ] for direct application on continuous speech']",0
"[', 6,  #TAUTHOR_TAG 8, 9, 10 ] is effective in extracting high - level properties from speech.', 'slp downstream tasks can be improved through speech representations']","['speech representation learning [ 2, 3, 4, 5, 6,  #TAUTHOR_TAG 8, 9, 10 ] is effective in extracting high - level properties from speech.', 'slp downstream tasks can be improved through speech representations']","['speech representation learning [ 2, 3, 4, 5, 6,  #TAUTHOR_TAG 8, 9, 10 ] is effective in extracting high - level properties from speech.', 'slp downstream tasks can be improved through speech representations']","['speech representation learning [ 2, 3, 4, 5, 6,  #TAUTHOR_TAG 8, 9, 10 ] is effective in extracting high - level properties from speech.', 'slp downstream tasks can be improved through speech representations because surface features such as log mel - spectrograms or waveform can poorly reveal the abundant information within speech.', 'contrastive predictive coding ( cpc ) [ 5 ] and wav2vec  #TAUTHOR_TAG use a multi - layer cnn to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task.', 'autoregressive predictive coding ( apc ) [ 6 ] uses autoregressive models to encode temporal information of past acoustic sequences ; the model predicts future frames like an rnn - based language model [ 11 ], optimized with reconstruction loss.', 'unidirectional models are commonly used in the previous approaches [ 2, 3, 4, 5, 6,  #TAUTHOR_TAG.', 'however, this constraint on model architectures limits the potential of speech representation learning.', 'the recently proposed vq - wav2vec [ 8 ] approach attempts to apply the well - performing natural language processing ( nlp ) algorithm bert [ 12 ] on continuous speech.', 'input speech is discretized to a k - way quantized embedding space, so continuous speech could act like discrete units similar to word tokens in nlp tasks.', 'in vq - wav2vec [ 8 ], an exhaustive two - stage training pipeline with massive computing resources are required to adapt speech to nlp algorithm, as the quantization process is against the continuous nature of speech.', 'unlike [ 8 ] that adapts speech to bert [ 12 ] through quantization, the proposed approach can be seen as a modified version of bert [ 12 ] for direct application on continuous speech']",0
"[', 6,  #TAUTHOR_TAG 8, 9, 10 ] is effective in extracting high - level properties from speech.', 'slp downstream tasks can be improved through speech representations']","['speech representation learning [ 2, 3, 4, 5, 6,  #TAUTHOR_TAG 8, 9, 10 ] is effective in extracting high - level properties from speech.', 'slp downstream tasks can be improved through speech representations']","['speech representation learning [ 2, 3, 4, 5, 6,  #TAUTHOR_TAG 8, 9, 10 ] is effective in extracting high - level properties from speech.', 'slp downstream tasks can be improved through speech representations']","['speech representation learning [ 2, 3, 4, 5, 6,  #TAUTHOR_TAG 8, 9, 10 ] is effective in extracting high - level properties from speech.', 'slp downstream tasks can be improved through speech representations because surface features such as log mel - spectrograms or waveform can poorly reveal the abundant information within speech.', 'contrastive predictive coding ( cpc ) [ 5 ] and wav2vec  #TAUTHOR_TAG use a multi - layer cnn to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task.', 'autoregressive predictive coding ( apc ) [ 6 ] uses autoregressive models to encode temporal information of past acoustic sequences ; the model predicts future frames like an rnn - based language model [ 11 ], optimized with reconstruction loss.', 'unidirectional models are commonly used in the previous approaches [ 2, 3, 4, 5, 6,  #TAUTHOR_TAG.', 'however, this constraint on model architectures limits the potential of speech representation learning.', 'the recently proposed vq - wav2vec [ 8 ] approach attempts to apply the well - performing natural language processing ( nlp ) algorithm bert [ 12 ] on continuous speech.', 'input speech is discretized to a k - way quantized embedding space, so continuous speech could act like discrete units similar to word tokens in nlp tasks.', 'in vq - wav2vec [ 8 ], an exhaustive two - stage training pipeline with massive computing resources are required to adapt speech to nlp algorithm, as the quantization process is against the continuous nature of speech.', 'unlike [ 8 ] that adapts speech to bert [ 12 ] through quantization, the proposed approach can be seen as a modified version of bert [ 12 ] for direct application on continuous speech']",0
"[', 6,  #TAUTHOR_TAG 8, 9, 10 ] is effective in extracting high - level properties from speech.', 'slp downstream tasks can be improved through speech representations']","['speech representation learning [ 2, 3, 4, 5, 6,  #TAUTHOR_TAG 8, 9, 10 ] is effective in extracting high - level properties from speech.', 'slp downstream tasks can be improved through speech representations']","['speech representation learning [ 2, 3, 4, 5, 6,  #TAUTHOR_TAG 8, 9, 10 ] is effective in extracting high - level properties from speech.', 'slp downstream tasks can be improved through speech representations']","['speech representation learning [ 2, 3, 4, 5, 6,  #TAUTHOR_TAG 8, 9, 10 ] is effective in extracting high - level properties from speech.', 'slp downstream tasks can be improved through speech representations because surface features such as log mel - spectrograms or waveform can poorly reveal the abundant information within speech.', 'contrastive predictive coding ( cpc ) [ 5 ] and wav2vec  #TAUTHOR_TAG use a multi - layer cnn to encode past context, representations are learned by predicting the future in latent space under a contrastive binary classification task.', 'autoregressive predictive coding ( apc ) [ 6 ] uses autoregressive models to encode temporal information of past acoustic sequences ; the model predicts future frames like an rnn - based language model [ 11 ], optimized with reconstruction loss.', 'unidirectional models are commonly used in the previous approaches [ 2, 3, 4, 5, 6,  #TAUTHOR_TAG.', 'however, this constraint on model architectures limits the potential of speech representation learning.', 'the recently proposed vq - wav2vec [ 8 ] approach attempts to apply the well - performing natural language processing ( nlp ) algorithm bert [ 12 ] on continuous speech.', 'input speech is discretized to a k - way quantized embedding space, so continuous speech could act like discrete units similar to word tokens in nlp tasks.', 'in vq - wav2vec [ 8 ], an exhaustive two - stage training pipeline with massive computing resources are required to adapt speech to nlp algorithm, as the quantization process is against the continuous nature of speech.', 'unlike [ 8 ] that adapts speech to bert [ 12 ] through quantization, the proposed approach can be seen as a modified version of bert [ 12 ] for direct application on continuous speech']",1
"['5, 6,  #TAUTHOR_TAG 8 ],']","['the pre - trained models to representation extraction only [ 5, 6,  #TAUTHOR_TAG 8 ],']","['5, 6,  #TAUTHOR_TAG 8 ],']","['previous left - to - right unidirectional approaches that only consider past sequences to predict information about future frames, the proposed method allows us to train a bidirectional speech representation model, alleviating the unidirectionality constraint of previous methods.', 'as a result, the mockingjay model obtains substantial improvements in several slp tasks.', 'moreover, as previous approaches restrict the power of the pre - trained models to representation extraction only [ 5, 6,  #TAUTHOR_TAG 8 ], the proposed method is robust and can be fine - tuned easily on downstream tasks.', 'we show that finetuning for 2 epochs easily acquires significant improvement.', '']",4
"['5,  #TAUTHOR_TAG 9 ] in both two tasks, which makes apc suitable as a strong baseline.', 'apc uses an unidirectional autoregressive model.', 'we compare']","['cpc representations [ 5,  #TAUTHOR_TAG 9 ] in both two tasks, which makes apc suitable as a strong baseline.', 'apc uses an unidirectional autoregressive model.', 'we compare']","['5,  #TAUTHOR_TAG 9 ] in both two tasks, which makes apc suitable as a strong baseline.', 'apc uses an unidirectional autoregressive model.', 'we compare']","['proposed approaches are mainly compared with apc [ 6 ] representations, as they also experiment on phone classification and speaker verification.', 'as reported in [ 6 ], the apc approach outperformed cpc representations [ 5,  #TAUTHOR_TAG 9 ] in both two tasks, which makes apc suitable as a strong baseline.', 'apc uses an unidirectional autoregressive model.', 'we compare the proposed approach with apc to show that our bidirectional approach has advantages in speech representation learning.', ""for fair comparison, we pre - train apc using their official implementations with the reported ideal parameters and settings, but expand the model's hidden size to h dim = 768 to match ours."", 'we also report results on 160 - dimensional log mel - features, which helps evaluate the accessibility of speech information from regular acoustic features']",4
"[', 6,  #TAUTHOR_TAG 8 ], we evaluate different features']","['works [ 2, 3, 4, 5, 6,  #TAUTHOR_TAG 8 ], we evaluate different features']","['2, 3, 4, 5, 6,  #TAUTHOR_TAG 8 ], we evaluate different features']","['previous works [ 2, 3, 4, 5, 6,  #TAUTHOR_TAG 8 ], we evaluate different features and representations on downstream tasks, including : phoneme classification, speaker recognition, and sentiment classification on spoken content.', 'for a fair comparison, each downstream task uses an identical model architecture and hyperparameters despite different input features.', 'we report results from 5 of our models : 1 ) base and 2 ) large where mockingjay representations are extracted from the last encoder layer, 3 ) the base - ft2 where we finetune base with random initialized downstream models for 2 epochs, and 4 ) the base - ft500 where we fine - tune for 500k steps, and finally 5 ) the large - ws where we incorporate hidden states from all encoder layers of the large model through a learnable weighted sum.', 'we did not fine - tune the large model, as it is meant for extracting representations.', 'empirically we found that even with supervised training, a random initialized mockingjay model followed by any downstream model is hard to be trained from scratch.', 'this shows that the proposed pre - training is essentially indispensable']",5
[' #TAUTHOR_TAG by human'],[' #TAUTHOR_TAG by human'],[' #TAUTHOR_TAG by'],"['', 'we propose a system for generating fluent and natural questions from a kb, which significantly reduces the human effort by leveraging massive web resources.', 'in more detail, a seed question set is first generated by applying a small number of hand - crafted templates on the input kb, then more questions are retrieved by iteratively forming already obtained questions as search queries into a standard search engine, before finally questions are selected by estimating their fluency and domain relevance.', 'evaluated by human graders on 500 random - selected triples from freebase, questions generated by our system are judged to be more fluent than those of  #TAUTHOR_TAG by human graders']",4
"[' #TAUTHOR_TAG relies on massive human -', 'labeled']","['work  #TAUTHOR_TAG relies on massive human -', 'labeled']","[' #TAUTHOR_TAG relies on massive human -', 'labeled data']","['', 'are already present in kbs but not for texts. question generation from kb is challenging as function words and morphological forms for entities are abstracted away when a kb is created. to tackle this challenge, previous work  #TAUTHOR_TAG relies on massive human -', 'labeled data. treating question generation as a machine translation problem,  #TAUTHOR_TAG train a neural machine translation ( nmt ) system with', '']",4
['system grammatical naturalness  #TAUTHOR_TAG 3. 36 3. 14 ours 3. 53 3. 31 table 1 : comparing'],['system grammatical naturalness  #TAUTHOR_TAG 3. 36 3. 14 ours 3. 53 3. 31 table 1 : comparing'],['system grammatical naturalness  #TAUTHOR_TAG 3. 36 3. 14 ours 3. 53 3. 31 table 1 : comparing'],"['in algorithm 1, the expanded question set e is initialized as the seed question set ( line 1 ).', 'in each iteration, an already - obtained question is expanded from web and the retrieved questions are added to e if e does not contain them ( lines 6 - 10 ).', 'as there may be a large number of questions generated in the loop, we limit the maximum number of iterations with i max ( line 4 ).', 'the questions collected from the web search engine may not be fluent or domain relevant ; especially the domain relevance drops significantly as the iteration goes on.', 'here we adopt a skip - gram model  #AUTHOR_TAG and a language model for evaluating the domain relevance and fluency of the expanded questions, respectively.', 'for system grammatical naturalness  #TAUTHOR_TAG 3. 36 3. 14 ours 3. 53 3. 31 table 1 : comparing generated questions domain relevance, we take the seed question set as the in - domain data d in, the domain relevance of expanded question q is defined as :', 'where v ( · ) is the document embedding defined as the averaged word embedding within the document.', 'for fluency, we define the averaged language model score as :', 'where lm ( · ) is the general - domain language model score ( log probability ), and len ( · ) is the word count.', 'we apply thresholds t rel and t f lu for domain relevance and fluency respectively, and filter out questions whose scores are below these thresholds']",4
['first compare our system with  #TAUTHOR_TAG on'],['first compare our system with  #TAUTHOR_TAG on'],['first compare our system with  #TAUTHOR_TAG on'],[' #TAUTHOR_TAG'],4
['first compare our system with  #TAUTHOR_TAG on'],['first compare our system with  #TAUTHOR_TAG on'],['first compare our system with  #TAUTHOR_TAG on'],[' #TAUTHOR_TAG'],4
['than these from  #TAUTHOR_TAG on 500 random - selected triples from'],['than these from  #TAUTHOR_TAG on 500 random - selected triples from'],['than these from  #TAUTHOR_TAG on 500 random - selected triples from'],"['presented a system to generate natural language questions from a knowledge base.', 'by leveraging rich web information, our system is able to generate domain - relevant questions in wide scope, while human effort is significantly reduced.', 'evaluated by human graders, questions generated by our system are significantly better than these from  #TAUTHOR_TAG on 500 random - selected triples from freebase.', 'we also demonstrated generated questions from our in - house kb of power tool domain, which are fluent and domain - relevant in general.', 'our current system only generates questions without answers, leaving automatic answer mining as our future work']",4
"[' #TAUTHOR_TAG relies on massive human -', 'labeled']","['work  #TAUTHOR_TAG relies on massive human -', 'labeled']","[' #TAUTHOR_TAG relies on massive human -', 'labeled data']","['', 'are already present in kbs but not for texts. question generation from kb is challenging as function words and morphological forms for entities are abstracted away when a kb is created. to tackle this challenge, previous work  #TAUTHOR_TAG relies on massive human -', 'labeled data. treating question generation as a machine translation problem,  #TAUTHOR_TAG train a neural machine translation ( nmt ) system with', '']",0
"[' #TAUTHOR_TAG relies on massive human -', 'labeled']","['work  #TAUTHOR_TAG relies on massive human -', 'labeled']","[' #TAUTHOR_TAG relies on massive human -', 'labeled data']","['', 'are already present in kbs but not for texts. question generation from kb is challenging as function words and morphological forms for entities are abstracted away when a kb is created. to tackle this challenge, previous work  #TAUTHOR_TAG relies on massive human -', 'labeled data. treating question generation as a machine translation problem,  #TAUTHOR_TAG train a neural machine translation ( nmt ) system with', '']",0
"[' #TAUTHOR_TAG relies on massive human -', 'labeled']","['work  #TAUTHOR_TAG relies on massive human -', 'labeled']","[' #TAUTHOR_TAG relies on massive human -', 'labeled data']","['', 'are already present in kbs but not for texts. question generation from kb is challenging as function words and morphological forms for entities are abstracted away when a kb is created. to tackle this challenge, previous work  #TAUTHOR_TAG relies on massive human -', 'labeled data. treating question generation as a machine translation problem,  #TAUTHOR_TAG train a neural machine translation ( nmt ) system with', '']",5
['method  #TAUTHOR_TAG on'],"['state - of - the - art method  #TAUTHOR_TAG on freebase  #AUTHOR_TAG, a domain - general kb.', 'in the second experiment, we validate']",[' #TAUTHOR_TAG on'],"['perform three experiments to evaluate our system qualitatively and quantitatively.', 'in the first experiment, we compare our end - to - end system with the previous state - of - the - art method  #TAUTHOR_TAG on freebase  #AUTHOR_TAG, a domain - general kb.', 'in the second experiment, we validate our domain relevance evaluation method on a standard dataset about short document classification.', 'in the final experiment, we run our endto - end system on a highly specialized in - house kb and present sample results, showing that our system is capable of generating questions from domain specific kbs']",5
['first compare our system with  #TAUTHOR_TAG on'],['first compare our system with  #TAUTHOR_TAG on'],['first compare our system with  #TAUTHOR_TAG on'],[' #TAUTHOR_TAG'],5
['first compare our system with  #TAUTHOR_TAG on'],['first compare our system with  #TAUTHOR_TAG on'],['first compare our system with  #TAUTHOR_TAG on'],[' #TAUTHOR_TAG'],5
['first compare our system with  #TAUTHOR_TAG on'],['first compare our system with  #TAUTHOR_TAG on'],['first compare our system with  #TAUTHOR_TAG on'],[' #TAUTHOR_TAG'],5
"['', 'actually we find inspired by  #TAUTHOR_TAG, we']","['is a concept only.', 'actually we find inspired by  #TAUTHOR_TAG, we']","['##6 _ channels, in, jurkat _ t _ cells ).', 'the subject or object in the tuple is formatted as { concept : attribute } where the attribute could be null if it is a concept only.', 'actually we find inspired by  #TAUTHOR_TAG, we formulate biocs as a sequence tagging problem.', 'we propose a new deep sequence tagging']","['', 'the expected outputs of the example sentence are : fact 1 : ( alkaline _ ph, increases, { trpv5 / v6 _ channels : activity } ), and condition 1 : ( trpv5 / v6 _ channels, in, jurkat _ t _ cells ).', 'the subject or object in the tuple is formatted as { concept : attribute } where the attribute could be null if it is a concept only.', 'actually we find inspired by  #TAUTHOR_TAG, we formulate biocs as a sequence tagging problem.', 'we propose a new deep sequence tagging framework to achieve the goal.', 'experiments on a data set of 141m sentences from pubmed paper abstracts show that the biological knowledge graph we constructed provide a good understanding of biological statements ( https : / / scikg. github. io )']",1
['parsing  #TAUTHOR_TAG and provided a fairly good'],"['constituency parsing  #TAUTHOR_TAG and provided a fairly good result.', 'however one obvious, intuitive drawback of seq2seq models']","['constituency parsing  #TAUTHOR_TAG and provided a fairly good result.', 'however one obvious, intuitive drawback of seq2seq models']","['- to - sequence ( seq2seq ) models have successfully improved many well - studied nlp tasks, especially for natural language generation ( nlg ) tasks, such as machine translation ( mt )  #AUTHOR_TAG and abstractive summarization  #AUTHOR_TAG.', 'seq2seq models have also been applied to constituency parsing  #TAUTHOR_TAG and provided a fairly good result.', 'however one obvious, intuitive drawback of seq2seq models when they are applied to constituency parsing is that they have no explicit architecture to model latent nested relationships among the words and phrases in constituency parse trees, thus, models that directly model them, such as rnng  #AUTHOR_TAG, are an intuitively more promising approach.', 'in fact, rnng and its extensions  #AUTHOR_TAG provide the current stateof - the - art performance.', 'sec2seq models are currently considered a simple baseline of neuralbased constituency parsing.', 'after the first proposal of an seq2seq constituency parser, many task - independent techniques have been developed, mainly in the nlg research area.', 'our aim is to update the seq2seq approach proposed in  #TAUTHOR_TAG as a stronger baseline of constituency parsing.', 'our motivation is basically identical to that described in  #AUTHOR_TAG.', 'a strong baseline is crucial for reporting reliable experimental results.', 'it offers a fair evaluation of promising new techniques if they solve new issues or simply resolve issues that have already been addressed by current generic technology.', 'more specifically, it might become possible to analyze what types of implicit linguistic structures are easier or harder to capture for neural models by comparing the outputs of strong seq2seq models and task - specific models, e. g., rnng.', 'the contributions of this paper are summarized as follows : ( 1 ) a strong baseline for constituency parsing based on general purpose seq2seq models 1, ( 2 ) an empirical investigation of several generic techniques that can ( or cannot ) contribute to improve the parser performance, ( 3 ) empirical evidence that seq2seq models implicitly learn parse tree structures well without knowing taskspecific and explicit tree structure information']",0
"['length controlling', 'as described in  #TAUTHOR_TAG, not']","['length controlling', 'as described in  #TAUTHOR_TAG, not']","['4 output length controlling', 'as described in  #TAUTHOR_TAG, not all the outputs']","['papers on the seq2seq approach  #AUTHOR_TAG have reported that the multi - task learning extension often improves the task performance if we can find effective auxiliary tasks related to the target task.', 'from this general knowledge, we re - consider jointly estimating pos - tags by incorporating the linearized forms without the pos - tag normalization as an auxiliary task.', 'in detail, the linearized forms with and without the pos - tag normalization are independently and simultaneously estimated as o j and q j, respectively, in the decoder output layer by following equation :', '3. 4 output length controlling', 'as described in  #TAUTHOR_TAG, not all the outputs ( predicted linearized parse trees ) obtained from the seq2seq parser are valid ( well - formed ) as a parse tree.', 'toward guaranteeing that every output is a valid tree, we introduce a simple extension of the method for controlling the seq2seq output length  #AUTHOR_TAG.', 'first, we introduce an additional bias term b in the decoder output layer to prevent the selection of certain output words :', '']",0
"['length controlling', 'as described in  #TAUTHOR_TAG, not']","['length controlling', 'as described in  #TAUTHOR_TAG, not']","['4 output length controlling', 'as described in  #TAUTHOR_TAG, not all the outputs']","['papers on the seq2seq approach  #AUTHOR_TAG have reported that the multi - task learning extension often improves the task performance if we can find effective auxiliary tasks related to the target task.', 'from this general knowledge, we re - consider jointly estimating pos - tags by incorporating the linearized forms without the pos - tag normalization as an auxiliary task.', 'in detail, the linearized forms with and without the pos - tag normalization are independently and simultaneously estimated as o j and q j, respectively, in the decoder output layer by following equation :', '3. 4 output length controlling', 'as described in  #TAUTHOR_TAG, not all the outputs ( predicted linearized parse trees ) obtained from the seq2seq parser are valid ( well - formed ) as a parse tree.', 'toward guaranteeing that every output is a valid tree, we introduce a simple extension of the method for controlling the seq2seq output length  #AUTHOR_TAG.', 'first, we introduce an additional bias term b in the decoder output layer to prevent the selection of certain output words :', '']",0
['parsing  #TAUTHOR_TAG and provided a fairly good'],"['constituency parsing  #TAUTHOR_TAG and provided a fairly good result.', 'however one obvious, intuitive drawback of seq2seq models']","['constituency parsing  #TAUTHOR_TAG and provided a fairly good result.', 'however one obvious, intuitive drawback of seq2seq models']","['- to - sequence ( seq2seq ) models have successfully improved many well - studied nlp tasks, especially for natural language generation ( nlg ) tasks, such as machine translation ( mt )  #AUTHOR_TAG and abstractive summarization  #AUTHOR_TAG.', 'seq2seq models have also been applied to constituency parsing  #TAUTHOR_TAG and provided a fairly good result.', 'however one obvious, intuitive drawback of seq2seq models when they are applied to constituency parsing is that they have no explicit architecture to model latent nested relationships among the words and phrases in constituency parse trees, thus, models that directly model them, such as rnng  #AUTHOR_TAG, are an intuitively more promising approach.', 'in fact, rnng and its extensions  #AUTHOR_TAG provide the current stateof - the - art performance.', 'sec2seq models are currently considered a simple baseline of neuralbased constituency parsing.', 'after the first proposal of an seq2seq constituency parser, many task - independent techniques have been developed, mainly in the nlg research area.', 'our aim is to update the seq2seq approach proposed in  #TAUTHOR_TAG as a stronger baseline of constituency parsing.', 'our motivation is basically identical to that described in  #AUTHOR_TAG.', 'a strong baseline is crucial for reporting reliable experimental results.', 'it offers a fair evaluation of promising new techniques if they solve new issues or simply resolve issues that have already been addressed by current generic technology.', 'more specifically, it might become possible to analyze what types of implicit linguistic structures are easier or harder to capture for neural models by comparing the outputs of strong seq2seq models and task - specific models, e. g., rnng.', 'the contributions of this paper are summarized as follows : ( 1 ) a strong baseline for constituency parsing based on general purpose seq2seq models 1, ( 2 ) an empirical investigation of several generic techniques that can ( or cannot ) contribute to improve the parser performance, ( 3 ) empirical evidence that seq2seq models implicitly learn parse tree structures well without knowing taskspecific and explicit tree structure information']",5
['to constituency parsing  #TAUTHOR_TAG'],['to constituency parsing  #TAUTHOR_TAG'],"['was applied to constituency parsing  #TAUTHOR_TAG.', 'we omit detailed descriptions']","['starting point is an rnn - based seq2seq model with an attention mechanism that was applied to constituency parsing  #TAUTHOR_TAG.', 'we omit detailed descriptions due to space limitations, but note that our model architecture is identical to the one introduced in  #AUTHOR_TAG a ) 2.', 'a key trick for applying seq2seq models to constituency parsing is the linearization of parse 1 our code and experimental configurations for reproducing our experiments are publicly available : https : / / github. com / nttcslab - nlp / strong s2s baseline parser 2 more specifically, our seq2seq model follows the one implemented in seq2seq - attn ( https : / / github. com / harvardnlp / seq2seq - attn ), which is the alpha - version of the opennmt tool ( http : / / opennmt. net )']",5
"['the evaluation settings explained in  #TAUTHOR_TAG.', '']","['the evaluation settings explained in  #TAUTHOR_TAG.', '']","['the evaluation settings explained in  #TAUTHOR_TAG.', '']","['experiments used the english penn treebank data  #AUTHOR_TAG, which are the most widely used benchmark data in the literature.', 'we used the standard split of training ( sec. 02 - 21 ), development ( sec. 22 ), and test data ( sec. 23 ) and strictly followed the instructions for the evaluation settings explained in  #TAUTHOR_TAG.', 'for data pre - processing, all the parse trees were transformed into linearized forms, which include standard unk replacement for oov words and pos - tag normalization by xx - tags.', 'as explained in  #TAUTHOR_TAG, we did not apply any parse tree binarization or special unary treatment, which were used as common techniques in the literature.', 'table 7 : list of bracketing f - measures on test data ( ptb sec. 23 ) reported in recent top - notch systems : scores with bold font represent our scores.', 'ments unless otherwise specified.', '']",5
"['the evaluation settings explained in  #TAUTHOR_TAG.', '']","['the evaluation settings explained in  #TAUTHOR_TAG.', '']","['the evaluation settings explained in  #TAUTHOR_TAG.', '']","['experiments used the english penn treebank data  #AUTHOR_TAG, which are the most widely used benchmark data in the literature.', 'we used the standard split of training ( sec. 02 - 21 ), development ( sec. 22 ), and test data ( sec. 23 ) and strictly followed the instructions for the evaluation settings explained in  #TAUTHOR_TAG.', 'for data pre - processing, all the parse trees were transformed into linearized forms, which include standard unk replacement for oov words and pos - tag normalization by xx - tags.', 'as explained in  #TAUTHOR_TAG, we did not apply any parse tree binarization or special unary treatment, which were used as common techniques in the literature.', 'table 7 : list of bracketing f - measures on test data ( ptb sec. 23 ) reported in recent top - notch systems : scores with bold font represent our scores.', 'ments unless otherwise specified.', '']",5
"['the evaluation settings explained in  #TAUTHOR_TAG.', '']","['the evaluation settings explained in  #TAUTHOR_TAG.', '']","['the evaluation settings explained in  #TAUTHOR_TAG.', '']","['experiments used the english penn treebank data  #AUTHOR_TAG, which are the most widely used benchmark data in the literature.', 'we used the standard split of training ( sec. 02 - 21 ), development ( sec. 22 ), and test data ( sec. 23 ) and strictly followed the instructions for the evaluation settings explained in  #TAUTHOR_TAG.', 'for data pre - processing, all the parse trees were transformed into linearized forms, which include standard unk replacement for oov words and pos - tag normalization by xx - tags.', 'as explained in  #TAUTHOR_TAG, we did not apply any parse tree binarization or special unary treatment, which were used as common techniques in the literature.', 'table 7 : list of bracketing f - measures on test data ( ptb sec. 23 ) reported in recent top - notch systems : scores with bold font represent our scores.', 'ments unless otherwise specified.', '']",5
"['length controlling', 'as described in  #TAUTHOR_TAG, not']","['length controlling', 'as described in  #TAUTHOR_TAG, not']","['4 output length controlling', 'as described in  #TAUTHOR_TAG, not all the outputs']","['papers on the seq2seq approach  #AUTHOR_TAG have reported that the multi - task learning extension often improves the task performance if we can find effective auxiliary tasks related to the target task.', 'from this general knowledge, we re - consider jointly estimating pos - tags by incorporating the linearized forms without the pos - tag normalization as an auxiliary task.', 'in detail, the linearized forms with and without the pos - tag normalization are independently and simultaneously estimated as o j and q j, respectively, in the decoder output layer by following equation :', '3. 4 output length controlling', 'as described in  #TAUTHOR_TAG, not all the outputs ( predicted linearized parse trees ) obtained from the seq2seq parser are valid ( well - formed ) as a parse tree.', 'toward guaranteeing that every output is a valid tree, we introduce a simple extension of the method for controlling the seq2seq output length  #AUTHOR_TAG.', 'first, we introduce an additional bias term b in the decoder output layer to prevent the selection of certain output words :', '']",1
"['length controlling', 'as described in  #TAUTHOR_TAG, not']","['length controlling', 'as described in  #TAUTHOR_TAG, not']","['4 output length controlling', 'as described in  #TAUTHOR_TAG, not all the outputs']","['papers on the seq2seq approach  #AUTHOR_TAG have reported that the multi - task learning extension often improves the task performance if we can find effective auxiliary tasks related to the target task.', 'from this general knowledge, we re - consider jointly estimating pos - tags by incorporating the linearized forms without the pos - tag normalization as an auxiliary task.', 'in detail, the linearized forms with and without the pos - tag normalization are independently and simultaneously estimated as o j and q j, respectively, in the decoder output layer by following equation :', '3. 4 output length controlling', 'as described in  #TAUTHOR_TAG, not all the outputs ( predicted linearized parse trees ) obtained from the seq2seq parser are valid ( well - formed ) as a parse tree.', 'toward guaranteeing that every output is a valid tree, we introduce a simple extension of the method for controlling the seq2seq output length  #AUTHOR_TAG.', 'first, we introduce an additional bias term b in the decoder output layer to prevent the selection of certain output words :', '']",1
"['( chelba and jelinek 2000 ; charniak 2001 ;  #TAUTHOR_TAG ; wang, stolck']","['( chelba and jelinek 2000 ; charniak 2001 ;  #TAUTHOR_TAG ; wang, stolcke, and harper 2004, among others ), and while this']","['over the past decade ( chelba and jelinek 2000 ; charniak 2001 ;  #TAUTHOR_TAG ; wang, stolcke, and har']","['', 'this half of the book will be more enjoyable for readers of this journal, who are presumably interested in more general questions of computation and language than the step - by - step tutorial format of the first half of the book.', 'the details of the approach are interesting, particularly the insights about how to build linguistically rich grammars that can be effectively compiled into high - utility context - free grammars for speech recognition.', 'the primary shortcoming of this presentation lies in perpetuating the false dichotomy between "" grammar - based "" and "" data - driven "" approaches to language modeling for speech recognition, which motivates the final chapter of the book.', ""in fact, the authors'approach is both grammar - based and data - driven, given the corpus - based grammar specialization and pcfg estimation, which the authors themselves demonstrate to be indispensable."", 'robust grammar - based language modeling is a topic that has received a fair bit of attention over the past decade ( chelba and jelinek 2000 ; charniak 2001 ;  #TAUTHOR_TAG ; wang, stolcke, and harper 2004, among others ), and while this line of research has not focused on the use of manually built, narrow - domain feature grammars, there is enough similarity between the approach described in this book and']",0
"['of parameters.', ' #TAUTHOR_TAG present a simple reparameterization of ibm model 2 that is very fast to train, and']","['of parameters.', ' #TAUTHOR_TAG present a simple reparameterization of ibm model 2 that is very fast to train, and']","['have a large number of parameters.', ' #TAUTHOR_TAG present a simple reparameterization of ibm model 2 that is very fast to train, and']","['alignment is at the basis of most statistical machine translation.', 'the models that are generally used are often slow to train, and have a large number of parameters.', ' #TAUTHOR_TAG present a simple reparameterization of ibm model 2 that is very fast to train, and achieves results similar to ibm model 4.', ""while this model is very effective, it also has a very low number of parameters, and as such doesn't have a large amount of expressive power."", 'for one thing, it forces the model to consider alignments on both sides of the diagonal equally likely.', ""however, it isn't clear that this is the case, as for some languages an alignment to earlier or later in the sentence ( above or below the diagonal ) could be common, due to word order differences."", 'for example, when aligning to dutch, it may be common for one verb to be aligned near the end of the sentence that would be at the beginning in english.', '']",0
"['of parameters.', ' #TAUTHOR_TAG present a simple reparameterization of ibm model 2 that is very fast to train, and']","['of parameters.', ' #TAUTHOR_TAG present a simple reparameterization of ibm model 2 that is very fast to train, and']","['have a large number of parameters.', ' #TAUTHOR_TAG present a simple reparameterization of ibm model 2 that is very fast to train, and']","['alignment is at the basis of most statistical machine translation.', 'the models that are generally used are often slow to train, and have a large number of parameters.', ' #TAUTHOR_TAG present a simple reparameterization of ibm model 2 that is very fast to train, and achieves results similar to ibm model 4.', ""while this model is very effective, it also has a very low number of parameters, and as such doesn't have a large amount of expressive power."", 'for one thing, it forces the model to consider alignments on both sides of the diagonal equally likely.', ""however, it isn't clear that this is the case, as for some languages an alignment to earlier or later in the sentence ( above or below the diagonal ) could be common, due to word order differences."", 'for example, when aligning to dutch, it may be common for one verb to be aligned near the end of the sentence that would be at the beginning in english.', '']",0
"['make use of a modified version of model 2, from  #TAUTHOR_TAG, which has an alignment model that is parameterised in']","['make use of a modified version of model 2, from  #TAUTHOR_TAG, which has an alignment model that is parameterised in']","['make use of a modified version of model 2, from  #TAUTHOR_TAG, which has an alignment model that is parameterised in']","['make use of a modified version of model 2, from  #TAUTHOR_TAG, which has an alignment model that is parameterised in its original form solely on the variable λ.', 'specifically, the probability of a sentence e given a sentence f is given as :', 'here, m is the length of the target sentence e, n the same for source sentence f, δ is the alignment model and θ is the translation model.', 'in this paper we are mainly concerned with the alignment model δ.', 'in the original formulation ( with a minor tweak to ensure symmetry through the center ), this function is defined as :', 'where, h ( · ) is defined as', 'e. a normalising function.', 'like the original model 2  #AUTHOR_TAG, this model is trained using expectation - maximisation.', 'however, it is not possible to directly update the λ parameter during training, as it cannot be computed analytically.', 'instead, a gradient - based approach is used during the m - step.', 'two different optimisations are employed, the first of which is used for calculating z λ.', 'this function forms a geometric series away from the diagonal ( for each target word ), which can be computed efficiently for each of the directions from the diagonal.', 'the second is used during the m - step when computing the derivative, and is very similar, but instead of using a geometric series, an arithmetico - geometric series is used.', 'in order to allow the model to have a different parameter above and below the diagonal, the only change needed is to redefine h ( · ) to use a different parameter for λ above and below the diagonal.', 'we denote these parameters as λ and γ for below and above the diagonal respectively.', 'further, the offset is denoted as ω.', 'we change the definition of h ( · ) to the following instead :', '+ ω otherwise j ↓ is the point closest to or on the diagonal here, calculated as :', 'here, ω can range from −1 to 1, and thus the calculation for the diagonal j ↓ is clamped to be in a valid range for alignments.', 'as the partition function ( z ( · ) ) used in  #TAUTHOR_TAG consists of 2 calculations for each target position i, one for above and one for below the diagonal, we can simply substitute γ for the geometric series calculations in order to use different parameters for each :', 'where j ↑ is j ↓ + 1']",0
"['make use of a modified version of model 2, from  #TAUTHOR_TAG, which has an alignment model that is parameterised in']","['make use of a modified version of model 2, from  #TAUTHOR_TAG, which has an alignment model that is parameterised in']","['make use of a modified version of model 2, from  #TAUTHOR_TAG, which has an alignment model that is parameterised in']","['make use of a modified version of model 2, from  #TAUTHOR_TAG, which has an alignment model that is parameterised in its original form solely on the variable λ.', 'specifically, the probability of a sentence e given a sentence f is given as :', 'here, m is the length of the target sentence e, n the same for source sentence f, δ is the alignment model and θ is the translation model.', 'in this paper we are mainly concerned with the alignment model δ.', 'in the original formulation ( with a minor tweak to ensure symmetry through the center ), this function is defined as :', 'where, h ( · ) is defined as', 'e. a normalising function.', 'like the original model 2  #AUTHOR_TAG, this model is trained using expectation - maximisation.', 'however, it is not possible to directly update the λ parameter during training, as it cannot be computed analytically.', 'instead, a gradient - based approach is used during the m - step.', 'two different optimisations are employed, the first of which is used for calculating z λ.', 'this function forms a geometric series away from the diagonal ( for each target word ), which can be computed efficiently for each of the directions from the diagonal.', 'the second is used during the m - step when computing the derivative, and is very similar, but instead of using a geometric series, an arithmetico - geometric series is used.', 'in order to allow the model to have a different parameter above and below the diagonal, the only change needed is to redefine h ( · ) to use a different parameter for λ above and below the diagonal.', 'we denote these parameters as λ and γ for below and above the diagonal respectively.', 'further, the offset is denoted as ω.', 'we change the definition of h ( · ) to the following instead :', '+ ω otherwise j ↓ is the point closest to or on the diagonal here, calculated as :', 'here, ω can range from −1 to 1, and thus the calculation for the diagonal j ↓ is clamped to be in a valid range for alignments.', 'as the partition function ( z ( · ) ) used in  #TAUTHOR_TAG consists of 2 calculations for each target position i, one for above and one for below the diagonal, we can simply substitute γ for the geometric series calculations in order to use different parameters for each :', 'where j ↑ is j ↓ + 1']",6
"['not be directly comparable to those found in  #TAUTHOR_TAG.', 'conditioning on the pos of the target words then becomes as simple as using a different λ']","['not be directly comparable to those found in  #TAUTHOR_TAG.', 'conditioning on the pos of the target words then becomes as simple as using a different λ, γ, and ω for']","['not be directly comparable to those found in  #TAUTHOR_TAG.', 'conditioning on the pos of the target words then becomes as simple as using a different λ, γ, and ω for each pos tag in the input, and calculating a separate derivative for each of them, using only the derivatives at those target words that use the pos tag.', 'a minor detail is to keep a count of alignment positions used for finding the derivative for each different parameter, and normalizing the resulting derivatives with those counts, so the step size can be kept constant across pos tags']","['in the original formulation, we need to use gradient - based optimisation in order to find good values for λ, γ and ω.', 'unfortunately, optimizing ω would require taking the derivative of h ( · ), and thus the derivative of the absolute value.', 'this is unfortunately undefined when the argument is 0, however we work around this by choosing a subgradient of 0 at that point.', 'this means the steps we take do not always improve the objective function, but in practice the method works well.', 'the first derivative of l with respect to λ at a single target word becomes :', 'and similar for finding the first derivative with respect to γ, but summing from j ↑ to n instead.', 'the first derivative with respect to ω then, is :', 'where h ( · ) is the first derivative of h ( · ) with respect to ω.', 'for obtaining this derivative, the arithmetico - geometric series  #AUTHOR_TAG was originally used as an optimization, and for the gradient with respect to omega a geometric series should suffice, as an optimization, as there is no conditioning on the source words.', 'this is not done in the current work however, so timing results will not be directly comparable to those found in  #TAUTHOR_TAG.', 'conditioning on the pos of the target words then becomes as simple as using a different λ, γ, and ω for each pos tag in the input, and calculating a separate derivative for each of them, using only the derivatives at those target words that use the pos tag.', 'a minor detail is to keep a count of alignment positions used for finding the derivative for each different parameter, and normalizing the resulting derivatives with those counts, so the step size can be kept constant across pos tags']",4
"['.', 'we use similar corpora as used in  #TAUTHOR_TAG : a']","['bleu scores are computed.', 'we use similar corpora as used in  #TAUTHOR_TAG : a french - english corpus']","['.', 'we use similar corpora as used in  #TAUTHOR_TAG : a french - english corpus']","['above described model is evaluated with experiments on a set of 3 language pairs, on which aer scores and bleu scores are computed.', 'we use similar corpora as used in  #TAUTHOR_TAG : a french - english corpus made up of europarl version 7 and news - commentary corpora, the arabicenglish parallel data consisting of the non - un portions of the nist training corpora, and the fbis chinese - english corpora.', ""the models that are compared are the original reparameterization of model 2, a version where λ is split around the diagonal ( split ), one where pos tags are used, but λ is not split around the diagonal ( pos ), one where an offset is used, but parameters aren't split about the diagonal ( offset ), one that's split about the diagonal and uses pos tags used as in  #TAUTHOR_TAG, with stepsize for updates to λ and γ during gradient ascent is 1000, and that for ω is 0. 03, decaying after every gradient descent step by 0. 9, using 8 steps every iteration."", 'both λ and γ are initialised to 6, and ω is initialised to 0.', 'for these experiments the pos and pos & split use pos tags generated using the stanford pos tagger  #AUTHOR_TAG, using the supplied models for all of the languages used in the experiments.', 'for comparison, model 4 is trained for 5 iterations using 5 iterations each of model 1 and model 3 as initialization, using giza + +  #AUTHOR_TAG.', 'for the comparisons in aer, the corpora are used as - is, but for the bleu comparisons, sentences longer than 50 words are filtered out.', 'in table 2 the sizes of the corpora before filtering are listed, as well as the time taken in hours to align the corpora for aer.', 'as the training times for the different versions barely differ, only the average is displayed for the models here described and model 4 training times are given for comparison.', 'note that the times for the models optimizing only λ and γ, and the model only optimizing ω still calculate the derivatives for the other parameters, and so could be made to be faster than here displayed.', 'for both the bleu and aer results, the alignments are generated in both directions, and symmetrised using the grow - diag - final - and heuristic, which in preliminary tests had shown to do best in terms of aer.', 'the results are given in table 2.', 'these scores were computed using the wmt2012 data as gold standard.', 'the different extensions to the model make no difference to the aer scores for chineseenglish, and']",3
"['.', 'we use similar corpora as used in  #TAUTHOR_TAG : a']","['bleu scores are computed.', 'we use similar corpora as used in  #TAUTHOR_TAG : a french - english corpus']","['.', 'we use similar corpora as used in  #TAUTHOR_TAG : a french - english corpus']","['above described model is evaluated with experiments on a set of 3 language pairs, on which aer scores and bleu scores are computed.', 'we use similar corpora as used in  #TAUTHOR_TAG : a french - english corpus made up of europarl version 7 and news - commentary corpora, the arabicenglish parallel data consisting of the non - un portions of the nist training corpora, and the fbis chinese - english corpora.', ""the models that are compared are the original reparameterization of model 2, a version where λ is split around the diagonal ( split ), one where pos tags are used, but λ is not split around the diagonal ( pos ), one where an offset is used, but parameters aren't split about the diagonal ( offset ), one that's split about the diagonal and uses pos tags used as in  #TAUTHOR_TAG, with stepsize for updates to λ and γ during gradient ascent is 1000, and that for ω is 0. 03, decaying after every gradient descent step by 0. 9, using 8 steps every iteration."", 'both λ and γ are initialised to 6, and ω is initialised to 0.', 'for these experiments the pos and pos & split use pos tags generated using the stanford pos tagger  #AUTHOR_TAG, using the supplied models for all of the languages used in the experiments.', 'for comparison, model 4 is trained for 5 iterations using 5 iterations each of model 1 and model 3 as initialization, using giza + +  #AUTHOR_TAG.', 'for the comparisons in aer, the corpora are used as - is, but for the bleu comparisons, sentences longer than 50 words are filtered out.', 'in table 2 the sizes of the corpora before filtering are listed, as well as the time taken in hours to align the corpora for aer.', 'as the training times for the different versions barely differ, only the average is displayed for the models here described and model 4 training times are given for comparison.', 'note that the times for the models optimizing only λ and γ, and the model only optimizing ω still calculate the derivatives for the other parameters, and so could be made to be faster than here displayed.', 'for both the bleu and aer results, the alignments are generated in both directions, and symmetrised using the grow - diag - final - and heuristic, which in preliminary tests had shown to do best in terms of aer.', 'the results are given in table 2.', 'these scores were computed using the wmt2012 data as gold standard.', 'the different extensions to the model make no difference to the aer scores for chineseenglish, and']",7
"[', sequence - to - sequence models  #TAUTHOR_TAG ; have gain superior']","[', sequence - to - sequence models  #TAUTHOR_TAG ; have gain superior']","[', sequence - to - sequence models  #TAUTHOR_TAG ; have gain superior performance in machine translation  #AUTHOR_TAG ].', 'yet these state - of - the - art neural machine translation ( nmt ) models still fail to generation target sentences with comparable quality as human translators.', 'to']","[', sequence - to - sequence models  #TAUTHOR_TAG ; have gain superior performance in machine translation  #AUTHOR_TAG ].', 'yet these state - of - the - art neural machine translation ( nmt ) models still fail to generation target sentences with comparable quality as human translators.', 'to obtain a high - quality translation, there are a few recent works attempt to incorporate human revision instructions into the algorithmic translation process.', 'for example, previous interactive nmt [  #TAUTHOR_TAG proposes to ask human to revise the translation output from the beginning of a sentence to the end ( i. e. from the left to right ), and regenerates the partial translation on the right side of the * equal contribution, part of this work was done while rongxiang weng was a research intern at bytedance ai lab.', '† corresponding author.', '1 source code is available at : https : / / github. com / wengrx / camit they discuss the issue of nuclear unk and free trade area they discuss the issue of nuclear non - proliferation and free trade area they to discuss the issue of nuclear unk and trade area they discuss nuclear non - proliferation and free trade area issues they to discuss the issue of nuclear unk and trade area ( they ) ( discuss ) ( prevent ) ( nuclear ) ( proliferate ) ( and ) ( free ) ( trade zone ) ( issue ) they discuss nuclear non - proliferation and free trade zone']",0
"[', sequence - to - sequence models  #TAUTHOR_TAG ; have gain superior']","[', sequence - to - sequence models  #TAUTHOR_TAG ; have gain superior']","[', sequence - to - sequence models  #TAUTHOR_TAG ; have gain superior performance in machine translation  #AUTHOR_TAG ].', 'yet these state - of - the - art neural machine translation ( nmt ) models still fail to generation target sentences with comparable quality as human translators.', 'to']","[', sequence - to - sequence models  #TAUTHOR_TAG ; have gain superior performance in machine translation  #AUTHOR_TAG ].', 'yet these state - of - the - art neural machine translation ( nmt ) models still fail to generation target sentences with comparable quality as human translators.', 'to obtain a high - quality translation, there are a few recent works attempt to incorporate human revision instructions into the algorithmic translation process.', 'for example, previous interactive nmt [  #TAUTHOR_TAG proposes to ask human to revise the translation output from the beginning of a sentence to the end ( i. e. from the left to right ), and regenerates the partial translation on the right side of the * equal contribution, part of this work was done while rongxiang weng was a research intern at bytedance ai lab.', '† corresponding author.', '1 source code is available at : https : / / github. com / wengrx / camit they discuss the issue of nuclear unk and free trade area they discuss the issue of nuclear non - proliferation and free trade area they to discuss the issue of nuclear unk and trade area they discuss nuclear non - proliferation and free trade area issues they to discuss the issue of nuclear unk and trade area ( they ) ( discuss ) ( prevent ) ( nuclear ) ( proliferate ) ( and ) ( free ) ( trade zone ) ( issue ) they discuss nuclear non - proliferation and free trade zone']",0
"['decoder  #TAUTHOR_TAG, our method is']","['decoder  #TAUTHOR_TAG, our method is']","['decoder  #TAUTHOR_TAG, our method is']","['', 'sentence after getting a revision. different from previous work about bi - directional decoder  #TAUTHOR_TAG, our method is designed to', '']",0
"['generation  #TAUTHOR_TAG ].', '']","['generation  #TAUTHOR_TAG ].', '']","['generation  #TAUTHOR_TAG ].', 'the encoder']","['machine translation ( nmt ) is based on a standard seq2seq model, which adopts an encoder - decoder architecture for sentence modeling and generation  #TAUTHOR_TAG ].', '']",0
"['generation  #TAUTHOR_TAG ].', '']","['generation  #TAUTHOR_TAG ].', '']","['generation  #TAUTHOR_TAG ].', 'the encoder']","['machine translation ( nmt ) is based on a standard seq2seq model, which adopts an encoder - decoder architecture for sentence modeling and generation  #TAUTHOR_TAG ].', '']",0
"['well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is']","['well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is computed by : the p ( y j | y > j, x ) and', 'l l are defined in equation 1 and']","['part, the new left part is expected to become better as well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is computed by : the p ( y j | y > j, x ) and', 'l l are defined in equation 1 and equation 4, respectively. | y | is the', 'length of y. we have shown how to update the sentence after', 'a single revision. however, another challenge for interactive nmt']","['the output is note that, the length of the new left part may be not equal to the original left part. in this way, the whole sentence is updated jointly by the two decoders', '. with the human revision, the new right part is expected to be better than the right part', '. based on this better right part, the new left part is expected to become better as well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is computed by : the p ( y j | y > j, x ) and', 'l l are defined in equation 1 and equation 4, respectively. | y | is the', 'length of y. we have shown how to update the sentence after', 'a single revision. however, another challenge for interactive nmt y r k + 1 happens when human translator performs several revisions in one round, because the interactive process should regenerate the translation with all the revisions considered', '. to solve the problem, we propose to combine the grid beam search  #TAUTHOR_TAG with our bi - directional decoder. intuitively, the grid beam search adopts a grid to store the partial translations that containing the previous revisions. after the gird search,', '']",0
"['translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently,']","['translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently,']","['to improve the translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently, researchers employ it in neural machine translation ( nmt ).', 'barrachina']","['machine translation has been widely exploited to improve the translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently, researchers employ it in neural machine translation ( nmt ).', 'barrachina et al. [ 2009 ], gonzalez - rubio et al. [ 2013 ] and  #TAUTHOR_TAG present an interactive nmt model with the uni - directional interaction protocol ( unidir ), in which users can only interact with the model from left to right.', '']",0
"['translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently,']","['translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently,']","['to improve the translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently, researchers employ it in neural machine translation ( nmt ).', 'barrachina']","['machine translation has been widely exploited to improve the translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently, researchers employ it in neural machine translation ( nmt ).', 'barrachina et al. [ 2009 ], gonzalez - rubio et al. [ 2013 ] and  #TAUTHOR_TAG present an interactive nmt model with the uni - directional interaction protocol ( unidir ), in which users can only interact with the model from left to right.', '']",0
"[', sequence - to - sequence models  #TAUTHOR_TAG ; have gain superior']","[', sequence - to - sequence models  #TAUTHOR_TAG ; have gain superior']","[', sequence - to - sequence models  #TAUTHOR_TAG ; have gain superior performance in machine translation  #AUTHOR_TAG ].', 'yet these state - of - the - art neural machine translation ( nmt ) models still fail to generation target sentences with comparable quality as human translators.', 'to']","[', sequence - to - sequence models  #TAUTHOR_TAG ; have gain superior performance in machine translation  #AUTHOR_TAG ].', 'yet these state - of - the - art neural machine translation ( nmt ) models still fail to generation target sentences with comparable quality as human translators.', 'to obtain a high - quality translation, there are a few recent works attempt to incorporate human revision instructions into the algorithmic translation process.', 'for example, previous interactive nmt [  #TAUTHOR_TAG proposes to ask human to revise the translation output from the beginning of a sentence to the end ( i. e. from the left to right ), and regenerates the partial translation on the right side of the * equal contribution, part of this work was done while rongxiang weng was a research intern at bytedance ai lab.', '† corresponding author.', '1 source code is available at : https : / / github. com / wengrx / camit they discuss the issue of nuclear unk and free trade area they discuss the issue of nuclear non - proliferation and free trade area they to discuss the issue of nuclear unk and trade area they discuss nuclear non - proliferation and free trade area issues they to discuss the issue of nuclear unk and trade area ( they ) ( discuss ) ( prevent ) ( nuclear ) ( proliferate ) ( and ) ( free ) ( trade zone ) ( issue ) they discuss nuclear non - proliferation and free trade zone']",1
"['decoder  #TAUTHOR_TAG, our method is']","['decoder  #TAUTHOR_TAG, our method is']","['decoder  #TAUTHOR_TAG, our method is']","['', 'sentence after getting a revision. different from previous work about bi - directional decoder  #TAUTHOR_TAG, our method is designed to', '']",1
"['well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is']","['well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is computed by : the p ( y j | y > j, x ) and', 'l l are defined in equation 1 and']","['part, the new left part is expected to become better as well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is computed by : the p ( y j | y > j, x ) and', 'l l are defined in equation 1 and equation 4, respectively. | y | is the', 'length of y. we have shown how to update the sentence after', 'a single revision. however, another challenge for interactive nmt']","['the output is note that, the length of the new left part may be not equal to the original left part. in this way, the whole sentence is updated jointly by the two decoders', '. with the human revision, the new right part is expected to be better than the right part', '. based on this better right part, the new left part is expected to become better as well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is computed by : the p ( y j | y > j, x ) and', 'l l are defined in equation 1 and equation 4, respectively. | y | is the', 'length of y. we have shown how to update the sentence after', 'a single revision. however, another challenge for interactive nmt y r k + 1 happens when human translator performs several revisions in one round, because the interactive process should regenerate the translation with all the revisions considered', '. to solve the problem, we propose to combine the grid beam search  #TAUTHOR_TAG with our bi - directional decoder. intuitively, the grid beam search adopts a grid to store the partial translations that containing the previous revisions. after the gird search,', '']",1
"['translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently,']","['translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently,']","['to improve the translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently, researchers employ it in neural machine translation ( nmt ).', 'barrachina']","['machine translation has been widely exploited to improve the translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently, researchers employ it in neural machine translation ( nmt ).', 'barrachina et al. [ 2009 ], gonzalez - rubio et al. [ 2013 ] and  #TAUTHOR_TAG present an interactive nmt model with the uni - directional interaction protocol ( unidir ), in which users can only interact with the model from left to right.', '']",1
"['decoder  #TAUTHOR_TAG, our method is']","['decoder  #TAUTHOR_TAG, our method is']","['decoder  #TAUTHOR_TAG, our method is']","['', 'sentence after getting a revision. different from previous work about bi - directional decoder  #TAUTHOR_TAG, our method is designed to', '']",4
"['well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is']","['well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is computed by : the p ( y j | y > j, x ) and', 'l l are defined in equation 1 and']","['part, the new left part is expected to become better as well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is computed by : the p ( y j | y > j, x ) and', 'l l are defined in equation 1 and equation 4, respectively. | y | is the', 'length of y. we have shown how to update the sentence after', 'a single revision. however, another challenge for interactive nmt']","['the output is note that, the length of the new left part may be not equal to the original left part. in this way, the whole sentence is updated jointly by the two decoders', '. with the human revision, the new right part is expected to be better than the right part', '. based on this better right part, the new left part is expected to become better as well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is computed by : the p ( y j | y > j, x ) and', 'l l are defined in equation 1 and equation 4, respectively. | y | is the', 'length of y. we have shown how to update the sentence after', 'a single revision. however, another challenge for interactive nmt y r k + 1 happens when human translator performs several revisions in one round, because the interactive process should regenerate the translation with all the revisions considered', '. to solve the problem, we propose to combine the grid beam search  #TAUTHOR_TAG with our bi - directional decoder. intuitively, the grid beam search adopts a grid to store the partial translations that containing the previous revisions. after the gird search,', '']",4
"['translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently,']","['translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently,']","['to improve the translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently, researchers employ it in neural machine translation ( nmt ).', 'barrachina']","['machine translation has been widely exploited to improve the translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently, researchers employ it in neural machine translation ( nmt ).', 'barrachina et al. [ 2009 ], gonzalez - rubio et al. [ 2013 ] and  #TAUTHOR_TAG present an interactive nmt model with the uni - directional interaction protocol ( unidir ), in which users can only interact with the model from left to right.', '']",4
"['translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently,']","['translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently,']","['to improve the translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently, researchers employ it in neural machine translation ( nmt ).', 'barrachina']","['machine translation has been widely exploited to improve the translation by using interaction feedback from human users  #TAUTHOR_TAG ].', 'recently, researchers employ it in neural machine translation ( nmt ).', 'barrachina et al. [ 2009 ], gonzalez - rubio et al. [ 2013 ] and  #TAUTHOR_TAG present an interactive nmt model with the uni - directional interaction protocol ( unidir ), in which users can only interact with the model from left to right.', '']",4
"['generation  #TAUTHOR_TAG ].', '']","['generation  #TAUTHOR_TAG ].', '']","['generation  #TAUTHOR_TAG ].', 'the encoder']","['machine translation ( nmt ) is based on a standard seq2seq model, which adopts an encoder - decoder architecture for sentence modeling and generation  #TAUTHOR_TAG ].', '']",7
"[' #TAUTHOR_TAG ], we focus on']","[' #TAUTHOR_TAG ], we focus on']","['one mistake in a sentence.', 'commonly, the revision can include several operations : replacement, insertion and deletion.', 'following previous work  #TAUTHOR_TAG ], we focus on']","['', 'one revision is the process that human translator manually corrects one mistake in a sentence.', 'commonly, the revision can include several operations : replacement, insertion and deletion.', 'following previous work  #TAUTHOR_TAG ], we focus on the replacement and others can be implemented with several replacement operations.', '']",5
"['with a copy mechanism  #TAUTHOR_TAG.', 'the']","['with a copy mechanism  #TAUTHOR_TAG.', 'the']","['with a copy mechanism  #TAUTHOR_TAG.', '']","['', 'when generating a word in decoding, the model will read the revision memory, trying to automatically fix mistakes with a copy mechanism  #TAUTHOR_TAG.', 'the final output distribution of word w is computed by distributions from the decoder and from the revision memory :', 'where r j, t is the probability of copying word y r t in current position j. it is calculated by the revision context < s j, c j > of current decoding state and revision contexts of items in the memory.', 'θ j is a weight computed at each decoding step :', 'if the output word w is an unknown word ( unk ), but has been corrected by human, our method could successfully generate w in future translation by copying it from the revision memory, which partially alleviate the problem of unk.', 'to keep the translation process going, the embedding of unk is fed into the decoder, as the input for the next time step']",5
"['score  #TAUTHOR_TAG.', 'we implement our interactive nmt model upon njunmt - pytorch 4.', 'the forward and backward decoders of the bi - directional model are']","['with the ibm - bleu score  #TAUTHOR_TAG.', 'we implement our interactive nmt model upon njunmt - pytorch 4.', 'the forward and backward decoders of the bi - directional model are']","['with the ibm - bleu score  #TAUTHOR_TAG.', 'we implement our interactive nmt model upon njunmt - pytorch 4.', 'the forward and backward decoders of the bi - directional model are trained together with a shared encoder.', 'the learning rate of online learning is 10 −5.', '']","['', 'formally, every time we obtain a correct translation pair { x, y } after interaction, we update the whole translation model for one step, according to equation 4.', 'by learning from the sentence level interaction history, our seq2seq model better fits we measure the translation quality with the ibm - bleu score  #TAUTHOR_TAG.', 'we implement our interactive nmt model upon njunmt - pytorch 4.', 'the forward and backward decoders of the bi - directional model are trained together with a shared encoder.', 'the learning rate of online learning is 10 −5.', 'following previous work [  #AUTHOR_TAG ], we train the revision memory by randomly sampling words and contexts in the same discourse.', 'the size of revision memory is 100 and it will be used when revision number is more than 20.', 'when the revision number is more than 100, the first part will be discarded until revision number is less than 100']",5
"['[  #TAUTHOR_TAG, we experiment on both the ideal and real environments.', 'because real - world human interactions are expensive and time - consuming to obtain, we first report results on the ideal environment, which generates simulated human interactions by identifying critical mistakes.', 'the simulated critical mistake is those lead to']","['[  #TAUTHOR_TAG, we experiment on both the ideal and real environments.', 'because real - world human interactions are expensive and time - consuming to obtain, we first report results on the ideal environment, which generates simulated human interactions by identifying critical mistakes.', 'the simulated critical mistake is those lead to']","['[  #TAUTHOR_TAG, we experiment on both the ideal and real environments.', 'because real - world human interactions are expensive and time - consuming to obtain, we first report results on the ideal environment, which generates simulated human interactions by identifying critical mistakes.', 'the simulated critical mistake is those lead to']","['previous work [  #TAUTHOR_TAG, we experiment on both the ideal and real environments.', 'because real - world human interactions are expensive and time - consuming to obtain, we first report results on the ideal environment, which generates simulated human interactions by identifying critical mistakes.', 'the simulated critical mistake is those lead to the most significant bleu score improvement after being corrected.', 'we also validate the interactive efficiency on the real environment, in which three human annotators are asked to revise the translation, until obtaining correct translations.', 'the three human annotators are asked to work with all different systems with no idea of which system they are working with']",5
"['well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is']","['well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is computed by : the p ( y j | y > j, x ) and', 'l l are defined in equation 1 and']","['part, the new left part is expected to become better as well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is computed by : the p ( y j | y > j, x ) and', 'l l are defined in equation 1 and equation 4, respectively. | y | is the', 'length of y. we have shown how to update the sentence after', 'a single revision. however, another challenge for interactive nmt']","['the output is note that, the length of the new left part may be not equal to the original left part. in this way, the whole sentence is updated jointly by the two decoders', '. with the human revision, the new right part is expected to be better than the right part', '. based on this better right part, the new left part is expected to become better as well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is computed by : the p ( y j | y > j, x ) and', 'l l are defined in equation 1 and equation 4, respectively. | y | is the', 'length of y. we have shown how to update the sentence after', 'a single revision. however, another challenge for interactive nmt y r k + 1 happens when human translator performs several revisions in one round, because the interactive process should regenerate the translation with all the revisions considered', '. to solve the problem, we propose to combine the grid beam search  #TAUTHOR_TAG with our bi - directional decoder. intuitively, the grid beam search adopts a grid to store the partial translations that containing the previous revisions. after the gird search,', '']",3
"['well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is']","['well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is computed by : the p ( y j | y > j, x ) and', 'l l are defined in equation 1 and']","['part, the new left part is expected to become better as well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is computed by : the p ( y j | y > j, x ) and', 'l l are defined in equation 1 and equation 4, respectively. | y | is the', 'length of y. we have shown how to update the sentence after', 'a single revision. however, another challenge for interactive nmt']","['the output is note that, the length of the new left part may be not equal to the original left part. in this way, the whole sentence is updated jointly by the two decoders', '. with the human revision, the new right part is expected to be better than the right part', '. based on this better right part, the new left part is expected to become better as well. the training stage is similar with multi - task models [  #TAUTHOR_TAG, both decoders could be trained using cross - entropy as the', 'objective : where l r is computed by : the p ( y j | y > j, x ) and', 'l l are defined in equation 1 and equation 4, respectively. | y | is the', 'length of y. we have shown how to update the sentence after', 'a single revision. however, another challenge for interactive nmt y r k + 1 happens when human translator performs several revisions in one round, because the interactive process should regenerate the translation with all the revisions considered', '. to solve the problem, we propose to combine the grid beam search  #TAUTHOR_TAG with our bi - directional decoder. intuitively, the grid beam search adopts a grid to store the partial translations that containing the previous revisions. after the gird search,', '']",6
"['by sentential constituents  #TAUTHOR_TAG.', '']","['by sentential constituents  #TAUTHOR_TAG.', '']","['by sentential constituents  #TAUTHOR_TAG.', '']","['- scale lexical semantic resources that provide relational information about words have recently received much focus in the field of natural language processing ( nlp ).', 'in particular, data - driven models for lexical semantics require the creation of broad - coverage, hand - annotated corpora with predicateargument information, i. e. rich information about words expressing a semantic relation having argument slots filled by the interpretations of their grammatical complements.', 'corpora combining semantic and syntactic annotations constitute the backbone for the development of probabilistic models that automatically identify the semantic relationships, or semantic roles, conveyed by sentential constituents  #TAUTHOR_TAG.', 'that is, given an input sentence and a target predicator the system labels constituents with general roles like agent, patient, theme, etc., or more specific roles, as in ( 1 ).', '( 1 ) 1 the task of automatic semantic role labelling ( or shallow semantic parsing ) is a first step towards text understanding and has found use in a variety of nlp applications including information extraction  #AUTHOR_TAG, machine translation  #AUTHOR_TAG, question answering  #AUTHOR_TAG, summarisation  #AUTHOR_TAG, recognition of textual entailment relations  #AUTHOR_TAG, etc.', 'corpora with semantic role labels additionally lend themselves to extraction of linguistic knowledge at the syntax - semantics interface.', 'the range of semantic and syntactic combinatorial properties ( valences ) of each word in each of its senses is documented in terms of annotated corpus attestations.', 'for instance, the valence pattern for the use of admire in ( 1 ) is shown in ( 2 ).', 'this data enables the quantitative study of various linguistic phenomena and the investigation of the relationship between the distinct linguistic layers comprised by predicate - argument analysis.', 'furthermore, the formulation of generalisations over predicate - specific annotations can capture how predicates relate in terms of both semantic and syntactic features.', 'such syntax - semantics mappings ( so - called linking generalisations ) encode regularities concerning the associations of semantic roles with grammatical functions and are essential for a linguistic knowledge base for nlp applications.', 'this paper addresses the problem of generalising over the valences of individual predicators and proposes an abstract semantic basis for the representation of participant roles.', 'the definition of semantic notions at an appropriate level of abstraction is the prerequisite for the formulation of a general, principled syntax - semantics interface.', 'this is in accordance with a somewhat intuitive conception of semantic roles as classificatory notions encoding semantic similarities across different types of events or situations in the world.', 'in effect, all conceptions of semantic roles as opposed to predicate - specific roles, such as admirer - admired, posit some sort of semantic classification of arguments across predicators while']",0
,,,,5
['recent exception is  #TAUTHOR_TAG which trained many -'],['recent exception is  #TAUTHOR_TAG which trained many - to - one models from 58 languages into english'],['recent exception is  #TAUTHOR_TAG which trained many - to - one models from 58 languages into english'],"['it is still unclear how far one can scale multilingual nmt in terms of the number of languages involved. previous works on multilingual nmt typically trained models with up to 7 languages  #AUTHOR_TAG b ;  #AUTHOR_TAG and up to 20', 'trained directions  #AUTHOR_TAG simultaneously. one recent exception is  #TAUTHOR_TAG which trained many - to - one models from 58 languages into english. while utilizing significantly more languages than previous works,', '']",0
"['massively multilingual models,  #TAUTHOR_TAG explored methods']","['massively multilingual models,  #TAUTHOR_TAG explored methods']","['models on only up to 7 languages  #AUTHOR_TAG and 20 trained directions  #AUTHOR_TAG in a single model, whereas we focus on scaling nmt to much larger numbers of languages and trained directions.', 'regarding massively multilingual models,  #TAUTHOR_TAG explored methods']","['the mentioned studies provide valuable contributions to improving multilingual models, they apply their models on only up to 7 languages  #AUTHOR_TAG and 20 trained directions  #AUTHOR_TAG in a single model, whereas we focus on scaling nmt to much larger numbers of languages and trained directions.', 'regarding massively multilingual models,  #TAUTHOR_TAG explored methods for rapid adaptation of nmt to new languages by training multilingual models on the 59 - language ted talks corpus and fine - tuning them using data from the new languages.', 'while modeling significantly more languages than previous studies, they only train many - to - one models, which we show are inferior in comparison to our proposed massively multilingual many - to - many models when evaluated into english on this dataset.', ' #AUTHOR_TAG trained an english - centric many - to - many model on translations of the bible including 927 languages.', 'while this work pointed to an interesting phenomena in the latent space learned by the model where it clusters representations of typologically - similar languages together, it did not include any evaluation of the produced translations.', ' #AUTHOR_TAG trained a many - to - english system including 1017 languages from bible translations, and used it to infer typological features for the different languages ( without evaluating the translation quality ).', 'in another relevant work,  #AUTHOR_TAG trained an nmt model on 93 languages and used the learned representations to perform cross - lingual transfer learning.', 'again, they did not report the performance of the translation model learned in that massively multilingual setting']",0
['recent exception is  #TAUTHOR_TAG which trained many -'],['recent exception is  #TAUTHOR_TAG which trained many - to - one models from 58 languages into english'],['recent exception is  #TAUTHOR_TAG which trained many - to - one models from 58 languages into english'],"['it is still unclear how far one can scale multilingual nmt in terms of the number of languages involved. previous works on multilingual nmt typically trained models with up to 7 languages  #AUTHOR_TAG b ;  #AUTHOR_TAG and up to 20', 'trained directions  #AUTHOR_TAG simultaneously. one recent exception is  #TAUTHOR_TAG which trained many - to - one models from 58 languages into english. while utilizing significantly more languages than previous works,', '']",5
['four languages as  #TAUTHOR_TAG -'],"['four languages as  #TAUTHOR_TAG - azerbeijani ( az ), belarusian ( be ), galician ( gl ) and slovak ( sk ).', 'these languages present an extreme low - resource case, with as few as 4. 5k training examples for belarusian - english.', 'in order to better understand the effect of training set size in these settings, we evaluate on four additional languages that have more than 167k training examples each - arabic ( ar ),']","['additional preprocessing other than applying joint subword segmentation  #AUTHOR_TAG with 32k symbols.', 'regarding the languages we evaluate on, we begin with the same four languages as  #TAUTHOR_TAG - azerbeijani ( az ), belarusian (']","['main question we wish to answer in this work is how well a single nmt model can scale to support a very large number of language pairs.', 'the answer is not trivial : on the one hand, training multiple language pairs together may result in transfer learning  #AUTHOR_TAG.', 'this may improve performance as we increase the number of language pairs, since more information can be shared between the different translation tasks, allowing the model to learn which information to share.', 'on the other hand, adding many language pairs may result in a bottleneck ; the model has a limited capacity while it needs to handle this large number of translation tasks, and sharing all parameters between the different languages can be sub - optimal  #AUTHOR_TAG especially if they are not from the same typological language family ( sachan and.', 'we begin tackling this question by experimenting with the ted talks parallel corpus compiled by  #AUTHOR_TAG 1, which is unique in that it includes parallel data from 59 languages.', 'for comparison, this is significantly "" more multilingual "" than the data available from all previous wmt news translation shared task evaluations throughout the years - the latest being  #AUTHOR_TAG bojar et al. (, 2017 bojar et al. (, 2018, which included 14 languages so far.', '2 we focus on the setting where we train "" english - centric "" models, i. e. training on all language pairs that contain english in either the source or the target, resulting in 116 translation directions.', 'this dataset is also highly imbalanced, with language pairs including between 3. 3k to 214k sentence pairs for training.', 'since the dataset is already tokenized we did not apply additional preprocessing other than applying joint subword segmentation  #AUTHOR_TAG with 32k symbols.', 'regarding the languages we evaluate on, we begin with the same four languages as  #TAUTHOR_TAG - azerbeijani ( az ), belarusian ( be ), galician ( gl ) and slovak ( sk ).', 'these languages present an extreme low - resource case, with as few as 4. 5k training examples for belarusian - english.', 'in order to better understand the effect of training set size in these settings, we evaluate on four additional languages that have more than 167k training examples each - arabic ( ar ), german ( de ), hebrew ( he ) and italian ( it )']",5
"['results on this dataset  #TAUTHOR_TAG ;  #AUTHOR_TAG ).', '']","['results on this dataset  #TAUTHOR_TAG ;  #AUTHOR_TAG ).', '']","['results on this dataset  #TAUTHOR_TAG ;  #AUTHOR_TAG ).', 'regarding the models, we focused on the transformer in the "" base "" configuration.', 'we refer the reader to  #AUTHOR_TAG']","['the same data, we trained three massively multilingual models : a many - to - many model which we train using all 116 translation directions with 58 languages to - and - from english, a one - tomany model from english into 58 languages, and a many - to - one model from 58 languages into english.', 'we follow the method of  #AUTHOR_TAG ;  #AUTHOR_TAG and add a target - language prefix token to each source sentence to enable many - to - many translation.', 'these different setups enable us to examine the effect of the number of translation tasks on the translation quality as measured in bleu  #AUTHOR_TAG.', 'we also compare our massively multilingual models to bilingual baselines and to two recently published results on this dataset  #TAUTHOR_TAG ;  #AUTHOR_TAG ).', 'regarding the models, we focused on the transformer in the "" base "" configuration.', 'we refer the reader to  #AUTHOR_TAG for more details on the model architecture.', 'specifically, we use 6 layers in both the encoder and the decoder, with model dimension set at 512, hidden dimension size of 2048 and 8 attention heads.', '']",5
['one models in  #TAUTHOR_TAG. we'],['one models in  #TAUTHOR_TAG. we'],['one models in  #TAUTHOR_TAG.'],"['', 'note the large gap in the many - to - one model between the training set bleu and the development set bleu, which points on', 'the generalization issue that is not present in the many - to - many setting. we also note that our many - to - one model is on average 0. 75 bleu behind the best many - to - one models in  #TAUTHOR_TAG. we attribute this to the fact that their models are fine - tuned using similarlanguage - regularization while our model is not. we find an additional difference between', 'the results on the resource - scarce languages ( table 1 ) and the higher - resource languages ( table 2 ). specifically, the bilingual baselines outperform the many -', 'to - one models only in the higherresource setting. this makes sense as in the lowresource setting the baselines have very few training examples to outperform the many - to - one mod - table 3 : en→x test bleu on the ted talks corpus els, while in the higher resource setting they have access to more training data. this corroborates the results of  #AUTHOR_TAG that showed the sensitivity of such', 'models to similar low resource conditions and the improvements gained from using many - to - one models ( however with much fewer language pairs ). table 3 shows the results of our massively multilingual models and bilingual baselines when evaluated out', '- of - english. in this case we see an opposite trend : the many - to - many model performs worse than the one - to - many model by 2', '. 53 bleu on average. while previous works  #AUTHOR_TAG discuss the phenomena of quality degradation in english - to - many settings, this shows that increasing the number of source languages also causes additional degradation in a many - to - many model. this degradation may be due to the english - centric setting : since most of the translation directions the model', 'is trained on are into english, this leaves less capacity for the other target languages ( while still performing better than the bilingual baselines on all 8 language pairs ). we also note that in this case', 'the results are consistent among the higher and lower resource pairs - the one - to - many model is better than the many - to - many model, which outperforms the bilingual baselines in all cases. this is unlike the difference we saw in the x→ en experiments since here we do not have the multi - way - parallel overfitting', 'issue']",5
['one models in  #TAUTHOR_TAG. we'],['one models in  #TAUTHOR_TAG. we'],['one models in  #TAUTHOR_TAG.'],"['', 'note the large gap in the many - to - one model between the training set bleu and the development set bleu, which points on', 'the generalization issue that is not present in the many - to - many setting. we also note that our many - to - one model is on average 0. 75 bleu behind the best many - to - one models in  #TAUTHOR_TAG. we attribute this to the fact that their models are fine - tuned using similarlanguage - regularization while our model is not. we find an additional difference between', 'the results on the resource - scarce languages ( table 1 ) and the higher - resource languages ( table 2 ). specifically, the bilingual baselines outperform the many -', 'to - one models only in the higherresource setting. this makes sense as in the lowresource setting the baselines have very few training examples to outperform the many - to - one mod - table 3 : en→x test bleu on the ted talks corpus els, while in the higher resource setting they have access to more training data. this corroborates the results of  #AUTHOR_TAG that showed the sensitivity of such', 'models to similar low resource conditions and the improvements gained from using many - to - one models ( however with much fewer language pairs ). table 3 shows the results of our massively multilingual models and bilingual baselines when evaluated out', '- of - english. in this case we see an opposite trend : the many - to - many model performs worse than the one - to - many model by 2', '. 53 bleu on average. while previous works  #AUTHOR_TAG discuss the phenomena of quality degradation in english - to - many settings, this shows that increasing the number of source languages also causes additional degradation in a many - to - many model. this degradation may be due to the english - centric setting : since most of the translation directions the model', 'is trained on are into english, this leaves less capacity for the other target languages ( while still performing better than the bilingual baselines on all 8 language pairs ). we also note that in this case', 'the results are consistent among the higher and lower resource pairs - the one - to - many model is better than the many - to - many model, which outperforms the bilingual baselines in all cases. this is unlike the difference we saw in the x→ en experiments since here we do not have the multi - way - parallel overfitting', 'issue']",4
['one models in  #TAUTHOR_TAG. we'],['one models in  #TAUTHOR_TAG. we'],['one models in  #TAUTHOR_TAG.'],"['', 'note the large gap in the many - to - one model between the training set bleu and the development set bleu, which points on', 'the generalization issue that is not present in the many - to - many setting. we also note that our many - to - one model is on average 0. 75 bleu behind the best many - to - one models in  #TAUTHOR_TAG. we attribute this to the fact that their models are fine - tuned using similarlanguage - regularization while our model is not. we find an additional difference between', 'the results on the resource - scarce languages ( table 1 ) and the higher - resource languages ( table 2 ). specifically, the bilingual baselines outperform the many -', 'to - one models only in the higherresource setting. this makes sense as in the lowresource setting the baselines have very few training examples to outperform the many - to - one mod - table 3 : en→x test bleu on the ted talks corpus els, while in the higher resource setting they have access to more training data. this corroborates the results of  #AUTHOR_TAG that showed the sensitivity of such', 'models to similar low resource conditions and the improvements gained from using many - to - one models ( however with much fewer language pairs ). table 3 shows the results of our massively multilingual models and bilingual baselines when evaluated out', '- of - english. in this case we see an opposite trend : the many - to - many model performs worse than the one - to - many model by 2', '. 53 bleu on average. while previous works  #AUTHOR_TAG discuss the phenomena of quality degradation in english - to - many settings, this shows that increasing the number of source languages also causes additional degradation in a many - to - many model. this degradation may be due to the english - centric setting : since most of the translation directions the model', 'is trained on are into english, this leaves less capacity for the other target languages ( while still performing better than the bilingual baselines on all 8 language pairs ). we also note that in this case', 'the results are consistent among the higher and lower resource pairs - the one - to - many model is better than the many - to - many model, which outperforms the bilingual baselines in all cases. this is unlike the difference we saw in the x→ en experiments since here we do not have the multi - way - parallel overfitting', 'issue']",4
"['noun  #TAUTHOR_TAG.', 'the principle of dependency length minimization predicts consistent branching, to']","['noun  #TAUTHOR_TAG.', 'the principle of dependency length minimization predicts consistent branching, to']","['noun  #TAUTHOR_TAG.', 'the principle of dependency length minimization predicts consistent branching, to']",[' #TAUTHOR_TAG'],0
"['noun  #TAUTHOR_TAG.', 'the principle of dependency length minimization predicts consistent branching, to']","['noun  #TAUTHOR_TAG.', 'the principle of dependency length minimization predicts consistent branching, to']","['noun  #TAUTHOR_TAG.', 'the principle of dependency length minimization predicts consistent branching, to']",[' #TAUTHOR_TAG'],0
"['of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words']","['3 the unit of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however, it could be measured in other linguistic units : syllables, morphemes, phonemes, etc. a higher']","['of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however,']","['this should make their dependency more robust against interference or decay. lower values of g ( u, v', ', d ) may facilitate the involvement of other word order principles. · a n. 1 2 3 4 5 6 7 8 8.', '5 9 10 11 12 13 13. 5 14 15. 1 2 3 4 5 6 7 7. 5 8 9 10 11 11. 5 12 13. 3 the unit of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however, it could be measured in other linguistic units : syllables, morphemes, phonemes, etc. a higher level of precision might illuminate inconsistencies between the dominant order according to the criteria in  #AUTHOR_TAG and other orders that appear recurrent', '##ly in particular circumstances.  #AUTHOR_TAG points out the case of italian, which is classified as svo  #AUTHOR_TAG but allows pronouns taking the role of the object to appear before', 'the verb. this is a common phenomenon occurring in other romance languages', '( e. g., catalan or french ). let us consider the case of french, which is "" svo when the object is lexical, but sov when the object is prepositional ""  #AUTHOR_TAG, as in sentences ( a ) and ( b ) in fig. 1. for simplicity, imagine that we measure dependency length in characters ( this may make sense in a reading task, for instance )', '. imagine that the center of a word of length λ is located at position ( λ + 1 ) / 2 and that the space between two words counts as one character. for simplicity again, let us assume that dependencies originate from the center of the word and that the length of a dependency is the difference between the positions of the centers of the words involved. the sum of dependency lengths of the sov sentence with a pronominal object ( fig', '. 1 ( b ) ) is in between that of the svo sentence with nominal object ( fig. 1 ( a ) ) and that of sov the sentence with nominal object ( fig. 1 ( c ) ).', 'this is due to the brevity of the pronoun', ', suggesting that pressure for online memory minimization reduces if short words are involved.', 'another factor that may influence pressure for dependency length minimization is sentence length : dependency length minimization has been argued to be less necessary in short sentences, where the maximization of the predictability of the verb might be the winning', 'principle ( ferrer - i -  #AUTHOR_TAG. sentence length and word length might', 'explain the tendency to adopt sov in romance languages when clitics are involved']",0
"['of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words']","['3 the unit of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however, it could be measured in other linguistic units : syllables, morphemes, phonemes, etc. a higher']","['of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however,']","['this should make their dependency more robust against interference or decay. lower values of g ( u, v', ', d ) may facilitate the involvement of other word order principles. · a n. 1 2 3 4 5 6 7 8 8.', '5 9 10 11 12 13 13. 5 14 15. 1 2 3 4 5 6 7 7. 5 8 9 10 11 11. 5 12 13. 3 the unit of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however, it could be measured in other linguistic units : syllables, morphemes, phonemes, etc. a higher level of precision might illuminate inconsistencies between the dominant order according to the criteria in  #AUTHOR_TAG and other orders that appear recurrent', '##ly in particular circumstances.  #AUTHOR_TAG points out the case of italian, which is classified as svo  #AUTHOR_TAG but allows pronouns taking the role of the object to appear before', 'the verb. this is a common phenomenon occurring in other romance languages', '( e. g., catalan or french ). let us consider the case of french, which is "" svo when the object is lexical, but sov when the object is prepositional ""  #AUTHOR_TAG, as in sentences ( a ) and ( b ) in fig. 1. for simplicity, imagine that we measure dependency length in characters ( this may make sense in a reading task, for instance )', '. imagine that the center of a word of length λ is located at position ( λ + 1 ) / 2 and that the space between two words counts as one character. for simplicity again, let us assume that dependencies originate from the center of the word and that the length of a dependency is the difference between the positions of the centers of the words involved. the sum of dependency lengths of the sov sentence with a pronominal object ( fig', '. 1 ( b ) ) is in between that of the svo sentence with nominal object ( fig. 1 ( a ) ) and that of sov the sentence with nominal object ( fig. 1 ( c ) ).', 'this is due to the brevity of the pronoun', ', suggesting that pressure for online memory minimization reduces if short words are involved.', 'another factor that may influence pressure for dependency length minimization is sentence length : dependency length minimization has been argued to be less necessary in short sentences, where the maximization of the predictability of the verb might be the winning', 'principle ( ferrer - i -  #AUTHOR_TAG. sentence length and word length might', 'explain the tendency to adopt sov in romance languages when clitics are involved']",0
"['of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words']","['3 the unit of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however, it could be measured in other linguistic units : syllables, morphemes, phonemes, etc. a higher']","['of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however,']","['this should make their dependency more robust against interference or decay. lower values of g ( u, v', ', d ) may facilitate the involvement of other word order principles. · a n. 1 2 3 4 5 6 7 8 8.', '5 9 10 11 12 13 13. 5 14 15. 1 2 3 4 5 6 7 7. 5 8 9 10 11 11. 5 12 13. 3 the unit of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however, it could be measured in other linguistic units : syllables, morphemes, phonemes, etc. a higher level of precision might illuminate inconsistencies between the dominant order according to the criteria in  #AUTHOR_TAG and other orders that appear recurrent', '##ly in particular circumstances.  #AUTHOR_TAG points out the case of italian, which is classified as svo  #AUTHOR_TAG but allows pronouns taking the role of the object to appear before', 'the verb. this is a common phenomenon occurring in other romance languages', '( e. g., catalan or french ). let us consider the case of french, which is "" svo when the object is lexical, but sov when the object is prepositional ""  #AUTHOR_TAG, as in sentences ( a ) and ( b ) in fig. 1. for simplicity, imagine that we measure dependency length in characters ( this may make sense in a reading task, for instance )', '. imagine that the center of a word of length λ is located at position ( λ + 1 ) / 2 and that the space between two words counts as one character. for simplicity again, let us assume that dependencies originate from the center of the word and that the length of a dependency is the difference between the positions of the centers of the words involved. the sum of dependency lengths of the sov sentence with a pronominal object ( fig', '. 1 ( b ) ) is in between that of the svo sentence with nominal object ( fig. 1 ( a ) ) and that of sov the sentence with nominal object ( fig. 1 ( c ) ).', 'this is due to the brevity of the pronoun', ', suggesting that pressure for online memory minimization reduces if short words are involved.', 'another factor that may influence pressure for dependency length minimization is sentence length : dependency length minimization has been argued to be less necessary in short sentences, where the maximization of the predictability of the verb might be the winning', 'principle ( ferrer - i -  #AUTHOR_TAG. sentence length and word length might', 'explain the tendency to adopt sov in romance languages when clitics are involved']",0
"['of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words']","['3 the unit of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however, it could be measured in other linguistic units : syllables, morphemes, phonemes, etc. a higher']","['of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however,']","['this should make their dependency more robust against interference or decay. lower values of g ( u, v', ', d ) may facilitate the involvement of other word order principles. · a n. 1 2 3 4 5 6 7 8 8.', '5 9 10 11 12 13 13. 5 14 15. 1 2 3 4 5 6 7 7. 5 8 9 10 11 11. 5 12 13. 3 the unit of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however, it could be measured in other linguistic units : syllables, morphemes, phonemes, etc. a higher level of precision might illuminate inconsistencies between the dominant order according to the criteria in  #AUTHOR_TAG and other orders that appear recurrent', '##ly in particular circumstances.  #AUTHOR_TAG points out the case of italian, which is classified as svo  #AUTHOR_TAG but allows pronouns taking the role of the object to appear before', 'the verb. this is a common phenomenon occurring in other romance languages', '( e. g., catalan or french ). let us consider the case of french, which is "" svo when the object is lexical, but sov when the object is prepositional ""  #AUTHOR_TAG, as in sentences ( a ) and ( b ) in fig. 1. for simplicity, imagine that we measure dependency length in characters ( this may make sense in a reading task, for instance )', '. imagine that the center of a word of length λ is located at position ( λ + 1 ) / 2 and that the space between two words counts as one character. for simplicity again, let us assume that dependencies originate from the center of the word and that the length of a dependency is the difference between the positions of the centers of the words involved. the sum of dependency lengths of the sov sentence with a pronominal object ( fig', '. 1 ( b ) ) is in between that of the svo sentence with nominal object ( fig. 1 ( a ) ) and that of sov the sentence with nominal object ( fig. 1 ( c ) ).', 'this is due to the brevity of the pronoun', ', suggesting that pressure for online memory minimization reduces if short words are involved.', 'another factor that may influence pressure for dependency length minimization is sentence length : dependency length minimization has been argued to be less necessary in short sentences, where the maximization of the predictability of the verb might be the winning', 'principle ( ferrer - i -  #AUTHOR_TAG. sentence length and word length might', 'explain the tendency to adopt sov in romance languages when clitics are involved']",0
"['of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words']","['3 the unit of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however, it could be measured in other linguistic units : syllables, morphemes, phonemes, etc. a higher']","['of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however,']","['this should make their dependency more robust against interference or decay. lower values of g ( u, v', ', d ) may facilitate the involvement of other word order principles. · a n. 1 2 3 4 5 6 7 8 8.', '5 9 10 11 12 13 13. 5 14 15. 1 2 3 4 5 6 7 7. 5 8 9 10 11 11. 5 12 13. 3 the unit of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however, it could be measured in other linguistic units : syllables, morphemes, phonemes, etc. a higher level of precision might illuminate inconsistencies between the dominant order according to the criteria in  #AUTHOR_TAG and other orders that appear recurrent', '##ly in particular circumstances.  #AUTHOR_TAG points out the case of italian, which is classified as svo  #AUTHOR_TAG but allows pronouns taking the role of the object to appear before', 'the verb. this is a common phenomenon occurring in other romance languages', '( e. g., catalan or french ). let us consider the case of french, which is "" svo when the object is lexical, but sov when the object is prepositional ""  #AUTHOR_TAG, as in sentences ( a ) and ( b ) in fig. 1. for simplicity, imagine that we measure dependency length in characters ( this may make sense in a reading task, for instance )', '. imagine that the center of a word of length λ is located at position ( λ + 1 ) / 2 and that the space between two words counts as one character. for simplicity again, let us assume that dependencies originate from the center of the word and that the length of a dependency is the difference between the positions of the centers of the words involved. the sum of dependency lengths of the sov sentence with a pronominal object ( fig', '. 1 ( b ) ) is in between that of the svo sentence with nominal object ( fig. 1 ( a ) ) and that of sov the sentence with nominal object ( fig. 1 ( c ) ).', 'this is due to the brevity of the pronoun', ', suggesting that pressure for online memory minimization reduces if short words are involved.', 'another factor that may influence pressure for dependency length minimization is sentence length : dependency length minimization has been argued to be less necessary in short sentences, where the maximization of the predictability of the verb might be the winning', 'principle ( ferrer - i -  #AUTHOR_TAG. sentence length and word length might', 'explain the tendency to adopt sov in romance languages when clitics are involved']",0
"['of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words']","['3 the unit of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however, it could be measured in other linguistic units : syllables, morphemes, phonemes, etc. a higher']","['of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however,']","['this should make their dependency more robust against interference or decay. lower values of g ( u, v', ', d ) may facilitate the involvement of other word order principles. · a n. 1 2 3 4 5 6 7 8 8.', '5 9 10 11 12 13 13. 5 14 15. 1 2 3 4 5 6 7 7. 5 8 9 10 11 11. 5 12 13. 3 the unit of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however, it could be measured in other linguistic units : syllables, morphemes, phonemes, etc. a higher level of precision might illuminate inconsistencies between the dominant order according to the criteria in  #AUTHOR_TAG and other orders that appear recurrent', '##ly in particular circumstances.  #AUTHOR_TAG points out the case of italian, which is classified as svo  #AUTHOR_TAG but allows pronouns taking the role of the object to appear before', 'the verb. this is a common phenomenon occurring in other romance languages', '( e. g., catalan or french ). let us consider the case of french, which is "" svo when the object is lexical, but sov when the object is prepositional ""  #AUTHOR_TAG, as in sentences ( a ) and ( b ) in fig. 1. for simplicity, imagine that we measure dependency length in characters ( this may make sense in a reading task, for instance )', '. imagine that the center of a word of length λ is located at position ( λ + 1 ) / 2 and that the space between two words counts as one character. for simplicity again, let us assume that dependencies originate from the center of the word and that the length of a dependency is the difference between the positions of the centers of the words involved. the sum of dependency lengths of the sov sentence with a pronominal object ( fig', '. 1 ( b ) ) is in between that of the svo sentence with nominal object ( fig. 1 ( a ) ) and that of sov the sentence with nominal object ( fig. 1 ( c ) ).', 'this is due to the brevity of the pronoun', ', suggesting that pressure for online memory minimization reduces if short words are involved.', 'another factor that may influence pressure for dependency length minimization is sentence length : dependency length minimization has been argued to be less necessary in short sentences, where the maximization of the predictability of the verb might be the winning', 'principle ( ferrer - i -  #AUTHOR_TAG. sentence length and word length might', 'explain the tendency to adopt sov in romance languages when clitics are involved']",1
"['of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words']","['3 the unit of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however, it could be measured in other linguistic units : syllables, morphemes, phonemes, etc. a higher']","['of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however,']","['this should make their dependency more robust against interference or decay. lower values of g ( u, v', ', d ) may facilitate the involvement of other word order principles. · a n. 1 2 3 4 5 6 7 8 8.', '5 9 10 11 12 13 13. 5 14 15. 1 2 3 4 5 6 7 7. 5 8 9 10 11 11. 5 12 13. 3 the unit of measurement of dependency length a limitation of  #TAUTHOR_TAG', 'is that dependency length is measured in words. however, it could be measured in other linguistic units : syllables, morphemes, phonemes, etc. a higher level of precision might illuminate inconsistencies between the dominant order according to the criteria in  #AUTHOR_TAG and other orders that appear recurrent', '##ly in particular circumstances.  #AUTHOR_TAG points out the case of italian, which is classified as svo  #AUTHOR_TAG but allows pronouns taking the role of the object to appear before', 'the verb. this is a common phenomenon occurring in other romance languages', '( e. g., catalan or french ). let us consider the case of french, which is "" svo when the object is lexical, but sov when the object is prepositional ""  #AUTHOR_TAG, as in sentences ( a ) and ( b ) in fig. 1. for simplicity, imagine that we measure dependency length in characters ( this may make sense in a reading task, for instance )', '. imagine that the center of a word of length λ is located at position ( λ + 1 ) / 2 and that the space between two words counts as one character. for simplicity again, let us assume that dependencies originate from the center of the word and that the length of a dependency is the difference between the positions of the centers of the words involved. the sum of dependency lengths of the sov sentence with a pronominal object ( fig', '. 1 ( b ) ) is in between that of the svo sentence with nominal object ( fig. 1 ( a ) ) and that of sov the sentence with nominal object ( fig. 1 ( c ) ).', 'this is due to the brevity of the pronoun', ', suggesting that pressure for online memory minimization reduces if short words are involved.', 'another factor that may influence pressure for dependency length minimization is sentence length : dependency length minimization has been argued to be less necessary in short sentences, where the maximization of the predictability of the verb might be the winning', 'principle ( ferrer - i -  #AUTHOR_TAG. sentence length and word length might', 'explain the tendency to adopt sov in romance languages when clitics are involved']",1
['##1 scores on the squad dataset  #TAUTHOR_TAG'],['f1 scores on the squad dataset  #TAUTHOR_TAG'],['##1 scores on the squad dataset  #TAUTHOR_TAG'],"['', 'experimental results show that dcr achieves stateof - the - art exact match and f1 scores on the squad dataset  #TAUTHOR_TAG']",5
"['. both models improved significantly over the', 'method proposed by  #TAUTHOR_TAG. our proposed model, called']","['. both models improved significantly over the', 'method proposed by  #TAUTHOR_TAG. our proposed model, called']","['. both models improved significantly over the', 'method proposed by  #TAUTHOR_TAG. our proposed model, called']","[""the answer's two boundary indices and the other classifies each passage word into answer / notanswer. both models improved significantly over the"", 'method proposed by  #TAUTHOR_TAG. our proposed model, called dynamic chunk reader ( dcr ), not only significantly differs from both the above systems in the way that answer candidates are generated and ranked, but also shares merits with both works. first, our model uses deep networks to learn better representations for candidate answer chunks, instead of using fixed feature representations as in  #TAUTHOR_TAG. second, it represents answer candidates as chunks, as in  #TAUTHOR_TAG, instead of word - level representations ( wang and jiang 2016 ), to make the model aware of the subtle differences among candidates ( importantly, overlapping candidates ). the contributions of this paper are', 'three - fold. ( 1 ) we pro - we also propose several simple but', 'effective features to strengthen the attention mechanism, which fundamentally improves candidate ranking, with the by - product of higher exact boundary match accuracy. the experiments on the stanford question answering dataset ( squad )  #TAUTHOR_TAG, which contains a variety of human - generated factoid and non', '']",5
"['inference purpose, and the squad dataset  #TAUTHOR_TAG used in this paper.', 'to the']","['inference purpose, and the squad dataset  #TAUTHOR_TAG used in this paper.', 'to the']","['inference purpose, and the squad dataset  #TAUTHOR_TAG used in this paper.', 'to the']","['.', 'an answer candidate for the i - th training example is defined as c m, n i, a sub - sequence in p i, that spans from position m to n ( 1 ≤ m ≤ n ≤ | p i | ).', 'the ground truth answer a i could be included in the set of all candidates c i = { c m, n i | ∀m, n ∈ n +, subj ( m, n, p i ) and 1 ≤ m ≤ n ≤ | p i | }, where subj ( m, n, p i ) is the constraint put on the candidate chunk for p i, such as, "" c m, n i can have at most 10 tokens "", or "" c m, n i must have a pre - defined pos pattern "".', ""to evaluate a system's performance, its top answer to a question is matched against the corresponding gold standard answer ( s )."", 'remark : categories of rc tasks other simpler variants of the aforementioned rc task were explored in the past.', 'for example, quiz - style datasets ( e. g., mctest ( richardson, burges, and renshaw 2013 ), movieqa ( tapaswi et al. 2015 ) ) have multiple - choice questions with answer options.', 'cloze - style datesets hill et al. 2015 ; onishi et al. 2016 ), usually automatically generated, have factoid "" question "" s created by replacing the answer in a sentence from the text with blank.', 'for the answer selection task this paper focuses on, several datasets exist, e. g. trec - qa for factoid answer extraction from multiple given passages, babi ( weston, chopra, and bordes 2014 ) designed for inference purpose, and the squad dataset  #TAUTHOR_TAG used in this paper.', 'to the best of our knowledge, the squad dataset is the only one for both factoid and nonfactoid answer extraction with a question distribution more close to real - world applications']",5
['like in  #TAUTHOR_TAG'],['ranker like in  #TAUTHOR_TAG'],"['like in  #TAUTHOR_TAG.', 'as a result, this baseline can be viewed as a deep learning based counterpart of']",[' #TAUTHOR_TAG'],5
,,,,5
"['. both models improved significantly over the', 'method proposed by  #TAUTHOR_TAG. our proposed model, called']","['. both models improved significantly over the', 'method proposed by  #TAUTHOR_TAG. our proposed model, called']","['. both models improved significantly over the', 'method proposed by  #TAUTHOR_TAG. our proposed model, called']","[""the answer's two boundary indices and the other classifies each passage word into answer / notanswer. both models improved significantly over the"", 'method proposed by  #TAUTHOR_TAG. our proposed model, called dynamic chunk reader ( dcr ), not only significantly differs from both the above systems in the way that answer candidates are generated and ranked, but also shares merits with both works. first, our model uses deep networks to learn better representations for candidate answer chunks, instead of using fixed feature representations as in  #TAUTHOR_TAG. second, it represents answer candidates as chunks, as in  #TAUTHOR_TAG, instead of word - level representations ( wang and jiang 2016 ), to make the model aware of the subtle differences among candidates ( importantly, overlapping candidates ). the contributions of this paper are', 'three - fold. ( 1 ) we pro - we also propose several simple but', 'effective features to strengthen the attention mechanism, which fundamentally improves candidate ranking, with the by - product of higher exact boundary match accuracy. the experiments on the stanford question answering dataset ( squad )  #TAUTHOR_TAG, which contains a variety of human - generated factoid and non', '']",0
"['. both models improved significantly over the', 'method proposed by  #TAUTHOR_TAG. our proposed model, called']","['. both models improved significantly over the', 'method proposed by  #TAUTHOR_TAG. our proposed model, called']","['. both models improved significantly over the', 'method proposed by  #TAUTHOR_TAG. our proposed model, called']","[""the answer's two boundary indices and the other classifies each passage word into answer / notanswer. both models improved significantly over the"", 'method proposed by  #TAUTHOR_TAG. our proposed model, called dynamic chunk reader ( dcr ), not only significantly differs from both the above systems in the way that answer candidates are generated and ranked, but also shares merits with both works. first, our model uses deep networks to learn better representations for candidate answer chunks, instead of using fixed feature representations as in  #TAUTHOR_TAG. second, it represents answer candidates as chunks, as in  #TAUTHOR_TAG, instead of word - level representations ( wang and jiang 2016 ), to make the model aware of the subtle differences among candidates ( importantly, overlapping candidates ). the contributions of this paper are', 'three - fold. ( 1 ) we pro - we also propose several simple but', 'effective features to strengthen the attention mechanism, which fundamentally improves candidate ranking, with the by - product of higher exact boundary match accuracy. the experiments on the stanford question answering dataset ( squad )  #TAUTHOR_TAG, which contains a variety of human - generated factoid and non', '']",0
"['. both models improved significantly over the', 'method proposed by  #TAUTHOR_TAG. our proposed model, called']","['. both models improved significantly over the', 'method proposed by  #TAUTHOR_TAG. our proposed model, called']","['. both models improved significantly over the', 'method proposed by  #TAUTHOR_TAG. our proposed model, called']","[""the answer's two boundary indices and the other classifies each passage word into answer / notanswer. both models improved significantly over the"", 'method proposed by  #TAUTHOR_TAG. our proposed model, called dynamic chunk reader ( dcr ), not only significantly differs from both the above systems in the way that answer candidates are generated and ranked, but also shares merits with both works. first, our model uses deep networks to learn better representations for candidate answer chunks, instead of using fixed feature representations as in  #TAUTHOR_TAG. second, it represents answer candidates as chunks, as in  #TAUTHOR_TAG, instead of word - level representations ( wang and jiang 2016 ), to make the model aware of the subtle differences among candidates ( importantly, overlapping candidates ). the contributions of this paper are', 'three - fold. ( 1 ) we pro - we also propose several simple but', 'effective features to strengthen the attention mechanism, which fundamentally improves candidate ranking, with the by - product of higher exact boundary match accuracy. the experiments on the stanford question answering dataset ( squad )  #TAUTHOR_TAG, which contains a variety of human - generated factoid and non', '']",1
"['. both models improved significantly over the', 'method proposed by  #TAUTHOR_TAG. our proposed model, called']","['. both models improved significantly over the', 'method proposed by  #TAUTHOR_TAG. our proposed model, called']","['. both models improved significantly over the', 'method proposed by  #TAUTHOR_TAG. our proposed model, called']","[""the answer's two boundary indices and the other classifies each passage word into answer / notanswer. both models improved significantly over the"", 'method proposed by  #TAUTHOR_TAG. our proposed model, called dynamic chunk reader ( dcr ), not only significantly differs from both the above systems in the way that answer candidates are generated and ranked, but also shares merits with both works. first, our model uses deep networks to learn better representations for candidate answer chunks, instead of using fixed feature representations as in  #TAUTHOR_TAG. second, it represents answer candidates as chunks, as in  #TAUTHOR_TAG, instead of word - level representations ( wang and jiang 2016 ), to make the model aware of the subtle differences among candidates ( importantly, overlapping candidates ). the contributions of this paper are', 'three - fold. ( 1 ) we pro - we also propose several simple but', 'effective features to strengthen the attention mechanism, which fundamentally improves candidate ranking, with the by - product of higher exact boundary match accuracy. the experiments on the stanford question answering dataset ( squad )  #TAUTHOR_TAG, which contains a variety of human - generated factoid and non', '']",4
,,,,4
"['of hate speech detection  #TAUTHOR_TAG gamback and  #AUTHOR_TAG.', ' #AUTHOR_TAG']","['of hate speech detection  #TAUTHOR_TAG gamback and  #AUTHOR_TAG.', ' #AUTHOR_TAG']","['research work have been reported since 2010 in this research field of hate speech detection  #TAUTHOR_TAG gamback and  #AUTHOR_TAG.', ' #AUTHOR_TAG']","['research work have been reported since 2010 in this research field of hate speech detection  #TAUTHOR_TAG gamback and  #AUTHOR_TAG.', ' #AUTHOR_TAG &  #AUTHOR_TAG reviewed the approaches used for hate speech detection.', ' #AUTHOR_TAG used bag of words and bi - gram features with machine learning approach to classify the tweets as "" racist "" or "" nonracist "".', ' #AUTHOR_TAG developed a supervised algorithm for hateful and antagonistic content in twitter using voted ensemble meta - classifier.', ' #AUTHOR_TAG learnt distributed low - dimensional representations of social media comments using neural language models for hate speech detection.', ' #AUTHOR_TAG used n - gram ( bigram, unigram, and trigram ) features with tf - idf score along with crowd - sourced hate speech lexicon and employed several classifiers including logistic regression with l1 regularization to separate hate speech from other offensive languages.', ' #AUTHOR_TAG used n - grams, skip - grams and clustering - based word representations as features with ensemble classifier for hate speech detection.', 'el  #AUTHOR_TAG performed linguistic and psycholinguistic analysis to detect the hate speech is either "" directed "" towards a target, or "" generalized "" towards a group.', 'gamback and  #AUTHOR_TAG used deep learning using cnn models to detect the hate speech as "" racism "", "" sexism "", "" both "" and "" nonhate - speech "".', 'they used character 4 - grams, word vectors based on word2vec, randomly generated word vectors, and word vectors combined with character n - grams as features in their approach.', ' #AUTHOR_TAG used convolution - gru based deep neural network for detecting hate speech.', 'many research work have been carried out in aggression detection  #AUTHOR_TAG b ).', ' #AUTHOR_TAG &  #AUTHOR_TAG used lstm and cnn respectively to detect aggression in text.', ' #AUTHOR_TAG b ) presented the findings of the shared task on aggression identification which aims to detect different scales of aggression namely "" overtly aggressive "", "" covertly aggressive "", and "" non - aggressive "".', ' #AUTHOR_TAG used cnn, lstm and bi - lstm to detect the above scales of aggression.', ' #AUTHOR_TAG &  #AUTHOR_TAG presented the methodologies on abusive language identification using deep neural networks.', 'research on identifying offensive languages has been focused on non - english languages like german  #AUTHOR_TAG, hindi  #AUTHOR_TAG b ), hinglish : hindi - english  #AUTHOR_TAG, slovene ( fiser et al., 2017 ) and chinese  #AUTHOR_TAG.', ' #AUTHOR_TAG presented an overview of germeval shared task on the identification of offensive language that focused on classification of german tweets from twitter.', ' #AUTHOR_TAG b ) focused on the shared task to identify aggression on hindi']",0
['may be improved further by incorporating external datasets  #AUTHOR_TAG a ;'],['may be improved further by incorporating external datasets  #AUTHOR_TAG a ;'],"['', 'the performance may be improved further by incorporating external datasets  #AUTHOR_TAG a ;']","['', 'we have employed 2 layered bi - directional lstm with scaled luong and normed bahdanau attention mechanisms to build the model for all the three sub tasks.', 'the instances are vectorized using tf - idf score for traditional machine learning models with minimum count two.', 'the classifiers namely multinomial naive bayes and support vector machine with stochastic gradient descent optimizer were employed to build the models for sub tasks b and c. deep learning with scaled luong attention, deep learning with normed bahdanau attention, traditional machine learning with svm give better results for task a, task b and task c respectively.', 'our models outperform the base line for all the three tasks.', 'the performance may be improved further by incorporating external datasets  #AUTHOR_TAG a ;']",2
"['than las rescoring  #TAUTHOR_TAG in vs wer, and']","['than las rescoring  #TAUTHOR_TAG in vs wer, and']","['relatively better than las rescoring  #TAUTHOR_TAG in vs wer,']","['', 'information for decoding. note that the first - pass hypotheses are sequences of wordpieces [ 19 ] and are usually short in vs, and thus the encoding should have limited impact on latency. our experiments are', 'conducted using the same training data as in [ 20, 21 ], which is from multiple domains such as voice search, youtube, farfield and telephony', '. we first analyze the behavior of the deliberation model, including performance when attending to multiple rnn - t hypotheses, contribution of different attention, and resc', '##oring vs. beam search. we apply additional encoder ( ae ) layers and minimum wer ( mwer ) training [ 22 ] to further improve quality.', 'the results show that our mwer trained 8 - hypothesis deliberation model performs 11 % relatively better than las rescoring  #TAUTHOR_TAG in vs wer, and up to 15', '% for proper noun recognition. joint training further improves vs slightly ( 2 % ) but significantly for a proper noun test set : 9 %. as a result, our best deliber', '']",0
"[', similar to  #TAUTHOR_TAG 16 ].', '']","['[ 1 ], and a deliberation decoder, similar to  #TAUTHOR_TAG 16 ].', '']","[', and a deliberation decoder, similar to  #TAUTHOR_TAG 16 ].', '']","['shown in fig. 1, our deliberation network consists of three major components : a shared encoder, an rnn - t decoder [ 1 ], and a deliberation decoder, similar to  #TAUTHOR_TAG 16 ].', '']",3
['in  #TAUTHOR_TAG but without a pre - trained decoder'],"['with mwer loss.', 'the joint training is similar to "" deep finetuning "" in  #TAUTHOR_TAG but without a pre - trained decoder']",['in  #TAUTHOR_TAG but without a pre - trained decoder'],"['the deliberation decoder while fixing rnn - t parameters is not optimal since the model components are not jointly updated.', 'we propose to use a combined loss to train all modules jointly :', 'where lrnnt ( · ) is the rnn - t loss, and lce ( · ) the ce loss for the deliberation decoder.', 'θe, θ1, and θ2 denote the parameters of shared encoder, rnn - t decoder, and deliberation decoder, respectively.', 'note that a jointly trained model can be further trained with mwer loss.', 'the joint training is similar to "" deep finetuning "" in  #TAUTHOR_TAG but without a pre - trained decoder']",3
"['from scratch by jointly optimizing all components [ 16 ].', 'however, we find training a two - pass model from scratch tends to be unstable in practice  #TAUTHOR_TAG']","['from scratch by jointly optimizing all components [ 16 ].', 'however, we find training a two - pass model from scratch tends to be unstable in practice  #TAUTHOR_TAG']","['from scratch by jointly optimizing all components [ 16 ].', 'however, we find training a two - pass model from scratch tends to be unstable in practice  #TAUTHOR_TAG']","['deliberation model is typically trained from scratch by jointly optimizing all components [ 16 ].', 'however, we find training a two - pass model from scratch tends to be unstable in practice  #TAUTHOR_TAG']",5
"['deliberation decoder on yr in a teacher - forcing mode  #TAUTHOR_TAG.', 'note the difference from  #TAUTHOR_TAG when rescoring a hypothesis is that the deliberation network sees all candidate hypotheses.', 'we compare rescoring and beam search in sect.', '4']","['deliberation decoder on yr in a teacher - forcing mode  #TAUTHOR_TAG.', 'note the difference from  #TAUTHOR_TAG when rescoring a hypothesis is that the deliberation network sees all candidate hypotheses.', 'we compare rescoring and beam search in sect.', '4']","['the second beam search to generate y d.', 'we are also curious how rescoring performs given bidirectional encoding from yr.', 'in rescoring, we run the deliberation decoder on yr in a teacher - forcing mode  #TAUTHOR_TAG.', 'note the difference from  #TAUTHOR_TAG when rescoring a hypothesis is that the deliberation network sees all candidate hypotheses.', 'we compare rescoring and beam search in sect.', '4']","['decoding consists of two passes : 1 ) decode using the rnn - t model to obtain the first - pass sequence yr, and 2 ) attend to both yr and e, and perform the second beam search to generate y d.', 'we are also curious how rescoring performs given bidirectional encoding from yr.', 'in rescoring, we run the deliberation decoder on yr in a teacher - forcing mode  #TAUTHOR_TAG.', 'note the difference from  #TAUTHOR_TAG when rescoring a hypothesis is that the deliberation network sees all candidate hypotheses.', 'we compare rescoring and beam search in sect.', '4']",5
['las rescoring model  #TAUTHOR_TAG to'],['las rescoring model  #TAUTHOR_TAG to'],['las rescoring model  #TAUTHOR_TAG to'],"['the above analysis, an mwer trained 8 - hypothesis deliberation model with ae layers performs the best, and thus we use that for comparison below.', 'in table 4, we compare deliberation models with an rnn - t [ 6 ] and las rescoring model  #TAUTHOR_TAG to understand where the improvement comes from, in fig. 2 we show an example of deliberation attention distribution on the rnn - t hypotheses ( x - axis ) at every step of the second - pass decoding ( yaxis ).', '']",5
['las rescoring model  #TAUTHOR_TAG to'],['las rescoring model  #TAUTHOR_TAG to'],['las rescoring model  #TAUTHOR_TAG to'],"['the above analysis, an mwer trained 8 - hypothesis deliberation model with ae layers performs the best, and thus we use that for comparison below.', 'in table 4, we compare deliberation models with an rnn - t [ 6 ] and las rescoring model  #TAUTHOR_TAG to understand where the improvement comes from, in fig. 2 we show an example of deliberation attention distribution on the rnn - t hypotheses ( x - axis ) at every step of the second - pass decoding ( yaxis ).', '']",7
['of  #TAUTHOR_TAG in that we use a fixed set of templates to'],['of  #TAUTHOR_TAG in that we use a fixed set of templates to'],"['', 'we build on the paraphrasing approach of  #TAUTHOR_TAG in that we use a fixed set of templates to']","['', 'knowing this on its own is not enough to build an effective system however.', 'we still need to be able to somehow identify that it is this particular term in the query that is associated with this logical form.', 'in this paper we demonstrate one way that this can be achieved.', 'we build on the paraphrasing approach of  #TAUTHOR_TAG in that we use a fixed set of templates to generate a set of candidate logical forms to answer a given query and map each logical form to a natural language expression, its canonical utterance.', 'instead of using a complex paraphrasing model however, we use tensor kernels to find relationships between terms occuring in the query and in the canonical utterance.', 'the virtue of our approach is in its simplicity, which both aids implementation and speeds up execution']",3
"['of paraphrasing techniques  #TAUTHOR_TAG 1 ], information']","['of paraphrasing techniques  #TAUTHOR_TAG 1 ], information']","['of paraphrasing techniques  #TAUTHOR_TAG 1 ], information extraction [ 12 ], learning low dimensional embeddings of words and']","['', 'there are two datasets of queries for this database : free917 consisting of 917 questions annotated with logical forms [ 9 ], and webquestions which consists of 5, 810 question - answer pairs, with no logical forms [ 10 ].', 'approaches to this task include schema matching [ 9 ], inducing latent logical forms [ 10 ], application of paraphrasing techniques  #TAUTHOR_TAG 1 ], information extraction [ 12 ], learning low dimensional embeddings of words and knowledge base constituents [ 13 ] and application of logical reasoning in conjunction with statistical techniques [ 11 ].', 'note that most of these approaches do not require annotated logical forms, and either induce logical forms when training using the given answers, or bypass them altogether']",3
"['built our implementation on top of the parasempre system  #TAUTHOR_TAG, and so our evaluation exactly matches theirs.', 'our implementation is freely available online.', '1 we substituted the paraphr']","['built our implementation on top of the parasempre system  #TAUTHOR_TAG, and so our evaluation exactly matches theirs.', 'our implementation is freely available online.', '1 we substituted the paraphrase system']","['built our implementation on top of the parasempre system  #TAUTHOR_TAG, and so our evaluation exactly matches theirs.', 'our implementation is freely available online.', '1 we substituted the paraphrase system']","['built our implementation on top of the parasempre system  #TAUTHOR_TAG, and so our evaluation exactly matches theirs.', 'our implementation is freely available online.', '1 we substituted the paraphrase system of parasempre with our tensor kernel - based system ( i. e. we excluded features from both the association and vector space models ), but we included the parasempre features derived from logical forms.', 'to implement our tensor kernel of unigram features, we simply added all pairs of terms in the query and canonical utterance as features ; in preliminary experiments we found that this was fast enough and we did not need to use the kernel trick, which could potentially provide further speed - ups.', 'we did not implement any feature selection methods which may also help with efficiency.', 'for evaluation, we report the average of the f1 score measured on the set of entities returned by the logical form when evaluated on the database, when compared to the correct set of entities.', 'this allows, for example, to get a nonzero score for returning a similar set of entities to the correct one.', ""for example, if we return the set { jaxon bieber } as an answer to the query who is justin bieber's brother? we allow a nonzero score ( the correct answer according to the dataset is { jazmyn bieber, jaxon bieber } )""]",3
"['same, 63 %  #TAUTHOR_TAG.', 'this is']","['same, 63 %  #TAUTHOR_TAG.', 'this is']","['oracle f1 score is the same, 63 %  #TAUTHOR_TAG.', 'this is']","['are reported in table 1.', ""our system achieves an average f1 score of 40. 1 %, compared to parasempre's 39. 9 %."", 'our system runs faster however, due to the simpler method of generating features.', 'evaluating using parasempre on the development set took 22h31m ; using the tensor kernel took 14h44m on a comparable machine.', 'since we have adopted the logical form templates of parasempre, our upper bound or oracle f1 score is the same, 63 %  #TAUTHOR_TAG.', 'this is the score that would be obtained if we knew which was the best logical form out of all those generated.', '']",3
['of  #TAUTHOR_TAG in that we use a fixed set of templates to'],['of  #TAUTHOR_TAG in that we use a fixed set of templates to'],"['', 'we build on the paraphrasing approach of  #TAUTHOR_TAG in that we use a fixed set of templates to']","['', 'knowing this on its own is not enough to build an effective system however.', 'we still need to be able to somehow identify that it is this particular term in the query that is associated with this logical form.', 'in this paper we demonstrate one way that this can be achieved.', 'we build on the paraphrasing approach of  #TAUTHOR_TAG in that we use a fixed set of templates to generate a set of candidate logical forms to answer a given query and map each logical form to a natural language expression, its canonical utterance.', 'instead of using a complex paraphrasing model however, we use tensor kernels to find relationships between terms occuring in the query and in the canonical utterance.', 'the virtue of our approach is in its simplicity, which both aids implementation and speeds up execution']",5
"['of paraphrasing techniques  #TAUTHOR_TAG 1 ], information']","['of paraphrasing techniques  #TAUTHOR_TAG 1 ], information']","['of paraphrasing techniques  #TAUTHOR_TAG 1 ], information extraction [ 12 ], learning low dimensional embeddings of words and']","['', 'there are two datasets of queries for this database : free917 consisting of 917 questions annotated with logical forms [ 9 ], and webquestions which consists of 5, 810 question - answer pairs, with no logical forms [ 10 ].', 'approaches to this task include schema matching [ 9 ], inducing latent logical forms [ 10 ], application of paraphrasing techniques  #TAUTHOR_TAG 1 ], information extraction [ 12 ], learning low dimensional embeddings of words and knowledge base constituents [ 13 ] and application of logical reasoning in conjunction with statistical techniques [ 11 ].', 'note that most of these approaches do not require annotated logical forms, and either induce logical forms when training using the given answers, or bypass them altogether']",5
"['built our implementation on top of the parasempre system  #TAUTHOR_TAG, and so our evaluation exactly matches theirs.', 'our implementation is freely available online.', '1 we substituted the paraphr']","['built our implementation on top of the parasempre system  #TAUTHOR_TAG, and so our evaluation exactly matches theirs.', 'our implementation is freely available online.', '1 we substituted the paraphrase system']","['built our implementation on top of the parasempre system  #TAUTHOR_TAG, and so our evaluation exactly matches theirs.', 'our implementation is freely available online.', '1 we substituted the paraphrase system']","['built our implementation on top of the parasempre system  #TAUTHOR_TAG, and so our evaluation exactly matches theirs.', 'our implementation is freely available online.', '1 we substituted the paraphrase system of parasempre with our tensor kernel - based system ( i. e. we excluded features from both the association and vector space models ), but we included the parasempre features derived from logical forms.', 'to implement our tensor kernel of unigram features, we simply added all pairs of terms in the query and canonical utterance as features ; in preliminary experiments we found that this was fast enough and we did not need to use the kernel trick, which could potentially provide further speed - ups.', 'we did not implement any feature selection methods which may also help with efficiency.', 'for evaluation, we report the average of the f1 score measured on the set of entities returned by the logical form when evaluated on the database, when compared to the correct set of entities.', 'this allows, for example, to get a nonzero score for returning a similar set of entities to the correct one.', ""for example, if we return the set { jaxon bieber } as an answer to the query who is justin bieber's brother? we allow a nonzero score ( the correct answer according to the dataset is { jazmyn bieber, jaxon bieber } )""]",5
"['same, 63 %  #TAUTHOR_TAG.', 'this is']","['same, 63 %  #TAUTHOR_TAG.', 'this is']","['oracle f1 score is the same, 63 %  #TAUTHOR_TAG.', 'this is']","['are reported in table 1.', ""our system achieves an average f1 score of 40. 1 %, compared to parasempre's 39. 9 %."", 'our system runs faster however, due to the simpler method of generating features.', 'evaluating using parasempre on the development set took 22h31m ; using the tensor kernel took 14h44m on a comparable machine.', 'since we have adopted the logical form templates of parasempre, our upper bound or oracle f1 score is the same, 63 %  #TAUTHOR_TAG.', 'this is the score that would be obtained if we knew which was the best logical form out of all those generated.', '']",5
['parasempre system of  #TAUTHOR_TAG is based on'],['parasempre system of  #TAUTHOR_TAG is based on'],['parasempre system of  #TAUTHOR_TAG is based on the idea of generating a set of candidate logical forms from'],"['parasempre system of  #TAUTHOR_TAG is based on the idea of generating a set of candidate logical forms from the query using a set of templates.', 'for example, the query who did brad pitt play in troy? would generate the logical form as well as many incorrect logical forms.', '']",0
"['logical form given a sentence  #TAUTHOR_TAG.', 'in order to ues this approach, we need a single feature']","['logical form given a sentence  #TAUTHOR_TAG.', 'in order to ues this approach, we need a single feature']","['to choose the best logical form given a sentence  #TAUTHOR_TAG.', 'in order to ues this approach, we need a single feature vector for each pair of']","['know that simple patterns or occurrences in the query can be used to identify a correct logical form with high probability, as with the "" currency "" example.', 'we still need some way of identifying these patterns and linking them up to appropriate logical forms.', 'in this section we discuss one approach for doing this.', 'our goal is to learn a mapping from queries to logical forms.', 'one way of doing this to consider a fixed number of logical forms for each query sentence, and train a classifier to choose the best logical form given a sentence  #TAUTHOR_TAG.', 'in order to ues this approach, we need a single feature vector for each pair of queries and logical forms.', 'our proposal is to extract features for each query and logical form indepdendently, and to take their tensor product as the combined vector.', 'explicitly, let q be the set of all possible queries and λ be the set of all possible logical forms.', '']",1
['7 parasempre  #TAUTHOR_TAG'],['parasempre  #TAUTHOR_TAG'],['7 parasempre  #TAUTHOR_TAG'],"['f1 score sempre [ 10 ] 35. 7 parasempre  #TAUTHOR_TAG 39. 9 facebook [ 13 ] 41. 8 deepqa [ 11 ] 45. 3 tensor kernel with unigrams 40. 1 in development, we found that ordering the training alphabetically by the text of the query lead to a large reduction in accuracy.', '2 ordering alphabetically when performing the split for cross validation ( instead of random ordering ) means that a lot of queries on the same topic are grouped together, increasing the likelihood that a query on a topic seen at test time would not have been seen at training time.', 'this validates our hypothesis that simple techniques work well because of the homogeneous nature of the dataset.', 'we would argue that this does not invalidate the techniques however, as it is likely that real - world datasets also have this property.', 'it is a feature of our tensor product model that there is no direct interaction between the features from the query and those from the logical form.', 'this is evidenced by the fact that the system has to learn that the term currency in the query maps to currency in the canonical utterance.', 'this hints at ways of improving over our current system.', 'more interestingly, it also means that we are currently making very light use of the canonical utterance generation ; in the canonical utterance, currency could be replaced by any symbol and our system would learn the same relationship.', 'this points at another route of investigation involving generating features for use in the tensor kernel directly from the logical form instead of via canonical utterances']",4
['7 parasempre  #TAUTHOR_TAG'],['parasempre  #TAUTHOR_TAG'],['7 parasempre  #TAUTHOR_TAG'],"['f1 score sempre [ 10 ] 35. 7 parasempre  #TAUTHOR_TAG 39. 9 facebook [ 13 ] 41. 8 deepqa [ 11 ] 45. 3 tensor kernel with unigrams 40. 1 in development, we found that ordering the training alphabetically by the text of the query lead to a large reduction in accuracy.', '2 ordering alphabetically when performing the split for cross validation ( instead of random ordering ) means that a lot of queries on the same topic are grouped together, increasing the likelihood that a query on a topic seen at test time would not have been seen at training time.', 'this validates our hypothesis that simple techniques work well because of the homogeneous nature of the dataset.', 'we would argue that this does not invalidate the techniques however, as it is likely that real - world datasets also have this property.', 'it is a feature of our tensor product model that there is no direct interaction between the features from the query and those from the logical form.', 'this is evidenced by the fact that the system has to learn that the term currency in the query maps to currency in the canonical utterance.', 'this hints at ways of improving over our current system.', 'more interestingly, it also means that we are currently making very light use of the canonical utterance generation ; in the canonical utterance, currency could be replaced by any symbol and our system would learn the same relationship.', 'this points at another route of investigation involving generating features for use in the tensor kernel directly from the logical form instead of via canonical utterances']",6
"['of homosexuality  #TAUTHOR_TAG.', 'neural word embeddings  #AUTHOR_TAG are probably']","['of homosexuality  #TAUTHOR_TAG.', 'neural word embeddings  #AUTHOR_TAG are probably']","[""the lexical item'gay'with the meaning dimension of homosexuality  #TAUTHOR_TAG."", 'neural word embeddings  #AUTHOR_TAG are probably']","['methods applied to large - sized, often temporally stratified corpora have markedly enhanced the methodological repertoire of both synchronic and diachronic computational linguistics and are getting more and more popular in the digital humanities ( see section 2. 2 ).', 'however, using such quantitative data as a basis for qualitative, empirically - grounded theories requires that measurements should not only be accurate, but also reliable.', 'only under such a guarantee, quantitative data can be assembled from different experiments as a foundation for trustful theories.', 'measuring word similarity by word neighborhoods in embedding space can be used to detect diachronic shifts or domain specific usage, by training word embeddings on suited corpora and comparing these representations.', 'additionally, lexical items near in the embedding space to the lexical item under scrutiny can be considered as approximating its meaning at a given point in time or in a specific domain.', ""these two lines of research converge in prior work to show, e. g., the increasing association of the lexical item'gay'with the meaning dimension of homosexuality  #TAUTHOR_TAG."", 'neural word embeddings  #AUTHOR_TAG are probably the most influential among all embedding types ( see section 2. 1 ).', 'yet, we gathered evidence that the inherent randomness involved in their generation affects the reliability of word neighborhood judgments and demonstrate how this hampers qualitative conclusions based on such models.', 'our investigation was performed on both historical ( for the time span of 1900 to 1904 ) and contemporary texts ( for the time span of 2005 to 2009 ) in two languages, english and german.', 'it is thus a continuation of prior work, in which we investigated historical english texts only  #AUTHOR_TAG a ), and also influenced by the design decisions of  #TAUTHOR_TAG and  #AUTHOR_TAG which were the first to use word embeddings in diachronic studies.', 'our results cast doubt on the reproducibility of such experiments where neighborhoods between words in embedding space are taken as a computationally valid indicator for properly capturing lexical meaning ( and, consequently, meaning shifts ).', 'linguistics.', 'the word2vec family of algorithms, developed from heavily trimmed artificial neural networks, is a widely used and robust way to generate such embeddings  #AUTHOR_TAG.', 'its skip - gram variant predicts plausible contexts for a given word, whereas the alternative continuous bag - of - words variant tries to predict words from contexts ; we focus on the former as it is generally reported to be superior ( see e. g.,  #AUTHOR_TAG ).', 'there are two strategies for managing the huge number of potential contexts a word can appear in.', 'skip - gram hierarchical softmax ( sghs ) uses a binary tree to more efficiently represent the vocabulary,']",0
"['predecessor  #TAUTHOR_TAG b ), or a mapping between models for different points in time']","['predecessor  #TAUTHOR_TAG b ), or a mapping between models for different points in time']","['predecessor  #TAUTHOR_TAG b ), or a mapping between models for different points in time must be calculated  #AUTHOR_TAG.', 'the first approach cannot be performed in parallel and is thus rather time - consuming, if texts are not subsampled.', 'we nevertheless discourage using samples instead of full corpora,']","['embeddings can be used rather directly for tracking semantic changes, namely by measuring the similarity of word representations generated for one word at different points in time - words which underwent semantic shifts will be dissimilar with themselves.', 'these models must either be trained in a continuous manner where the model for each time span is initialized with its predecessor  #TAUTHOR_TAG b ), or a mapping between models for different points in time must be calculated  #AUTHOR_TAG.', 'the first approach cannot be performed in parallel and is thus rather time - consuming, if texts are not subsampled.', 'we nevertheless discourage using samples instead of full corpora, as we observed extremely low reliability values between different samples  #AUTHOR_TAG a ).', 'word embeddings can also be used in diachronic studies without any kind of mapping to track clusters of similar words over time and, thus, model the evolution of topics  #AUTHOR_TAG or compare neighborhoods in embedding space for preselected words  #AUTHOR_TAG.', 'besides temporal variations, word embeddings can also used to analyze geographic ones, e. g., the distinction between us american and british english variants  #AUTHOR_TAG.', 'most of these studies were performed with algorithms from the word2vec family, respectively glove in  #AUTHOR_TAG, and are thus likely to be affected by the same systematic reliability problems on which we focus here.', ' #AUTHOR_TAG used svd ppmi in some of their very recent experiments and showed it to be adequate for exploring historical semantics.', 'the google books ngram corpus ( gbn ;  #AUTHOR_TAG,  #AUTHOR_TAG is used in most of the studies we already mentioned, including our current study and its predecessor  #AUTHOR_TAG a ).', 'it contains about 6 % of all books published between 1500 and 2009 in the form of n - grams ( up to pentagrams ), together with their frequency for each year.', 'this corpus has often been criticized for its opaque sampling strategy, as its constituent books remain unknown and can be shown to form an unbalanced collection  #AUTHOR_TAG.', 'gbn is multilingual, with its english part being subdivided into regional segments ( british, us ) and topic categories ( general language and fiction texts ).', 'diachronic research focuses on the english fiction part, with the exception of some work relating to german data  #AUTHOR_TAG b )']",0
"['of homosexuality  #TAUTHOR_TAG.', 'neural word embeddings  #AUTHOR_TAG are probably']","['of homosexuality  #TAUTHOR_TAG.', 'neural word embeddings  #AUTHOR_TAG are probably']","[""the lexical item'gay'with the meaning dimension of homosexuality  #TAUTHOR_TAG."", 'neural word embeddings  #AUTHOR_TAG are probably']","['methods applied to large - sized, often temporally stratified corpora have markedly enhanced the methodological repertoire of both synchronic and diachronic computational linguistics and are getting more and more popular in the digital humanities ( see section 2. 2 ).', 'however, using such quantitative data as a basis for qualitative, empirically - grounded theories requires that measurements should not only be accurate, but also reliable.', 'only under such a guarantee, quantitative data can be assembled from different experiments as a foundation for trustful theories.', 'measuring word similarity by word neighborhoods in embedding space can be used to detect diachronic shifts or domain specific usage, by training word embeddings on suited corpora and comparing these representations.', 'additionally, lexical items near in the embedding space to the lexical item under scrutiny can be considered as approximating its meaning at a given point in time or in a specific domain.', ""these two lines of research converge in prior work to show, e. g., the increasing association of the lexical item'gay'with the meaning dimension of homosexuality  #TAUTHOR_TAG."", 'neural word embeddings  #AUTHOR_TAG are probably the most influential among all embedding types ( see section 2. 1 ).', 'yet, we gathered evidence that the inherent randomness involved in their generation affects the reliability of word neighborhood judgments and demonstrate how this hampers qualitative conclusions based on such models.', 'our investigation was performed on both historical ( for the time span of 1900 to 1904 ) and contemporary texts ( for the time span of 2005 to 2009 ) in two languages, english and german.', 'it is thus a continuation of prior work, in which we investigated historical english texts only  #AUTHOR_TAG a ), and also influenced by the design decisions of  #TAUTHOR_TAG and  #AUTHOR_TAG which were the first to use word embeddings in diachronic studies.', 'our results cast doubt on the reproducibility of such experiments where neighborhoods between words in embedding space are taken as a computationally valid indicator for properly capturing lexical meaning ( and, consequently, meaning shifts ).', 'linguistics.', 'the word2vec family of algorithms, developed from heavily trimmed artificial neural networks, is a widely used and robust way to generate such embeddings  #AUTHOR_TAG.', 'its skip - gram variant predicts plausible contexts for a given word, whereas the alternative continuous bag - of - words variant tries to predict words from contexts ; we focus on the former as it is generally reported to be superior ( see e. g.,  #AUTHOR_TAG ).', 'there are two strategies for managing the huge number of potential contexts a word can appear in.', 'skip - gram hierarchical softmax ( sghs ) uses a binary tree to more efficiently represent the vocabulary,']",5
"['by us and  #TAUTHOR_TAG. svd ppmi, which are conceptually not bothered by the reliability problems we discussed here, were not a good fit for the hyperparameters we adopted from  #AUTHOR_TAG.  #AUTHOR_TAG reports similarity']","['by us and  #TAUTHOR_TAG. svd ppmi, which are conceptually not bothered by the reliability problems we discussed here, were not a good fit for the hyperparameters we adopted from  #AUTHOR_TAG.  #AUTHOR_TAG reports similarity']","['training', 'epoch, as was done by us and  #TAUTHOR_TAG. svd ppmi, which are conceptually not bothered by the reliability problems we discussed here, were not a good fit for the hyperparameters we adopted from  #AUTHOR_TAG.  #AUTHOR_TAG reports similarity']","['', 'e., c = 0', "". 9999, was never reached ( this observation might be explained by kulkarni et al.'s decision not to reset the learning rate for each training"", 'epoch, as was done by us and  #TAUTHOR_TAG. svd ppmi, which are conceptually not bothered by the reliability problems we discussed here, were not a good fit for the hyperparameters we adopted from  #AUTHOR_TAG.  #AUTHOR_TAG reports similarity accuracy superior to sgns, whereas for our set - up results in pretests were about 10 percent points', 'worse than skip - gram embeddings, e. g., only 0. 35 for 1900 - 1904 english fiction. finally, to want', 'to illustrate how this reliability problem affects qualitative conclusions. in table 3 we provide some examples in which three negative', 'sampling models for 1900', ""- 1904 english fiction did not agree on the closest neighbor for words in question ( mostly drawn from the list in footnote 9 ). the most inconsistent word neighborhoods are provided for'romantic'which is connected"", ""to'lazzaroni ', 12'fanciful'and'melancholies '. this"", ""holds despite the high frequency ( 94th percentile ) and moderate ambiguity ( 5 synsets ) of the target item'romantic '"", '']",5
"['predecessor  #TAUTHOR_TAG b ), or a mapping between models for different points in time']","['predecessor  #TAUTHOR_TAG b ), or a mapping between models for different points in time']","['predecessor  #TAUTHOR_TAG b ), or a mapping between models for different points in time must be calculated  #AUTHOR_TAG.', 'the first approach cannot be performed in parallel and is thus rather time - consuming, if texts are not subsampled.', 'we nevertheless discourage using samples instead of full corpora,']","['embeddings can be used rather directly for tracking semantic changes, namely by measuring the similarity of word representations generated for one word at different points in time - words which underwent semantic shifts will be dissimilar with themselves.', 'these models must either be trained in a continuous manner where the model for each time span is initialized with its predecessor  #TAUTHOR_TAG b ), or a mapping between models for different points in time must be calculated  #AUTHOR_TAG.', 'the first approach cannot be performed in parallel and is thus rather time - consuming, if texts are not subsampled.', 'we nevertheless discourage using samples instead of full corpora, as we observed extremely low reliability values between different samples  #AUTHOR_TAG a ).', 'word embeddings can also be used in diachronic studies without any kind of mapping to track clusters of similar words over time and, thus, model the evolution of topics  #AUTHOR_TAG or compare neighborhoods in embedding space for preselected words  #AUTHOR_TAG.', 'besides temporal variations, word embeddings can also used to analyze geographic ones, e. g., the distinction between us american and british english variants  #AUTHOR_TAG.', 'most of these studies were performed with algorithms from the word2vec family, respectively glove in  #AUTHOR_TAG, and are thus likely to be affected by the same systematic reliability problems on which we focus here.', ' #AUTHOR_TAG used svd ppmi in some of their very recent experiments and showed it to be adequate for exploring historical semantics.', 'the google books ngram corpus ( gbn ;  #AUTHOR_TAG,  #AUTHOR_TAG is used in most of the studies we already mentioned, including our current study and its predecessor  #AUTHOR_TAG a ).', 'it contains about 6 % of all books published between 1500 and 2009 in the form of n - grams ( up to pentagrams ), together with their frequency for each year.', 'this corpus has often been criticized for its opaque sampling strategy, as its constituent books remain unknown and can be shown to form an unbalanced collection  #AUTHOR_TAG.', 'gbn is multilingual, with its english part being subdivided into regional segments ( british, us ) and topic categories ( general language and fiction texts ).', 'diachronic research focuses on the english fiction part, with the exception of some work relating to german data  #AUTHOR_TAG b )']",1
"['by us and  #TAUTHOR_TAG. svd ppmi, which are conceptually not bothered by the reliability problems we discussed here, were not a good fit for the hyperparameters we adopted from  #AUTHOR_TAG.  #AUTHOR_TAG reports similarity']","['by us and  #TAUTHOR_TAG. svd ppmi, which are conceptually not bothered by the reliability problems we discussed here, were not a good fit for the hyperparameters we adopted from  #AUTHOR_TAG.  #AUTHOR_TAG reports similarity']","['training', 'epoch, as was done by us and  #TAUTHOR_TAG. svd ppmi, which are conceptually not bothered by the reliability problems we discussed here, were not a good fit for the hyperparameters we adopted from  #AUTHOR_TAG.  #AUTHOR_TAG reports similarity']","['', 'e., c = 0', "". 9999, was never reached ( this observation might be explained by kulkarni et al.'s decision not to reset the learning rate for each training"", 'epoch, as was done by us and  #TAUTHOR_TAG. svd ppmi, which are conceptually not bothered by the reliability problems we discussed here, were not a good fit for the hyperparameters we adopted from  #AUTHOR_TAG.  #AUTHOR_TAG reports similarity accuracy superior to sgns, whereas for our set - up results in pretests were about 10 percent points', 'worse than skip - gram embeddings, e. g., only 0. 35 for 1900 - 1904 english fiction. finally, to want', 'to illustrate how this reliability problem affects qualitative conclusions. in table 3 we provide some examples in which three negative', 'sampling models for 1900', ""- 1904 english fiction did not agree on the closest neighbor for words in question ( mostly drawn from the list in footnote 9 ). the most inconsistent word neighborhoods are provided for'romantic'which is connected"", ""to'lazzaroni ', 12'fanciful'and'melancholies '. this"", ""holds despite the high frequency ( 94th percentile ) and moderate ambiguity ( 5 synsets ) of the target item'romantic '"", '']",3
[' #TAUTHOR_TAG presents a comparative style'],"[' #TAUTHOR_TAG presents a comparative style analysis of hyperpartisan news, evaluating features']",[' #TAUTHOR_TAG presents a comparative style'],"['"" is the concept of bias [ 2 ], which refers to the presentation of information according to the', 'standpoints or interests of the journalists and the news agencies. detecting bias is very important to help users to acquire balanced information', '. moreover, how a piece of information is reported has the capacity to evoke different sentiments in the audience,', 'which may have large social implications ( especially in very controversial topics such as terror attacks and religion issues ). in', 'this paper, we approach this very broad topic by focusing on the problem of detecting hyperpartisan news, namely news written with an extreme manipulation of the reality on the basis of', 'an underlying, typically extreme, ideology. this problem has received little attention in the context of the automatic detection of fake news, despite the potential correlation between them. seminal work from  #TAUTHOR_TAG presents a comparative style analysis of hyperpartisan news, evaluating features such as characters n - grams, stop words, part - of - speech, readability scores, and ratios of quoted words and external', 'links. the results indicate that a topic - based model outperforms a style - based one to separate the left, right', 'and mainstream orientations. we build upon  #TAUTHOR_TAG : this way we can investigate hyperpartisan - biased news ( i. e., extremely one - sided ) that have been manually', 'fact - checked by professional journalists from buzzfeed. the articles originated from 9 well - known political publishers, three each from the mainstream, the hyperpartisan left - wing, and the hyperpartisan right - wing. to detect hyperpartisanship, we apply a masking technique that transforms the original texts in a form', 'where the textual structure is maintained, while letting the learning algorithm focus on the writing style or the topic - related information. this technique makes it possible for us to corroborate previous results that content matters more than style. however, perhaps', 'surprisingly, we are able to achieve the overall best performance by simply', 'using higher - length n - grams than those used in the original work from  #TAUTHOR_TAG : this seems to indicate a strong lexical overlap between different sources with the same orientation, which, in turn, calls for more challenging datasets', 'and task formulations to encourage the development of models covering more subtle, i. e., implicit, forms of bias. the rest of the paper is structured as follows. in section 2 we describe our method to hyperparti', '##san news detection based on masking. section 3 presents details on the dataset, experimental results and a discussion of our results. finally', '']",0
[' #TAUTHOR_TAG presents a comparative style'],"[' #TAUTHOR_TAG presents a comparative style analysis of hyperpartisan news, evaluating features']",[' #TAUTHOR_TAG presents a comparative style'],"['"" is the concept of bias [ 2 ], which refers to the presentation of information according to the', 'standpoints or interests of the journalists and the news agencies. detecting bias is very important to help users to acquire balanced information', '. moreover, how a piece of information is reported has the capacity to evoke different sentiments in the audience,', 'which may have large social implications ( especially in very controversial topics such as terror attacks and religion issues ). in', 'this paper, we approach this very broad topic by focusing on the problem of detecting hyperpartisan news, namely news written with an extreme manipulation of the reality on the basis of', 'an underlying, typically extreme, ideology. this problem has received little attention in the context of the automatic detection of fake news, despite the potential correlation between them. seminal work from  #TAUTHOR_TAG presents a comparative style analysis of hyperpartisan news, evaluating features such as characters n - grams, stop words, part - of - speech, readability scores, and ratios of quoted words and external', 'links. the results indicate that a topic - based model outperforms a style - based one to separate the left, right', 'and mainstream orientations. we build upon  #TAUTHOR_TAG : this way we can investigate hyperpartisan - biased news ( i. e., extremely one - sided ) that have been manually', 'fact - checked by professional journalists from buzzfeed. the articles originated from 9 well - known political publishers, three each from the mainstream, the hyperpartisan left - wing, and the hyperpartisan right - wing. to detect hyperpartisanship, we apply a masking technique that transforms the original texts in a form', 'where the textual structure is maintained, while letting the learning algorithm focus on the writing style or the topic - related information. this technique makes it possible for us to corroborate previous results that content matters more than style. however, perhaps', 'surprisingly, we are able to achieve the overall best performance by simply', 'using higher - length n - grams than those used in the original work from  #TAUTHOR_TAG : this seems to indicate a strong lexical overlap between different sources with the same orientation, which, in turn, calls for more challenging datasets', 'and task formulations to encourage the development of models covering more subtle, i. e., implicit, forms of bias. the rest of the paper is structured as follows. in section 2 we describe our method to hyperparti', '##san news detection based on masking. section 3 presents details on the dataset, experimental results and a discussion of our results. finally', '']",5
['used the  #TAUTHOR_TAG whose articles were labeled with'],['used the  #TAUTHOR_TAG whose articles were labeled with'],['used the  #TAUTHOR_TAG whose articles were labeled with'],"['used the  #TAUTHOR_TAG whose articles were labeled with respect to three political orientations : mainstream, left - wing, and right - wing ( see table 2 ).', 'each article was taken from one of 9 publishers known as hyperpartisan left / right or mainstream in a period close to the us presidential elections of 2016.', 'therefore, the content of all the articles is related to the same topic.', ""during initial data analysis and prototyping we identified a variety of issues with the  #TAUTHOR_TAG we cleaned the data excluding articles with empty or bogus texts, e. g.'the document has moved here'( 23 and 14 articles respectively )."", 'additionally, we removed duplicates ( 33 ) and files with the same text but inconsistent labels ( 2 ).', 'as a result, we obtained a new dataset with 1555 articles out of 1627.', '4 following the settings of  #TAUTHOR_TAG, we balance the training set using random duplicate oversampling.', '4 the dataset is available at https : / / github. com / jjsjunquera / unmaskingbiasinnews']",5
['used the  #TAUTHOR_TAG whose articles were labeled with'],['used the  #TAUTHOR_TAG whose articles were labeled with'],['used the  #TAUTHOR_TAG whose articles were labeled with'],"['used the  #TAUTHOR_TAG whose articles were labeled with respect to three political orientations : mainstream, left - wing, and right - wing ( see table 2 ).', 'each article was taken from one of 9 publishers known as hyperpartisan left / right or mainstream in a period close to the us presidential elections of 2016.', 'therefore, the content of all the articles is related to the same topic.', ""during initial data analysis and prototyping we identified a variety of issues with the  #TAUTHOR_TAG we cleaned the data excluding articles with empty or bogus texts, e. g.'the document has moved here'( 23 and 14 articles respectively )."", 'additionally, we removed duplicates ( 33 ) and files with the same text but inconsistent labels ( 2 ).', 'as a result, we obtained a new dataset with 1555 articles out of 1627.', '4 following the settings of  #TAUTHOR_TAG, we balance the training set using random duplicate oversampling.', '4 the dataset is available at https : / / github. com / jjsjunquera / unmaskingbiasinnews']",5
"['configuration used in  #TAUTHOR_TAG.', 'therefore,']","['configuration used in  #TAUTHOR_TAG.', 'therefore,']","['with the same configuration used in  #TAUTHOR_TAG.', 'therefore,']","['', 'with that intention we selected the k most frequent words from the target language, and then we transformed the texts by masking the occurrences of the rest of the words.', 'in another setting, we masked style - related information to allow the system to focus only on the topic - related differences between the orientations.', 'we call this a topic - based model.', 'for this, we masked the k most frequent words and maintained intact the rest.', 'after the text transformation by the masking process in both the training and test sets, we represented the documents with character n - grams and compared the results obtained with the style - based and the topic - related models.', 'machine ( svm ) and random forest ( rf ) ; for the three classifiers we used the versions implemented in sklearn with the parameters set by default.', 'evaluation : we performed 3 - fold cross - validation with the same configuration used in  #TAUTHOR_TAG.', ""therefore, each fold comprised one publisher from each orientation ( the classifiers did not learn a publisher's style )."", '']",5
"['configuration used in  #TAUTHOR_TAG.', 'therefore,']","['configuration used in  #TAUTHOR_TAG.', 'therefore,']","['with the same configuration used in  #TAUTHOR_TAG.', 'therefore,']","['', 'with that intention we selected the k most frequent words from the target language, and then we transformed the texts by masking the occurrences of the rest of the words.', 'in another setting, we masked style - related information to allow the system to focus only on the topic - related differences between the orientations.', 'we call this a topic - based model.', 'for this, we masked the k most frequent words and maintained intact the rest.', 'after the text transformation by the masking process in both the training and test sets, we represented the documents with character n - grams and compared the results obtained with the style - based and the topic - related models.', 'machine ( svm ) and random forest ( rf ) ; for the three classifiers we used the versions implemented in sklearn with the parameters set by default.', 'evaluation : we performed 3 - fold cross - validation with the same configuration used in  #TAUTHOR_TAG.', ""therefore, each fold comprised one publisher from each orientation ( the classifiers did not learn a publisher's style )."", '']",5
"['configuration used in  #TAUTHOR_TAG.', 'therefore,']","['configuration used in  #TAUTHOR_TAG.', 'therefore,']","['with the same configuration used in  #TAUTHOR_TAG.', 'therefore,']","['', 'with that intention we selected the k most frequent words from the target language, and then we transformed the texts by masking the occurrences of the rest of the words.', 'in another setting, we masked style - related information to allow the system to focus only on the topic - related differences between the orientations.', 'we call this a topic - based model.', 'for this, we masked the k most frequent words and maintained intact the rest.', 'after the text transformation by the masking process in both the training and test sets, we represented the documents with character n - grams and compared the results obtained with the style - based and the topic - related models.', 'machine ( svm ) and random forest ( rf ) ; for the three classifiers we used the versions implemented in sklearn with the parameters set by default.', 'evaluation : we performed 3 - fold cross - validation with the same configuration used in  #TAUTHOR_TAG.', ""therefore, each fold comprised one publisher from each orientation ( the classifiers did not learn a publisher's style )."", '']",5
"['configuration used in  #TAUTHOR_TAG.', 'therefore,']","['configuration used in  #TAUTHOR_TAG.', 'therefore,']","['with the same configuration used in  #TAUTHOR_TAG.', 'therefore,']","['', 'with that intention we selected the k most frequent words from the target language, and then we transformed the texts by masking the occurrences of the rest of the words.', 'in another setting, we masked style - related information to allow the system to focus only on the topic - related differences between the orientations.', 'we call this a topic - based model.', 'for this, we masked the k most frequent words and maintained intact the rest.', 'after the text transformation by the masking process in both the training and test sets, we represented the documents with character n - grams and compared the results obtained with the style - based and the topic - related models.', 'machine ( svm ) and random forest ( rf ) ; for the three classifiers we used the versions implemented in sklearn with the parameters set by default.', 'evaluation : we performed 3 - fold cross - validation with the same configuration used in  #TAUTHOR_TAG.', ""therefore, each fold comprised one publisher from each orientation ( the classifiers did not learn a publisher's style )."", '']",5
"['configuration used in  #TAUTHOR_TAG.', 'therefore,']","['configuration used in  #TAUTHOR_TAG.', 'therefore,']","['with the same configuration used in  #TAUTHOR_TAG.', 'therefore,']","['', 'with that intention we selected the k most frequent words from the target language, and then we transformed the texts by masking the occurrences of the rest of the words.', 'in another setting, we masked style - related information to allow the system to focus only on the topic - related differences between the orientations.', 'we call this a topic - based model.', 'for this, we masked the k most frequent words and maintained intact the rest.', 'after the text transformation by the masking process in both the training and test sets, we represented the documents with character n - grams and compared the results obtained with the style - based and the topic - related models.', 'machine ( svm ) and random forest ( rf ) ; for the three classifiers we used the versions implemented in sklearn with the parameters set by default.', 'evaluation : we performed 3 - fold cross - validation with the same configuration used in  #TAUTHOR_TAG.', ""therefore, each fold comprised one publisher from each orientation ( the classifiers did not learn a publisher's style )."", '']",5
"['configuration used in  #TAUTHOR_TAG.', 'therefore,']","['configuration used in  #TAUTHOR_TAG.', 'therefore,']","['with the same configuration used in  #TAUTHOR_TAG.', 'therefore,']","['', 'with that intention we selected the k most frequent words from the target language, and then we transformed the texts by masking the occurrences of the rest of the words.', 'in another setting, we masked style - related information to allow the system to focus only on the topic - related differences between the orientations.', 'we call this a topic - based model.', 'for this, we masked the k most frequent words and maintained intact the rest.', 'after the text transformation by the masking process in both the training and test sets, we represented the documents with character n - grams and compared the results obtained with the style - based and the topic - related models.', 'machine ( svm ) and random forest ( rf ) ; for the three classifiers we used the versions implemented in sklearn with the parameters set by default.', 'evaluation : we performed 3 - fold cross - validation with the same configuration used in  #TAUTHOR_TAG.', ""therefore, each fold comprised one publisher from each orientation ( the classifiers did not learn a publisher's style )."", '']",3
"['out in  #TAUTHOR_TAG,']","['out in  #TAUTHOR_TAG,']","['was already pointed out in  #TAUTHOR_TAG,']","['line with what was already pointed out in  #TAUTHOR_TAG, the left - wing orientation is harder to predict, possibly because this class is represented with fewer examples in the dataset.', 'another reason why our masking approach achieves better results could be that we use a higher length of character n - grams.', 'in fact, comparing the results of  #TAUTHOR_TAG against our baseline model, it is possible to note that even without masking any word, the classifier obtains better results.', 'this suggests that the good results are due to the length of the character n - grams rather than the use of the masking technique.', 'robustness of the approach to different values of k and n. with the goals of : ( i ) understanding the robustness of the approach to different parameter values ; and to see if ( ii ) it is possible to overcome the f 1 = 0. 70 from the baseline model, we vary the values of k and n and evaluate the macro f 1 using svm.', 'figures 1 shows the results of the variation of k ∈ { 100, 200,..., 5000 }. when k > 5000, we clearly can see that the topic - related model, in which the k most frequent terms are masked, is decreasing the performance.', '']",3
"['configuration used in  #TAUTHOR_TAG.', 'therefore,']","['configuration used in  #TAUTHOR_TAG.', 'therefore,']","['with the same configuration used in  #TAUTHOR_TAG.', 'therefore,']","['', 'with that intention we selected the k most frequent words from the target language, and then we transformed the texts by masking the occurrences of the rest of the words.', 'in another setting, we masked style - related information to allow the system to focus only on the topic - related differences between the orientations.', 'we call this a topic - based model.', 'for this, we masked the k most frequent words and maintained intact the rest.', 'after the text transformation by the masking process in both the training and test sets, we represented the documents with character n - grams and compared the results obtained with the style - based and the topic - related models.', 'machine ( svm ) and random forest ( rf ) ; for the three classifiers we used the versions implemented in sklearn with the parameters set by default.', 'evaluation : we performed 3 - fold cross - validation with the same configuration used in  #TAUTHOR_TAG.', ""therefore, each fold comprised one publisher from each orientation ( the classifiers did not learn a publisher's style )."", '']",4
"['out in  #TAUTHOR_TAG,']","['out in  #TAUTHOR_TAG,']","['was already pointed out in  #TAUTHOR_TAG,']","['line with what was already pointed out in  #TAUTHOR_TAG, the left - wing orientation is harder to predict, possibly because this class is represented with fewer examples in the dataset.', 'another reason why our masking approach achieves better results could be that we use a higher length of character n - grams.', 'in fact, comparing the results of  #TAUTHOR_TAG against our baseline model, it is possible to note that even without masking any word, the classifier obtains better results.', 'this suggests that the good results are due to the length of the character n - grams rather than the use of the masking technique.', 'robustness of the approach to different values of k and n. with the goals of : ( i ) understanding the robustness of the approach to different parameter values ; and to see if ( ii ) it is possible to overcome the f 1 = 0. 70 from the baseline model, we vary the values of k and n and evaluate the macro f 1 using svm.', 'figures 1 shows the results of the variation of k ∈ { 100, 200,..., 5000 }. when k > 5000, we clearly can see that the topic - related model, in which the k most frequent terms are masked, is decreasing the performance.', '']",4
"['out in  #TAUTHOR_TAG,']","['out in  #TAUTHOR_TAG,']","['was already pointed out in  #TAUTHOR_TAG,']","['line with what was already pointed out in  #TAUTHOR_TAG, the left - wing orientation is harder to predict, possibly because this class is represented with fewer examples in the dataset.', 'another reason why our masking approach achieves better results could be that we use a higher length of character n - grams.', 'in fact, comparing the results of  #TAUTHOR_TAG against our baseline model, it is possible to note that even without masking any word, the classifier obtains better results.', 'this suggests that the good results are due to the length of the character n - grams rather than the use of the masking technique.', 'robustness of the approach to different values of k and n. with the goals of : ( i ) understanding the robustness of the approach to different parameter values ; and to see if ( ii ) it is possible to overcome the f 1 = 0. 70 from the baseline model, we vary the values of k and n and evaluate the macro f 1 using svm.', 'figures 1 shows the results of the variation of k ∈ { 100, 200,..., 5000 }. when k > 5000, we clearly can see that the topic - related model, in which the k most frequent terms are masked, is decreasing the performance.', '']",4
"['of natural language generation  #TAUTHOR_TAG.', 'narrative is generally acknowledged as a fundamental mode of presenting and communicating information between humans, with different manifestations across media but with a very significant presence in textual form.', 'yet efforts in natural language generation research have generally side stepped the issue.', 'aside']","['of natural language generation  #TAUTHOR_TAG.', 'narrative is generally acknowledged as a fundamental mode of presenting and communicating information between humans, with different manifestations across media but with a very significant presence in textual form.', 'yet efforts in natural language generation research have generally side stepped the issue.', 'aside']","['of natural language generation  #TAUTHOR_TAG.', 'narrative is generally acknowledged as a fundamental mode of presenting and communicating information between humans, with different manifestations across media but with a very significant presence in textual form.', 'yet efforts in natural language generation research have generally side stepped the issue.', 'aside']","['last few years have seen an increased interest in narrative within the field of natural language generation  #TAUTHOR_TAG.', 'narrative is generally acknowledged as a fundamental mode of presenting and communicating information between humans, with different manifestations across media but with a very significant presence in textual form.', 'yet efforts in natural language generation research have generally side stepped the issue.', 'aside from the pioneering work of  #AUTHOR_TAG and an early attempt to bridge the gap between narratology and natural language generation ( lonneker, 2005 ), the field had mostly avoided narrative until recent times.', '']",0
"['of natural language generation  #TAUTHOR_TAG.', 'narrative is generally acknowledged as a fundamental mode of presenting and communicating information between humans, with different manifestations across media but with a very significant presence in textual form.', 'yet efforts in natural language generation research have generally side stepped the issue.', 'aside']","['of natural language generation  #TAUTHOR_TAG.', 'narrative is generally acknowledged as a fundamental mode of presenting and communicating information between humans, with different manifestations across media but with a very significant presence in textual form.', 'yet efforts in natural language generation research have generally side stepped the issue.', 'aside']","['of natural language generation  #TAUTHOR_TAG.', 'narrative is generally acknowledged as a fundamental mode of presenting and communicating information between humans, with different manifestations across media but with a very significant presence in textual form.', 'yet efforts in natural language generation research have generally side stepped the issue.', 'aside']","['last few years have seen an increased interest in narrative within the field of natural language generation  #TAUTHOR_TAG.', 'narrative is generally acknowledged as a fundamental mode of presenting and communicating information between humans, with different manifestations across media but with a very significant presence in textual form.', 'yet efforts in natural language generation research have generally side stepped the issue.', 'aside from the pioneering work of  #AUTHOR_TAG and an early attempt to bridge the gap between narratology and natural language generation ( lonneker, 2005 ), the field had mostly avoided narrative until recent times.', '']",0
"['( daume  #TAUTHOR_TAG, named']","['( daume  #TAUTHOR_TAG, named']","['( daume  #TAUTHOR_TAG, named']","['', 'domain adaptation is very important in the field of natural language processing ( nlp ) as it can reduce the expensive manual annotation effort in the target domain.', 'various nlp tasks have benefited from domain adaptation techniques, including part - ofspeech tagging  #AUTHOR_TAG a ), chunking ( daume  #TAUTHOR_TAG, named entity recognition  #AUTHOR_TAG, dependency parsing  #AUTHOR_TAG and semantic role labeling  #AUTHOR_TAG b ).', 'in a typical domain adaptation scenario of nlp, the source and target domains contain text data of different genres ( e. g., newswire vs biomedical  #AUTHOR_TAG ).', 'under such circumstances, the original lexical features may not perform well in cross - domain learning since different genres of text may use very different vocabularies and produce cross - domain feature distribution divergence and feature sparsity issue.', 'a number of techniques have been developed in the literature to tackle the problem of cross - domain feature divergence and feature sparsity, including clustering based word representation learning methods  #TAUTHOR_TAG, word embedding based representation learning methods  #AUTHOR_TAG and some other representation learning methods  #AUTHOR_TAG.', '']",0
"['( daume  #TAUTHOR_TAG, named']","['( daume  #TAUTHOR_TAG, named']","['( daume  #TAUTHOR_TAG, named']","['', 'domain adaptation is very important in the field of natural language processing ( nlp ) as it can reduce the expensive manual annotation effort in the target domain.', 'various nlp tasks have benefited from domain adaptation techniques, including part - ofspeech tagging  #AUTHOR_TAG a ), chunking ( daume  #TAUTHOR_TAG, named entity recognition  #AUTHOR_TAG, dependency parsing  #AUTHOR_TAG and semantic role labeling  #AUTHOR_TAG b ).', 'in a typical domain adaptation scenario of nlp, the source and target domains contain text data of different genres ( e. g., newswire vs biomedical  #AUTHOR_TAG ).', 'under such circumstances, the original lexical features may not perform well in cross - domain learning since different genres of text may use very different vocabularies and produce cross - domain feature distribution divergence and feature sparsity issue.', 'a number of techniques have been developed in the literature to tackle the problem of cross - domain feature divergence and feature sparsity, including clustering based word representation learning methods  #TAUTHOR_TAG, word embedding based representation learning methods  #AUTHOR_TAG and some other representation learning methods  #AUTHOR_TAG.', '']",0
"[',  #TAUTHOR_TAG used the discrete hidden state of a word under hmms as augmenting features']","['address domain adaptation problems.', 'for example,  #TAUTHOR_TAG used the discrete hidden state of a word under hmms as augmenting features']","[',  #TAUTHOR_TAG used the discrete hidden state of a word under hmms as augmenting features']","['variety of representation learning approaches have been developed in the literature to address nlp domain adaptation problems.', 'the clustering based word representation learning methods perform word clustering within the sentence structure and use word cluster indicators as generalizable features to address domain adaptation problems.', 'for example,  #TAUTHOR_TAG used the discrete hidden state of a word under hmms as augmenting features for cross - domain pos tagging and np chunking.', 'brown clusters  #AUTHOR_TAG, which was used as latent features for simple in - domain dependency parsing  #AUTHOR_TAG, has recently been exploited for out - ofdomain statistical parsing  #AUTHOR_TAG.', 'the word embedding based representation learning methods learn a dense real - valued representation vector for each word as latent features for domain adaptation.', ' #AUTHOR_TAG empirically studied using word embeddings learned from hierarchical log - bilinear models  #AUTHOR_TAG and neural language models  #AUTHOR_TAG for cross - domain ner tasks.', ' #AUTHOR_TAG used the word embeddings learned from the skip - gram model ( sgm )  #AUTHOR_TAG to develop a pos tagger for twitter data with labeled newswire training data.', 'some other representation learning methods have been developed to tackle nlp cross - domain problems as well.', 'for example,  #AUTHOR_TAG proposed a structural correspondence learning method for pos tagging, which first selects a set of pivot features ( occurring frequently in figure 1 : hidden markov models with distributed state representations ( dhmm ).', 'the two domains ) and then models the correlations between pivot features and non - pivot features to induce generalizable features.', 'in terms of performing distributed representation learning for output variables, our proposed model shares similarity with the structured output representation learning approach developed by  #AUTHOR_TAG, which extends the structured support vector machines to simultaneously learn the prediction model and the distributed representations of the output labels.', 'however, the approach in  #AUTHOR_TAG assumes the training labels ( i. e., output values ) are given and performs learning in the standard supervised in - domain setting, while our proposed distributed hmms address cross - domain learning problems by performing unsupervised representation learning.', 'there are also a few works that extended standard hmms in the literature, including the observable operator models  #AUTHOR_TAG, and the spectral learning method  #AUTHOR_TAG. but none of them performs representation learning to address cross - domain adaptation problems']",0
"['of discrete hidden states induced from a hmm on addressing feature sparsity in domain adaptation  #TAUTHOR_TAG.', 'however, expressing a semantic word by a single discrete state value is too restrictive,']","['of discrete hidden states induced from a hmm on addressing feature sparsity in domain adaptation  #TAUTHOR_TAG.', 'however, expressing a semantic word by a single discrete state value is too restrictive,']","['m ∈ r h×m be the state embedding matrix where the i - th row m i : denotes the m - dimensional representation vector for the i - th state.', 'previous works have demonstrated the usefulness of discrete hidden states induced from a hmm on addressing feature sparsity in domain adaptation  #TAUTHOR_TAG.', 'however, expressing a semantic word by a single discrete state value is too restrictive,']","['this paper, we propose a novel distributed hidden markov model ( dhmm ) for representation learning over sequence data.', 'this model extends the hidden markov models  #AUTHOR_TAG to learn distributed state representations.', 'similar as hmms, a dhmm ( shown in figure 1 ) is a two - layer generative graphical model, which generates a sequence of observations from a sequence of latent state variables using markov properties.', 'let o = { o 1, o 2,..., o t } be the sequence of observations with length t, where each observation o t ∈ r d is a d - dimensional feature vector.', 'let s = { s 1, s 2,..', '., s t } be the sequence of t hidden states, where each hidden state s t has a discrete state value from a total h hidden states h = { 1, 2,..., h }. besides, we assume that there is a low - dimensional distributed representation vector associated with each hidden state.', 'let m ∈ r h×m be the state embedding matrix where the i - th row m i : denotes the m - dimensional representation vector for the i - th state.', 'previous works have demonstrated the usefulness of discrete hidden states induced from a hmm on addressing feature sparsity in domain adaptation  #TAUTHOR_TAG.', 'however, expressing a semantic word by a single discrete state value is too restrictive, as it has been shown in the literature that words have many different features in a multidimensional space where they could be separately characterized as number, pos tag, gender, tense, voice and other aspects  #AUTHOR_TAG.', 'our proposed model aims to overcome this inherent drawback of standard hmms on learning word representations.', 'given a set of observation sequences in two domains, the dhmm induces a distributed representation vector with continuous real values for each observation word as generalizable features, which has the capacity of capturing multi - aspect latent characteristics of the word clusters']",0
"['( daume  #TAUTHOR_TAG, named']","['( daume  #TAUTHOR_TAG, named']","['( daume  #TAUTHOR_TAG, named']","['', 'domain adaptation is very important in the field of natural language processing ( nlp ) as it can reduce the expensive manual annotation effort in the target domain.', 'various nlp tasks have benefited from domain adaptation techniques, including part - ofspeech tagging  #AUTHOR_TAG a ), chunking ( daume  #TAUTHOR_TAG, named entity recognition  #AUTHOR_TAG, dependency parsing  #AUTHOR_TAG and semantic role labeling  #AUTHOR_TAG b ).', 'in a typical domain adaptation scenario of nlp, the source and target domains contain text data of different genres ( e. g., newswire vs biomedical  #AUTHOR_TAG ).', 'under such circumstances, the original lexical features may not perform well in cross - domain learning since different genres of text may use very different vocabularies and produce cross - domain feature distribution divergence and feature sparsity issue.', 'a number of techniques have been developed in the literature to tackle the problem of cross - domain feature divergence and feature sparsity, including clustering based word representation learning methods  #TAUTHOR_TAG, word embedding based representation learning methods  #AUTHOR_TAG and some other representation learning methods  #AUTHOR_TAG.', '']",3
"['tasks.', 'we used the same experimental datasets as in  #TAUTHOR_TAG']","['tasks.', 'we used the same experimental datasets as in  #TAUTHOR_TAG']","['tasks.', 'we used the same experimental datasets as in  #TAUTHOR_TAG']","['conducted experiments on cross - domain partof - speech ( pos ) tagging and noun - phrase ( np ) chunking tasks.', 'we used the same experimental datasets as in  #TAUTHOR_TAG for cross - domain pos tagging from wall street journal ( wsj ) domain  #AUTHOR_TAG to med - line domain ( pennbioie, 2005 ) and for crossdomain np chunking from conll shared task dataset  #AUTHOR_TAG to open american national corpus ( oanc )  #AUTHOR_TAG']",3
"['( daume  #TAUTHOR_TAG, named']","['( daume  #TAUTHOR_TAG, named']","['( daume  #TAUTHOR_TAG, named']","['', 'domain adaptation is very important in the field of natural language processing ( nlp ) as it can reduce the expensive manual annotation effort in the target domain.', 'various nlp tasks have benefited from domain adaptation techniques, including part - ofspeech tagging  #AUTHOR_TAG a ), chunking ( daume  #TAUTHOR_TAG, named entity recognition  #AUTHOR_TAG, dependency parsing  #AUTHOR_TAG and semantic role labeling  #AUTHOR_TAG b ).', 'in a typical domain adaptation scenario of nlp, the source and target domains contain text data of different genres ( e. g., newswire vs biomedical  #AUTHOR_TAG ).', 'under such circumstances, the original lexical features may not perform well in cross - domain learning since different genres of text may use very different vocabularies and produce cross - domain feature distribution divergence and feature sparsity issue.', 'a number of techniques have been developed in the literature to tackle the problem of cross - domain feature divergence and feature sparsity, including clustering based word representation learning methods  #TAUTHOR_TAG, word embedding based representation learning methods  #AUTHOR_TAG and some other representation learning methods  #AUTHOR_TAG.', '']",5
"['tasks.', 'we used the same experimental datasets as in  #TAUTHOR_TAG']","['tasks.', 'we used the same experimental datasets as in  #TAUTHOR_TAG']","['tasks.', 'we used the same experimental datasets as in  #TAUTHOR_TAG']","['conducted experiments on cross - domain partof - speech ( pos ) tagging and noun - phrase ( np ) chunking tasks.', 'we used the same experimental datasets as in  #TAUTHOR_TAG for cross - domain pos tagging from wall street journal ( wsj ) domain  #AUTHOR_TAG to med - line domain ( pennbioie, 2005 ) and for crossdomain np chunking from conll shared task dataset  #AUTHOR_TAG to open american national corpus ( oanc )  #AUTHOR_TAG']",5
"['discrete hidden state based clustering system  #TAUTHOR_TAG.', 'we used the word id']","['discrete hidden state based clustering system  #TAUTHOR_TAG.', 'we used the word id']","['a discrete hidden state based clustering system  #TAUTHOR_TAG.', 'we used the word id']","['used the induced distributed state representations of each observation as augmenting features to train conditional random fields ( crf ) with the crfsuite package  #AUTHOR_TAG on the labeled source sentences and perform prediction on the target test sentences.', 'we compared with the following systems : a baseline system without representation learning, a sgm based word embedding system  #AUTHOR_TAG, and a discrete hidden state based clustering system  #TAUTHOR_TAG.', 'we used the word id and orthographic features as the baseline features for pos tagging and added pos tags for np chunking.', '']",7
"['of non - compositional mwe token instances  #AUTHOR_TAG, or the prediction of the compositionality of mwe types  #TAUTHOR_TAG.', '']","['of non - compositional mwe token instances  #AUTHOR_TAG, or the prediction of the compositionality of mwe types  #TAUTHOR_TAG.', '']","['the identification of non - compositional mwe token instances  #AUTHOR_TAG, or the prediction of the compositionality of mwe types  #TAUTHOR_TAG.', 'the identification of non - compositional mwe tokens is an important task']","['studies which have considered mwe compositionality have focused on either the identification of non - compositional mwe token instances  #AUTHOR_TAG, or the prediction of the compositionality of mwe types  #TAUTHOR_TAG.', 'the identification of non - compositional mwe tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional ( generally non - mwe ) and non - compositional mwe usage.', 'approaches have ranged from the unsupervised learning of type - level preferences  #AUTHOR_TAG to supervised methods specific to particular mwe constructions  #AUTHOR_TAG or applicable across multiple constructions using features similar to those used in all - words word sense disambiguation  #AUTHOR_TAG.', 'the prediction of the compositionality of mwe types has traditionally been couched as a binary classification task ( compositional or non - compositional :  #AUTHOR_TAG,  #AUTHOR_TAG ), but more recent work has moved towards a regression setup, where the degree of the compositionality is predicted on a continuous scale  #TAUTHOR_TAG.', 'in either case, the modelling has been done either over the whole mwe  #TAUTHOR_TAG, or relative to each component within the mwe  #AUTHOR_TAG.', 'in this paper, we focus on the binary classification of mwe types relative to each component of']",0
"['of non - compositional mwe token instances  #AUTHOR_TAG, or the prediction of the compositionality of mwe types  #TAUTHOR_TAG.', '']","['of non - compositional mwe token instances  #AUTHOR_TAG, or the prediction of the compositionality of mwe types  #TAUTHOR_TAG.', '']","['the identification of non - compositional mwe token instances  #AUTHOR_TAG, or the prediction of the compositionality of mwe types  #TAUTHOR_TAG.', 'the identification of non - compositional mwe tokens is an important task']","['studies which have considered mwe compositionality have focused on either the identification of non - compositional mwe token instances  #AUTHOR_TAG, or the prediction of the compositionality of mwe types  #TAUTHOR_TAG.', 'the identification of non - compositional mwe tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional ( generally non - mwe ) and non - compositional mwe usage.', 'approaches have ranged from the unsupervised learning of type - level preferences  #AUTHOR_TAG to supervised methods specific to particular mwe constructions  #AUTHOR_TAG or applicable across multiple constructions using features similar to those used in all - words word sense disambiguation  #AUTHOR_TAG.', 'the prediction of the compositionality of mwe types has traditionally been couched as a binary classification task ( compositional or non - compositional :  #AUTHOR_TAG,  #AUTHOR_TAG ), but more recent work has moved towards a regression setup, where the degree of the compositionality is predicted on a continuous scale  #TAUTHOR_TAG.', 'in either case, the modelling has been done either over the whole mwe  #TAUTHOR_TAG, or relative to each component within the mwe  #AUTHOR_TAG.', 'in this paper, we focus on the binary classification of mwe types relative to each component of']",0
"['of non - compositional mwe token instances  #AUTHOR_TAG, or the prediction of the compositionality of mwe types  #TAUTHOR_TAG.', '']","['of non - compositional mwe token instances  #AUTHOR_TAG, or the prediction of the compositionality of mwe types  #TAUTHOR_TAG.', '']","['the identification of non - compositional mwe token instances  #AUTHOR_TAG, or the prediction of the compositionality of mwe types  #TAUTHOR_TAG.', 'the identification of non - compositional mwe tokens is an important task']","['studies which have considered mwe compositionality have focused on either the identification of non - compositional mwe token instances  #AUTHOR_TAG, or the prediction of the compositionality of mwe types  #TAUTHOR_TAG.', 'the identification of non - compositional mwe tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional ( generally non - mwe ) and non - compositional mwe usage.', 'approaches have ranged from the unsupervised learning of type - level preferences  #AUTHOR_TAG to supervised methods specific to particular mwe constructions  #AUTHOR_TAG or applicable across multiple constructions using features similar to those used in all - words word sense disambiguation  #AUTHOR_TAG.', 'the prediction of the compositionality of mwe types has traditionally been couched as a binary classification task ( compositional or non - compositional :  #AUTHOR_TAG,  #AUTHOR_TAG ), but more recent work has moved towards a regression setup, where the degree of the compositionality is predicted on a continuous scale  #TAUTHOR_TAG.', 'in either case, the modelling has been done either over the whole mwe  #TAUTHOR_TAG, or relative to each component within the mwe  #AUTHOR_TAG.', 'in this paper, we focus on the binary classification of mwe types relative to each component of']",0
"['work that is perhaps most closely related to this paper is that of  #TAUTHOR_TAG and  #AUTHOR_TAG, who use translation']","['work that is perhaps most closely related to this paper is that of  #TAUTHOR_TAG and  #AUTHOR_TAG, who use translation']","['work that is perhaps most closely related to this paper is that of  #TAUTHOR_TAG and  #AUTHOR_TAG, who use translation data']","['work that is perhaps most closely related to this paper is that of  #TAUTHOR_TAG and  #AUTHOR_TAG, who use translation data to predict the compositionality of a given mwe relative to each of its components, and then combine those scores to derive an overall compositionality score.', 'in both cases, translations of the mwe and its components are sourced from panlex  #AUTHOR_TAG, and if there is greater similarity between the translated components and mwe in a range of languages, the mwe is predicted to be more compositional.', 'the basis of the similarity calculation is unsupervised, using either string similarity  #TAUTHOR_TAG or distributional similarity  #AUTHOR_TAG.', 'however, the overall method is supervised, as training data is used to select the languages to aggregate scores across for a given mwe construction.', 'to benchmark our method, we use two of the same datasets as these two papers, and repurpose the best - performing methods of  #TAUTHOR_TAG and  #AUTHOR_TAG for classification of the compositionality of each mwe component']",3
['datasets as  #TAUTHOR_TAG ('],['datasets as  #TAUTHOR_TAG ( which'],['as  #TAUTHOR_TAG ('],"['mentioned above, we evaluate our method over the same two datasets as  #TAUTHOR_TAG ( which were later used, in addition to a third dataset of german noun compounds, in  #AUTHOR_TAG ) : ( 1 ) 90 binary english noun compounds ( encs, e. g. spelling bee or swimming pool ) ; and ( 2 ) 160 english verb particle constructions ( evpcs, e. g. stand up and give away ).', 'our results are not directly comparable with those of  #TAUTHOR_TAG and  #AUTHOR_TAG, however, who evaluated in terms of a regression task, modelling the overall compositionality of the mwe.', 'in our case, the task setup is a binary classification task relative to each of the two components of the mwe.', 'the enc dataset was originally constructed by  #AUTHOR_TAG, and annotated on a continuous [ 0, 5 ] scale for both overall compositionality and the component - wise compositionality of each of the modifier and head noun.', 'the sampling was random in an attempt to make the dataset balanced, with 48 % of compositional english noun compounds, of which 51 % are compositional in the first component and 60 % are compositional in the second component.', 'we generate discrete labels by discretising the component - wise compositionality scores based on the partitions [ 0, 2. 5 ] and ( 2. 5, 5 ].', 'on average, each nc in this dataset has 1. 4 senses ( definitions ) in wiktionary.', 'the evpc dataset was constructed by  #AUTHOR_TAG, and manually annotated for compositionality on a binary scale for each of the head verb and particle.', 'for the 160 evpcs, 76 % are verb - compositional and 48 % are particlecompositional.', 'on average, each evpc in this dataset has 3. 0 senses ( definitions ) in wiktionary']",3
['datasets as  #TAUTHOR_TAG ('],['datasets as  #TAUTHOR_TAG ( which'],['as  #TAUTHOR_TAG ('],"['mentioned above, we evaluate our method over the same two datasets as  #TAUTHOR_TAG ( which were later used, in addition to a third dataset of german noun compounds, in  #AUTHOR_TAG ) : ( 1 ) 90 binary english noun compounds ( encs, e. g. spelling bee or swimming pool ) ; and ( 2 ) 160 english verb particle constructions ( evpcs, e. g. stand up and give away ).', 'our results are not directly comparable with those of  #TAUTHOR_TAG and  #AUTHOR_TAG, however, who evaluated in terms of a regression task, modelling the overall compositionality of the mwe.', 'in our case, the task setup is a binary classification task relative to each of the two components of the mwe.', 'the enc dataset was originally constructed by  #AUTHOR_TAG, and annotated on a continuous [ 0, 5 ] scale for both overall compositionality and the component - wise compositionality of each of the modifier and head noun.', 'the sampling was random in an attempt to make the dataset balanced, with 48 % of compositional english noun compounds, of which 51 % are compositional in the first component and 60 % are compositional in the second component.', 'we generate discrete labels by discretising the component - wise compositionality scores based on the partitions [ 0, 2. 5 ] and ( 2. 5, 5 ].', 'on average, each nc in this dataset has 1. 4 senses ( definitions ) in wiktionary.', 'the evpc dataset was constructed by  #AUTHOR_TAG, and manually annotated for compositionality on a binary scale for each of the head verb and particle.', 'for the 160 evpcs, 76 % are verb - compositional and 48 % are particlecompositional.', 'on average, each evpc in this dataset has 3. 0 senses ( definitions ) in wiktionary']",3
"['work that is perhaps most closely related to this paper is that of  #TAUTHOR_TAG and  #AUTHOR_TAG, who use translation']","['work that is perhaps most closely related to this paper is that of  #TAUTHOR_TAG and  #AUTHOR_TAG, who use translation']","['work that is perhaps most closely related to this paper is that of  #TAUTHOR_TAG and  #AUTHOR_TAG, who use translation data']","['work that is perhaps most closely related to this paper is that of  #TAUTHOR_TAG and  #AUTHOR_TAG, who use translation data to predict the compositionality of a given mwe relative to each of its components, and then combine those scores to derive an overall compositionality score.', 'in both cases, translations of the mwe and its components are sourced from panlex  #AUTHOR_TAG, and if there is greater similarity between the translated components and mwe in a range of languages, the mwe is predicted to be more compositional.', 'the basis of the similarity calculation is unsupervised, using either string similarity  #TAUTHOR_TAG or distributional similarity  #AUTHOR_TAG.', 'however, the overall method is supervised, as training data is used to select the languages to aggregate scores across for a given mwe construction.', 'to benchmark our method, we use two of the same datasets as these two papers, and repurpose the best - performing methods of  #TAUTHOR_TAG and  #AUTHOR_TAG for classification of the compositionality of each mwe component']",5
"['work that is perhaps most closely related to this paper is that of  #TAUTHOR_TAG and  #AUTHOR_TAG, who use translation']","['work that is perhaps most closely related to this paper is that of  #TAUTHOR_TAG and  #AUTHOR_TAG, who use translation']","['work that is perhaps most closely related to this paper is that of  #TAUTHOR_TAG and  #AUTHOR_TAG, who use translation data']","['work that is perhaps most closely related to this paper is that of  #TAUTHOR_TAG and  #AUTHOR_TAG, who use translation data to predict the compositionality of a given mwe relative to each of its components, and then combine those scores to derive an overall compositionality score.', 'in both cases, translations of the mwe and its components are sourced from panlex  #AUTHOR_TAG, and if there is greater similarity between the translated components and mwe in a range of languages, the mwe is predicted to be more compositional.', 'the basis of the similarity calculation is unsupervised, using either string similarity  #TAUTHOR_TAG or distributional similarity  #AUTHOR_TAG.', 'however, the overall method is supervised, as training data is used to select the languages to aggregate scores across for a given mwe construction.', 'to benchmark our method, we use two of the same datasets as these two papers, and repurpose the best - performing methods of  #TAUTHOR_TAG and  #AUTHOR_TAG for classification of the compositionality of each mwe component']",5
"['component of interest, use the lcsbased method of  #TAUTHOR_TAG to measure the string']","['component of interest, use the lcsbased method of  #TAUTHOR_TAG to measure the string']","['the component of interest, use the lcsbased method of  #TAUTHOR_TAG to measure the string similarity']","['third information source in wiktionary that can be used to predict compositionality is sense - level translation data.', 'due to the user - generated nature of wiktionary, the set of languages for which 1 although the recall of these tags is low  #AUTHOR_TAG translations are provided varies greatly across lexical entries.', 'our approach is to take whatever translations happen to exist in wiktionary for a given mwe, and where there are translations in that language for the component of interest, use the lcsbased method of  #TAUTHOR_TAG to measure the string similarity between the translation of the mwe and the translation of the components.', 'unlike  #TAUTHOR_TAG, however, we do not use development data to select the optimal set of languages in a supervised manner, and instead simply take the average of the string similarity scores across the available languages.', 'in the case of more than one translation in a given language, we use the maximum string similarity for each pairing of mwe and component translation.', 'unlike the definition and synonym - based approach, the translation - based approach will produce real rather than binary values.', 'to combine the two approaches, we discretise the scores given by the translation approach.', 'in the case of disagreement between the two approaches, we label the given mwe as non - compositional.', 'this results in higher recall and lower precision for the task of detecting compositionality']",5
"[' #TAUTHOR_TAG, in which 54 languages are used ; (']","[' #TAUTHOR_TAG, in which 54 languages are used ; (']","['##s "", the string similarity - based method of  #TAUTHOR_TAG, in which 54 languages are used ; ( 2 ) "" ds']","['idiom inclusion, 3 computer chess, basketball player, telephone box ). we also compare our method with : ( 1 ) "" lcs "", the string similarity - based method of  #TAUTHOR_TAG, in which 54 languages are used ; ( 2 ) "" ds "", the monol', '##ingual distributional similarity method of  #AUTHOR_TAG ; ( 3 ) "" ds + dsl2 "", the multilingual distributional similarity method of  #AUTHOR_TAG, including supervised language selection for a given dataset, based on crossvalidation ;', 'and ( 4 ) "" lcs + ds + dsl2 "", whereby the first three methods are combined using a supervised support vector regression model. in each case, the continuous output of the model is equal - width discretised to generate a binary classification. we additionally present results for the combination of each of the six methods proposed', 'in this paper with lcs, ds and dsl2, using a linear - kernel support vector machine ( represented with the suffix "" comb ( lcs + ds + dsl2 ) "" for a given method ). the results are based on cross - 3 http : / / en. wiktionary. org / wiki / wiktionary : idioms _ that _ survived _ rfd validation, and for direct comparability, the partitions are exactly the same as  #AUTHOR_TAG. tables 2 and 3 provide the results when our proposed method for detecting', 'non - compositionality is applied to the enc and evpc datasets, respectively. the inclusion of translation data was', 'found to improve all of precision, recall and f - score across the board for all of the proposed methods. for reasons of space, results without translation data are therefore omitted from the paper. overall, the simple unsupervised methods proposed in this paper are comparable with the unsupervised and supervised state - of -', '']",5
"[' #TAUTHOR_TAG, in which 54 languages are used ; (']","[' #TAUTHOR_TAG, in which 54 languages are used ; (']","['##s "", the string similarity - based method of  #TAUTHOR_TAG, in which 54 languages are used ; ( 2 ) "" ds']","['idiom inclusion, 3 computer chess, basketball player, telephone box ). we also compare our method with : ( 1 ) "" lcs "", the string similarity - based method of  #TAUTHOR_TAG, in which 54 languages are used ; ( 2 ) "" ds "", the monol', '##ingual distributional similarity method of  #AUTHOR_TAG ; ( 3 ) "" ds + dsl2 "", the multilingual distributional similarity method of  #AUTHOR_TAG, including supervised language selection for a given dataset, based on crossvalidation ;', 'and ( 4 ) "" lcs + ds + dsl2 "", whereby the first three methods are combined using a supervised support vector regression model. in each case, the continuous output of the model is equal - width discretised to generate a binary classification. we additionally present results for the combination of each of the six methods proposed', 'in this paper with lcs, ds and dsl2, using a linear - kernel support vector machine ( represented with the suffix "" comb ( lcs + ds + dsl2 ) "" for a given method ). the results are based on cross - 3 http : / / en. wiktionary. org / wiki / wiktionary : idioms _ that _ survived _ rfd validation, and for direct comparability, the partitions are exactly the same as  #AUTHOR_TAG. tables 2 and 3 provide the results when our proposed method for detecting', 'non - compositionality is applied to the enc and evpc datasets, respectively. the inclusion of translation data was', 'found to improve all of precision, recall and f - score across the board for all of the proposed methods. for reasons of space, results without translation data are therefore omitted from the paper. overall, the simple unsupervised methods proposed in this paper are comparable with the unsupervised and supervised state - of -', '']",5
"[' #TAUTHOR_TAG, in which 54 languages are used ; (']","[' #TAUTHOR_TAG, in which 54 languages are used ; (']","['##s "", the string similarity - based method of  #TAUTHOR_TAG, in which 54 languages are used ; ( 2 ) "" ds']","['idiom inclusion, 3 computer chess, basketball player, telephone box ). we also compare our method with : ( 1 ) "" lcs "", the string similarity - based method of  #TAUTHOR_TAG, in which 54 languages are used ; ( 2 ) "" ds "", the monol', '##ingual distributional similarity method of  #AUTHOR_TAG ; ( 3 ) "" ds + dsl2 "", the multilingual distributional similarity method of  #AUTHOR_TAG, including supervised language selection for a given dataset, based on crossvalidation ;', 'and ( 4 ) "" lcs + ds + dsl2 "", whereby the first three methods are combined using a supervised support vector regression model. in each case, the continuous output of the model is equal - width discretised to generate a binary classification. we additionally present results for the combination of each of the six methods proposed', 'in this paper with lcs, ds and dsl2, using a linear - kernel support vector machine ( represented with the suffix "" comb ( lcs + ds + dsl2 ) "" for a given method ). the results are based on cross - 3 http : / / en. wiktionary. org / wiki / wiktionary : idioms _ that _ survived _ rfd validation, and for direct comparability, the partitions are exactly the same as  #AUTHOR_TAG. tables 2 and 3 provide the results when our proposed method for detecting', 'non - compositionality is applied to the enc and evpc datasets, respectively. the inclusion of translation data was', 'found to improve all of precision, recall and f - score across the board for all of the proposed methods. for reasons of space, results without translation data are therefore omitted from the paper. overall, the simple unsupervised methods proposed in this paper are comparable with the unsupervised and supervised state - of -', '']",5
"['component of interest, use the lcsbased method of  #TAUTHOR_TAG to measure the string']","['component of interest, use the lcsbased method of  #TAUTHOR_TAG to measure the string']","['the component of interest, use the lcsbased method of  #TAUTHOR_TAG to measure the string similarity']","['third information source in wiktionary that can be used to predict compositionality is sense - level translation data.', 'due to the user - generated nature of wiktionary, the set of languages for which 1 although the recall of these tags is low  #AUTHOR_TAG translations are provided varies greatly across lexical entries.', 'our approach is to take whatever translations happen to exist in wiktionary for a given mwe, and where there are translations in that language for the component of interest, use the lcsbased method of  #TAUTHOR_TAG to measure the string similarity between the translation of the mwe and the translation of the components.', 'unlike  #TAUTHOR_TAG, however, we do not use development data to select the optimal set of languages in a supervised manner, and instead simply take the average of the string similarity scores across the available languages.', 'in the case of more than one translation in a given language, we use the maximum string similarity for each pairing of mwe and component translation.', 'unlike the definition and synonym - based approach, the translation - based approach will produce real rather than binary values.', 'to combine the two approaches, we discretise the scores given by the translation approach.', 'in the case of disagreement between the two approaches, we label the given mwe as non - compositional.', 'this results in higher recall and lower precision for the task of detecting compositionality']",4
['datasets as  #TAUTHOR_TAG ('],['datasets as  #TAUTHOR_TAG ( which'],['as  #TAUTHOR_TAG ('],"['mentioned above, we evaluate our method over the same two datasets as  #TAUTHOR_TAG ( which were later used, in addition to a third dataset of german noun compounds, in  #AUTHOR_TAG ) : ( 1 ) 90 binary english noun compounds ( encs, e. g. spelling bee or swimming pool ) ; and ( 2 ) 160 english verb particle constructions ( evpcs, e. g. stand up and give away ).', 'our results are not directly comparable with those of  #TAUTHOR_TAG and  #AUTHOR_TAG, however, who evaluated in terms of a regression task, modelling the overall compositionality of the mwe.', 'in our case, the task setup is a binary classification task relative to each of the two components of the mwe.', 'the enc dataset was originally constructed by  #AUTHOR_TAG, and annotated on a continuous [ 0, 5 ] scale for both overall compositionality and the component - wise compositionality of each of the modifier and head noun.', 'the sampling was random in an attempt to make the dataset balanced, with 48 % of compositional english noun compounds, of which 51 % are compositional in the first component and 60 % are compositional in the second component.', 'we generate discrete labels by discretising the component - wise compositionality scores based on the partitions [ 0, 2. 5 ] and ( 2. 5, 5 ].', 'on average, each nc in this dataset has 1. 4 senses ( definitions ) in wiktionary.', 'the evpc dataset was constructed by  #AUTHOR_TAG, and manually annotated for compositionality on a binary scale for each of the head verb and particle.', 'for the 160 evpcs, 76 % are verb - compositional and 48 % are particlecompositional.', 'on average, each evpc in this dataset has 3. 0 senses ( definitions ) in wiktionary']",4
"[' #TAUTHOR_TAG, in which 54 languages are used ; (']","[' #TAUTHOR_TAG, in which 54 languages are used ; (']","['##s "", the string similarity - based method of  #TAUTHOR_TAG, in which 54 languages are used ; ( 2 ) "" ds']","['idiom inclusion, 3 computer chess, basketball player, telephone box ). we also compare our method with : ( 1 ) "" lcs "", the string similarity - based method of  #TAUTHOR_TAG, in which 54 languages are used ; ( 2 ) "" ds "", the monol', '##ingual distributional similarity method of  #AUTHOR_TAG ; ( 3 ) "" ds + dsl2 "", the multilingual distributional similarity method of  #AUTHOR_TAG, including supervised language selection for a given dataset, based on crossvalidation ;', 'and ( 4 ) "" lcs + ds + dsl2 "", whereby the first three methods are combined using a supervised support vector regression model. in each case, the continuous output of the model is equal - width discretised to generate a binary classification. we additionally present results for the combination of each of the six methods proposed', 'in this paper with lcs, ds and dsl2, using a linear - kernel support vector machine ( represented with the suffix "" comb ( lcs + ds + dsl2 ) "" for a given method ). the results are based on cross - 3 http : / / en. wiktionary. org / wiki / wiktionary : idioms _ that _ survived _ rfd validation, and for direct comparability, the partitions are exactly the same as  #AUTHOR_TAG. tables 2 and 3 provide the results when our proposed method for detecting', 'non - compositionality is applied to the enc and evpc datasets, respectively. the inclusion of translation data was', 'found to improve all of precision, recall and f - score across the board for all of the proposed methods. for reasons of space, results without translation data are therefore omitted from the paper. overall, the simple unsupervised methods proposed in this paper are comparable with the unsupervised and supervised state - of -', '']",4
"['##ume iii and  #AUTHOR_TAG daume  #TAUTHOR_TAG.', 'in this study, we examine']","['other databases, from known entries ( daume iii and  #AUTHOR_TAG daume  #TAUTHOR_TAG.', 'in this study, we examine']","['##ume iii and  #AUTHOR_TAG daume  #TAUTHOR_TAG.', 'in this study, we examine']","['typology is the classification of human languages according to syntactic, phonological, and other classes of features, and the investigation of the relationships and correlations between these classes / features.', 'this study has been a scientific pursuit in its own right since the 19th century  #AUTHOR_TAG, but recently typology has borne practical fruit within various subfields of nlp, particularly on problems involving lower - resource languages.', "" #AUTHOR_TAG, has proven useful in many nlp tasks ( o' #AUTHOR_TAG, such as multilingual dependency parsing  #AUTHOR_TAG, generative parsing in low - resource settings  #AUTHOR_TAG tackstrom et al., 2013 ), phonological language modeling and loanword prediction  #AUTHOR_TAG, postagging  #AUTHOR_TAG, and machine translation  #AUTHOR_TAG."", 'however, the needs of nlp tasks differ in many ways from the needs of scientific typology, and typological databases are often only sparsely populated, by necessity or by design.', '2 in nlp, on the other hand, what is important is having a relatively full set of features for the particular group of languages you are working on.', 'this mismatch of needs has motivated various proposals to reconstruct missing entries, in wals and other databases, from known entries ( daume iii and  #AUTHOR_TAG daume  #TAUTHOR_TAG.', '']",0
"['language ( daume  #TAUTHOR_TAG.', 'as an alternative that does not']","['language ( daume  #TAUTHOR_TAG.', 'as an alternative that does not']","['help infer other unknown features of the language ( daume  #TAUTHOR_TAG.', 'as an alternative that does not necessarily require pre - existing knowledge of the typological features in the language at hand,  #AUTHOR_TAG have proposed a method']","['##ology database : to perform our analysis, we use the uriel language typology database  #AUTHOR_TAG, which is a collection of binary features extracted from multiple typological, phylogenetic, and geographical databases such as wals ( world atlas of language structures )  #AUTHOR_TAG, phoible  #AUTHOR_TAG, ethnologue  #AUTHOR_TAG, and glottolog ( hammarstrom et al., 2015 ).', 'these features are divided into separate classes regarding syntax ( e. g. whether a language has prepositions or postpositions ), phonology ( e. g. whether a language has complex syllabic onset clusters ), and phonetic inventory ( e. g. whether a language has interdental fricatives ).', 'there are 103 syntactical features, 28 phonology features and 158 phonetic inventory features in the database.', 'baseline feature vectors : several previous methods take advantage of typological implicature, the fact that some typological traits correlate strongly with others, to use known features of a language to help infer other unknown features of the language ( daume  #TAUTHOR_TAG.', ""as an alternative that does not necessarily require pre - existing knowledge of the typological features in the language at hand,  #AUTHOR_TAG have proposed a method for inferring typological features directly from the language's k nearest neighbors ( k - nn ) according to geodesic distance ( distance on the earth's surface ) and genetic distance ( distance according to a phylogenetic family tree )."", 'in our experiments, our baseline uses this method by taking the 3 - nn for each language according to normalized geodesic + genetic distance, and calculating an average feature vector of these three neighbors.', 'typology prediction : to perform prediction, we trained a logistic regression classifier 3 with the baseline k - nn feature vectors described above and the proposed nmt feature vectors described in the next section.', '']",0
"['##ling, 2015 ;  #TAUTHOR_TAG, and we examine']","['that has used specifically engineered alignment - based models  #AUTHOR_TAG ostling, 2015 ;  #TAUTHOR_TAG, and we examine']","['##ling, 2015 ;  #TAUTHOR_TAG, and we examine']","['this section we describe three methods for learning representations for typology prediction with multilingual neural models.', 'lm language vector several methods have been proposed to learn multilingual language models ( lms ) that utilize vector representations of languages  #AUTHOR_TAG ostling and  #AUTHOR_TAG.', 'specifically, these models train a recurrent neural network lm ( rnnlm ;  #AUTHOR_TAG ) using long short - term memory ( lstm ;  #AUTHOR_TAG ) with an additional vector representing the current language as an input.', 'the expectation is that this vector will be able to capture the features of the language and improve lm accuracy.', 'ostling and  #AUTHOR_TAG noted that, intriguingly, agglomerative clustering of these language vectors results in something that looks roughly like a phylogenetic tree, but stopped short of performing typological inference.', 'we train this vector by appending a special token representing the source language ( e. g. "" fra "" for french ) to the beginning of the source sentence, as shown in fig. 1, then using the word representation learned for this token as a representation of the language.', 'we will call this first set of feature vectors lmvec, and examine their utility for typology prediction.', 'nmt language vector in our second set of feature vectors, mtvec, we similarly use a language embedding vector, but instead learn a multilingual neural mt model trained to translate from many languages to english, in a similar fashion to  #AUTHOR_TAG ;  #AUTHOR_TAG.', 'in contrast to lmvec, we hypothesize that the alignments to an identical sentence in english, the model will have a stronger signal allowing it to more accurately learn vectors that reflect the syntactic, phonetic, or semantic consistencies of various languages.', 'this has been demonstrated to some extent in previous work that has used specifically engineered alignment - based models  #AUTHOR_TAG ostling, 2015 ;  #TAUTHOR_TAG, and we examine whether these results apply to neural network feature extractors and expand beyond word order and syntax to other types of typology as well.', 'table 1 : accuracy of syntactic, phonological, and inventory features using lm language vectors ( lmvec ), mt language vectors ( mtvec ), mt encoder cell averages ( mtcell ) or both mt feature vectors ( mtboth ).', 'aux indicates auxiliary information of geodesic / genetic nearest neighbors ; "" none - aux "" is the majority class chance rate, while "" none + aux "" is a 3 - nn classification']",6
"['inventory ( "" i "" ) classes.', '2008 ; ostling, 2015 ;  #TAUTHOR_TAG, our proposed method is also able to infer information']","['inventory ( "" i "" ) classes.', '2008 ; ostling, 2015 ;  #TAUTHOR_TAG, our proposed method is also able to infer information']","['inventory ( "" i "" ) classes.', '2008 ; ostling, 2015 ;  #TAUTHOR_TAG, our proposed method is also able to infer information']","['results of the experiments can be found in tab.', '1. first, focusing on the "" - aux "" results, we can see that all feature vectors obtained by the neural models improve over the chance rate, demonstrating that indeed it is possible to extract information about linguistic typology from unsupervised neural models.', 'comparing lmvec to mtvec, we can see a convincing improvement of 2 - 3 % across the board, indicating that the use of bilingual information does indeed provide a stronger signal, allowing the network to extract more salient features.', 'next, we can see that mtcell further outperforms mtvec, indicating that the proposed method of investigating the hidden cell dynamics is more effective than using a statically learned language vector.', 'finally, combining both feature vectors as mtboth leads to further improvements.', 'to measure statistical significance of the results, we performed a paired bootstrap test to measure the gain between none + aux and mtboth + aux and found that the gains for syntax and inventory were significant ( p = 0. 05 ), but phonology was not, perhaps because the number of phonological features was fewer than the other classes ( only 28 ).', 'when further using the geodesic / genetic distance neighbor feature vectors, we can see that these trends largely hold although gains are much smaller, indicating that the proposed method is still useful in the case where we have a - priori knowledge about the environment in which the language exists.', 'it should be noted, however, that the gains of lmvec evaporate, indicating that access to aligned data may be essential when inferring the typology of a new language.', 'we also noted that the accuracies of certain features decreased from none - aux to mtboth - aux, particularly gender markers, case suffix and negative affix, but these decreases were to a lesser extent in magnitude than the improvements.', 'interestingly, and in contrast to previous methods for inferring typology from raw text, which have been specifically designed for inducing word order or other syntactic features ( lewis and xia, table 2 : top 5 improvements from "" none - aux "" to "" mtboth - aux "" in the syntax ( "" s "" ), phonology ( "" p "" ), and inventory ( "" i "" ) classes.', '2008 ; ostling, 2015 ;  #TAUTHOR_TAG, our proposed method is also able to infer information about phonological or phonetic inventory features.', 'this may seem surprising or even counterintuitive, but a look at the most - improved phonology / inventory features ( tab. 2 ) shows a number of features in which languages with the "" nondefault "" option ( e.']",4
['trees  #TAUTHOR_TAG'],['trees  #TAUTHOR_TAG'],"['the elementary trees  #TAUTHOR_TAG.', 'in section 5 we present some figures that show how the size of the encoding of']","['', 'in developing the lexsys grammar we have explored the consequences of giving the grammar writer the freedom to write a grammar that maximizes analysis quality without any regard for grammar size.', 'in the next three sections we present detailed statistics for the current lexsys grammar that give an indication of what the grammar contains, its current size, and why it has grown to this size.', 'in order to ease the process of engineering such a large grammar, we have made use of the lexical knowledge representation language datr  #AUTHOR_TAG to compactly encode the elementary trees  #TAUTHOR_TAG.', 'in section 5 we present some figures that show how the size of the encoding of the grammar has increased during the grammar development process as the number and complexity of elementary trees has grown.', 'we have addressed problems that result from trying to parse with such a large grammar by using a technique proposed by  #AUTHOR_TAG and  #AUTHOR_TAG in which all the trees that each word can anchor are compactly represented using a collection of finite state automata.', 'in section 6 we give some data that shows the extent to which this technique is successful in compacting the grammar']",5
[' #TAUTHOR_TAG and  #AUTHOR_TAG the lexsys grammar is encoded using'],[' #TAUTHOR_TAG and  #AUTHOR_TAG the lexsys grammar is encoded using'],[' #TAUTHOR_TAG and  #AUTHOR_TAG the lexsys grammar is encoded using'],"[' #TAUTHOR_TAG and  #AUTHOR_TAG the lexsys grammar is encoded using datr, a non - monotonic knowledge representation language.', 'in 1998, the grammar contained 620 trees organized into 44 tree families and produced using 35 rules.', 'this grammar was encoded in 2200 datr statements, giving an average of 3 : 55 datr statements per tree.', 'the grammar currently contains around 4000 trees in 143 families produced with 88 rules.', 'this grammar is encoded with around 5300 4 datr statements, giving an average of 1 : 325 statements per tree.', 'thus, as the grammar has grown the number of datr statements needed to encode it has grown, but not as rapidly']",5
[' #TAUTHOR_TAG and  #AUTHOR_TAG the lexsys grammar is encoded using'],[' #TAUTHOR_TAG and  #AUTHOR_TAG the lexsys grammar is encoded using'],[' #TAUTHOR_TAG and  #AUTHOR_TAG the lexsys grammar is encoded using'],"[' #TAUTHOR_TAG and  #AUTHOR_TAG the lexsys grammar is encoded using datr, a non - monotonic knowledge representation language.', 'in 1998, the grammar contained 620 trees organized into 44 tree families and produced using 35 rules.', 'this grammar was encoded in 2200 datr statements, giving an average of 3 : 55 datr statements per tree.', 'the grammar currently contains around 4000 trees in 143 families produced with 88 rules.', 'this grammar is encoded with around 5300 4 datr statements, giving an average of 1 : 325 statements per tree.', 'thus, as the grammar has grown the number of datr statements needed to encode it has grown, but not as rapidly']",3
"[' #TAUTHOR_TAG 5, 18 ]']","[' #TAUTHOR_TAG 5, 18 ]']","['2, 15, 16 ] ).', 'while useful, citation texts might lack the appropriate context from the reference article  #TAUTHOR_TAG 5, 18 ].', '']","['scienti c literature, related work is often referenced along with a short textual description regarding that work which we call citation text.', 'citation texts usually highlight certain contributions of the referenced paper and a set of citation texts to a reference paper can provide useful information about that paper.', 'therefore, citation texts have been previously used to enhance many downstream tasks in ir / nlp such as search and summarization ( e. g. [ 2, 15, 16 ] ).', 'while useful, citation texts might lack the appropriate context from the reference article  #TAUTHOR_TAG 5, 18 ].', 'for example, details of the methods, assumptions or conditions for the obtained results are often not mentioned.', '']",0
"[', the only published results on tac 201  #TAUTHOR_TAG, where the']","['citation text are annotated by 4 experts.', 'baselines.', 'to our knowledge, the only published results on tac 201  #TAUTHOR_TAG, where the']","[', the only published results on tac 201  #TAUTHOR_TAG, where the authors']","['.', 'we use the tac 2014 biomedical summarization benchmark 3.', 'this dataset contains 220 scienti c biomedical journal articles and 313 total citation texts where the relevant contexts for each citation text are annotated by 4 experts.', 'baselines.', 'to our knowledge, the only published results on tac 201  #TAUTHOR_TAG, where the authors utilized query reformulation ( qr ) based on umls ontology.', 'in addition to  #TAUTHOR_TAG lmd - lda : language modeling with lda smoothing which is a recent extension of the lmd to also account for the latent topics [ 10 ].', 'all the baseline parameters are tuned for the best performance, and the same preprocessing is applied to all the baselines and our methods.', 'our methods.', 'we rst report results based on training the embeddings on wikipedia ( we wiki ).', 'since tac dataset is in biomedical domain, many of the biomedical terms might be either outof - vocabulary or not captured in the correct context using general embeddings, therefore we also train biomedical embeddings ( we bio ) 4.', 'in addition, we report results for biomedical embeddings with retro tting ( we bio + rtrft ), as well as interpolating domain knowledge ( we bio + dmn']",0
['reformulation ( qr ) method by  #TAUTHOR_TAG which improves'],['reformulation ( qr ) method by  #TAUTHOR_TAG which improves'],['( qr ) method by  #TAUTHOR_TAG which improves'],"['', '( iv ) character precision at k ( c - p @ k ) : since we are usually interested in the top retrieved spans, we consider character o set precision only for the top k spans and we denote it with "" c - p @ k "".', 'results.', 'the results of intrinsic evaluation of contextualization are presented in table 1.', 'our models ( last 4 rows of table 1 ) achieve signi cant improvements over the baselines consistently across most of the metrics.', 'this shows the e ectiveness of our models viewed from di erent aspects in comparison with the baselines.', 'the best baseline performance is the query reformulation ( qr ) method by  #TAUTHOR_TAG which improves over other baselines.', 'we observe that using general domain embeddings does not provide much advantage in comparison with the best baseline ( compare we wiki and qr in the table ).', 'however, using the domain speci c embeddings ( we bio ) results in 10 % c - f improvement over the best baseline.', 'this is expected since word relations in the biomedical context are better captured with biomedical embeddings.', 'in table 2 an illustrative word "" expression "" gives better intuition why is']",0
"[', the only published results on tac 201  #TAUTHOR_TAG, where the']","['citation text are annotated by 4 experts.', 'baselines.', 'to our knowledge, the only published results on tac 201  #TAUTHOR_TAG, where the']","[', the only published results on tac 201  #TAUTHOR_TAG, where the authors']","['.', 'we use the tac 2014 biomedical summarization benchmark 3.', 'this dataset contains 220 scienti c biomedical journal articles and 313 total citation texts where the relevant contexts for each citation text are annotated by 4 experts.', 'baselines.', 'to our knowledge, the only published results on tac 201  #TAUTHOR_TAG, where the authors utilized query reformulation ( qr ) based on umls ontology.', 'in addition to  #TAUTHOR_TAG lmd - lda : language modeling with lda smoothing which is a recent extension of the lmd to also account for the latent topics [ 10 ].', 'all the baseline parameters are tuned for the best performance, and the same preprocessing is applied to all the baselines and our methods.', 'our methods.', 'we rst report results based on training the embeddings on wikipedia ( we wiki ).', 'since tac dataset is in biomedical domain, many of the biomedical terms might be either outof - vocabulary or not captured in the correct context using general embeddings, therefore we also train biomedical embeddings ( we bio ) 4.', 'in addition, we report results for biomedical embeddings with retro tting ( we bio + rtrft ), as well as interpolating domain knowledge ( we bio + dmn']",4
"[', the only published results on tac 201  #TAUTHOR_TAG, where the']","['citation text are annotated by 4 experts.', 'baselines.', 'to our knowledge, the only published results on tac 201  #TAUTHOR_TAG, where the']","[', the only published results on tac 201  #TAUTHOR_TAG, where the authors']","['.', 'we use the tac 2014 biomedical summarization benchmark 3.', 'this dataset contains 220 scienti c biomedical journal articles and 313 total citation texts where the relevant contexts for each citation text are annotated by 4 experts.', 'baselines.', 'to our knowledge, the only published results on tac 201  #TAUTHOR_TAG, where the authors utilized query reformulation ( qr ) based on umls ontology.', 'in addition to  #TAUTHOR_TAG lmd - lda : language modeling with lda smoothing which is a recent extension of the lmd to also account for the latent topics [ 10 ].', 'all the baseline parameters are tuned for the best performance, and the same preprocessing is applied to all the baselines and our methods.', 'our methods.', 'we rst report results based on training the embeddings on wikipedia ( we wiki ).', 'since tac dataset is in biomedical domain, many of the biomedical terms might be either outof - vocabulary or not captured in the correct context using general embeddings, therefore we also train biomedical embeddings ( we bio ) 4.', 'in addition, we report results for biomedical embeddings with retro tting ( we bio + rtrft ), as well as interpolating domain knowledge ( we bio + dmn']",6
['to ours is  #TAUTHOR_TAG where the authors approached the problem using a vector space model similarity ranking and'],['to ours is  #TAUTHOR_TAG where the authors approached the problem using a vector space model similarity ranking and'],"['given documents ; we furthermore incorporate ways to utilize domain speci c knowledge in our model.', 'the most relevant prior work to ours is  #TAUTHOR_TAG where the authors approached the problem using a vector space model similarity ranking and query reformulations']","['work has mostly focused on extracting the citation text in the citing article ( e. g. [ 1 ] ).', 'in this work, given the citation texts, we focus on extracting its relevant context from the reference paper.', 'related work have also shown that citation texts can be used in di erent applications such as summarization [ 2, 3, 9, 11, 15, 20 ].', 'our proposed model utilizes word embeddings and the domain knowledge.', 'embeddings have been recently used in general information retrieval models.', 'vulic and moens [ 19 ] proposed an architecture for learning word embeddings in multilingual settings and used them in document and query representation.', 'mitra et al. [ 12 ] proposed dual embedded space model that predicts document aboutness by comparing the centroid of word vectors to query terms.', 'ganguly et al. [ 7 ] used embeddings to transform term weights in a translation model for retrieval.', 'their model uses embeddings to expand documents and use co - occurrences for estimation.', 'unlike these works, we directly use embeddings in estimating the likelihood of query given documents ; we furthermore incorporate ways to utilize domain speci c knowledge in our model.', 'the most relevant prior work to ours is  #TAUTHOR_TAG where the authors approached the problem using a vector space model similarity ranking and query reformulations']",3
"['problem by  #TAUTHOR_TAG.', 'while they use a']","['problem by  #TAUTHOR_TAG.', 'while they use a feed - forward neural language']","['by  #TAUTHOR_TAG.', 'while they use a feed - forward neural language model']",[' #TAUTHOR_TAG'],6
[' #TAUTHOR_TAG to a recurrent neural'],[' #TAUTHOR_TAG to a recurrent neural'],"[' #TAUTHOR_TAG to a recurrent neural network architecture.', 'our model is a simplified version of']","['extend the state - of - the - art model for abstractive sentence summarization  #TAUTHOR_TAG to a recurrent neural network architecture.', 'our model is a simplified version of the encoder - decoder framework for machine translation.', 'the model is trained on the gigaword corpus to generate headlines based on the first line of each news article.', 'we comfortably outperform the previous state - of - the - art on both gigaword data and the duc - 2004 challenge even though our model does not rely on additional extractive features']",6
"['problem by  #TAUTHOR_TAG.', 'while they use a']","['problem by  #TAUTHOR_TAG.', 'while they use a feed - forward neural language']","['by  #TAUTHOR_TAG.', 'while they use a feed - forward neural language model']",[' #TAUTHOR_TAG'],4
[') of  #TAUTHOR_TAG which relies on a'],"['( abs ) of  #TAUTHOR_TAG which relies on a feed - forward network decoder.', 'additionally, we compare to']","[') of  #TAUTHOR_TAG which relies on a feed - forward network decoder.', 'additionally, we compare to']","['the gigaword corpus we evaluate our models in terms of perplexity on a held - out set.', 'we then pick the model with best perplexity on the held - out set and use it to compute the f1 - score of rouge - 1, rouge - 2, and rouge - l on the test sets, all of which we report.', 'for the duc corpus however, inline with the standard, we report the recall - only rouge.', 'as baseline we use the state - of - the - art attention - based system ( abs ) of  #TAUTHOR_TAG which relies on a feed - forward network decoder.', 'additionally, we compare to an enhanced version of their system ( abs + ), which relies on a range of separate extractive summarization features that are added as log - linear features in a secondary learning step with minimum error rate training  #AUTHOR_TAG.', 'table 1 shows that both our ras - elman and ras - lstm models achieve lower perplexity than abs as well as other models reported in  #TAUTHOR_TAG.', '']",4
[' #TAUTHOR_TAG to a recurrent neural'],[' #TAUTHOR_TAG to a recurrent neural'],"[' #TAUTHOR_TAG to a recurrent neural network architecture.', 'our model is a simplified version of']","['extend the state - of - the - art model for abstractive sentence summarization  #TAUTHOR_TAG to a recurrent neural network architecture.', 'our model is a simplified version of the encoder - decoder framework for machine translation.', 'the model is trained on the gigaword corpus to generate headlines based on the first line of each news article.', 'we comfortably outperform the previous state - of - the - art on both gigaword data and the duc - 2004 challenge even though our model does not rely on additional extractive features']",4
"['problem by  #TAUTHOR_TAG.', 'while they use a']","['problem by  #TAUTHOR_TAG.', 'while they use a feed - forward neural language']","['by  #TAUTHOR_TAG.', 'while they use a feed - forward neural language model']",[' #TAUTHOR_TAG'],3
['as  #TAUTHOR_TAG and'],['as  #TAUTHOR_TAG and'],"['as  #TAUTHOR_TAG and we use the same splits for training, validation,']","['models are trained on the annotated version of the gigaword corpus  #AUTHOR_TAG and we use only the annotations for tokenization and sentence separation while discarding other annotations such as tags and parses.', 'we pair the first sentence of each article with its headline to form sentence - summary pairs.', 'the data is pre - processed in the same way as  #TAUTHOR_TAG and we use the same splits for training, validation, and testing.', 'for gigaword we report results on the same randomly held - out test set of 2000 sentence - summary pairs as  #TAUTHOR_TAG.', '1 we also evaluate our models on the duc - 2004 evaluation data set comprising 500 pairs  #AUTHOR_TAG.', 'our evaluation is based on three variants of rouge  #AUTHOR_TAG, namely, rouge - 1 ( unigrams ), rouge - 2 ( bigrams ), and rouge - l ( longest - common substring )']",3
['as  #TAUTHOR_TAG and'],['as  #TAUTHOR_TAG and'],"['as  #TAUTHOR_TAG and we use the same splits for training, validation,']","['models are trained on the annotated version of the gigaword corpus  #AUTHOR_TAG and we use only the annotations for tokenization and sentence separation while discarding other annotations such as tags and parses.', 'we pair the first sentence of each article with its headline to form sentence - summary pairs.', 'the data is pre - processed in the same way as  #TAUTHOR_TAG and we use the same splits for training, validation, and testing.', 'for gigaword we report results on the same randomly held - out test set of 2000 sentence - summary pairs as  #TAUTHOR_TAG.', '1 we also evaluate our models on the duc - 2004 evaluation data set comprising 500 pairs  #AUTHOR_TAG.', 'our evaluation is based on three variants of rouge  #AUTHOR_TAG, namely, rouge - 1 ( unigrams ), rouge - 2 ( bigrams ), and rouge - l ( longest - common substring )']",3
[') of  #TAUTHOR_TAG which relies on a'],"['( abs ) of  #TAUTHOR_TAG which relies on a feed - forward network decoder.', 'additionally, we compare to']","[') of  #TAUTHOR_TAG which relies on a feed - forward network decoder.', 'additionally, we compare to']","['the gigaword corpus we evaluate our models in terms of perplexity on a held - out set.', 'we then pick the model with best perplexity on the held - out set and use it to compute the f1 - score of rouge - 1, rouge - 2, and rouge - l on the test sets, all of which we report.', 'for the duc corpus however, inline with the standard, we report the recall - only rouge.', 'as baseline we use the state - of - the - art attention - based system ( abs ) of  #TAUTHOR_TAG which relies on a feed - forward network decoder.', 'additionally, we compare to an enhanced version of their system ( abs + ), which relies on a range of separate extractive summarization features that are added as log - linear features in a secondary learning step with minimum error rate training  #AUTHOR_TAG.', 'table 1 shows that both our ras - elman and ras - lstm models achieve lower perplexity than abs as well as other models reported in  #TAUTHOR_TAG.', '']",3
"['include  #AUTHOR_TAG.', 'very recently  #TAUTHOR_TAG proposed a neural attention model for this']","['of sentence summarization include  #AUTHOR_TAG.', 'very recently  #TAUTHOR_TAG proposed a neural attention model for this']","['was used directly as a method for text simplification by  #AUTHOR_TAG.', 'other works which have recently been proposed for the problem of sentence summarization include  #AUTHOR_TAG.', 'very recently  #TAUTHOR_TAG proposed a neural attention model for this problem using a new data set for training and showing state - of - the - art performance on the duc tasks.', 'our model can be seen as an extension of their model']","['', 'more recently  #AUTHOR_TAG and later  #AUTHOR_TAG proposed systems which made heavy use of the syntactic features of the sentence - summary pairs.', 'later, along the lines of  #AUTHOR_TAG, moses was used directly as a method for text simplification by  #AUTHOR_TAG.', 'other works which have recently been proposed for the problem of sentence summarization include  #AUTHOR_TAG.', 'very recently  #TAUTHOR_TAG proposed a neural attention model for this problem using a new data set for training and showing state - of - the - art performance on the duc tasks.', 'our model can be seen as an extension of their model']",0
['as  #TAUTHOR_TAG and'],['as  #TAUTHOR_TAG and'],"['as  #TAUTHOR_TAG and we use the same splits for training, validation,']","['models are trained on the annotated version of the gigaword corpus  #AUTHOR_TAG and we use only the annotations for tokenization and sentence separation while discarding other annotations such as tags and parses.', 'we pair the first sentence of each article with its headline to form sentence - summary pairs.', 'the data is pre - processed in the same way as  #TAUTHOR_TAG and we use the same splits for training, validation, and testing.', 'for gigaword we report results on the same randomly held - out test set of 2000 sentence - summary pairs as  #TAUTHOR_TAG.', '1 we also evaluate our models on the duc - 2004 evaluation data set comprising 500 pairs  #AUTHOR_TAG.', 'our evaluation is based on three variants of rouge  #AUTHOR_TAG, namely, rouge - 1 ( unigrams ), rouge - 2 ( bigrams ), and rouge - l ( longest - common substring )']",5
['as  #TAUTHOR_TAG and'],['as  #TAUTHOR_TAG and'],"['as  #TAUTHOR_TAG and we use the same splits for training, validation,']","['models are trained on the annotated version of the gigaword corpus  #AUTHOR_TAG and we use only the annotations for tokenization and sentence separation while discarding other annotations such as tags and parses.', 'we pair the first sentence of each article with its headline to form sentence - summary pairs.', 'the data is pre - processed in the same way as  #TAUTHOR_TAG and we use the same splits for training, validation, and testing.', 'for gigaword we report results on the same randomly held - out test set of 2000 sentence - summary pairs as  #TAUTHOR_TAG.', '1 we also evaluate our models on the duc - 2004 evaluation data set comprising 500 pairs  #AUTHOR_TAG.', 'our evaluation is based on three variants of rouge  #AUTHOR_TAG, namely, rouge - 1 ( unigrams ), rouge - 2 ( bigrams ), and rouge - l ( longest - common substring )']",5
[') of  #TAUTHOR_TAG which relies on a'],"['( abs ) of  #TAUTHOR_TAG which relies on a feed - forward network decoder.', 'additionally, we compare to']","[') of  #TAUTHOR_TAG which relies on a feed - forward network decoder.', 'additionally, we compare to']","['the gigaword corpus we evaluate our models in terms of perplexity on a held - out set.', 'we then pick the model with best perplexity on the held - out set and use it to compute the f1 - score of rouge - 1, rouge - 2, and rouge - l on the test sets, all of which we report.', 'for the duc corpus however, inline with the standard, we report the recall - only rouge.', 'as baseline we use the state - of - the - art attention - based system ( abs ) of  #TAUTHOR_TAG which relies on a feed - forward network decoder.', 'additionally, we compare to an enhanced version of their system ( abs + ), which relies on a range of separate extractive summarization features that are added as log - linear features in a secondary learning step with minimum error rate training  #AUTHOR_TAG.', 'table 1 shows that both our ras - elman and ras - lstm models achieve lower perplexity than abs as well as other models reported in  #TAUTHOR_TAG.', '']",5
"['steps into a single scan process.', 'the algorithm, which is an extension of  #TAUTHOR_TAG, allows us to chunk morphemes into base phrases']","['steps into a single scan process.', 'the algorithm, which is an extension of  #TAUTHOR_TAG, allows us to chunk morphemes into base phrases']","['into a single scan process.', 'the algorithm, which is an extension of  #TAUTHOR_TAG, allows us to chunk morphemes into base phrases']","['scan algorithms of parsing are important for interactive applications of nlp.', 'for instance, such algorithms would be more suitable for robots accepting speech inputs or chatbots handling natural language inputs which should respond quickly in some situations even when human inputs are not clearly ended.', 'japanese sentence analysis typically consists of three major steps, namely morphological analysis, bunsetsu ( base phrase ) chunking, and dependency parsing.', 'in this paper, we describe a novel algorithm that combines the last two steps into a single scan process.', 'the algorithm, which is an extension of  #TAUTHOR_TAG, allows us to chunk morphemes into base phrases and decide dependency relations of the phrases in a strict left - toright manner.', 'we show a pseudo code of the algorithm and evaluate its performance empirically with the voted perceptron on the kyoto university corpus  #AUTHOR_TAG']",5
"['algorithm that we propose is based on  #TAUTHOR_TAG, which is considered to be a simple form of shift - reduce parsing.', 'the pseudo code of our algorithm is presented in']","['algorithm that we propose is based on  #TAUTHOR_TAG, which is considered to be a simple form of shift - reduce parsing.', 'the pseudo code of our algorithm is presented in']","['algorithm that we propose is based on  #TAUTHOR_TAG, which is considered to be a simple form of shift - reduce parsing.', 'the pseudo code of our algorithm is presented in figure 3.', 'important variables here are h j and t j where j is an index of morphemes']","['algorithm that we propose is based on  #TAUTHOR_TAG, which is considered to be a simple form of shift - reduce parsing.', 'the pseudo code of our algorithm is presented in figure 3.', 'important variables here are h j and t j where j is an index of morphemes.', 'the variable h j holds the head id and the variable t j has the type of dependency relation.', 'for example, the head and the dependency relation type of "" meg "" in figure 2 are represented as h 0 = 1 and t 0 = "" b "" respectively.', 'the flow of the algorithm, which has the same structure as  #TAUTHOR_TAG, is controlled with a stack that holds ids for modifier morphemes.', 'decision of the relation between two morphemes is made in dep ( ), which uses a machine learning - based classifier that supports multiclass prediction.', 'the presented algorithm runs in a left - to - right manner and its upper bound of the time complexity is o ( n ).', 'due to space limitation, we do not discuss its complexity here.', 'see  #TAUTHOR_TAG for further details']",5
"['algorithm that we propose is based on  #TAUTHOR_TAG, which is considered to be a simple form of shift - reduce parsing.', 'the pseudo code of our algorithm is presented in']","['algorithm that we propose is based on  #TAUTHOR_TAG, which is considered to be a simple form of shift - reduce parsing.', 'the pseudo code of our algorithm is presented in']","['algorithm that we propose is based on  #TAUTHOR_TAG, which is considered to be a simple form of shift - reduce parsing.', 'the pseudo code of our algorithm is presented in figure 3.', 'important variables here are h j and t j where j is an index of morphemes']","['algorithm that we propose is based on  #TAUTHOR_TAG, which is considered to be a simple form of shift - reduce parsing.', 'the pseudo code of our algorithm is presented in figure 3.', 'important variables here are h j and t j where j is an index of morphemes.', 'the variable h j holds the head id and the variable t j has the type of dependency relation.', 'for example, the head and the dependency relation type of "" meg "" in figure 2 are represented as h 0 = 1 and t 0 = "" b "" respectively.', 'the flow of the algorithm, which has the same structure as  #TAUTHOR_TAG, is controlled with a stack that holds ids for modifier morphemes.', 'decision of the relation between two morphemes is made in dep ( ), which uses a machine learning - based classifier that supports multiclass prediction.', 'the presented algorithm runs in a left - to - right manner and its upper bound of the time complexity is o ( n ).', 'due to space limitation, we do not discuss its complexity here.', 'see  #TAUTHOR_TAG for further details']",5
"['have designed rather simple features based on the common feature set  #TAUTHOR_TAG for bunsetsu - based parsers.', 'we use the following features for']","['have designed rather simple features based on the common feature set  #TAUTHOR_TAG for bunsetsu - based parsers.', 'we use the following features for']","['have designed rather simple features based on the common feature set  #TAUTHOR_TAG for bunsetsu - based parsers.', 'we use the following features for each morpheme : gap features between two morphemes are also used']","['have designed rather simple features based on the common feature set  #TAUTHOR_TAG for bunsetsu - based parsers.', 'we use the following features for each morpheme : gap features between two morphemes are also used since they have proven to be very useful and contribute to the accuracy  #AUTHOR_TAG.', 'they are represented as a binary feature and include distance ( 1, 2, 3, 4 - 10, or 11 ≤ ), particles, parentheses, and punctuation.', 'in our proposed algorithm basically two morphemes are examined to estimate their dependency relation.', 'context information about the current morphemes to be estimated would be very useful and we can incorporate such information into our model.', 'we assume that we have the j - th morpheme and the i - th one in figure 3.', 'we also use the j − n,..., j − 1, j + 1,..., j + n morphemes and the i − n,..., i − 1, i + 1,..., i + n ones, where n table 2 : dependency accuracy.', 'the system with the previous method employs the algorithm  #TAUTHOR_TAG with the voted perceptron.', 'is the size of the context window.', 'we examined 0, 1, 2 and 3 for n']",5
"['have designed rather simple features based on the common feature set  #TAUTHOR_TAG for bunsetsu - based parsers.', 'we use the following features for']","['have designed rather simple features based on the common feature set  #TAUTHOR_TAG for bunsetsu - based parsers.', 'we use the following features for']","['have designed rather simple features based on the common feature set  #TAUTHOR_TAG for bunsetsu - based parsers.', 'we use the following features for each morpheme : gap features between two morphemes are also used']","['have designed rather simple features based on the common feature set  #TAUTHOR_TAG for bunsetsu - based parsers.', 'we use the following features for each morpheme : gap features between two morphemes are also used since they have proven to be very useful and contribute to the accuracy  #AUTHOR_TAG.', 'they are represented as a binary feature and include distance ( 1, 2, 3, 4 - 10, or 11 ≤ ), particles, parentheses, and punctuation.', 'in our proposed algorithm basically two morphemes are examined to estimate their dependency relation.', 'context information about the current morphemes to be estimated would be very useful and we can incorporate such information into our model.', 'we assume that we have the j - th morpheme and the i - th one in figure 3.', 'we also use the j − n,..., j − 1, j + 1,..., j + n morphemes and the i − n,..., i − 1, i + 1,..., i + n ones, where n table 2 : dependency accuracy.', 'the system with the previous method employs the algorithm  #TAUTHOR_TAG with the voted perceptron.', 'is the size of the context window.', 'we examined 0, 1, 2 and 3 for n']",5
"['of  #TAUTHOR_TAG originally used.', 'his parser, which cannot']","['of  #TAUTHOR_TAG originally used.', 'his parser, which cannot']","['employs the algorithm of  #TAUTHOR_TAG originally used.', 'his parser, which cannot']","['', 'we implemented a parser that employs the algorithm of  #TAUTHOR_TAG originally used.', '']",5
"['of  #TAUTHOR_TAG originally used.', 'his parser, which cannot']","['of  #TAUTHOR_TAG originally used.', 'his parser, which cannot']","['employs the algorithm of  #TAUTHOR_TAG originally used.', 'his parser, which cannot']","['', 'we implemented a parser that employs the algorithm of  #TAUTHOR_TAG originally used.', '']",5
"['. g.,  #TAUTHOR_TAG,']","['of algorithms of japanese dependency parsing, e. g.,  #TAUTHOR_TAG,']","['. g.,  #TAUTHOR_TAG,']","['japanese nlp, it is often assumed that the structure of a sentence is given by dependency relations meg - ga kare - ni ano pen - wo age - ta.', 'meg - subj to him that pen - acc give - past.', 'among bunsetsus.', 'a bunsetsu is a base phrasal unit and consists of one or more content words followed by zero or more function words.', 'in addition, most of algorithms of japanese dependency parsing, e. g.,  #TAUTHOR_TAG, assume the three constraints below.', '']",0
"[') before dependency parsing  #TAUTHOR_TAG.', 'although wordbased']","['( base phrase chunking ) before dependency parsing  #TAUTHOR_TAG.', 'although wordbased parsers']","['( base phrase chunking ) before dependency parsing  #TAUTHOR_TAG.', 'although wordbased parsers are proposed in  #AUTHOR_TAG,']","['far as we know, there is no dependency parser that does simultaneously both bunsetsu chunking and dependency parsing and, in addition, does them with a single scan.', 'most of the modern dependency parsers for japanese require bunsetsu chunking ( base phrase chunking ) before dependency parsing  #TAUTHOR_TAG.', 'although wordbased parsers are proposed in  #AUTHOR_TAG, they do not build bunsetsus and are not compatible with other japanese dependency parsers.', 'multilingual parsers of participants in the conll 2006 shared task  #AUTHOR_TAG can handle japanese sentences. but they are basically word - based.', 'meg ga kare ni ano pen wo age - ta.', 'meg subj him to that pen acc give - past']",0
"['algorithm that we propose is based on  #TAUTHOR_TAG, which is considered to be a simple form of shift - reduce parsing.', 'the pseudo code of our algorithm is presented in']","['algorithm that we propose is based on  #TAUTHOR_TAG, which is considered to be a simple form of shift - reduce parsing.', 'the pseudo code of our algorithm is presented in']","['algorithm that we propose is based on  #TAUTHOR_TAG, which is considered to be a simple form of shift - reduce parsing.', 'the pseudo code of our algorithm is presented in figure 3.', 'important variables here are h j and t j where j is an index of morphemes']","['algorithm that we propose is based on  #TAUTHOR_TAG, which is considered to be a simple form of shift - reduce parsing.', 'the pseudo code of our algorithm is presented in figure 3.', 'important variables here are h j and t j where j is an index of morphemes.', 'the variable h j holds the head id and the variable t j has the type of dependency relation.', 'for example, the head and the dependency relation type of "" meg "" in figure 2 are represented as h 0 = 1 and t 0 = "" b "" respectively.', 'the flow of the algorithm, which has the same structure as  #TAUTHOR_TAG, is controlled with a stack that holds ids for modifier morphemes.', 'decision of the relation between two morphemes is made in dep ( ), which uses a machine learning - based classifier that supports multiclass prediction.', 'the presented algorithm runs in a left - to - right manner and its upper bound of the time complexity is o ( n ).', 'due to space limitation, we do not discuss its complexity here.', 'see  #TAUTHOR_TAG for further details']",0
"['have designed rather simple features based on the common feature set  #TAUTHOR_TAG for bunsetsu - based parsers.', 'we use the following features for']","['have designed rather simple features based on the common feature set  #TAUTHOR_TAG for bunsetsu - based parsers.', 'we use the following features for']","['have designed rather simple features based on the common feature set  #TAUTHOR_TAG for bunsetsu - based parsers.', 'we use the following features for each morpheme : gap features between two morphemes are also used']","['have designed rather simple features based on the common feature set  #TAUTHOR_TAG for bunsetsu - based parsers.', 'we use the following features for each morpheme : gap features between two morphemes are also used since they have proven to be very useful and contribute to the accuracy  #AUTHOR_TAG.', 'they are represented as a binary feature and include distance ( 1, 2, 3, 4 - 10, or 11 ≤ ), particles, parentheses, and punctuation.', 'in our proposed algorithm basically two morphemes are examined to estimate their dependency relation.', 'context information about the current morphemes to be estimated would be very useful and we can incorporate such information into our model.', 'we assume that we have the j - th morpheme and the i - th one in figure 3.', 'we also use the j − n,..., j − 1, j + 1,..., j + n morphemes and the i − n,..., i − 1, i + 1,..., i + n ones, where n table 2 : dependency accuracy.', 'the system with the previous method employs the algorithm  #TAUTHOR_TAG with the voted perceptron.', 'is the size of the context window.', 'we examined 0, 1, 2 and 3 for n']",6
"['of  #TAUTHOR_TAG originally used.', 'his parser, which cannot']","['of  #TAUTHOR_TAG originally used.', 'his parser, which cannot']","['employs the algorithm of  #TAUTHOR_TAG originally used.', 'his parser, which cannot']","['', 'we implemented a parser that employs the algorithm of  #TAUTHOR_TAG originally used.', '']",6
"['of  #TAUTHOR_TAG originally used.', 'his parser, which cannot']","['of  #TAUTHOR_TAG originally used.', 'his parser, which cannot']","['employs the algorithm of  #TAUTHOR_TAG originally used.', 'his parser, which cannot']","['', 'we implemented a parser that employs the algorithm of  #TAUTHOR_TAG originally used.', '']",4
"['of  #TAUTHOR_TAG originally used.', 'his parser, which cannot']","['of  #TAUTHOR_TAG originally used.', 'his parser, which cannot']","['employs the algorithm of  #TAUTHOR_TAG originally used.', 'his parser, which cannot']","['', 'we implemented a parser that employs the algorithm of  #TAUTHOR_TAG originally used.', '']",1
"['of  #TAUTHOR_TAG, but tried to']","['of  #TAUTHOR_TAG, but tried to']","['of  #TAUTHOR_TAG, but tried']","['', 'the target words. [ 9 ] followed the method of  #TAUTHOR_TAG, but tried to resolve the ambiguous relative problem by using just', 'unambiguous relatives. that is, the ambiguous relative rail is not utilized to build a training data of the word', 'crane because the word rail is ambiguous. another difference from  #TAUTHOR_TAG is on a lexical database : they utilized wordnet as a lexical database for', ""acquiring relatives of target words instead of international roget's thesaurus."", 'since wordnet is freely available for research, various kinds of wsd studies based', 'on wordnet can be compared with the method of [ 9 ]. they evaluated their method on 14 ambiguous', 'nouns and achieved a good performance comparable to the methods based on the sense tagged corpus.', '']",0
"['of  #TAUTHOR_TAG, but tried to']","['of  #TAUTHOR_TAG, but tried to']","['of  #TAUTHOR_TAG, but tried']","['', 'the target words. [ 9 ] followed the method of  #TAUTHOR_TAG, but tried to resolve the ambiguous relative problem by using just', 'unambiguous relatives. that is, the ambiguous relative rail is not utilized to build a training data of the word', 'crane because the word rail is ambiguous. another difference from  #TAUTHOR_TAG is on a lexical database : they utilized wordnet as a lexical database for', ""acquiring relatives of target words instead of international roget's thesaurus."", 'since wordnet is freely available for research, various kinds of wsd studies based', 'on wordnet can be compared with the method of [ 9 ]. they evaluated their method on 14 ambiguous', 'nouns and achieved a good performance comparable to the methods based on the sense tagged corpus.', '']",0
"['of  #TAUTHOR_TAG, but tried to']","['of  #TAUTHOR_TAG, but tried to']","['of  #TAUTHOR_TAG, but tried']","['', 'the target words. [ 9 ] followed the method of  #TAUTHOR_TAG, but tried to resolve the ambiguous relative problem by using just', 'unambiguous relatives. that is, the ambiguous relative rail is not utilized to build a training data of the word', 'crane because the word rail is ambiguous. another difference from  #TAUTHOR_TAG is on a lexical database : they utilized wordnet as a lexical database for', ""acquiring relatives of target words instead of international roget's thesaurus."", 'since wordnet is freely available for research, various kinds of wsd studies based', 'on wordnet can be compared with the method of [ 9 ]. they evaluated their method on 14 ambiguous', 'nouns and achieved a good performance comparable to the methods based on the sense tagged corpus.', '']",0
"['of  #TAUTHOR_TAG, but tried to']","['of  #TAUTHOR_TAG, but tried to']","['of  #TAUTHOR_TAG, but tried']","['', 'the target words. [ 9 ] followed the method of  #TAUTHOR_TAG, but tried to resolve the ambiguous relative problem by using just', 'unambiguous relatives. that is, the ambiguous relative rail is not utilized to build a training data of the word', 'crane because the word rail is ambiguous. another difference from  #TAUTHOR_TAG is on a lexical database : they utilized wordnet as a lexical database for', ""acquiring relatives of target words instead of international roget's thesaurus."", 'since wordnet is freely available for research, various kinds of wsd studies based', 'on wordnet can be compared with the method of [ 9 ]. they evaluated their method on 14 ambiguous', 'nouns and achieved a good performance comparable to the methods based on the sense tagged corpus.', '']",0
"['of  #TAUTHOR_TAG, but tried to']","['of  #TAUTHOR_TAG, but tried to']","['of  #TAUTHOR_TAG, but tried']","['', 'the target words. [ 9 ] followed the method of  #TAUTHOR_TAG, but tried to resolve the ambiguous relative problem by using just', 'unambiguous relatives. that is, the ambiguous relative rail is not utilized to build a training data of the word', 'crane because the word rail is ambiguous. another difference from  #TAUTHOR_TAG is on a lexical database : they utilized wordnet as a lexical database for', ""acquiring relatives of target words instead of international roget's thesaurus."", 'since wordnet is freely available for research, various kinds of wsd studies based', 'on wordnet can be compared with the method of [ 9 ]. they evaluated their method on 14 ambiguous', 'nouns and achieved a good performance comparable to the methods based on the sense tagged corpus.', '']",0
"['of  #TAUTHOR_TAG, but tried to']","['of  #TAUTHOR_TAG, but tried to']","['of  #TAUTHOR_TAG, but tried']","['', 'the target words. [ 9 ] followed the method of  #TAUTHOR_TAG, but tried to resolve the ambiguous relative problem by using just', 'unambiguous relatives. that is, the ambiguous relative rail is not utilized to build a training data of the word', 'crane because the word rail is ambiguous. another difference from  #TAUTHOR_TAG is on a lexical database : they utilized wordnet as a lexical database for', ""acquiring relatives of target words instead of international roget's thesaurus."", 'since wordnet is freely available for research, various kinds of wsd studies based', 'on wordnet can be compared with the method of [ 9 ]. they evaluated their method on 14 ambiguous', 'nouns and achieved a good performance comparable to the methods based on the sense tagged corpus.', '']",0
['reimplemented methods of  #TAUTHOR_TAG and'],"['reimplemented methods of  #TAUTHOR_TAG and [ 9 ], respectively. it is observed in the']",['reimplemented methods of  #TAUTHOR_TAG and'],[' #TAUTHOR_TAG'],0
['reimplemented methods of  #TAUTHOR_TAG and'],"['reimplemented methods of  #TAUTHOR_TAG and [ 9 ], respectively. it is observed in the']",['reimplemented methods of  #TAUTHOR_TAG and'],[' #TAUTHOR_TAG'],0
"['', 'this is similar to the relevance model trained in expressiontree  #TAUTHOR_TAG and unitdep']","['whether a number is relevant.', 'this is similar to the relevance model trained in expressiontree  #TAUTHOR_TAG and unitdep [ 28 ]']","['', 'this is similar to the relevance model trained in expressiontree  #TAUTHOR_TAG and unit']","['', 'this seq2seq model translates math word problems to equation templates, followed by a number mapping step to fill the slots in the equation with the quantities extracted from the text.', 'to ensure that the output equations by the model are syntactically correct, five rules are pre - defined as validity constraints.', 'for example, if the i th character in the output sequence is an operator in { +, −, ×, ÷ }, then the model cannot result in c ∈ { +, −, ×, ÷, ), = } for the ( i + 1 ) th character.', 'to further improve the accuracy, dns enhances the model in two ways.', 'first, it builds a lstm - based binary classification model to determine whether a number is relevant.', 'this is similar to the relevance model trained in expressiontree  #TAUTHOR_TAG and unitdep [ 28 ].', 'the difference is that dns uses lstm as the classifier with unsupervised word - embedding features whereas expressiontree and unitdep use svm with handcrafted features.', 'second, the seq2seq model is integrated with a similarity - based method [ 12 ] introduced in section 3. 2.', 'given a pre -']",3
"['dependent verb.', ' #TAUTHOR_TAG [ 27 ] [ 28 ] [ 15 ] directly use dependent']","['dependent verb.', ' #TAUTHOR_TAG [ 27 ] [ 28 ] [ 15 ] directly use dependent']","['dependent verb.', ' #TAUTHOR_TAG [ 27 ] [ 28 ] [ 15 ] directly use dependent verb as one of the features.', 'another widely - adopted verb - related feature is a vector capturing the distance']","['are important indicators for correct operator determination.', 'for example, "" lose "" is a verb indicating quantity loss for an entity and related to the subtraction operator.', 'given a quantity, we call the verb closest to it in the dependency tree as its dependent verb.', ' #TAUTHOR_TAG [ 27 ] [ 28 ] [ 15 ] directly use dependent verb as one of the features.', 'another widely - adopted verb - related feature is a vector capturing the distance between the dependent verb and a small pre - defined collection of verbs that are found to be useful in categorizing arithmetic operations.', 'again, the remaining features come from the works  #TAUTHOR_TAG, [ 28 ], [ 15 ].', 'the features indicate whether two quantities have the same dependent verbs or whether their dependent verbs refer to the same verb mention.', 'as we can see from the examples in table 5, the difference between these two types of features is the occurrence number of the dependent verb in the sentence']",3
"['as equivalent constraint  #TAUTHOR_TAG 7 ] is not restrictive to languages with distinctive grammar structure.', 'in this paper, we propose a novel language - agnostic method to learn']","['as equivalent constraint  #TAUTHOR_TAG 7 ] is not restrictive to languages with distinctive grammar structure.', 'in this paper, we propose a novel language - agnostic method to learn']","['as equivalent constraint  #TAUTHOR_TAG 7 ] is not restrictive to languages with distinctive grammar structure.', 'in this paper, we propose a novel language - agnostic method to learn']","['mixing has been a common phenomenon in multilingual communities.', 'it is motivated in response to social factors as a way of communication in a multicultural society.', 'from a sociolinguistic perspective, individuals do code - switching in order to construct an optimal interaction by accomplishing the conceptual, relational - interpersonal, and discourse - presentational meaning of conversation [ 1 ].', 'in its practice, the variation of code - switching will vary due to the traditions, beliefs, and normative values in the respective communities.', 'a number of studies [ 2, 3, 4, 5 ] found that code - switching is not produced indiscriminately, but follows syntactic constraints.', 'many linguists formulated various constraints to define a general rule for code - switching [ 2, 4, 5 ].', 'however, the constraints are not enough to make a good generalization of real code - switching constraints, and they have not been tested in large - scale corpora for many language pairs.', 'one of the biggest problem in code - switching is collecting large scale corpora.', 'speech data have to be collected from a spontaneous speech by bilingual speakers and the codeswitching has to be triggered naturally during the conversation.', 'in order to solve the data scarcity issue, code - switching data generation is useful to increase the volume and variance.', 'a linguistics constraint - driven generation approach such as equivalent constraint  #TAUTHOR_TAG 7 ] is not restrictive to languages with distinctive grammar structure.', 'in this paper, we propose a novel language - agnostic method to learn how to generate code - switching sentences by using a pointer - generator network [ 8 ].', 'the model is trained from concatenated sequences of parallel sentences to generate code - switching sentences, constrained by codeswitching texts.', 'the pointer network copies words from both languages and pastes them into the output, generating code switching sentences in matrix language to embedded language and vice versa.', 'the attention mechanism helps the decoder to generate meaningful and grammatical sentences without needing any sequence alignment.', 'this idea is also in line with code - mixing by borrowing words from the embedded language [ 9 ] and intuitively, the copying mechanism can be seen as an end - to - end approach to translate, align, and reorder the given words into a grammatical code - switching sentence.', 'this approach is the unification of all components in the work of  #TAUTHOR_TAG into a single computational model.', 'a code - switching language model learned in this way is able to capture the patterns and constraints of the switches and mitigate the out - of - vocabulary ( oov ) issue during sequence generation.', 'by adding the generated sentences and incorporating syntactic information to the']",0
"[') model  #TAUTHOR_TAG.', '']","['( asr ) model  #TAUTHOR_TAG.', '[ 11 ] explored functional head constraint,']","[') model  #TAUTHOR_TAG.', '']","['synthetic code - switching generation approach was introduced by adapting equivalence constraint on monolingual sentence pairs during the decoding step on an automatic speech recognition ( asr ) model  #TAUTHOR_TAG.', '[ 11 ] explored functional head constraint, which was found to be more restrictive than the equivalence constraint, but complex to be implemented, by using a lattice parser with a weighted finitestate transducer.', '[ 12 ] extended the rnn by adding pos information to the input layer and factorized output layer with a language identifier.', 'then, factorized rnn networks were combined with an n - gram backoff model using linear interpolation [ 13 ].', '[ 14 ] added syntactic and semantic features to the factorized rnn networks.', '[ 15 ] adapted an effective curriculum learning by training a network with monolingual corpora of both languages, and subsequently train on codeswitched data.', 'a further investigation of equivalence constraint and curriculum learning showed an improvement in language modeling [ 7 ].', 'a multi - task learning approach was introduced to train the syntax representation of languages by constraining the language generator [ 10 ].', 'a copy mechanism was proposed to copy words directly from the input to the output using an attention mechanism [ 16 ].', 'this mechanism has proven to be effective in several nlp tasks including text summarization [ 8 ], and dialog systems [ 17 ].', 'the common characteristic of these tasks is parts of the output are exactly the same as the input source.', 'for example, in dialog systems the responses most of the time have appeared in the previous dialog steps']",0
"['as equivalent constraint  #TAUTHOR_TAG 7 ] is not restrictive to languages with distinctive grammar structure.', 'in this paper, we propose a novel language - agnostic method to learn']","['as equivalent constraint  #TAUTHOR_TAG 7 ] is not restrictive to languages with distinctive grammar structure.', 'in this paper, we propose a novel language - agnostic method to learn']","['as equivalent constraint  #TAUTHOR_TAG 7 ] is not restrictive to languages with distinctive grammar structure.', 'in this paper, we propose a novel language - agnostic method to learn']","['mixing has been a common phenomenon in multilingual communities.', 'it is motivated in response to social factors as a way of communication in a multicultural society.', 'from a sociolinguistic perspective, individuals do code - switching in order to construct an optimal interaction by accomplishing the conceptual, relational - interpersonal, and discourse - presentational meaning of conversation [ 1 ].', 'in its practice, the variation of code - switching will vary due to the traditions, beliefs, and normative values in the respective communities.', 'a number of studies [ 2, 3, 4, 5 ] found that code - switching is not produced indiscriminately, but follows syntactic constraints.', 'many linguists formulated various constraints to define a general rule for code - switching [ 2, 4, 5 ].', 'however, the constraints are not enough to make a good generalization of real code - switching constraints, and they have not been tested in large - scale corpora for many language pairs.', 'one of the biggest problem in code - switching is collecting large scale corpora.', 'speech data have to be collected from a spontaneous speech by bilingual speakers and the codeswitching has to be triggered naturally during the conversation.', 'in order to solve the data scarcity issue, code - switching data generation is useful to increase the volume and variance.', 'a linguistics constraint - driven generation approach such as equivalent constraint  #TAUTHOR_TAG 7 ] is not restrictive to languages with distinctive grammar structure.', 'in this paper, we propose a novel language - agnostic method to learn how to generate code - switching sentences by using a pointer - generator network [ 8 ].', 'the model is trained from concatenated sequences of parallel sentences to generate code - switching sentences, constrained by codeswitching texts.', 'the pointer network copies words from both languages and pastes them into the output, generating code switching sentences in matrix language to embedded language and vice versa.', 'the attention mechanism helps the decoder to generate meaningful and grammatical sentences without needing any sequence alignment.', 'this idea is also in line with code - mixing by borrowing words from the embedded language [ 9 ] and intuitively, the copying mechanism can be seen as an end - to - end approach to translate, align, and reorder the given words into a grammatical code - switching sentence.', 'this approach is the unification of all components in the work of  #TAUTHOR_TAG into a single computational model.', 'a code - switching language model learned in this way is able to capture the patterns and constraints of the switches and mitigate the out - of - vocabulary ( oov ) issue during sequence generation.', 'by adding the generated sentences and incorporating syntactic information to the']",5
"['( 2 ) we generate sequences that satisfy equivalence constraint  #TAUTHOR_TAG.', ""the constraint doesn't allow any switch within a crossing of two word alignments."", 'we use fastalign']","['use seq2seq with attention ; ( 2 ) we generate sequences that satisfy equivalence constraint  #TAUTHOR_TAG.', ""the constraint doesn't allow any switch within a crossing of two word alignments."", 'we use fastalign [ 19 ] as the word aligner 1 ; ( 3 )']","['( 2 ) we generate sequences that satisfy equivalence constraint  #TAUTHOR_TAG.', ""the constraint doesn't allow any switch within a crossing of two word alignments."", 'we use fastalign [ 19 ] as the word aligner 1 ; ( 3 )']","['of generating words from a large vocabulary space using a seq2seq model with attention [ 18 ], pointer - generator network [ 8 ] is proposed to copy words from the input to the output using an attention mechanism and generate the output sequence using decoders.', 'the network is depicted in figure 1.', 'for each decoder step, a generation probability p gen ∈ [ 0, 1 ] is calculated, which weights the probability of generating words from the vocabulary, and copying words from the source text.', 'p gen is a soft gating probability to decide whether generating the next token from the decoder or copying the word from the input instead.', 'the attention distribution a t is a standard attention with general scoring [ 18 ].', 'it considers all encoder hidden states to derive the context vector.', 'the vocabulary distribution p vocab ( w ) is calculated by concatenating the decoder state s t and the context vector h * t.', 'where w h *, w s, w x are trainable parameters and b ptr is the scalar bias.', 'the vocabulary distribution p vocab ( w ) and the attention distribution a t are weighted and summed to obtain the final distribution p ( w ).', 'the final distribution is calculated as follows : we use a beam search to select n - best code - switching sentences and concatenate the generated sentence with the training set to form a larger dataset.', 'the result of the generated code - switching sentences is showed in table 1.', 'as our baseline, we compare our proposed method with three other models : ( 1 ) we use seq2seq with attention ; ( 2 ) we generate sequences that satisfy equivalence constraint  #TAUTHOR_TAG.', ""the constraint doesn't allow any switch within a crossing of two word alignments."", 'we use fastalign [ 19 ] as the word aligner 1 ; ( 3 ) we also form sentences using the alignments without any constraint.', 'the number of the generated sentences are equivalent to 3 - best data from the pointer - generator model.', 'to increase the generation variance, we randomly permute each alignment to form a new sequence']",5
"['of a system  #TAUTHOR_TAG.', '']","['of a system  #TAUTHOR_TAG.', '']","['on distributional methods is that the definition of context is often critical to the success of a system  #TAUTHOR_TAG.', 'some distributional representations,']","['', 'intuitively, the dih states that we should be able to replace any occurrence of "" cat "" with "" animal "" and still have a valid utterance.', 'an important insight from work on distributional methods is that the definition of context is often critical to the success of a system  #TAUTHOR_TAG.', 'some distributional representations, like positional or dependency - based contexts, may even capture crude hearst pattern - like features  #AUTHOR_TAG.', 'while both approaches for hypernym detection rely on co - occurrences within certain contexts, they differ in their context selection strategy : pattern - based methods use predefined manuallycurated patterns to generate high - precision extractions while dih methods rely on unconstrained word co - occurrences in large corpora.', 'here, we revisit the idea of using pattern - based methods for hypernym detection.', 'we evaluate several pattern - based models on modern, large corpora and compare them to methods based on the dih.', 'we find that simple pattern - based methods consistently outperform specialized dih methods on several difficult hypernymy tasks, including detection, direction prediction, and graded entailment ranking.', 'moreover']",0
"['inclusion hypothesis  #TAUTHOR_TAG.', 'here, we compare to two methods with strong empirical results.', 'as with']","['inclusion hypothesis  #TAUTHOR_TAG.', 'here, we compare to two methods with strong empirical results.', 'as with']","['hypernymy detection are based on variants of the distributional inclusion hypothesis  #TAUTHOR_TAG.', 'here, we compare to two methods with strong empirical results.', 'as with most dih measures, they are only defined for large, sparse, positively - valued distributional spaces.', 'first,']","['unsupervised distributional approaches for hypernymy detection are based on variants of the distributional inclusion hypothesis  #TAUTHOR_TAG.', 'here, we compare to two methods with strong empirical results.', 'as with most dih measures, they are only defined for large, sparse, positively - valued distributional spaces.', '']",0
"['inclusion hypothesis  #TAUTHOR_TAG.', 'here, we compare to two methods with strong empirical results.', 'as with']","['inclusion hypothesis  #TAUTHOR_TAG.', 'here, we compare to two methods with strong empirical results.', 'as with']","['hypernymy detection are based on variants of the distributional inclusion hypothesis  #TAUTHOR_TAG.', 'here, we compare to two methods with strong empirical results.', 'as with most dih measures, they are only defined for large, sparse, positively - valued distributional spaces.', 'first,']","['unsupervised distributional approaches for hypernymy detection are based on variants of the distributional inclusion hypothesis  #TAUTHOR_TAG.', 'here, we compare to two methods with strong empirical results.', 'as with most dih measures, they are only defined for large, sparse, positively - valued distributional spaces.', '']",5
['is consistent with evaluations in  #TAUTHOR_TAG'],['is consistent with evaluations in  #TAUTHOR_TAG'],"['is consistent with evaluations in  #TAUTHOR_TAG.', 'direction : in direction prediction, the task is to']","['', 'we chose to evaluate the global ranking using average precision.', 'this allowed us to use the same metric on all detection benchmarks, and is consistent with evaluations in  #TAUTHOR_TAG.', 'direction : in direction prediction, the task is to identify which term is broader in a given pair of words.', 'for this task, we evaluate all models on three datasets described by  #AUTHOR_TAG : on bless, the task is to predict the direction for all 1337 positive pairs in the dataset.', 'pairs are only counted correct if the hypernymy direction scores higher than the reverse direction, i. e. score ( x, y ) > score ( y, x ).', 'we reserve 10 % of the data for validation, and test on the remaining 90 %.', 'on wbless, we follow prior work  #AUTHOR_TAG vulic and mrksic, 2017 ) and perform 1000 random iterations in which 2 % of the data is used as a validation set to learn a classification threshold, and test on the remainder of the data.', 'we report average accuracy across all iterations.', 'finally, we evaluate on bibless  #AUTHOR_TAG, a variant of wbless with hypernymy and hyponymy pairs explicitly annotated for their direction']",5
"['of  #TAUTHOR_TAG, which is']","['of  #TAUTHOR_TAG, which is']","['the distributional baselines, we employ the large, sparse distributional space of  #TAUTHOR_TAG, which is computed from ukwac']","['- based models : we extract hearst patterns from the concatenation of gigaword and wikipedia, and prepare our corpus by tokenizing, lemmatizing, and pos tagging using corenlp 3. 8. 0.', 'the full set of hearst patterns is provided in table 1.', 'our selected patterns match prototypical hearst patterns, like "" animals such as cats, "" but also include broader patterns like "" new year is the most important holiday. "" leading and following noun phrases are allowed to match limited modifiers ( compound nouns, adjectives, etc. ), in which case we also generate a hit for the head of the noun phrase.', 'during postprocessing, we remove pairs which were not extracted by at least two distinct patterns.', 'we also remove any pair ( y, x ) if p ( y, x ) < p ( x, y ).', 'the final corpus contains roughly 4. 5m matched pairs, 431k unique pairs, and 243k unique terms.', 'for svd - based models, we select the rank from r ∈ { 5, 10, 15, 20, 25, 50, 100, 150, 200, 250, 300, 500, 1000 } on the validation set.', 'the other pattern - based models do not have any hyperparameters.', 'distributional models : for the distributional baselines, we employ the large, sparse distributional space of  #TAUTHOR_TAG, which is computed from ukwac and wikipedia, and is known to have strong performance on several of the detection tasks.', 'the corpus was pos tagged and dependency parsed.', 'distributional contexts were constructed from adjacent words in dependency parses ( pado and  #AUTHOR_TAG.', 'targets and contexts which appeared fewer than 100 times in the corpus were filtered, and the resulting co - occurrence matrix was ppmi transformed.', '1 the resulting space contains representations for 218k words over 732k context dimensions.', 'for the slqs model, we selected the number of contexts n from the same set of options as the svd rank in pattern - based models.', 'table 2 shows the results from all three experimental settings.', 'in nearly all cases, we find that patternbased approaches substantially outperform all three distributional models.', 'particularly strong improvements can be observed on bless ( 0. 76 average precision vs 0. 19 ) and wbless ( 0. 96 vs. 0. 69 ) for the detection tasks and on all directionality tasks.', 'for directionality prediction on bless, the svd models surpass even the state - of - the - art supervised model of vulic and mrksic ( 2017 )']",5
"[' #TAUTHOR_TAG ] ), i believe that the creation of novel semantic - based', 'measurements would improve the']","[' #TAUTHOR_TAG ] ), i believe that the creation of novel semantic - based', 'measurements would improve the']","['in [  #TAUTHOR_TAG ] ), i believe that the creation of novel semantic - based', 'measurements would improve the state of the']","['', '11 ] ). however, when deep analyzes are performed, network - based strategies usually do not perform better than other techniques making extensive use of semantic resources and tools. in order to improve the performance of network', '- based applications, i suggest a twofold research line : ( i ) the introduction of measurements consistent with the nature', 'of the problem ; and ( ii ) the combination of topological strategies with other traditional natural language processing methods. more specifically, in ( i ), i propose 1', 'e - mail : diego. raphael @ gmail. com, diego', '@ icmc. usp. br december 4, 2014 the conception of measurements', 'that are able to capture semantic aspects, since the topological measurements of co - occurrence', 'networks capture mostly syntactic factors [ 8 ]. although such networks have proved useful in some semantical - dependent tasks ( see e. g. a', 'topological approach to word sense disambiguation in [  #TAUTHOR_TAG ] ), i believe that the creation of novel semantic - based', 'measurements would improve the state of the art. alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space. in ( ii ), i suggest, for example, the', 'introduction of a hybrid classifier that could consider both linguistic ( deeper linguistic processing [ 18 ] ) and topological attributes at the same time in a hybrid way. examples of combinations of distinct strategies are described in [ 9 ], [ 19 ] and [ 20 ]. in sum,', 'the network framework has proven applicable to understand the properties of the language and its applications, especially those related to the textual classification in', 'several levels. despite the limitations imposed by the restrict understanding of the mechanisms behind the classification, it is worth noting that the such representation remains entirely generic, being therefore useful to', 'many tasks as well as for analyzing the evolution of languages, cultures and emotional trends. for this reason, i believe that the', 'use of complex networks in both practical and theoretical investigations shall yield novels insights into the mechanisms behind the language']",0
"[' #TAUTHOR_TAG ] ), i believe that the creation of novel semantic - based measurements would improve the state of the']","[' #TAUTHOR_TAG ] ), i believe that the creation of novel semantic - based measurements would improve the state of the art.', 'alternative forms to create the network']","['- occurrence networks capture mostly syntactic factors [ 8 ].', 'although such networks have proved useful in some semantical - dependent tasks ( see e. g. a topological approach to word sense disambiguation in [  #TAUTHOR_TAG ] ), i believe that the creation of novel semantic - based measurements would improve the state of the art.', 'alternative forms to create the network']","['conception of measurements that are able to capture semantic aspects, since the topological measurements of co - occurrence networks capture mostly syntactic factors [ 8 ].', 'although such networks have proved useful in some semantical - dependent tasks ( see e. g. a topological approach to word sense disambiguation in [  #TAUTHOR_TAG ] ), i believe that the creation of novel semantic - based measurements would improve the state of the art.', 'alternative forms to create the network could also be useful to grasp semantical features hidden in the topological space.', 'in ( ii ), i suggest, for example, the introduction of a hybrid classifier that could consider both linguistic ( deeper linguistic processing [ 18 ] ) and topological attributes at the same time in a hybrid way.', 'examples of combinations of distinct strategies are described in [ 9 ], [ 19 ] and [ 20 ].', 'in sum, the network framework has proven applicable to understand the properties of the language and its applications, especially those related to the textual classification in several levels.', 'despite the limitations imposed by the restrict understanding of the mechanisms behind the classification, it is worth noting that the such representation remains entirely generic, being therefore useful to many tasks as well as for analyzing the evolution of languages, cultures and emotional trends.', 'for this reason, i believe that the use of complex networks in both practical and theoretical investigations shall yield novels insights into the mechanisms behind the language']",0
"['of  #TAUTHOR_TAG, who use partial least squares regression ( plsr ) to']","['of  #TAUTHOR_TAG, who use partial least squares regression ( plsr ) to']","['of  #TAUTHOR_TAG, who use partial least squares regression ( plsr ) to']","['property - listing studies have been conducted with human participants in order to build property norms - datasets of normalized humanverbalizable feature listings for lexical concepts ( mc  #AUTHOR_TAG.', 'one use of feature norms is to critically examine distributional semantic models on their ability to encode grounded, human - elicited semantic knowledge.', 'for example,  #AUTHOR_TAG demonstrated that state - of - the - art distributional semantic models fail to predict attributive properties of concept words ( e. g. the properties is - red and is - round for the word apple ) as accurately as taxonomic properties ( e. g. is - a - fruit ).', ' #AUTHOR_TAG investigated the types of semantic knowledge encoded within pretrained word embeddings, concluding that some properties cannot be learned by supervised classifiers.', ' #AUTHOR_TAG compared linguistic and visual representations of object concepts on their ability to represent different types of property knowledge.', 'research has shown that state - of - the - art distributional semantic models built from text corpora fail to capture important aspects of meaning related to grounded perceptual information, as this kind of information is not adequately represented in the statistical regularities of text data  #AUTHOR_TAG.', 'motivated by these issues,  #AUTHOR_TAG constructed multimodal semantic models from text and image data, with the goal of grounding word meaning using visual attributes.', 'more recently,  #AUTHOR_TAG built similar models with the added constraint of sparsity, demonstrating that sparse multimodal vectors provide a more faithful representation of human semantic representations.', 'finally, the work that most resembles ours is that of  #TAUTHOR_TAG, who use partial least squares regression ( plsr ) to learn a mapping from a word embedding model onto specific conceptual properties.', 'concurrent work recently undertaken by li and summers -  #AUTHOR_TAG replaces the plsr model with a feedforward neural network.', 'in our work, we instead map property knowledge directly into vector space models of word meaning, rather than learning a supervised predictive function from concept embedding dimensions to feature terms']",5
"[' #TAUTHOR_TAG, although their approach differs from ours in that they map from an embedding space onto the feature space, while we learn a mapping from the feature domain onto the embedding space.', 'we outline both methods below']","[' #TAUTHOR_TAG, although their approach differs from ours in that they map from an embedding space onto the feature space, while we learn a mapping from the feature domain onto the embedding space.', 'we outline both methods below']","[' #TAUTHOR_TAG, although their approach differs from ours in that they map from an embedding space onto the feature space, while we learn a mapping from the feature domain onto the embedding space.', 'we outline both methods below']","['make primary comparison with the work of  #TAUTHOR_TAG, although their approach differs from ours in that they map from an embedding space onto the feature space, while we learn a mapping from the feature domain onto the embedding space.', 'we outline both methods below']",5
"['size to 50, following  #TAUTHOR_TAG.', 'we also build a plsr model using 120 dimensions, which in preliminary experimentation we found gave the best']","['size to 50, following  #TAUTHOR_TAG.', 'we also build a plsr model using 120 dimensions, which in preliminary experimentation we found gave the best']","['predicting vectors in the feature space from vectors in the embedding space.', 'in this work, we use the plsr approach as a baseline for our model.', 'in implementing plsr, we set the intermediate dimension size to 50, following  #TAUTHOR_TAG.', 'we also build a plsr model using 120 dimensions, which in preliminary experimentation we found gave the best performance from a range of values tested']","['used partial least squares regression ( plsr ) to map between the glove embedding space and property norm vectors.', 'suppose we have two real - valued matrices g ∈ r n×m and f ∈ r n×k.', 'in this context, g and f represent glove embedding vectors and property norm feature vectors, respectively.', 'for n available concept words, g is a matrix which consists of stacked pretrained embeddings from glove and f is the ( sparse ) matrix of production frequencies for each concept×feature pair.', 'g and f share the same row indexing for concept words.', 'for a new dimension size p ∈ n, a partial least squared regression learns two new subspaces with dimensions n × p, which have maximal covariance between them.', 'the algorithm solves this problem by learning a mapping from the matrix g onto f, similar to a regression model.', 'the fitted regression model thus provides a framework for predicting vectors in the feature space from vectors in the embedding space.', 'in this work, we use the plsr approach as a baseline for our model.', 'in implementing plsr, we set the intermediate dimension size to 50, following  #TAUTHOR_TAG.', 'we also build a plsr model using 120 dimensions, which in preliminary experimentation we found gave the best performance from a range of values tested']",5
['on the feature vector reconstruction task used by  #TAUTHOR_TAG'],['on the feature vector reconstruction task used by  #TAUTHOR_TAG'],"['first evaluate how well the baseline plsr model performs on the feature vector reconstruction task used by  #TAUTHOR_TAG.', 'in this']","['first evaluate how well the baseline plsr model performs on the feature vector reconstruction task used by  #TAUTHOR_TAG.', 'in this evaluation, the feature vector for a test concept is predicted and we test whether the real concept vector is within the top n most similar neighbours of the predicted vector.', 'we report results over both 50 ( as in  #TAUTHOR_TAG and 120 dimensions for a range of values of n ( table 1 )']",5
"['of  #TAUTHOR_TAG, who use partial least squares regression ( plsr ) to']","['of  #TAUTHOR_TAG, who use partial least squares regression ( plsr ) to']","['of  #TAUTHOR_TAG, who use partial least squares regression ( plsr ) to']","['property - listing studies have been conducted with human participants in order to build property norms - datasets of normalized humanverbalizable feature listings for lexical concepts ( mc  #AUTHOR_TAG.', 'one use of feature norms is to critically examine distributional semantic models on their ability to encode grounded, human - elicited semantic knowledge.', 'for example,  #AUTHOR_TAG demonstrated that state - of - the - art distributional semantic models fail to predict attributive properties of concept words ( e. g. the properties is - red and is - round for the word apple ) as accurately as taxonomic properties ( e. g. is - a - fruit ).', ' #AUTHOR_TAG investigated the types of semantic knowledge encoded within pretrained word embeddings, concluding that some properties cannot be learned by supervised classifiers.', ' #AUTHOR_TAG compared linguistic and visual representations of object concepts on their ability to represent different types of property knowledge.', 'research has shown that state - of - the - art distributional semantic models built from text corpora fail to capture important aspects of meaning related to grounded perceptual information, as this kind of information is not adequately represented in the statistical regularities of text data  #AUTHOR_TAG.', 'motivated by these issues,  #AUTHOR_TAG constructed multimodal semantic models from text and image data, with the goal of grounding word meaning using visual attributes.', 'more recently,  #AUTHOR_TAG built similar models with the added constraint of sparsity, demonstrating that sparse multimodal vectors provide a more faithful representation of human semantic representations.', 'finally, the work that most resembles ours is that of  #TAUTHOR_TAG, who use partial least squares regression ( plsr ) to learn a mapping from a word embedding model onto specific conceptual properties.', 'concurrent work recently undertaken by li and summers -  #AUTHOR_TAG replaces the plsr model with a feedforward neural network.', 'in our work, we instead map property knowledge directly into vector space models of word meaning, rather than learning a supervised predictive function from concept embedding dimensions to feature terms']",6
"['of  #TAUTHOR_TAG, who use partial least squares regression ( plsr ) to']","['of  #TAUTHOR_TAG, who use partial least squares regression ( plsr ) to']","['of  #TAUTHOR_TAG, who use partial least squares regression ( plsr ) to']","['property - listing studies have been conducted with human participants in order to build property norms - datasets of normalized humanverbalizable feature listings for lexical concepts ( mc  #AUTHOR_TAG.', 'one use of feature norms is to critically examine distributional semantic models on their ability to encode grounded, human - elicited semantic knowledge.', 'for example,  #AUTHOR_TAG demonstrated that state - of - the - art distributional semantic models fail to predict attributive properties of concept words ( e. g. the properties is - red and is - round for the word apple ) as accurately as taxonomic properties ( e. g. is - a - fruit ).', ' #AUTHOR_TAG investigated the types of semantic knowledge encoded within pretrained word embeddings, concluding that some properties cannot be learned by supervised classifiers.', ' #AUTHOR_TAG compared linguistic and visual representations of object concepts on their ability to represent different types of property knowledge.', 'research has shown that state - of - the - art distributional semantic models built from text corpora fail to capture important aspects of meaning related to grounded perceptual information, as this kind of information is not adequately represented in the statistical regularities of text data  #AUTHOR_TAG.', 'motivated by these issues,  #AUTHOR_TAG constructed multimodal semantic models from text and image data, with the goal of grounding word meaning using visual attributes.', 'more recently,  #AUTHOR_TAG built similar models with the added constraint of sparsity, demonstrating that sparse multimodal vectors provide a more faithful representation of human semantic representations.', 'finally, the work that most resembles ours is that of  #TAUTHOR_TAG, who use partial least squares regression ( plsr ) to learn a mapping from a word embedding model onto specific conceptual properties.', 'concurrent work recently undertaken by li and summers -  #AUTHOR_TAG replaces the plsr model with a feedforward neural network.', 'in our work, we instead map property knowledge directly into vector space models of word meaning, rather than learning a supervised predictive function from concept embedding dimensions to feature terms']",4
"[' #TAUTHOR_TAG, although their approach differs from ours in that they map from an embedding space onto the feature space, while we learn a mapping from the feature domain onto the embedding space.', 'we outline both methods below']","[' #TAUTHOR_TAG, although their approach differs from ours in that they map from an embedding space onto the feature space, while we learn a mapping from the feature domain onto the embedding space.', 'we outline both methods below']","[' #TAUTHOR_TAG, although their approach differs from ours in that they map from an embedding space onto the feature space, while we learn a mapping from the feature domain onto the embedding space.', 'we outline both methods below']","['make primary comparison with the work of  #TAUTHOR_TAG, although their approach differs from ours in that they map from an embedding space onto the feature space, while we learn a mapping from the feature domain onto the embedding space.', 'we outline both methods below']",4
['on the feature vector reconstruction task used by  #TAUTHOR_TAG'],['on the feature vector reconstruction task used by  #TAUTHOR_TAG'],"['first evaluate how well the baseline plsr model performs on the feature vector reconstruction task used by  #TAUTHOR_TAG.', 'in this']","['first evaluate how well the baseline plsr model performs on the feature vector reconstruction task used by  #TAUTHOR_TAG.', 'in this evaluation, the feature vector for a test concept is predicted and we test whether the real concept vector is within the top n most similar neighbours of the predicted vector.', 'we report results over both 50 ( as in  #TAUTHOR_TAG and 120 dimensions for a range of values of n ( table 1 )']",3
"['errors  #TAUTHOR_TAG.', 'moreover, the set of features used within the norms are dependent on the concepts that were presented to the human participants.', 'it is therefore notable that the conceptual representations predicted by our model for the two outof - norms concept words are particularly plausible, even though the attributes were never intended to conceptually represent these words.', 'our analysis supports the view that such supervised models could']","['errors  #TAUTHOR_TAG.', 'moreover, the set of features used within the norms are dependent on the concepts that were presented to the human participants.', 'it is therefore notable that the conceptual representations predicted by our model for the two outof - norms concept words are particularly plausible, even though the attributes were never intended to conceptually represent these words.', 'our analysis supports the view that such supervised models could']","[', and predicted properties not in the norms are not necessarily errors  #TAUTHOR_TAG.', 'moreover, the set of features used within the norms are dependent on the concepts that were presented to the human participants.', 'it is therefore notable that the conceptual representations predicted by our model for the two outof - norms concept words are particularly plausible, even though the attributes were never intended to conceptually represent these words.', 'our analysis supports the view that such supervised models could be utilised as an assistive tool']","['previous work, we provide the top 10 feature predictions for a few sample concepts, displayed in table 3.', 'properties underlined and in bold represent features that match the available ground truth data ( i. e., the concept×feature pair occurs in the norms ).', 'the first two words in table 3 were sampled from the cslb norms test set, whilst the last two words were randomly sampled from the word embedding lexicon and are not concept words appearing in the cslb norms.', 'we find that the predicted features that are not contained within the ground truth property set still tend to be quite reasonable, even for the two concepts not in the test dataset.', 'as property norms do not represent an exhaustive listing of property knowledge, this is not surprising, and predicted properties not in the norms are not necessarily errors  #TAUTHOR_TAG.', 'moreover, the set of features used within the norms are dependent on the concepts that were presented to the human participants.', 'it is therefore notable that the conceptual representations predicted by our model for the two outof - norms concept words are particularly plausible, even though the attributes were never intended to conceptually represent these words.', 'our analysis supports the view that such supervised models could be utilised as an assistive tool for surveying much larger vocabularies of words']",0
"['predicting concept features on two property norm datasets.', 'as discussed by  #TAUTHOR_TAG and others, it is clear that property norm datasets provide only a semi - complete picture of human conceptual']","['predicting concept features on two property norm datasets.', 'as discussed by  #TAUTHOR_TAG and others, it is clear that property norm datasets provide only a semi - complete picture of human conceptual knowledge, and more extensive surveys']","['predicting concept features on two property norm datasets.', 'as discussed by  #TAUTHOR_TAG and others, it is clear that property norm datasets provide only a semi - complete picture of human conceptual knowledge, and more extensive surveys may provide additional useful property knowledge information.', 'by predicting plausible semantic features']","['proposed a method for constructing distributional semantic vectors for human property norms from a pretrained vector space model of word meaning, which outperforms previous methods for predicting concept features on two property norm datasets.', 'as discussed by  #TAUTHOR_TAG and others, it is clear that property norm datasets provide only a semi - complete picture of human conceptual knowledge, and more extensive surveys may provide additional useful property knowledge information.', 'by predicting plausible semantic features for concepts through the leveraging of corpus - derived word embedding data, our method offers a useful tool for guiding the expensive and laborious process of collecting property norm listings.', 'for example, existing property norm datasets can be extended through human verification of features predicted with high confidence by feature2vec, with these features being added to the norms and subsequently incorporated into feature2vec in an iterative, semi - supervised manner  #AUTHOR_TAG.', 'thus, feature2vec provides a useful heuristic to add interpretable feature - based information to these datasets for new words in a practical and efficient way']",1
"['by  #TAUTHOR_TAG through incorporating recall - based machine translation and part - of - speech - tagging features.', 'our system ranked first in the binary']","['by  #TAUTHOR_TAG through incorporating recall - based machine translation and part - of - speech - tagging features.', 'our system ranked first in the binary']","['by  #TAUTHOR_TAG through incorporating recall - based machine translation and part - of - speech - tagging features.', 'our system ranked first in the binary classification ( ii ) task']","['describe a machine learning based method to identify incorrect entries in translation memories.', 'it extends previous work by  #TAUTHOR_TAG through incorporating recall - based machine translation and part - of - speech - tagging features.', 'our system ranked first in the binary classification ( ii ) task for two out of three language pairs : english - italian and english - spanish']",4
['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],"['', 'in this paper, we describe our submitted system for distinguishing correct from incorrect tus.', ""rather than tailoring it to individual languages, we aimed at a languageindependent solution to cover all of the language pairs in this shared task or, looking to the future, autodesk's production environments."", 'the system is based on previous work by  #TAUTHOR_TAG and uses language - independent features with language - specific plug - ins, such as machine translation, part - of - speech tagging, and language classification.', 'specifics about previous work are given in the next section.', 'in section 3, we describe our method and, in section 4, show how it compares to  #TAUTHOR_TAG approach as well as other submissions to this shared task.', '']",4
"['by  #TAUTHOR_TAG through incorporating recall - based machine translation and part - of - speech - tagging features.', 'our system ranked first in the binary']","['by  #TAUTHOR_TAG through incorporating recall - based machine translation and part - of - speech - tagging features.', 'our system ranked first in the binary']","['by  #TAUTHOR_TAG through incorporating recall - based machine translation and part - of - speech - tagging features.', 'our system ranked first in the binary classification ( ii ) task']","['describe a machine learning based method to identify incorrect entries in translation memories.', 'it extends previous work by  #TAUTHOR_TAG through incorporating recall - based machine translation and part - of - speech - tagging features.', 'our system ranked first in the binary classification ( ii ) task for two out of three language pairs : english - italian and english - spanish']",6
['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],"['', 'in this paper, we describe our submitted system for distinguishing correct from incorrect tus.', ""rather than tailoring it to individual languages, we aimed at a languageindependent solution to cover all of the language pairs in this shared task or, looking to the future, autodesk's production environments."", 'the system is based on previous work by  #TAUTHOR_TAG and uses language - independent features with language - specific plug - ins, such as machine translation, part - of - speech tagging, and language classification.', 'specifics about previous work are given in the next section.', 'in section 3, we describe our method and, in section 4, show how it compares to  #TAUTHOR_TAG approach as well as other submissions to this shared task.', '']",6
['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG'],"['', 'in this paper, we describe our submitted system for distinguishing correct from incorrect tus.', ""rather than tailoring it to individual languages, we aimed at a languageindependent solution to cover all of the language pairs in this shared task or, looking to the future, autodesk's production environments."", 'the system is based on previous work by  #TAUTHOR_TAG and uses language - independent features with language - specific plug - ins, such as machine translation, part - of - speech tagging, and language classification.', 'specifics about previous work are given in the next section.', 'in section 3, we describe our method and, in section 4, show how it compares to  #TAUTHOR_TAG approach as well as other submissions to this shared task.', '']",3
"[' #TAUTHOR_TAG.', 'with our participation to']","[' #TAUTHOR_TAG.', 'with our participation to']","['a neglected research area ""  #TAUTHOR_TAG.', 'with our participation to']","['cleaning functionality in commercial tools is mostly rule - based, centering around the removal of duplicate entries, ensuring markup validity ( e. g., no unclosed tags ), or controlling for client or project specific terminology.', 'although helpful, these methods fall short of identifying spurious entries that contain language errors or partial translations.', 'with crowd - sourced and automatically constructed tms in particular, it is also necessary to identify translation units with source and target segments that do not correspond at all ( e. g.,  #AUTHOR_TAG.', ' #AUTHOR_TAG has proposed to cast the identification of such incorrect translations as a supervised classification problem.', 'in his work, 1, 243 labelled tus were used to train binary classifiers based on 17 features.', 'the "" most important "" of them, according to the author, were bisegment _ similarity and lang _ diff : the former is defined as the cosine similarity between a target segment and its machine translated source segment, while the latter denotes whether the language codes declared in a translation unit correspond with the codes detected by a language detector.', 'the best classifier, a support vector machine with linear kernel, achieved 82 % precision and 81 % recall on a held - out test set of 309 tus.', 'to the best of our knowledge, barbu provided the first and so far only research contribution on automatic tm cleaning, which the author himself described as "" a neglected research area ""  #TAUTHOR_TAG.', 'with our participation to this shared task, we seek to extend his work by examining new features based on statistical mt and pos tagging.', 'as outlined above, comparing machine translated source segments to their actual target segments has proven effective in  #TAUTHOR_TAG experiments.', 'we propose to complement or replace the similarity function used for this comparison ( cosine similarity ) by two automatic mt evaluation metrics, bleu  #AUTHOR_TAG and characterbased levenshtein distance, in order to reward higher - order n - gram ( n > 1 ) and partial word overlaps, respectively.', 'furthermore, we introduce a recall - based mt feature that takes multiple mt hypotheses ( n - best translations ) of a given source segment into account, based on the assumption that alternative translations of words ( such as "" buy "" and "" purchase "" ) or phrases ( such as "" despite "" and "" in spite of "" ) should not be punished.', 'we also experiment with part - of - speech information to identify spurious translation units.', 'with closely related languages in particular, the rationale would be that adjectives ( to name an example ) in a source segment are likely to be reflected in the corresponding target segment in case of a valid translation']",3
"['feature extraction pipeline, including  #TAUTHOR_TAG as well as our own features ( see section 3. 1 ), is implemented in scala.', 'this pipeline is used to']","['feature extraction pipeline, including  #TAUTHOR_TAG as well as our own features ( see section 3. 1 ), is implemented in scala.', 'this pipeline is used to']","['feature extraction pipeline, including  #TAUTHOR_TAG as well as our own features ( see section 3. 1 ), is implemented in scala.', 'this pipeline is used to']","['feature extraction pipeline, including  #TAUTHOR_TAG as well as our own features ( see section 3. 1 ), is implemented in scala.', 'this pipeline is used to transform translation units into feature vectors and train classifiers using the scikitlearn framework  #AUTHOR_TAG.', 'from the various classification algorithms we tested, random forests performed best with our selection of features ( see below )']",3
"['chars ( as described in section 3. 1 ), alongside cg _ score, only _ capletters _ dif, and punctuation _ similarity ( from  #TAUTHOR_TAG.', 'evaluation results are given in the next section']","['the following to be most successful : ratio _ words, pos _ sim _ all, language _ detection, mt _ cfs, mt _ bleu, ratio _ chars ( as described in section 3. 1 ), alongside cg _ score, only _ capletters _ dif, and punctuation _ similarity ( from  #TAUTHOR_TAG.', 'evaluation results are given in the next section']","['. e., the binary classification ( ii ) task - by optimising the weighted f 1 - score ( f 1 ) on training data ( see tables 2a and 2b ).', 'from the various feature combinations we tested, we found the following to be most successful : ratio _ words, pos _ sim _ all, language _ detection, mt _ cfs, mt _ bleu, ratio _ chars ( as described in section 3. 1 ), alongside cg _ score, only _ capletters _ dif, and punctuation _ similarity ( from  #TAUTHOR_TAG.', 'evaluation results are given in the next section']","['the reasons mentioned in section 1, we aimed at finding a combination of features that would perform well with all language pairs rather than tailoring solutions to individual languages.', 'we focused on gearing our classifiers to distinguish correct or almost correct ( classes 1, 2 ) from incorrect tus ( class 3 ) - i. e., the binary classification ( ii ) task - by optimising the weighted f 1 - score ( f 1 ) on training data ( see tables 2a and 2b ).', 'from the various feature combinations we tested, we found the following to be most successful : ratio _ words, pos _ sim _ all, language _ detection, mt _ cfs, mt _ bleu, ratio _ chars ( as described in section 3. 1 ), alongside cg _ score, only _ capletters _ dif, and punctuation _ similarity ( from  #TAUTHOR_TAG.', 'evaluation results are given in the next section']",3
"['by  #TAUTHOR_TAG ( baseline 2 ).', 'more importantly, however, we compared our']","['by  #TAUTHOR_TAG ( baseline 2 ).', 'more importantly, however, we compared our']","['by  #TAUTHOR_TAG ( baseline 2 ).', 'more importantly, however, we compared our']","['rationale for focusing on telling apart correct or almost correct from incorrect tus was that a first application of our method, if successful, would most likely be the filtering of tm data for mt training.', 'while eliminating almost correct tus might decrease rather than increase mt quality, filtering out incorrect segments can have a positive impact  #AUTHOR_TAG.', 'prior to submission, we benchmarked our system against the two baselines provided by the organizers : a dummy classifier assigning random classes according to the overall class distribution in the training data ( baseline 1 ), and a classifier based on the church - gale algorithm as adapted by  #TAUTHOR_TAG ( baseline 2 ).', 'more importantly, however, we compared our system to  #TAUTHOR_TAG approach, using the classification algorithms which reportedly worked best with the 17 features in his work.', ""our system performed well in this comparison, surpassing barbu's approach in all language pairs except en - de, where both systems were en par."", 'details are shown in table 2a, where we report weighted precision ( p ), recall ( r ), and f 1 - scores averaged over 5 - fold cross - validation with 2 / 3 - 1 / 3 splits of the training data.', 'the final evaluation and ranking produced by the organizers, shown in table 3a, confirms our findings from experimenting with training data : our system performs well on the en - es and en - it test sets ( best in class ), while performance is substantially lower on the en - de test set.', 'the reasons for this are yet to be ascertained ( see also section 5 )']",3
"['by  #TAUTHOR_TAG ( baseline 2 ).', 'more importantly, however, we compared our']","['by  #TAUTHOR_TAG ( baseline 2 ).', 'more importantly, however, we compared our']","['by  #TAUTHOR_TAG ( baseline 2 ).', 'more importantly, however, we compared our']","['rationale for focusing on telling apart correct or almost correct from incorrect tus was that a first application of our method, if successful, would most likely be the filtering of tm data for mt training.', 'while eliminating almost correct tus might decrease rather than increase mt quality, filtering out incorrect segments can have a positive impact  #AUTHOR_TAG.', 'prior to submission, we benchmarked our system against the two baselines provided by the organizers : a dummy classifier assigning random classes according to the overall class distribution in the training data ( baseline 1 ), and a classifier based on the church - gale algorithm as adapted by  #TAUTHOR_TAG ( baseline 2 ).', 'more importantly, however, we compared our system to  #TAUTHOR_TAG approach, using the classification algorithms which reportedly worked best with the 17 features in his work.', ""our system performed well in this comparison, surpassing barbu's approach in all language pairs except en - de, where both systems were en par."", 'details are shown in table 2a, where we report weighted precision ( p ), recall ( r ), and f 1 - scores averaged over 5 - fold cross - validation with 2 / 3 - 1 / 3 splits of the training data.', 'the final evaluation and ranking produced by the organizers, shown in table 3a, confirms our findings from experimenting with training data : our system performs well on the en - es and en - it test sets ( best in class ), while performance is substantially lower on the en - de test set.', 'the reasons for this are yet to be ascertained ( see also section 5 )']",3
"['to  #TAUTHOR_TAG method, using 2']","['to  #TAUTHOR_TAG method, using 2 / 3 - 1 / 3 splits of the training']","['##ed to the binary classification ( ii ) task ( see above ), we also assessed our system on the fine - grained classification task.', 'here, the goal was to distinguish between all of the three classes, i. e., determine whether a tu is correct, almost correct, or incorrect.', ""again, we compared our system's performance to  #TAUTHOR_TAG method, using 2""]","['geared to the binary classification ( ii ) task ( see above ), we also assessed our system on the fine - grained classification task.', 'here, the goal was to distinguish between all of the three classes, i. e., determine whether a tu is correct, almost correct, or incorrect.', ""again, we compared our system's performance to  #TAUTHOR_TAG method, using 2 / 3 - 1 / 3 splits of the training data ( 5 - fold cross - validation )."", 'the results, shown in table 2b, implied that the nine features we selected would not suffice for a more fine - grained classification of tus.', 'this was confirmed in the official evaluation and ranking : our system scored low on en - de and mediocre on en - es and en - it.', 'further work will be needed to analyse these results in more detail']",3
"[' #TAUTHOR_TAG.', 'with our participation to']","[' #TAUTHOR_TAG.', 'with our participation to']","['a neglected research area ""  #TAUTHOR_TAG.', 'with our participation to']","['cleaning functionality in commercial tools is mostly rule - based, centering around the removal of duplicate entries, ensuring markup validity ( e. g., no unclosed tags ), or controlling for client or project specific terminology.', 'although helpful, these methods fall short of identifying spurious entries that contain language errors or partial translations.', 'with crowd - sourced and automatically constructed tms in particular, it is also necessary to identify translation units with source and target segments that do not correspond at all ( e. g.,  #AUTHOR_TAG.', ' #AUTHOR_TAG has proposed to cast the identification of such incorrect translations as a supervised classification problem.', 'in his work, 1, 243 labelled tus were used to train binary classifiers based on 17 features.', 'the "" most important "" of them, according to the author, were bisegment _ similarity and lang _ diff : the former is defined as the cosine similarity between a target segment and its machine translated source segment, while the latter denotes whether the language codes declared in a translation unit correspond with the codes detected by a language detector.', 'the best classifier, a support vector machine with linear kernel, achieved 82 % precision and 81 % recall on a held - out test set of 309 tus.', 'to the best of our knowledge, barbu provided the first and so far only research contribution on automatic tm cleaning, which the author himself described as "" a neglected research area ""  #TAUTHOR_TAG.', 'with our participation to this shared task, we seek to extend his work by examining new features based on statistical mt and pos tagging.', 'as outlined above, comparing machine translated source segments to their actual target segments has proven effective in  #TAUTHOR_TAG experiments.', 'we propose to complement or replace the similarity function used for this comparison ( cosine similarity ) by two automatic mt evaluation metrics, bleu  #AUTHOR_TAG and characterbased levenshtein distance, in order to reward higher - order n - gram ( n > 1 ) and partial word overlaps, respectively.', 'furthermore, we introduce a recall - based mt feature that takes multiple mt hypotheses ( n - best translations ) of a given source segment into account, based on the assumption that alternative translations of words ( such as "" buy "" and "" purchase "" ) or phrases ( such as "" despite "" and "" in spite of "" ) should not be punished.', 'we also experiment with part - of - speech information to identify spurious translation units.', 'with closely related languages in particular, the rationale would be that adjectives ( to name an example ) in a source segment are likely to be reflected in the corresponding target segment in case of a valid translation']",0
"['by  #TAUTHOR_TAG ( baseline 2 ).', 'more importantly, however, we compared our']","['by  #TAUTHOR_TAG ( baseline 2 ).', 'more importantly, however, we compared our']","['by  #TAUTHOR_TAG ( baseline 2 ).', 'more importantly, however, we compared our']","['rationale for focusing on telling apart correct or almost correct from incorrect tus was that a first application of our method, if successful, would most likely be the filtering of tm data for mt training.', 'while eliminating almost correct tus might decrease rather than increase mt quality, filtering out incorrect segments can have a positive impact  #AUTHOR_TAG.', 'prior to submission, we benchmarked our system against the two baselines provided by the organizers : a dummy classifier assigning random classes according to the overall class distribution in the training data ( baseline 1 ), and a classifier based on the church - gale algorithm as adapted by  #TAUTHOR_TAG ( baseline 2 ).', 'more importantly, however, we compared our system to  #TAUTHOR_TAG approach, using the classification algorithms which reportedly worked best with the 17 features in his work.', ""our system performed well in this comparison, surpassing barbu's approach in all language pairs except en - de, where both systems were en par."", 'details are shown in table 2a, where we report weighted precision ( p ), recall ( r ), and f 1 - scores averaged over 5 - fold cross - validation with 2 / 3 - 1 / 3 splits of the training data.', 'the final evaluation and ranking produced by the organizers, shown in table 3a, confirms our findings from experimenting with training data : our system performs well on the en - es and en - it test sets ( best in class ), while performance is substantially lower on the en - de test set.', 'the reasons for this are yet to be ascertained ( see also section 5 )']",5
"['be easily captured in twitter conversations  #TAUTHOR_TAG. in this paper, our main']","['be easily captured in twitter conversations  #TAUTHOR_TAG. in this paper, our main']","['be easily captured in twitter conversations  #TAUTHOR_TAG. in this paper, our main']","['', 'unimportant. we can further argue that commonsense drives an adaptation in extracting knowledge. to measure commonsense for a particular situation is hard, however, adaptations can be easily captured in twitter conversations  #TAUTHOR_TAG. in this paper, our main concerns are firstly to construct discussion groups including agents', 'having different social powers and serving opposite aims. secondly, we investigate how we can track the progress of opinions together with their influences on decisions in oral conversations. we claim that', 'linguistic relations ( poria et al. 2015 ) preserve all rich phenomena, shortly discussed above, including collective', 'voice, reshaping', 'arguments, and so adaptation. to analyze adaptation induced by both cooperation and competition, we consider court conversations : they are held in clearly stated winner and loser groups with distinct hierarchy in', 'decisionmaking due to the presence of justices and', 'lawyers. to this end, we evaluate the open access data of the united states supreme court ( hawes, lin, and', 'resnik 2009 ; hawes 2009 ;  #TAUTHOR_TAG, prepare conversation groups with different adaptation levels, implement a suitable algorithm to extract linguistic relations in these group conversations, and', 'finally provide a comparison between the groups and the discovered linguistic relations. the rest of the paper is organized as follows : the first section presents the dataset we consider and designed conversation groups out', 'of the data ; the second section describes our algorithm in detail ; the following section explains how we implement pointwise mutual information for the conversation groups and then link with linguistic', '']",0
"['be easily captured in twitter conversations  #TAUTHOR_TAG. in this paper, our main']","['be easily captured in twitter conversations  #TAUTHOR_TAG. in this paper, our main']","['be easily captured in twitter conversations  #TAUTHOR_TAG. in this paper, our main']","['', 'unimportant. we can further argue that commonsense drives an adaptation in extracting knowledge. to measure commonsense for a particular situation is hard, however, adaptations can be easily captured in twitter conversations  #TAUTHOR_TAG. in this paper, our main concerns are firstly to construct discussion groups including agents', 'having different social powers and serving opposite aims. secondly, we investigate how we can track the progress of opinions together with their influences on decisions in oral conversations. we claim that', 'linguistic relations ( poria et al. 2015 ) preserve all rich phenomena, shortly discussed above, including collective', 'voice, reshaping', 'arguments, and so adaptation. to analyze adaptation induced by both cooperation and competition, we consider court conversations : they are held in clearly stated winner and loser groups with distinct hierarchy in', 'decisionmaking due to the presence of justices and', 'lawyers. to this end, we evaluate the open access data of the united states supreme court ( hawes, lin, and', 'resnik 2009 ; hawes 2009 ;  #TAUTHOR_TAG, prepare conversation groups with different adaptation levels, implement a suitable algorithm to extract linguistic relations in these group conversations, and', 'finally provide a comparison between the groups and the discovered linguistic relations. the rest of the paper is organized as follows : the first section presents the dataset we consider and designed conversation groups out', 'of the data ; the second section describes our algorithm in detail ; the following section explains how we implement pointwise mutual information for the conversation groups and then link with linguistic', '']",0
"['updated version used here are publicly available  #TAUTHOR_TAG.', 'the data gathers oral speech']","['updated version used here are publicly available  #TAUTHOR_TAG.', 'the data gathers oral speeches before the']","['updated version used here are publicly available  #TAUTHOR_TAG.', 'the data gathers oral speeches before']","['borrow the textual data of the conversations in the united states supreme court pre - processed by ( hawes, lin, and resnik 2009 ; hawes 2009 ) and enriched by ( danescuniculescu - mizil et al. 2012 ) including the final votes of justices.', 'both the original data and the most updated version used here are publicly available  #TAUTHOR_TAG.', 'the data gathers oral speeches before the supreme court and hosts 50, 389 conversational exchanges among justices and lawyers.', 'distinct hierarchy between justices ( high power ) and lawyers ( low power ) impose lawyers to tune their arguments under the perspective and understandings of justices, and as a result, speech adaptation and linguistic coordination leaves their traces in a sudden occurrence of sharing the same adverbs, conjunctions, and pronouns.', ""tracking initial utterances, the sides present a unique and personal speaking, but after a while in the communication, word selections, their forms, and frequencies mirror each other's language preference."", 'the linguistic coordination is systematically quantified by  #TAUTHOR_TAG and the arguments follow the principles of exchange theory examining behavior dynamics in low and high power groups ( willer 1999 ; thye, willer, and markovsky 2006 ) : lawyers tend to cooperate more to justices than conversely and demonstrate strong linguistic coordination in their speech.', '']",0
"['updated version used here are publicly available  #TAUTHOR_TAG.', 'the data gathers oral speech']","['updated version used here are publicly available  #TAUTHOR_TAG.', 'the data gathers oral speeches before the']","['updated version used here are publicly available  #TAUTHOR_TAG.', 'the data gathers oral speeches before']","['borrow the textual data of the conversations in the united states supreme court pre - processed by ( hawes, lin, and resnik 2009 ; hawes 2009 ) and enriched by ( danescuniculescu - mizil et al. 2012 ) including the final votes of justices.', 'both the original data and the most updated version used here are publicly available  #TAUTHOR_TAG.', 'the data gathers oral speeches before the supreme court and hosts 50, 389 conversational exchanges among justices and lawyers.', 'distinct hierarchy between justices ( high power ) and lawyers ( low power ) impose lawyers to tune their arguments under the perspective and understandings of justices, and as a result, speech adaptation and linguistic coordination leaves their traces in a sudden occurrence of sharing the same adverbs, conjunctions, and pronouns.', ""tracking initial utterances, the sides present a unique and personal speaking, but after a while in the communication, word selections, their forms, and frequencies mirror each other's language preference."", 'the linguistic coordination is systematically quantified by  #TAUTHOR_TAG and the arguments follow the principles of exchange theory examining behavior dynamics in low and high power groups ( willer 1999 ; thye, willer, and markovsky 2006 ) : lawyers tend to cooperate more to justices than conversely and demonstrate strong linguistic coordination in their speech.', '']",6
"['##er, and markovsky 2006 ) and the measured coordination  #TAUTHOR_TAG, one can order']","['1999 ; thye, willer, and markovsky 2006 ) and the measured coordination  #TAUTHOR_TAG, one can order']","['thye, willer, and markovsky 2006 ) and the measured coordination  #TAUTHOR_TAG, one can order']","['- respondent side table 1 : the segregation schema of the roles in conversations : support sides of justices and sides of lawyers.', '1 - 6 summarize all potential roles present in the data.', 'in 1 - 4, who supported by the justice is given in the middle.', 'furthermore, the last indicates the side of lawyer the justice speaks to.', 'referring exchange theory ( willer 1999 ; thye, willer, and markovsky 2006 ) and the measured coordination  #TAUTHOR_TAG, one can order the relative power of each justice and lawyer pair', '']",4
"['rare presence in the other conversation groups.', 'unlike the previous study  #TAUTHOR_TAG, entirely tracking back and forth utterances and proving the adaptation,']","['rare presence in the other conversation groups.', 'unlike the previous study  #TAUTHOR_TAG, entirely tracking back and forth utterances and proving the adaptation, e. g., linguistic coordination, by identifying the frequency of selected keywords, we directly utilize their overall conclusion and claim that linguistic relations']","['rare presence in the other conversation groups.', 'unlike the previous study  #TAUTHOR_TAG, entirely tracking back and forth utterances and proving the adaptation,']","['##wise mutual information ( pmi ) is a metric to measure coincidence of two discrete random events.', 'it combines individual probabilities of events and their joined probability to determine how often the two events occur at the same occasion.', 'we quantify to what extend linguistic relations r are addressed by conversation groups κ and whether we observe any variation in the selections.', 'to this end, pmi between r and κ is introduced ( pantel, ravichandran, and hovy 2004 )', 'here, f ( r, κ ) represents the frequency of occurrence for certain r in particular κ and n is the total number of all r in all κ.', 'so, while the numerator describes the probabilistic occurrence of r in κ, the denominator provides individual probability of r and that of κ in the pool.', 'we expect high m i ( r, κ ) while r appears in a specific κ and that is an indicator of its rare presence in the other conversation groups.', 'unlike the previous study  #TAUTHOR_TAG, entirely tracking back and forth utterances and proving the adaptation, e. g., linguistic coordination, by identifying the frequency of selected keywords, we directly utilize their overall conclusion and claim that linguistic relations already preserve the adaptation and any other complex collective linguistic process induced by both cooperation and competition in different power groups.', 'we expect that the variation in m i ( r, κ ) of gathered utterances of each relative power group, independent of the utterance order, suggests which relations can distinguish the difference in the groups and the magnitude of m i ( r, κ ) of that difference highlights which relative power groups drastically influence the applied language.', 'we will analyze m i ( r, κ ) following this discussed understanding in coming section']",4
"['a model.', 'in contrast,  #TAUTHOR_TAG make a study involving training a model but use']","['a model.', 'in contrast,  #TAUTHOR_TAG make a study involving training a model but use']","['a model.', 'in contrast,  #TAUTHOR_TAG make a study involving training a model but use']","['', 'the question is what quality can be obtained using such a noisy training set.', 'to the best of our knowledge, we cannot find the answer for french in literature.', 'indeed, compares the performance of mwetoolkit with another toolkit on english and french corpora, but they never use the data generated by mwetoolkit to train a model.', 'in contrast,  #TAUTHOR_TAG make a study involving training a model but use it only on english and use extra lexical resources to complement the machine learning method, so their study does not focus just on classifier evaluation.', 'this paper presents the first evaluation of mwetoolkit on french together with two resources very commonly used by the french nlp community : the tagger treetagger  #AUTHOR_TAG and the dictionary dela.', '1 training and test data are taken from the french europarl corpus  #AUTHOR_TAG and classifiers are trained using the weka machine learning toolkit  #AUTHOR_TAG.', 'the primary goal is to evaluate what level of precision can be achieved for nominal mwes, using a manual evaluation of mwes extracted, and to what extent the mwes extracted are novel and can be used to']",0
"['system with the association measures as features with a minimum of implementation.', ' #AUTHOR_TAG b ) provide experiments on portuguese, english and greek.', ' #TAUTHOR_TAG provide experiments with this']","['system with the association measures as features with a minimum of implementation.', ' #AUTHOR_TAG b ) provide experiments on portuguese, english and greek.', ' #TAUTHOR_TAG provide experiments with this']","['system with the association measures as features with a minimum of implementation.', ' #AUTHOR_TAG b ) provide experiments on portuguese, english and greek.', ' #TAUTHOR_TAG provide experiments with this tool as well.', 'in the latter study, after having trained a machine on bigram mwes, they try to']","['the tools developed for extracting mwes, mwetoolkit is one of the most recent.', 'developed by  #AUTHOR_TAG b ) it aims not only at extracting candidates for potential mwes, but also at extracting their association measures.', 'provided that a lexicon of mwes is available and provided a preprocessed corpus, mwetoolkit makes it possible to train a machine learning system with the association measures as features with a minimum of implementation.', ' #AUTHOR_TAG b ) provide experiments on portuguese, english and greek.', ' #TAUTHOR_TAG provide experiments with this tool as well.', 'in the latter study, after having trained a machine on bigram mwes, they try to extract full n - gram expressions from the europarl corpus.', 'they then reuse the model obtained on bigrams for extraction of full n - gram mwes.', 'finally, they apply a second filter for getting back the false negatives by checking every mwe annotated as false by the algorithm against a online dictionary.', 'this method gets a very good precision ( over 87 % ) and recall ( over 84 % ).', 'however, we do not really know if this result is mostly due to the coverage of the dictionary online. what is the contribution of machine learning in itself?', 'another question raised by this study is the ability of a machine trained on one kind of pattern ( e. g., noun - adjective ) to extract correctly another kind of mwe pattern ( e. g., noun - noun ).', 'that is the reason why we will run three experiments close to the one of  #TAUTHOR_TAG but were the only changing parameter is the pattern that we train our classifiers on']",0
"['a model.', 'in contrast,  #TAUTHOR_TAG make a study involving training a model but use']","['a model.', 'in contrast,  #TAUTHOR_TAG make a study involving training a model but use']","['a model.', 'in contrast,  #TAUTHOR_TAG make a study involving training a model but use']","['', 'the question is what quality can be obtained using such a noisy training set.', 'to the best of our knowledge, we cannot find the answer for french in literature.', 'indeed, compares the performance of mwetoolkit with another toolkit on english and french corpora, but they never use the data generated by mwetoolkit to train a model.', 'in contrast,  #TAUTHOR_TAG make a study involving training a model but use it only on english and use extra lexical resources to complement the machine learning method, so their study does not focus just on classifier evaluation.', 'this paper presents the first evaluation of mwetoolkit on french together with two resources very commonly used by the french nlp community : the tagger treetagger  #AUTHOR_TAG and the dictionary dela.', '1 training and test data are taken from the french europarl corpus  #AUTHOR_TAG and classifiers are trained using the weka machine learning toolkit  #AUTHOR_TAG.', 'the primary goal is to evaluate what level of precision can be achieved for nominal mwes, using a manual evaluation of mwes extracted, and to what extent the mwes extracted are novel and can be used to']",1
"['system with the association measures as features with a minimum of implementation.', ' #AUTHOR_TAG b ) provide experiments on portuguese, english and greek.', ' #TAUTHOR_TAG provide experiments with this']","['system with the association measures as features with a minimum of implementation.', ' #AUTHOR_TAG b ) provide experiments on portuguese, english and greek.', ' #TAUTHOR_TAG provide experiments with this']","['system with the association measures as features with a minimum of implementation.', ' #AUTHOR_TAG b ) provide experiments on portuguese, english and greek.', ' #TAUTHOR_TAG provide experiments with this tool as well.', 'in the latter study, after having trained a machine on bigram mwes, they try to']","['the tools developed for extracting mwes, mwetoolkit is one of the most recent.', 'developed by  #AUTHOR_TAG b ) it aims not only at extracting candidates for potential mwes, but also at extracting their association measures.', 'provided that a lexicon of mwes is available and provided a preprocessed corpus, mwetoolkit makes it possible to train a machine learning system with the association measures as features with a minimum of implementation.', ' #AUTHOR_TAG b ) provide experiments on portuguese, english and greek.', ' #TAUTHOR_TAG provide experiments with this tool as well.', 'in the latter study, after having trained a machine on bigram mwes, they try to extract full n - gram expressions from the europarl corpus.', 'they then reuse the model obtained on bigrams for extraction of full n - gram mwes.', 'finally, they apply a second filter for getting back the false negatives by checking every mwe annotated as false by the algorithm against a online dictionary.', 'this method gets a very good precision ( over 87 % ) and recall ( over 84 % ).', 'however, we do not really know if this result is mostly due to the coverage of the dictionary online. what is the contribution of machine learning in itself?', 'another question raised by this study is the ability of a machine trained on one kind of pattern ( e. g., noun - adjective ) to extract correctly another kind of mwe pattern ( e. g., noun - noun ).', 'that is the reason why we will run three experiments close to the one of  #TAUTHOR_TAG but were the only changing parameter is the pattern that we train our classifiers on']",1
"['system with the association measures as features with a minimum of implementation.', ' #AUTHOR_TAG b ) provide experiments on portuguese, english and greek.', ' #TAUTHOR_TAG provide experiments with this']","['system with the association measures as features with a minimum of implementation.', ' #AUTHOR_TAG b ) provide experiments on portuguese, english and greek.', ' #TAUTHOR_TAG provide experiments with this']","['system with the association measures as features with a minimum of implementation.', ' #AUTHOR_TAG b ) provide experiments on portuguese, english and greek.', ' #TAUTHOR_TAG provide experiments with this tool as well.', 'in the latter study, after having trained a machine on bigram mwes, they try to']","['the tools developed for extracting mwes, mwetoolkit is one of the most recent.', 'developed by  #AUTHOR_TAG b ) it aims not only at extracting candidates for potential mwes, but also at extracting their association measures.', 'provided that a lexicon of mwes is available and provided a preprocessed corpus, mwetoolkit makes it possible to train a machine learning system with the association measures as features with a minimum of implementation.', ' #AUTHOR_TAG b ) provide experiments on portuguese, english and greek.', ' #TAUTHOR_TAG provide experiments with this tool as well.', 'in the latter study, after having trained a machine on bigram mwes, they try to extract full n - gram expressions from the europarl corpus.', 'they then reuse the model obtained on bigrams for extraction of full n - gram mwes.', 'finally, they apply a second filter for getting back the false negatives by checking every mwe annotated as false by the algorithm against a online dictionary.', 'this method gets a very good precision ( over 87 % ) and recall ( over 84 % ).', 'however, we do not really know if this result is mostly due to the coverage of the dictionary online. what is the contribution of machine learning in itself?', 'another question raised by this study is the ability of a machine trained on one kind of pattern ( e. g., noun - adjective ) to extract correctly another kind of mwe pattern ( e. g., noun - noun ).', 'that is the reason why we will run three experiments close to the one of  #TAUTHOR_TAG but were the only changing parameter is the pattern that we train our classifiers on']",5
"['preprocessing we used the same processes as described in  #TAUTHOR_TAG.', 'first we ran the sentence splitter and the tokenizer provided with the europarl corpus.', 'then we ran treetagger  #AUTHOR_TAG to']","['preprocessing we used the same processes as described in  #TAUTHOR_TAG.', 'first we ran the sentence splitter and the tokenizer provided with the europarl corpus.', 'then we ran treetagger  #AUTHOR_TAG to']","['preprocessing we used the same processes as described in  #TAUTHOR_TAG.', 'first we ran the sentence splitter and the tokenizer provided with the europarl corpus.', 'then we ran treetagger  #AUTHOR_TAG to']","['preprocessing we used the same processes as described in  #TAUTHOR_TAG.', 'first we ran the sentence splitter and the tokenizer provided with the europarl corpus.', 'then we ran treetagger  #AUTHOR_TAG to obtain the tags and the lemmas']",5
"['well as the training options suggested by  #TAUTHOR_TAG.', 'we also tried to remove some features']","['well as the training options suggested by  #TAUTHOR_TAG.', 'we also tried to remove some features']","['well as the training options suggested by  #TAUTHOR_TAG.', 'we also tried to remove some features']","['tested several algorithms offered by weka as well as the training options suggested by  #TAUTHOR_TAG.', 'we also tried to remove some features and to keep only the most informative ones ( mle, t - score and log - likelihood according to information gain ratio ) but we noticed each time a loss in the recall.', 'at the end with all the features kept and for the purpose of evaluating na mwe candidates the best classification algorithm was the bayesian network']",5
"['contrast to  #TAUTHOR_TAG we run our experiment on french.', 'the choice of a different language requires an adaptation of the patterns.', 'french indeed, as a latin language, does not show the']","['contrast to  #TAUTHOR_TAG we run our experiment on french.', 'the choice of a different language requires an adaptation of the patterns.', 'french indeed, as a latin language, does not show the']","['contrast to  #TAUTHOR_TAG we run our experiment on french.', 'the choice of a different language requires an adaptation of the patterns.', 'french indeed, as a latin language, does not show the same characteristic patterns as english.', 'we know that there is a strong recurrence of the pattern noun - adjective in bigram mwes in our lexicon ( silberztein and']","['contrast to  #TAUTHOR_TAG we run our experiment on french.', 'the choice of a different language requires an adaptation of the patterns.', 'french indeed, as a latin language, does not show the same characteristic patterns as english.', 'we know that there is a strong recurrence of the pattern noun - adjective in bigram mwes in our lexicon ( silberztein and l. a. d. l., 1990, p. 82 ), and the next most frequent pattern is nounnoun.', 'therefore we extract only candidates that correspond to these patterns.', 'and, since we have two patterns, we will run two extra experiments where our models will be trained only on one of the patterns.', 'in this way, we will discover how sensitive the method is to the choice of pattern']",4
"['of  #TAUTHOR_TAG,']","['of  #TAUTHOR_TAG,']","['of  #TAUTHOR_TAG,']","['comprehension of text is the overarching goal of a great deal of research in natural language processing.', 'the machine comprehension test  #AUTHOR_TAG was recently proposed to assess methods on an open - domain, extensible, and easy - to - evaluate task consisting of two datasets.', 'in this paper we develop a lexical matching method that takes into account multiple context windows, question types and coreference resolution.', 'we show that the proposed method outperforms the baseline of  #TAUTHOR_TAG, and despite its relative simplicity, is comparable to recent work using machine learning.', 'we hope that our approach will inform future work on this task.', 'furthermore, we argue that mc500 is harder than mc160 due to the way question answer pairs were created']",4
"[',  #TAUTHOR_TAG proposed']","['of text.', 'to this end,  #TAUTHOR_TAG proposed']","['on each task individually, rather than on overall progress towards machine comprehension of text.', 'to this end,  #TAUTHOR_TAG proposed']","['', 'however, these tasks assess performance on each task individually, rather than on overall progress towards machine comprehension of text.', 'to this end,  #TAUTHOR_TAG proposed the machine comprehension test ( mctest ), a new challenge that aims at evaluating machine comprehension.', 'it does so through an opendomain multiple - choice question answering task on fictional stories requiring the common sense reasoning typical of a 7 - year - old child.', 'it is easy to evaluate as it consists of multiple choice questions.', ' #TAUTHOR_TAG also showed how the creation of stories and questions can be crowdsourced efficiently, constructing two datasets for the task, namely mc160 and mc500.', 'in addition, the authors presented a lexical matching baseline which is combined with the textual entailment recognition system biutee  #AUTHOR_TAG.', 'in this paper we develop an approach based on lexical matching which we extend by taking into account the type of the question and coreference resolution.', '']",4
['of  #TAUTHOR_TAG by 4 and 3 points in'],['of  #TAUTHOR_TAG by 4 and 3 points in'],['proposed baseline outperforms the baseline of  #TAUTHOR_TAG by 4 and 3 points in'],"['evaluated our system on mc160 and mc500 test sets and the results are shown in table 2.', 'our proposed baseline outperforms the baseline of  #TAUTHOR_TAG by 4 and 3 points in accuracy on mc160 and mc500 respectively.', '3 our system is comparable to the msr baseline with the rte system biutee  #AUTHOR_TAG.', 'if we linearly combine the rte scores used in the msr baseline with our method, we achieve 5 and 2. 5 accuracy points higher than the best results achieved by  #TAUTHOR_TAG.', 'concurrently with ours, three other approaches to solving mctest were developed and subsequently published a few months before our method.', ' #AUTHOR_TAG presented a discourse - level approach, which chooses an answer by utilising relations between sentences chosen as important.', 'despite is simplicity, our method is comparable in performance, suggesting that better lexical matching could help improve their model.', ' #AUTHOR_TAG treated mctest as a structured prediction problem, searching for a latent structure connecting the question, answer and the text, dubbed the answer - entailing structure.', 'their model performs better on mc500 ( was  #AUTHOR_TAG is the most similar to ours, in the sense that they combine a baseline feature set with more advanced linguistic analyses, namely syntax, frame semantics, coreference, and word embeddings.', 'instead of a rule - based approach, they combine them through a latent - variable classifier achieving the current state - of - the - art performance on mctest']",4
['of  #TAUTHOR_TAG by 4 and 3 points in'],['of  #TAUTHOR_TAG by 4 and 3 points in'],['proposed baseline outperforms the baseline of  #TAUTHOR_TAG by 4 and 3 points in'],"['evaluated our system on mc160 and mc500 test sets and the results are shown in table 2.', 'our proposed baseline outperforms the baseline of  #TAUTHOR_TAG by 4 and 3 points in accuracy on mc160 and mc500 respectively.', '3 our system is comparable to the msr baseline with the rte system biutee  #AUTHOR_TAG.', 'if we linearly combine the rte scores used in the msr baseline with our method, we achieve 5 and 2. 5 accuracy points higher than the best results achieved by  #TAUTHOR_TAG.', 'concurrently with ours, three other approaches to solving mctest were developed and subsequently published a few months before our method.', ' #AUTHOR_TAG presented a discourse - level approach, which chooses an answer by utilising relations between sentences chosen as important.', 'despite is simplicity, our method is comparable in performance, suggesting that better lexical matching could help improve their model.', ' #AUTHOR_TAG treated mctest as a structured prediction problem, searching for a latent structure connecting the question, answer and the text, dubbed the answer - entailing structure.', 'their model performs better on mc500 ( was  #AUTHOR_TAG is the most similar to ours, in the sense that they combine a baseline feature set with more advanced linguistic analyses, namely syntax, frame semantics, coreference, and word embeddings.', 'instead of a rule - based approach, they combine them through a latent - variable classifier achieving the current state - of - the - art performance on mctest']",4
"[',  #TAUTHOR_TAG proposed']","['of text.', 'to this end,  #TAUTHOR_TAG proposed']","['on each task individually, rather than on overall progress towards machine comprehension of text.', 'to this end,  #TAUTHOR_TAG proposed']","['', 'however, these tasks assess performance on each task individually, rather than on overall progress towards machine comprehension of text.', 'to this end,  #TAUTHOR_TAG proposed the machine comprehension test ( mctest ), a new challenge that aims at evaluating machine comprehension.', 'it does so through an opendomain multiple - choice question answering task on fictional stories requiring the common sense reasoning typical of a 7 - year - old child.', 'it is easy to evaluate as it consists of multiple choice questions.', ' #TAUTHOR_TAG also showed how the creation of stories and questions can be crowdsourced efficiently, constructing two datasets for the task, namely mc160 and mc500.', 'in addition, the authors presented a lexical matching baseline which is combined with the textual entailment recognition system biutee  #AUTHOR_TAG.', 'in this paper we develop an approach based on lexical matching which we extend by taking into account the type of the question and coreference resolution.', '']",0
"[',  #TAUTHOR_TAG proposed']","['of text.', 'to this end,  #TAUTHOR_TAG proposed']","['on each task individually, rather than on overall progress towards machine comprehension of text.', 'to this end,  #TAUTHOR_TAG proposed']","['', 'however, these tasks assess performance on each task individually, rather than on overall progress towards machine comprehension of text.', 'to this end,  #TAUTHOR_TAG proposed the machine comprehension test ( mctest ), a new challenge that aims at evaluating machine comprehension.', 'it does so through an opendomain multiple - choice question answering task on fictional stories requiring the common sense reasoning typical of a 7 - year - old child.', 'it is easy to evaluate as it consists of multiple choice questions.', ' #TAUTHOR_TAG also showed how the creation of stories and questions can be crowdsourced efficiently, constructing two datasets for the task, namely mc160 and mc500.', 'in addition, the authors presented a lexical matching baseline which is combined with the textual entailment recognition system biutee  #AUTHOR_TAG.', 'in this paper we develop an approach based on lexical matching which we extend by taking into account the type of the question and coreference resolution.', '']",0
"[' #TAUTHOR_TAG proposed a sliding window algorithm that ranks', '']","[' #TAUTHOR_TAG proposed a sliding window algorithm that ranks', '']","['results. 3 scoring function  #TAUTHOR_TAG proposed a sliding window algorithm that ranks', '']","['', '500 stories respectively, with 4 questions per story, and 4 candidate answers per question ( figure 1 ).', 'all stories and questions were crowd - sourced using amazon mechanical turk. 2 mc160 was manually curated by richardson et al.,', 'while mc500 was curated by crowdworkers. both datasets are divided into training, development, and test sets. all development was conducted on the training and', 'development sets ; the test sets were used only to report the final results. 3 scoring function  #TAUTHOR_TAG proposed a sliding window algorithm that ranks', '']",0
"[' #TAUTHOR_TAG proposed a sliding window algorithm that ranks', '']","[' #TAUTHOR_TAG proposed a sliding window algorithm that ranks', '']","['results. 3 scoring function  #TAUTHOR_TAG proposed a sliding window algorithm that ranks', '']","['', '500 stories respectively, with 4 questions per story, and 4 candidate answers per question ( figure 1 ).', 'all stories and questions were crowd - sourced using amazon mechanical turk. 2 mc160 was manually curated by richardson et al.,', 'while mc500 was curated by crowdworkers. both datasets are divided into training, development, and test sets. all development was conducted on the training and', 'development sets ; the test sets were used only to report the final results. 3 scoring function  #TAUTHOR_TAG proposed a sliding window algorithm that ranks', '']",0
"['are, that two or more misleading choices', 'included.  #TAUTHOR_TAG demonstrate that the']","['are, that two or more misleading choices', 'included.  #TAUTHOR_TAG demonstrate that the']","['process of the mc500 dataset, which stip', '##ulated that answers must not be contained directly within the story text, or if they are, that two or more misleading choices', 'included.  #TAUTHOR_TAG demonstrate that the']","['', 'components such as co - reference. this is a consequence of the design and curation process of the mc500 dataset, which stip', '##ulated that answers must not be contained directly within the story text, or if they are, that two or more misleading choices', 'included.  #TAUTHOR_TAG demonstrate that the mc160 and mc500 have similar ratings for clarity and grammar, and that humans perform equally well on both. however, in many cases mc500 appears to be', 'designed in such a way to confuse lexical algorithms and encourage the use of more sophisticated techniques necessary to deal with phenomena such as elimination questions, negation, and common knowledge not explicitly written in the story']",0
"[' #TAUTHOR_TAG proposed a sliding window algorithm that ranks', '']","[' #TAUTHOR_TAG proposed a sliding window algorithm that ranks', '']","['results. 3 scoring function  #TAUTHOR_TAG proposed a sliding window algorithm that ranks', '']","['', '500 stories respectively, with 4 questions per story, and 4 candidate answers per question ( figure 1 ).', 'all stories and questions were crowd - sourced using amazon mechanical turk. 2 mc160 was manually curated by richardson et al.,', 'while mc500 was curated by crowdworkers. both datasets are divided into training, development, and test sets. all development was conducted on the training and', 'development sets ; the test sets were used only to report the final results. 3 scoring function  #TAUTHOR_TAG proposed a sliding window algorithm that ranks', '']",3
"[' #TAUTHOR_TAG proposed a sliding window algorithm that ranks', '']","[' #TAUTHOR_TAG proposed a sliding window algorithm that ranks', '']","['results. 3 scoring function  #TAUTHOR_TAG proposed a sliding window algorithm that ranks', '']","['', '500 stories respectively, with 4 questions per story, and 4 candidate answers per question ( figure 1 ).', 'all stories and questions were crowd - sourced using amazon mechanical turk. 2 mc160 was manually curated by richardson et al.,', 'while mc500 was curated by crowdworkers. both datasets are divided into training, development, and test sets. all development was conducted on the training and', 'development sets ; the test sets were used only to report the final results. 3 scoring function  #TAUTHOR_TAG proposed a sliding window algorithm that ranks', '']",5
['is  #TAUTHOR_TAG'],['is  #TAUTHOR_TAG'],['is  #TAUTHOR_TAG'],"['problem of music mood recognition is about utilizing machine learning, data mining and other techniques to automatically classify songs in 2 or more emotion categories with highest possible accuracy.', 'different combinations of features such as audio or lyrics are involved in the process.', 'in this study we make use of song lyrics exploiting the dataset described in [ 9 ] ( here am628 ).', 'the original dataset contains 771 song texts collected from allmusic portal.', 'allmusic tags and 3 human experts were used for the annotation of songs.', 'we balanced the dataset obtaining 314 positive and 314 negative lyrics.', 'we also utilize moodylyrics ( here ml3k ), a dataset of 3, 000 mood labeled songs from different genres and epochs described in [ 3 ].', 'pioneering work in movie review polarity analysis has been conducted by pang and lee in [ 14 ] and [ 13 ].', 'the authors released sentiment polarity dataset, a collection of 2, 000 movie reviews categorized as positive or negative.', 'deep learning techniques and distributed word representations appeared on recent studies like [ 17 ] where the role of rnns ( recurrent neural networks ), and cnns ( convolutional neural networks ) is explored.', 'the author reports that cnns perform best.', 'an important work that has relevance here is  #TAUTHOR_TAG where authors present an even larger movie review dataset of 50, 000 movie reviews from imbd.', 'this dataset has been used in various works such as [ 5 ], [ 16 ] etc.', 'for our experiments we used a chunk of 10k ( mr10k ) as well as the full set ( mr50k ).', 'we first cleaned and tokenized texts of the datasets.', 'the dataset of the current run is loaded and a set of unique text words is created.', 'all 14 models are also loaded in the script.', 'we train a 15th ( self w2v ) model using the corpus of the current run and skip - gram method.', 'the script iterates in every line of the pretrained models splitting apart the words and the float vectors and building { word : vec } dictionaries later used as classification feature sets.', 'next we prepare the classification models using tf - idf vectorizer which has been successfully applied in similar studies like [ 4 ].', 'instead of applying tf - idf in words only as in other text classifiers, we vectorize both word ( for semantic relevance ) and corresponding vector ( for syntactic and contextual relevance ).', 'random forest was used as classifier and 5 - fold cross - validation accuracy is computed for each of the']",0
"['text8corpus.', 'our scores on this task are somehow lower than those reported from various studies that explore advanced deep learning constructs on same dataset.', 'in  #TAUTHOR_TAG for example, authors who created movie review dataset try on it their probabilistic model that is able to capture semantic similarities between words.', 'they report a maximal']","['text8corpus.', 'our scores on this task are somehow lower than those reported from various studies that explore advanced deep learning constructs on same dataset.', 'in  #TAUTHOR_TAG for example, authors who created movie review dataset try on it their probabilistic model that is able to capture semantic similarities between words.', 'they report a maximal']","['text8corpus.', 'our scores on this task are somehow lower than those reported from various studies that explore advanced deep learning constructs on same dataset.', 'in  #TAUTHOR_TAG for example, authors who created movie review dataset try on it their probabilistic model that is able to capture semantic similarities between words.', 'they report a maximal accuracy of 0. 88.', 'a study that uses a very similar method is [ 16 ] where authors combine random forest with word vector average values.', 'on movie review dataset they achieve an accuracy of']","['', 'it is interesting to see how self w2v goes up from the last to the top, with scores edging between 0. 61 and 0. 83.', 'this model is trained with the data of each experiment and depends on the size of that dataset which grows significantly ( see table 2 ).', 'we see that accuracy values we got here are in line with reports from other similar works such as [ 6 ] where they use a dataset of 1032 lyrics from allmusic to perform content analysis with text features.', 'accuracy scores for movie review polarity prediction are presented in figures 3 and 4.', 'again we see that crawl 840 performs very well.', 'google news is also among the top whereas twitter models are positioned in the middle of the list.', 'once again self w2v grows considerably, this time from the 3rd place to the top.', 'on mr50k it has a discrete margin of more than 0. 03 from the 2nd position.', 'again wikigiga models are positioned in the middle of the list and the worst performing models are moodycorpus and text8corpus.', 'our scores on this task are somehow lower than those reported from various studies that explore advanced deep learning constructs on same dataset.', 'in  #TAUTHOR_TAG for example, authors who created movie review dataset try on it their probabilistic model that is able to capture semantic similarities between words.', 'they report a maximal accuracy of 0. 88.', 'a study that uses a very similar method is [ 16 ] where authors combine random forest with word vector average values.', 'on movie review dataset they achieve an accuracy of 0. 84 which is about what we got here']",0
"['text8corpus.', 'our scores on this task are somehow lower than those reported from various studies that explore advanced deep learning constructs on same dataset.', 'in  #TAUTHOR_TAG for example, authors who created movie review dataset try on it their probabilistic model that is able to capture semantic similarities between words.', 'they report a maximal']","['text8corpus.', 'our scores on this task are somehow lower than those reported from various studies that explore advanced deep learning constructs on same dataset.', 'in  #TAUTHOR_TAG for example, authors who created movie review dataset try on it their probabilistic model that is able to capture semantic similarities between words.', 'they report a maximal']","['text8corpus.', 'our scores on this task are somehow lower than those reported from various studies that explore advanced deep learning constructs on same dataset.', 'in  #TAUTHOR_TAG for example, authors who created movie review dataset try on it their probabilistic model that is able to capture semantic similarities between words.', 'they report a maximal accuracy of 0. 88.', 'a study that uses a very similar method is [ 16 ] where authors combine random forest with word vector average values.', 'on movie review dataset they achieve an accuracy of']","['', 'it is interesting to see how self w2v goes up from the last to the top, with scores edging between 0. 61 and 0. 83.', 'this model is trained with the data of each experiment and depends on the size of that dataset which grows significantly ( see table 2 ).', 'we see that accuracy values we got here are in line with reports from other similar works such as [ 6 ] where they use a dataset of 1032 lyrics from allmusic to perform content analysis with text features.', 'accuracy scores for movie review polarity prediction are presented in figures 3 and 4.', 'again we see that crawl 840 performs very well.', 'google news is also among the top whereas twitter models are positioned in the middle of the list.', 'once again self w2v grows considerably, this time from the 3rd place to the top.', 'on mr50k it has a discrete margin of more than 0. 03 from the 2nd position.', 'again wikigiga models are positioned in the middle of the list and the worst performing models are moodycorpus and text8corpus.', 'our scores on this task are somehow lower than those reported from various studies that explore advanced deep learning constructs on same dataset.', 'in  #TAUTHOR_TAG for example, authors who created movie review dataset try on it their probabilistic model that is able to capture semantic similarities between words.', 'they report a maximal accuracy of 0. 88.', 'a study that uses a very similar method is [ 16 ] where authors combine random forest with word vector average values.', 'on movie review dataset they achieve an accuracy of 0. 84 which is about what we got here']",4
['1  #TAUTHOR_TAG is a pattern - based'],['1  #TAUTHOR_TAG is a pattern - based'],['##t 1  #TAUTHOR_TAG is a pattern - based framework'],[' #TAUTHOR_TAG'],0
['1  #TAUTHOR_TAG is a pattern - based'],['1  #TAUTHOR_TAG is a pattern - based'],['##t 1  #TAUTHOR_TAG is a pattern - based framework'],[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
['1  #TAUTHOR_TAG is a pattern - based'],['1  #TAUTHOR_TAG is a pattern - based'],['##t 1  #TAUTHOR_TAG is a pattern - based framework'],[' #TAUTHOR_TAG'],1
['1  #TAUTHOR_TAG is a pattern - based'],['1  #TAUTHOR_TAG is a pattern - based'],['##t 1  #TAUTHOR_TAG is a pattern - based framework'],[' #TAUTHOR_TAG'],6
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],6
"['', 'recently,  #TAUTHOR_TAG showed that combining a bert - based reader with passage']","['to sequence labeling.', 'recently,  #TAUTHOR_TAG showed that combining a bert - based reader with passage']","['', 'recently,  #TAUTHOR_TAG showed that combining a bert - based reader with passage retrieval using the anserini ir toolkit']","[' #AUTHOR_TAG represents the latest refinement in a series of neural models that take advantage of pretraining on a language modeling task  #AUTHOR_TAG.', 'researchers have demonstrated impressive gains in a broad range of nlp tasks, from sentence classification to sequence labeling.', 'recently,  #TAUTHOR_TAG showed that combining a bert - based reader with passage retrieval using the anserini ir toolkit yields a large improvement in question answering directly from a wikipedia corpus, measured in terms of exact match on a standard benchmark  #AUTHOR_TAG.', 'interestingly, the approach of  #TAUTHOR_TAG represents a simple method to combining bert with off - the - shelf ir.', 'in this paper, we build on these initial successes to explore how much further we can push this simple architecture by data augmentation, taking advantage of distant supervision techniques to gather more and higher - quality * equal contribution training data to fine tune bert.', 'experiments show that, using the same reader model as  #TAUTHOR_TAG, our simple data - augmentation techniques yield additional large improvements.', 'to illustrate the robustness of our methods, we also demonstrate consistent gains on another english qa dataset and present baselines for two additional chinese qa datasets ( which have not to date been evaluated in an "" end - to - end "" manner ).', '']",5
"['bertserini  #TAUTHOR_TAG, where the input corpus is pre - segmented into paragraphs at index time, each of which is']","['bertserini  #TAUTHOR_TAG, where the input corpus is pre - segmented into paragraphs at index time, each of which is']","['bertserini  #TAUTHOR_TAG, where the input corpus is pre - segmented into paragraphs at index time, each of which is treated as']","['this work, we fix the underlying model and focus on data augmentation techniques to explore how to best fine - tune bert.', 'we use the same exact setup as the "" paragraph "" variant of bertserini  #TAUTHOR_TAG, where the input corpus is pre - segmented into paragraphs at index time, each of which is treated as a "" document "" for retrieval purposes.', '']",5
"['.', 'following  #TAUTHOR_TAG,']","['', 'following  #TAUTHOR_TAG,']","['.', 'following  #TAUTHOR_TAG,']","['', ""for these, we use the 2018 - 12 - 01 dump of chinese wikipedia, tokenized with lucene's cjkanalyzer into overlapping bigrams."", 'we apply hanziconv 1 to transform the corpus into simplified characters for cmrc and traditional characters for drcd.', 'following  #TAUTHOR_TAG, to evaluate answers in an end - to - end setup, we disregard the paragraph context from the original datasets and use only the answer spans.', 'as in previous work, exact match ( em ) score and f 1 score ( at the token level ) serve as the two primary evaluation metrics.', 'in addition, we compute recall ( r ), the fraction of questions for which the correct answer appears in any retrieved paragraph ; to make our results comparable to  #TAUTHOR_TAG, anserini returns the top k = 100 paragraphs to feed into the bert reader.', '']",5
['matches the bertserini condition of  #TAUTHOR_TAG ; we report higher scores'],['matches the bertserini condition of  #TAUTHOR_TAG ; we report higher scores'],['matches the bertserini condition of  #TAUTHOR_TAG ; we report higher scores'],"['main results on squad are shown in table 2.', 'the row marked "" src "" indicates fine tuning with squad data only and matches the bertserini condition of  #TAUTHOR_TAG ; we report higher scores due to engineering improvements ( primarily a lucene version upgrade ).', 'as expected, fine tuning with augmented data improves effectiveness, and experiments show that while training with positive examples using ds ( + ) definitely  #AUTHOR_TAG 29. 1 37. 5 -  #AUTHOR_TAG 29. 8 - - par.', '']",5
"['', 'recently,  #TAUTHOR_TAG showed that combining a bert - based reader with passage']","['to sequence labeling.', 'recently,  #TAUTHOR_TAG showed that combining a bert - based reader with passage']","['', 'recently,  #TAUTHOR_TAG showed that combining a bert - based reader with passage retrieval using the anserini ir toolkit']","[' #AUTHOR_TAG represents the latest refinement in a series of neural models that take advantage of pretraining on a language modeling task  #AUTHOR_TAG.', 'researchers have demonstrated impressive gains in a broad range of nlp tasks, from sentence classification to sequence labeling.', 'recently,  #TAUTHOR_TAG showed that combining a bert - based reader with passage retrieval using the anserini ir toolkit yields a large improvement in question answering directly from a wikipedia corpus, measured in terms of exact match on a standard benchmark  #AUTHOR_TAG.', 'interestingly, the approach of  #TAUTHOR_TAG represents a simple method to combining bert with off - the - shelf ir.', 'in this paper, we build on these initial successes to explore how much further we can push this simple architecture by data augmentation, taking advantage of distant supervision techniques to gather more and higher - quality * equal contribution training data to fine tune bert.', 'experiments show that, using the same reader model as  #TAUTHOR_TAG, our simple data - augmentation techniques yield additional large improvements.', 'to illustrate the robustness of our methods, we also demonstrate consistent gains on another english qa dataset and present baselines for two additional chinese qa datasets ( which have not to date been evaluated in an "" end - to - end "" manner ).', '']",4
"['bertserini  #TAUTHOR_TAG, where the input corpus is pre - segmented into paragraphs at index time, each of which is']","['bertserini  #TAUTHOR_TAG, where the input corpus is pre - segmented into paragraphs at index time, each of which is']","['bertserini  #TAUTHOR_TAG, where the input corpus is pre - segmented into paragraphs at index time, each of which is treated as']","['this work, we fix the underlying model and focus on data augmentation techniques to explore how to best fine - tune bert.', 'we use the same exact setup as the "" paragraph "" variant of bertserini  #TAUTHOR_TAG, where the input corpus is pre - segmented into paragraphs at index time, each of which is treated as a "" document "" for retrieval purposes.', '']",4
['matches the bertserini condition of  #TAUTHOR_TAG ; we report higher scores'],['matches the bertserini condition of  #TAUTHOR_TAG ; we report higher scores'],['matches the bertserini condition of  #TAUTHOR_TAG ; we report higher scores'],"['main results on squad are shown in table 2.', 'the row marked "" src "" indicates fine tuning with squad data only and matches the bertserini condition of  #TAUTHOR_TAG ; we report higher scores due to engineering improvements ( primarily a lucene version upgrade ).', 'as expected, fine tuning with augmented data improves effectiveness, and experiments show that while training with positive examples using ds ( + ) definitely  #AUTHOR_TAG 29. 1 37. 5 -  #AUTHOR_TAG 29. 8 - - par.', '']",4
"['.', 'following  #TAUTHOR_TAG,']","['', 'following  #TAUTHOR_TAG,']","['.', 'following  #TAUTHOR_TAG,']","['', ""for these, we use the 2018 - 12 - 01 dump of chinese wikipedia, tokenized with lucene's cjkanalyzer into overlapping bigrams."", 'we apply hanziconv 1 to transform the corpus into simplified characters for cmrc and traditional characters for drcd.', 'following  #TAUTHOR_TAG, to evaluate answers in an end - to - end setup, we disregard the paragraph context from the original datasets and use only the answer spans.', 'as in previous work, exact match ( em ) score and f 1 score ( at the token level ) serve as the two primary evaluation metrics.', 'in addition, we compute recall ( r ), the fraction of questions for which the correct answer appears in any retrieved paragraph ; to make our results comparable to  #TAUTHOR_TAG, anserini returns the top k = 100 paragraphs to feed into the bert reader.', '']",3
['matches the bertserini condition of  #TAUTHOR_TAG ; we report higher scores'],['matches the bertserini condition of  #TAUTHOR_TAG ; we report higher scores'],['matches the bertserini condition of  #TAUTHOR_TAG ; we report higher scores'],"['main results on squad are shown in table 2.', 'the row marked "" src "" indicates fine tuning with squad data only and matches the bertserini condition of  #TAUTHOR_TAG ; we report higher scores due to engineering improvements ( primarily a lucene version upgrade ).', 'as expected, fine tuning with augmented data improves effectiveness, and experiments show that while training with positive examples using ds ( + ) definitely  #AUTHOR_TAG 29. 1 37. 5 -  #AUTHOR_TAG 29. 8 - - par.', '']",7
"['directly from parallel corpora  #TAUTHOR_TAG,']","['directly from parallel corpora  #TAUTHOR_TAG,']","['to alignment model translation as a sequence of probabilistic operations transforming the syntactic parse tree of a sentence in one language into that of the other.', 'the trees may be learned directly from parallel corpora  #TAUTHOR_TAG,']","['- based approaches to alignment model translation as a sequence of probabilistic operations transforming the syntactic parse tree of a sentence in one language into that of the other.', 'the trees may be learned directly from parallel corpora  #TAUTHOR_TAG, or provided by a parser trained on hand - annotated treebanks  #AUTHOR_TAG.', 'in this paper, we compare these approaches on chinese - english and french - english datasets, and find that automatically derived trees result in better agreement with human - annotated word - level alignments for unseen test data']",0
"['', ' #TAUTHOR_TAG modeled']","['new sentence.', ' #TAUTHOR_TAG modeled']","['finding the highest probability translation given a new sentence.', ' #TAUTHOR_TAG modeled']","['', 'the tree - based approach allows us to represent the fact that syntactic constituents tend to move as unit, as well as systematic differences in word order in the grammars of the two languages.', 'furthermore, the tree structure allows us to make probabilistic independence assumptions that result in polynomial time algorithms for estimating a translation model from parallel training data, and for finding the highest probability translation given a new sentence.', ' #TAUTHOR_TAG']",0
"['', ' #TAUTHOR_TAG modeled']","['new sentence.', ' #TAUTHOR_TAG modeled']","['finding the highest probability translation given a new sentence.', ' #TAUTHOR_TAG modeled']","['', 'the tree - based approach allows us to represent the fact that syntactic constituents tend to move as unit, as well as systematic differences in word order in the grammars of the two languages.', 'furthermore, the tree structure allows us to make probabilistic independence assumptions that result in polynomial time algorithms for estimating a translation model from parallel training data, and for finding the highest probability translation given a new sentence.', ' #TAUTHOR_TAG']",0
['inversion transduction grammar of  #TAUTHOR_TAG can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous'],"['inversion transduction grammar of  #TAUTHOR_TAG can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous context - free grammar productions.', 'the grammar is restricted to binary rules, which can have']","['inversion transduction grammar of  #TAUTHOR_TAG can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous context - free grammar productions.', 'the grammar is restricted to binary']","['inversion transduction grammar of  #TAUTHOR_TAG can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous context - free grammar productions.', 'the grammar is restricted to binary rules, which can have the symbols in the right hand side appear in the same order in both languages, represented with square brackets :', 'or the symbols may appear in reverse order in the two languages, indicated by angle brackets :', '']",0
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
"['many of the resulting alignments fall within the hard constraints of both  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'they find']","['many of the resulting alignments fall within the hard constraints of both  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'they find higher coverage']","['many of the resulting alignments fall within the hard constraints of both  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'they find']","['the translation model is likely to significantly improve the performance of syntactically supervised alignment. the', 'syntactically supervised model has been found to outperform the ibm word -', 'level alignment models of  #AUTHOR_TAG for translation by  #AUTHOR_TAG. an evaluation for the alignment task,', 'measuring agreement with human judges, also found the syntax - based model to outperform the ibm models. however, a relatively small corpus was used to train both models ( 2121 japanese - english sentence pairs )', ', and the evaluations were performed on the same data for training, meaning', 'that one or both models might be significantly overfitting.  #AUTHOR_TAG provide a thorough analysis', 'of alignment constraints from the perspective of decoding algorithms. they train the models of  #AUTHOR_TAG. decoding, meaning exact computation of the highest probability translation given a foreign sentence, is not possible in polynomial time for the ibm models, and in practice', 'decoders search through the space of hypothesis translations using', 'a set of additional, hard alignment constraints.  #AUTHOR_TAG compute the viterbi alignments for german - english and french - english sentences pairs using ibm model 5, and then measure how many of the resulting alignments fall within the hard constraints of both  #TAUTHOR_TAG and  #AUTHOR_TAG.', '']",0
"['directly from parallel corpora  #TAUTHOR_TAG,']","['directly from parallel corpora  #TAUTHOR_TAG,']","['to alignment model translation as a sequence of probabilistic operations transforming the syntactic parse tree of a sentence in one language into that of the other.', 'the trees may be learned directly from parallel corpora  #TAUTHOR_TAG,']","['- based approaches to alignment model translation as a sequence of probabilistic operations transforming the syntactic parse tree of a sentence in one language into that of the other.', 'the trees may be learned directly from parallel corpora  #TAUTHOR_TAG, or provided by a parser trained on hand - annotated treebanks  #AUTHOR_TAG.', 'in this paper, we compare these approaches on chinese - english and french - english datasets, and find that automatically derived trees result in better agreement with human - annotated word - level alignments for unseen test data']",5
"['', ' #TAUTHOR_TAG modeled']","['new sentence.', ' #TAUTHOR_TAG modeled']","['finding the highest probability translation given a new sentence.', ' #TAUTHOR_TAG modeled']","['', 'the tree - based approach allows us to represent the fact that syntactic constituents tend to move as unit, as well as systematic differences in word order in the grammars of the two languages.', 'furthermore, the tree structure allows us to make probabilistic independence assumptions that result in polynomial time algorithms for estimating a translation model from parallel training data, and for finding the highest probability translation given a new sentence.', ' #TAUTHOR_TAG']",5
['inversion transduction grammar of  #TAUTHOR_TAG can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous'],"['inversion transduction grammar of  #TAUTHOR_TAG can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous context - free grammar productions.', 'the grammar is restricted to binary rules, which can have']","['inversion transduction grammar of  #TAUTHOR_TAG can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous context - free grammar productions.', 'the grammar is restricted to binary']","['inversion transduction grammar of  #TAUTHOR_TAG can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous context - free grammar productions.', 'the grammar is restricted to binary rules, which can have the symbols in the right hand side appear in the same order in both languages, represented with square brackets :', 'or the symbols may appear in reverse order in the two languages, indicated by angle brackets :', '']",5
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],5
['is very sparse  #TAUTHOR_TAG'],['is very sparse  #TAUTHOR_TAG'],"['feature space is very sparse  #TAUTHOR_TAG.', 'this problem']","['date, standard approaches to named entity classification rely on supervised models, that typically require a large - scale annotated corpus and a widecoverage dictionary.', 'however, since new named entities arise regularly, it becomes increasingly difficult to maintain an up - to - date dictionary and / or adapt a named entity classifier to a new domain ; for example, sequence labeling techniques that use feature templates  #AUTHOR_TAG are not robust for unknown named entities because their feature space is very sparse  #TAUTHOR_TAG.', 'this problem worsens when we attempt to use a combination of features for sparse named entity classification.', 'therefore, in this paper, we propose the use of matrix factorization for named entity classification to consider the relationships between sparse features.', 'through our experiments, we achieved competitive accuracy to models developed in previous works in terms of using fewer features and compactness using factorization machines  #AUTHOR_TAG.', 'the main contributions of this paper are as follows :', '• we address the data sparseness problem in unknown named entity classification using factorization machines.', '• we demonstrate that factorization machines achieve state - of - the - art performance in sparse named entity classification task using a reduced feature set and a compact model']",0
"[',  #TAUTHOR_TAG explored the']","['classification,  #TAUTHOR_TAG explored the']","['unknown named entity classification,  #TAUTHOR_TAG explored the use of sparse combinatorial features.', 'they proposed a log - bilinear model']","['standard approach to named entity classification is to formulate a task as a sequence labeling problem and use a supervised method, such as conditional random fields  #AUTHOR_TAG.', 'these studies heavily rely on feature templates for learning combinations of features ; however, since combinations of features in conventional supervised learning are treated independently, this approach is not robust for named entities that do not appear in the training data.', 'to address the task of unknown named entity classification,  #TAUTHOR_TAG explored the use of sparse combinatorial features.', 'they proposed a log - bilinear model that defines a score function considering interactions between features ; the score function is regularized via a nuclear norm on a feature weight matrix.', 'further, heir method employs singular value decomposition ( svd ) - based regularization to handle the combination of features.', 'they reported that their regularization achieved higher accuracy than l1 and l2 regularization, frequently used in natural language processing  #AUTHOR_TAG.', 'however, nuclear norm regularization ( i. e., svdbased regularization ) is not necessarily the best way to incorporate interactions between features, because it does not directly optimize classification accuracy.', 'therefore, our proposed method treats sparse features using matrix factorization from a different perspective : we decompose a feature weight matrix using factorization machines as to directly optimize classification accuracy using a large margin method similar to support vector machines ( svms ) and passive - agressive algorithms  #AUTHOR_TAG']",0
"['the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion']","['the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion matrix,']","['the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion matrix,']","['results show that performance on org was improved.', 'for example, the term "" vicepresident "" appears in both contexts of org and o, and our method correctly handled this sparse combination of context and entity features.', 'the accuracy of loc, however, was lower than that of the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion matrix, we found that the loc tag was often misclassified as per.', 'we therefore conclude here that clustering and pos features are necessary to distinguish these tags.', 'figure 1 plots the f1 - score of our proposed method as dimension k changes for matrix factor - ization using the same development data as that of  #TAUTHOR_TAG.', '']",0
['is very sparse  #TAUTHOR_TAG'],['is very sparse  #TAUTHOR_TAG'],"['feature space is very sparse  #TAUTHOR_TAG.', 'this problem']","['date, standard approaches to named entity classification rely on supervised models, that typically require a large - scale annotated corpus and a widecoverage dictionary.', 'however, since new named entities arise regularly, it becomes increasingly difficult to maintain an up - to - date dictionary and / or adapt a named entity classifier to a new domain ; for example, sequence labeling techniques that use feature templates  #AUTHOR_TAG are not robust for unknown named entities because their feature space is very sparse  #TAUTHOR_TAG.', 'this problem worsens when we attempt to use a combination of features for sparse named entity classification.', 'therefore, in this paper, we propose the use of matrix factorization for named entity classification to consider the relationships between sparse features.', 'through our experiments, we achieved competitive accuracy to models developed in previous works in terms of using fewer features and compactness using factorization machines  #AUTHOR_TAG.', 'the main contributions of this paper are as follows :', '• we address the data sparseness problem in unknown named entity classification using factorization machines.', '• we demonstrate that factorization machines achieve state - of - the - art performance in sparse named entity classification task using a reduced feature set and a compact model']",1
['log - bilinear model using nuclear norm for regularization  #TAUTHOR_TAG'],"['given training corpus.', 'we compared factorization machines with a log - linear model, a polynomial - kernel svm, and a state - ofthe - art log - bilinear model using nuclear norm for regularization  #TAUTHOR_TAG']","['given training corpus.', 'we compared factorization machines with a log - linear model, a polynomial - kernel svm, and a state - ofthe - art log - bilinear model using nuclear norm for regularization  #TAUTHOR_TAG']","['described above, we aim to classify named entities that rarely appear in a given training corpus.', 'we compared factorization machines with a log - linear model, a polynomial - kernel svm, and a state - ofthe - art log - bilinear model using nuclear norm for regularization  #TAUTHOR_TAG']",5
[' #TAUTHOR_TAG ; this dataset was created'],[' #TAUTHOR_TAG ; this dataset was created'],[' #TAUTHOR_TAG ; this dataset was created'],"['.', 'we used the dataset provided by  #TAUTHOR_TAG ; this dataset was created for evaluating unknown named entity classification and is context features : right and left contexts of the candidate in a sentence ( do not take the order into account ).', 'cap = 1, cap = 0 : whether the first letter of the candidate is uppercase, or not. all - low = 1, all - low = 0 : whether all letters of the candidate are lowercase, or not.', 'all - cap1 = 1, all - cap1 = 0 : whether all letters of the candidate are uppercase, or not.', 'all - cap2 = 1, all - cap2 = 0 : whether all letters of the candidate are uppercase and periods, or not.', 'num - tokens = 1, num - tokens = 2, num - tokens > 2 : whether the candidate consists of 1, 2, or more tokens.', 'dummy : dummy feature to capture context features.', 'based on the conll - 2003 english dataset, which omits named entity candidates that appear in the training data from the development and test data.', 'table 1 shows the number of tokens and types in the given dataset.', 'this dataset contains five tags : person ( per ), location ( loc ), organization ( org ), miscellaneous ( misc ), and non - entities ( o ).', 'features.', 'we used a subset of features from experiments performed by  #TAUTHOR_TAG.', 'table 3 summarizes the features used in our experiment, including context and entity features.', 'tools.', 'in terms of tools, we used scikit - learn 0. 17 to implement a log - linear model and polynomial kernel in an svm.', 'further, we employed libfm 1. 4. 2 1  #AUTHOR_TAG to build a named entity classifier using factorization machines.', 'in the interaction of both the svm and the factorization machine, we fixed the degree of the polynomial kernel to d = 2.', 'we also tuned other parameters such as learning methods, learning rate and regularization methods based on development data.', 'further, we used a one - versus - all strategy to build a multiclass classifier.', 'evaluation metrics.', 'for our evaluation, we used precision, recall, and f1 - score.', 'the scores were calculated on all tags except for non - entities ( o )']",5
[' #TAUTHOR_TAG ; this dataset was created'],[' #TAUTHOR_TAG ; this dataset was created'],[' #TAUTHOR_TAG ; this dataset was created'],"['.', 'we used the dataset provided by  #TAUTHOR_TAG ; this dataset was created for evaluating unknown named entity classification and is context features : right and left contexts of the candidate in a sentence ( do not take the order into account ).', 'cap = 1, cap = 0 : whether the first letter of the candidate is uppercase, or not. all - low = 1, all - low = 0 : whether all letters of the candidate are lowercase, or not.', 'all - cap1 = 1, all - cap1 = 0 : whether all letters of the candidate are uppercase, or not.', 'all - cap2 = 1, all - cap2 = 0 : whether all letters of the candidate are uppercase and periods, or not.', 'num - tokens = 1, num - tokens = 2, num - tokens > 2 : whether the candidate consists of 1, 2, or more tokens.', 'dummy : dummy feature to capture context features.', 'based on the conll - 2003 english dataset, which omits named entity candidates that appear in the training data from the development and test data.', 'table 1 shows the number of tokens and types in the given dataset.', 'this dataset contains five tags : person ( per ), location ( loc ), organization ( org ), miscellaneous ( misc ), and non - entities ( o ).', 'features.', 'we used a subset of features from experiments performed by  #TAUTHOR_TAG.', 'table 3 summarizes the features used in our experiment, including context and entity features.', 'tools.', 'in terms of tools, we used scikit - learn 0. 17 to implement a log - linear model and polynomial kernel in an svm.', 'further, we employed libfm 1. 4. 2 1  #AUTHOR_TAG to build a named entity classifier using factorization machines.', 'in the interaction of both the svm and the factorization machine, we fixed the degree of the polynomial kernel to d = 2.', 'we also tuned other parameters such as learning methods, learning rate and regularization methods based on development data.', 'further, we used a one - versus - all strategy to build a multiclass classifier.', 'evaluation metrics.', 'for our evaluation, we used precision, recall, and f1 - score.', 'the scores were calculated on all tags except for non - entities ( o )']",5
"['the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion']","['the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion matrix,']","['the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion matrix,']","['results show that performance on org was improved.', 'for example, the term "" vicepresident "" appears in both contexts of org and o, and our method correctly handled this sparse combination of context and entity features.', 'the accuracy of loc, however, was lower than that of the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion matrix, we found that the loc tag was often misclassified as per.', 'we therefore conclude here that clustering and pos features are necessary to distinguish these tags.', 'figure 1 plots the f1 - score of our proposed method as dimension k changes for matrix factor - ization using the same development data as that of  #TAUTHOR_TAG.', '']",5
"['2 presents results of our experiments.', 'note that  #TAUTHOR_TAG used additional features']","['2 presents results of our experiments.', 'note that  #TAUTHOR_TAG used additional features']","['2 presents results of our experiments.', 'note that  #TAUTHOR_TAG used additional features']","['2 presents results of our experiments.', 'note that  #TAUTHOR_TAG used additional features such as brown clustering and parts - of - speech ( pos ) features, which we did not use.', '']",4
"['the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion']","['the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion matrix,']","['the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion matrix,']","['results show that performance on org was improved.', 'for example, the term "" vicepresident "" appears in both contexts of org and o, and our method correctly handled this sparse combination of context and entity features.', 'the accuracy of loc, however, was lower than that of the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion matrix, we found that the loc tag was often misclassified as per.', 'we therefore conclude here that clustering and pos features are necessary to distinguish these tags.', 'figure 1 plots the f1 - score of our proposed method as dimension k changes for matrix factor - ization using the same development data as that of  #TAUTHOR_TAG.', '']",4
"['the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion']","['the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion matrix,']","['the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion matrix,']","['results show that performance on org was improved.', 'for example, the term "" vicepresident "" appears in both contexts of org and o, and our method correctly handled this sparse combination of context and entity features.', 'the accuracy of loc, however, was lower than that of the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion matrix, we found that the loc tag was often misclassified as per.', 'we therefore conclude here that clustering and pos features are necessary to distinguish these tags.', 'figure 1 plots the f1 - score of our proposed method as dimension k changes for matrix factor - ization using the same development data as that of  #TAUTHOR_TAG.', '']",4
"['2 presents results of our experiments.', 'note that  #TAUTHOR_TAG used additional features']","['2 presents results of our experiments.', 'note that  #TAUTHOR_TAG used additional features']","['2 presents results of our experiments.', 'note that  #TAUTHOR_TAG used additional features']","['2 presents results of our experiments.', 'note that  #TAUTHOR_TAG used additional features such as brown clustering and parts - of - speech ( pos ) features, which we did not use.', '']",3
"['the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion']","['the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion matrix,']","['the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion matrix,']","['results show that performance on org was improved.', 'for example, the term "" vicepresident "" appears in both contexts of org and o, and our method correctly handled this sparse combination of context and entity features.', 'the accuracy of loc, however, was lower than that of the log - bilinear model  #TAUTHOR_TAG.', 'upon investigating the confusion matrix, we found that the loc tag was often misclassified as per.', 'we therefore conclude here that clustering and pos features are necessary to distinguish these tags.', 'figure 1 plots the f1 - score of our proposed method as dimension k changes for matrix factor - ization using the same development data as that of  #TAUTHOR_TAG.', '']",3
[' #TAUTHOR_TAG 25 ]'],[' #TAUTHOR_TAG 25 ]'],"['in accuracy  #TAUTHOR_TAG 25 ]. unfortunately,', 'these statistical regularities are hard to avoid when collecting real datasets. as illustrated in figure 1, there is a crucial need to develop new strategies to reduce', '']","['looking at the image, detecting a banana and assessing its color, it is much easier to learn from the statistical shortcut linking the words what, color and bananas with the most occurring answer yellow. one way to', 'quantify the amount of statistical shortcuts from each modality is to train unimodal', 'models. for instance, a question - only model trained on the widely used v', '##qa v2 dataset [ 9 ] predicts the correct answer approximately 44 % of the time over the test set. vqa models are not discouraged to exploit these', 'statistical shortcuts from the question modality, because their training set often follows the same distribution as their testing set. however, when evaluated on a test set that displays different statistical regularities, they usually suffer from a significant drop in accuracy  #TAUTHOR_TAG 25 ]. unfortunately,', 'these statistical regularities are hard to avoid when collecting real datasets. as illustrated in figure 1, there is a crucial need to develop new strategies to reduce', '']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],['can be leveraged  #TAUTHOR_TAG'],"['', 'however, even with this additional balancing, statistical biases from the question', 'remain and can be leveraged  #TAUTHOR_TAG. that is why we propose an approach to reduce unimodal biases during training. it is designed to learn unbiased models from biased datasets. our learning strategy dynamically modifies the loss values to reduce biases from the question.', 'by doing so, we reduce the importance of certain examples, similarly to the rejection sampling approach, while increasing the', 'importance of complementary examples which are already in the training set. architectures and learning strategies to reduce unimodal biases in parallel of these previous works on balancing datasets, an important effort has been carried out to design vqa models to overcome biases from datasets. [', '']",0
"['models are inclined to learn unimodal biases from the datasets  #TAUTHOR_TAG.', 'this can be shown by evaluating models on datasets that have different distributions of answers for the test set, such']","['of size n.', 'vqa models are inclined to learn unimodal biases from the datasets  #TAUTHOR_TAG.', 'this can be shown by evaluating models on datasets that have different distributions of answers for the test set, such']","['in minimizing the standard cross - entropy criterion over a dataset of size n.', 'vqa models are inclined to learn unimodal biases from the datasets  #TAUTHOR_TAG.', 'this can be shown by evaluating models on datasets that have different distributions of answers for the test set, such']","['', 'these functions are composed as follows :', 'each one of them can be defined to instantiate most of the state of the art models, such as [ 26, 38, 19, 39, 17, 40, 16 ] to cite a few.', 'classical learning strategy and pitfall the classical learning strategy of vqa models, depicted in figure 2, consists in minimizing the standard cross - entropy criterion over a dataset of size n.', 'vqa models are inclined to learn unimodal biases from the datasets  #TAUTHOR_TAG.', 'this can be shown by evaluating models on datasets that have different distributions of answers for the test set, such as vqa - cp v2.', 'in other words, they rely on statistical regularities from one modality to provide accurate predictions without having to consider the other modality.', 'as an extreme example, strongly biased models towards the question modality always output yellow to the question what color is the banana.', 'they do not learn to use the image information because there are too few examples in the dataset where the banana is not yellow.', 'once trained, their inability to use the two modalities adequately makes them inoperable on data coming from different distributions such as real - world data.', 'our contribution consists in modifying this cost function to avoid the learning of these biases']",0
[' #TAUTHOR_TAG 25 ]'],[' #TAUTHOR_TAG 25 ]'],"['in accuracy  #TAUTHOR_TAG 25 ]. unfortunately,', 'these statistical regularities are hard to avoid when collecting real datasets. as illustrated in figure 1, there is a crucial need to develop new strategies to reduce', '']","['looking at the image, detecting a banana and assessing its color, it is much easier to learn from the statistical shortcut linking the words what, color and bananas with the most occurring answer yellow. one way to', 'quantify the amount of statistical shortcuts from each modality is to train unimodal', 'models. for instance, a question - only model trained on the widely used v', '##qa v2 dataset [ 9 ] predicts the correct answer approximately 44 % of the time over the test set. vqa models are not discouraged to exploit these', 'statistical shortcuts from the question modality, because their training set often follows the same distribution as their testing set. however, when evaluated on a test set that displays different statistical regularities, they usually suffer from a significant drop in accuracy  #TAUTHOR_TAG 25 ]. unfortunately,', 'these statistical regularities are hard to avoid when collecting real datasets. as illustrated in figure 1, there is a crucial need to develop new strategies to reduce', '']",1
[' #TAUTHOR_TAG 25 ]'],[' #TAUTHOR_TAG 25 ]'],"['in accuracy  #TAUTHOR_TAG 25 ]. unfortunately,', 'these statistical regularities are hard to avoid when collecting real datasets. as illustrated in figure 1, there is a crucial need to develop new strategies to reduce', '']","['looking at the image, detecting a banana and assessing its color, it is much easier to learn from the statistical shortcut linking the words what, color and bananas with the most occurring answer yellow. one way to', 'quantify the amount of statistical shortcuts from each modality is to train unimodal', 'models. for instance, a question - only model trained on the widely used v', '##qa v2 dataset [ 9 ] predicts the correct answer approximately 44 % of the time over the test set. vqa models are not discouraged to exploit these', 'statistical shortcuts from the question modality, because their training set often follows the same distribution as their testing set. however, when evaluated on a test set that displays different statistical regularities, they usually suffer from a significant drop in accuracy  #TAUTHOR_TAG 25 ]. unfortunately,', 'these statistical regularities are hard to avoid when collecting real datasets. as illustrated in figure 1, there is a crucial need to develop new strategies to reduce', '']",1
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],['can be leveraged  #TAUTHOR_TAG'],"['', 'however, even with this additional balancing, statistical biases from the question', 'remain and can be leveraged  #TAUTHOR_TAG. that is why we propose an approach to reduce unimodal biases during training. it is designed to learn unbiased models from biased datasets. our learning strategy dynamically modifies the loss values to reduce biases from the question.', 'by doing so, we reduce the importance of certain examples, similarly to the rejection sampling approach, while increasing the', 'importance of complementary examples which are already in the training set. architectures and learning strategies to reduce unimodal biases in parallel of these previous works on balancing datasets, an important effort has been carried out to design vqa models to overcome biases from datasets. [', '']",1
[' #TAUTHOR_TAG 25 ]'],[' #TAUTHOR_TAG 25 ]'],"['in accuracy  #TAUTHOR_TAG 25 ]. unfortunately,', 'these statistical regularities are hard to avoid when collecting real datasets. as illustrated in figure 1, there is a crucial need to develop new strategies to reduce', '']","['looking at the image, detecting a banana and assessing its color, it is much easier to learn from the statistical shortcut linking the words what, color and bananas with the most occurring answer yellow. one way to', 'quantify the amount of statistical shortcuts from each modality is to train unimodal', 'models. for instance, a question - only model trained on the widely used v', '##qa v2 dataset [ 9 ] predicts the correct answer approximately 44 % of the time over the test set. vqa models are not discouraged to exploit these', 'statistical shortcuts from the question modality, because their training set often follows the same distribution as their testing set. however, when evaluated on a test set that displays different statistical regularities, they usually suffer from a significant drop in accuracy  #TAUTHOR_TAG 25 ]. unfortunately,', 'these statistical regularities are hard to avoid when collecting real datasets. as illustrated in figure 1, there is a crucial need to develop new strategies to reduce', '']",5
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],['can be leveraged  #TAUTHOR_TAG'],"['', 'however, even with this additional balancing, statistical biases from the question', 'remain and can be leveraged  #TAUTHOR_TAG. that is why we propose an approach to reduce unimodal biases during training. it is designed to learn unbiased models from biased datasets. our learning strategy dynamically modifies the loss values to reduce biases from the question.', 'by doing so, we reduce the importance of certain examples, similarly to the rejection sampling approach, while increasing the', 'importance of complementary examples which are already in the training set. architectures and learning strategies to reduce unimodal biases in parallel of these previous works on balancing datasets, an important effort has been carried out to design vqa models to overcome biases from datasets. [', '']",5
"['- cp v2  #TAUTHOR_TAG.', 'this dataset was developed to']","['our models on vqa - cp v2  #TAUTHOR_TAG.', 'this dataset was developed to']","['- cp v2  #TAUTHOR_TAG.', 'this dataset was developed to']","['setup we train and evaluate our models on vqa - cp v2  #TAUTHOR_TAG.', 'this dataset was developed to evaluate the models robustness to question biases.', 'we follow the same training and evaluation protocol as [ 25 ], who also propose a learning strategy to reduce biases.', 'for each model, we report the standard vqa evaluation metric [ 8 ].', 'we also evaluate our models on the standard vqa v2 [ 9 ].', 'further implementation details are included in the supplementary materials']",5
"['gvqa  #TAUTHOR_TAG, which is a specific']","['gvqa  #TAUTHOR_TAG, which is a specific']","['gvqa  #TAUTHOR_TAG, which is a specific architecture designed for v']","['', 'this accuracy corresponds to a gain of + 5. 94 percentage points over the current state - of - the - art updn + q - adv + doe. it also corresponds to a gain of + 15. 88 over gvqa  #TAUTHOR_TAG, which is a specific architecture designed for vqa - cp.', 'rubi reaches a + 8. 65 improvement over our baseline model trained with the classical cross - entropy.', '']",4
"['to our baseline, while  #TAUTHOR_TAG report a']","['to our baseline, while  #TAUTHOR_TAG report a']","['to our baseline, while  #TAUTHOR_TAG report a drop']","['report the impact of our method on the standard vqa v2 dataset in table 3.', 'it uses the same data as vqa - cp v2, but includes the same statistical regularities in its train, val and test sets.', 'in this context, we usually observe a drop in accuracy using approaches focused on reducing biases.', 'this is due to the fact that exploiting unwanted correlations from the vqa v2 train set is not discouraged and often leads to a higher accuracy on the test set.', 'nevertheless, our rubi approach leads to a comparable drop to what can be seen in the state - of - the - art.', 'we report a drop of 1. 94 percentage points with respect to our baseline, while  #TAUTHOR_TAG report a drop of 3. 78 between gvqa and their san baseline.', '[ 25 ] report drops of 0. 05, 0. 73 and 2. 95 for their three learning strategies with the updn architecture which uses the same visual features as rubi.', 'as shown in this section, rubi improves the accuracy on vqa - cp v2 from a large margin, while maintaining competitive performance on the standard vqa v2 dataset compared to similar approaches']",4
"['statistical parser  #TAUTHOR_TAG', ', whose architecture and properties']","['statistical parser  #TAUTHOR_TAG', ', whose architecture and properties']","['art statistical parser  #TAUTHOR_TAG', ', whose architecture and properties']","[""statistical parser can output such richer information without any degradation of the parser's accuracy on the original parsing task. briefly, our method consists in augmenting a state - of - the - art statistical parser  #TAUTHOR_TAG"", ', whose architecture and properties make it particularly adaptive to new tasks. we achieve state - of - the - art results both for parsing and', 'function labelling. statistical parsers trained on the penn treebank ( ptb )  #AUTHOR_TAG produce trees annotated with bare phrase structure labels  #AUTHOR_TAG. the trees of the penn treebank, however, are also decorated with function', ""labels. figure 1 shows the simplified tree representation with function labels for a sample sentence from the penn treebank corpus ( section 00 ) the government's borrowing authority dropped at midnight tuesday to 2. 8 trillion"", 'from 2. 87 trillion. tion labels are context - dependent and encode a shallow level of phr', '##asal and lexical semantics, as observed first in  #AUTHOR_TAG. 1 to a large extent, they overlap with semantic role labels as defined in propbank. current statistical parsers do not use this', '']",6
"['statistical parser  #TAUTHOR_TAG', ', whose architecture and properties']","['statistical parser  #TAUTHOR_TAG', ', whose architecture and properties']","['art statistical parser  #TAUTHOR_TAG', ', whose architecture and properties']","[""statistical parser can output such richer information without any degradation of the parser's accuracy on the original parsing task. briefly, our method consists in augmenting a state - of - the - art statistical parser  #TAUTHOR_TAG"", ', whose architecture and properties make it particularly adaptive to new tasks. we achieve state - of - the - art results both for parsing and', 'function labelling. statistical parsers trained on the penn treebank ( ptb )  #AUTHOR_TAG produce trees annotated with bare phrase structure labels  #AUTHOR_TAG. the trees of the penn treebank, however, are also decorated with function', ""labels. figure 1 shows the simplified tree representation with function labels for a sample sentence from the penn treebank corpus ( section 00 ) the government's borrowing authority dropped at midnight tuesday to 2. 8 trillion"", 'from 2. 87 trillion. tion labels are context - dependent and encode a shallow level of phr', '##asal and lexical semantics, as observed first in  #AUTHOR_TAG. 1 to a large extent, they overlap with semantic role labels as defined in propbank. current statistical parsers do not use this', '']",6
"['simple synchrony network ( ssn ) parsers  #TAUTHOR_TAG,']","['simple synchrony network ( ssn ) parsers  #TAUTHOR_TAG,']","['', 'it is therefore important to choose a statistical parser that can model our augmented labelling problem.', 'we use a family of statistical parsers, the simple synchrony network ( ssn ) parsers  #TAUTHOR_TAG,']","['main hypothesis says that function labels can be successfully and automatically recovered while parsing, without affecting negatively the performance of the parser.', 'it is possible that attempting to solve the function labelling and the parsing problem at the same time would require modifying existing parsing models, since their underlying independence assumptions might no longer hold.', 'moreover, many more parameters are to be estimated.', 'it is therefore important to choose a statistical parser that can model our augmented labelling problem.', 'we use a family of statistical parsers, the simple synchrony network ( ssn ) parsers  #TAUTHOR_TAG, which crucially do not make any explicit independence assumptions, and learn to smooth across rare feature combinations.', 'they are therefore likely to adapt without much modification to the current problem.', 'this architecture has shown state - of - the - art performance and is very adaptive to properties of the input.', 'the architecture of an ssn parser comprises two components, one which estimates the parameters of a stochastic model for syntactic trees, and one which searches for the most probable syntactic tree given the parameter estimates.', 'as with many other statistical parsers  #AUTHOR_TAG, the model of parsing is history - based.', 'its events are derivation moves.', 'the set of well - formed sequences of derivation moves in this parser is defined by a predictive lr pushdown automaton  #AUTHOR_TAG, which implements a form of left - corner parsing strategy.', '2 the probability of a phrase - structure tree is equated to the probability of a finite ( but unbounded ) sequence of derivation moves.', 'to bound the number of parameters, standard history - based models partition the set of prefixes of well - formed sequences of transitions into equivalence classes.', '']",5
,,,,5
,,,,5
"['simple synchrony network ( ssn ) parsers  #TAUTHOR_TAG,']","['simple synchrony network ( ssn ) parsers  #TAUTHOR_TAG,']","['', 'it is therefore important to choose a statistical parser that can model our augmented labelling problem.', 'we use a family of statistical parsers, the simple synchrony network ( ssn ) parsers  #TAUTHOR_TAG,']","['main hypothesis says that function labels can be successfully and automatically recovered while parsing, without affecting negatively the performance of the parser.', 'it is possible that attempting to solve the function labelling and the parsing problem at the same time would require modifying existing parsing models, since their underlying independence assumptions might no longer hold.', 'moreover, many more parameters are to be estimated.', 'it is therefore important to choose a statistical parser that can model our augmented labelling problem.', 'we use a family of statistical parsers, the simple synchrony network ( ssn ) parsers  #TAUTHOR_TAG, which crucially do not make any explicit independence assumptions, and learn to smooth across rare feature combinations.', 'they are therefore likely to adapt without much modification to the current problem.', 'this architecture has shown state - of - the - art performance and is very adaptive to properties of the input.', 'the architecture of an ssn parser comprises two components, one which estimates the parameters of a stochastic model for syntactic trees, and one which searches for the most probable syntactic tree given the parameter estimates.', 'as with many other statistical parsers  #AUTHOR_TAG, the model of parsing is history - based.', 'its events are derivation moves.', 'the set of well - formed sequences of derivation moves in this parser is defined by a predictive lr pushdown automaton  #AUTHOR_TAG, which implements a form of left - corner parsing strategy.', '2 the probability of a phrase - structure tree is equated to the probability of a finite ( but unbounded ) sequence of derivation moves.', 'to bound the number of parameters, standard history - based models partition the set of prefixes of well - formed sequences of transitions into equivalence classes.', '']",0
,,,,0
,,,,3
"[')  #TAUTHOR_TAG transition system, which covers']","['of )  #TAUTHOR_TAG transition system, which covers']","['for ( an isomorphic variant of )  #TAUTHOR_TAG transition system, which covers a subset of non - projective trees.', 'the exact inference algorithm runs in opn 7 q time, where n denotes sentence length.', ""in this paper, we show how cohen et al.'s ( 2011 ) system can be modified to""]","['- projective dependency trees are those containing crossing edges.', 'they account for 12. 59 % of all training sentences in the annotated universal dependencies ( ud ) 2. 1 data  #AUTHOR_TAG, and more than 20 % in each of 10 languages among the 54 in ud 2. 1 with training treebanks.', 'but modeling non - projectivity is computationally costly ( mc  #AUTHOR_TAG.', 'some transition - based dependency parsers have deduction systems that use dynamic programming to enable exact inference in polynomial time and space  #AUTHOR_TAG.', 'for non - projective parsing, though, the only tabularization of a transition - based parser is, to our knowledge, that of  #AUTHOR_TAG.', 'they define a deduction system for ( an isomorphic variant of )  #TAUTHOR_TAG transition system, which covers a subset of non - projective trees.', 'the exact inference algorithm runs in opn 7 q time, where n denotes sentence length.', ""in this paper, we show how cohen et al.'s ( 2011 ) system can be modified to generate a new family of deduction systems with corresponding transition systems."", 'in particular, we present three novel variants of the degree - 2 attardi parser, summarized in fig. 1 ( our technique can also be applied to generalized  #TAUTHOR_TAG systems ; see § 3. 2 ).', '']",0
"['now introduce the widely - used  #TAUTHOR_TAG system,']","['now introduce the widely - used  #TAUTHOR_TAG system,']","['now introduce the widely - used  #TAUTHOR_TAG system,']","['now introduce the widely - used  #TAUTHOR_TAG system, which includes transitions that create arcs between non - consecutive subtrees, thus allowing it to produce some non - projective trees.', ""to simplify exposition, here we present cohen et al.'s ( 2011 ) isomorphic version."", 'the set of transitions consists of a shift transition ( sh ) and four reduce transitions ( re ).', '']",0
"['now introduce the widely - used  #TAUTHOR_TAG system,']","['now introduce the widely - used  #TAUTHOR_TAG system,']","['now introduce the widely - used  #TAUTHOR_TAG system,']","['now introduce the widely - used  #TAUTHOR_TAG system, which includes transitions that create arcs between non - consecutive subtrees, thus allowing it to produce some non - projective trees.', ""to simplify exposition, here we present cohen et al.'s ( 2011 ) isomorphic version."", 'the set of transitions consists of a shift transition ( sh ) and four reduce transitions ( re ).', '']",0
['handle more non - projective treebank trees  #TAUTHOR_TAG ; go'],['handle more non - projective treebank trees  #TAUTHOR_TAG ;'],['1 can handle more non - projective treebank trees  #TAUTHOR_TAG ; go'],"['key observation is that a degree - d attardi system does not contain all possible transitions of degree within d. since prior empirical work has ascertained that transition systems using more transitions with degree greater than 1 can handle more non - projective treebank trees  #TAUTHOR_TAG ; gomez - rodriguez, 2016 ), we hypothesize that adding some of these "" missing "" reduce transitions into the system\'s inventory should increase coverage.', ""the challenge is to simultaneously maintain run - time guarantees, as there exists a known trade - off between coverage and complexity ( gomez - rodriguez, 2016  #AUTHOR_TAG, rather than opn [UNK] ` 1 q ; and ( ii ) another has degree 2 but better runtime than cohen et al.'s ( 2011 ) system."", 'here, we first sketch the existing exact inference algorithm, 3 and then present our variants']",0
"['1 transitions from r into the  #TAUTHOR_TAG', 'system. surprisingly, using alldeg1']","['1 transitions from r into the  #TAUTHOR_TAG', 'system. surprisingly, using alldeg1 improves non - projective coverage from 87']","['- 1 transitions from r into the  #TAUTHOR_TAG', 'system. surprisingly, using alldeg1 improves non - projective coverage from 87. 24 % to 93. 32 %.', 'furthermore, recall that we argued above that']","['are exactly nine reduce transitions r "" tre s 0, s 1, re s 1, s 0, re s 0, s 2', ', re s 2, s 0, re s 1, s 2, re s 2, s 1, re b 0,', 's 0, re b 0, s 1, re b 0, s 2 u that can be used in cohen et al.', '\' s ( 2011 ) exact inference algorithm, without allowing a reduction with head b i for i e 1. 6 ( note that cohen et al.\'s ( 2011 ) reduce rules are precisely the first four elements of r. ) from fig. 3 we infer that the concatenation of i - computations rh 1, i, h 2, h 3, ks and rh 3, k, h 4, h 5, js yields a configuration of the form pσ | h 2 | h 4 | h 5, j | β, aq. for the application of a reduce rule to yield a valid i - computation, by condition ( 1 ) of the i - computation definition, first, the head and modifier must be selected from the "" exposed "" elements h 2, h 4, h 5, and j, corresponding to s 2, s 1, s 0, b 0, respectively ; and second, the modifier can only come from the', 'stack. r is precisely the set of rules satisfying these criteria. further, every reduce transition from r is compatible with  #AUTHOR_TAG "" hook trick "". this gives us the satisfactory result that the opn 7 q running time upper bound still holds for transitions in r, even though one of them has degree 3. next, we consider three notable variants within the family of r - based non - projective transitionbased dependency parsers. they are', 'given in fig. 1, along with their time complexities', 'and empirical coverage statistics. the latter is computed using static oracles  #AUTHOR_TAG on the', 'ud 2. 1 dataset  #AUTHOR_TAG. 7 we report the global coverage over the 76, 084 non - projective sentences from all the training treebanks. one', ""might assume that adding more degree - 1 transitions wouldn't improve coverage of trees with non - crossing edges."", ""on the other hand, since their addition doesn't affect the asymptotic run - time,"", 'we define alldeg1 to include all five degree - 1 transitions from r into the  #TAUTHOR_TAG', 'system. surprisingly, using alldeg1 improves non - projective coverage from 87. 24 % to 93. 32 %.', 'furthermore, recall that we argued above that']",0
"[')  #TAUTHOR_TAG transition system, which covers']","['of )  #TAUTHOR_TAG transition system, which covers']","['for ( an isomorphic variant of )  #TAUTHOR_TAG transition system, which covers a subset of non - projective trees.', 'the exact inference algorithm runs in opn 7 q time, where n denotes sentence length.', ""in this paper, we show how cohen et al.'s ( 2011 ) system can be modified to""]","['- projective dependency trees are those containing crossing edges.', 'they account for 12. 59 % of all training sentences in the annotated universal dependencies ( ud ) 2. 1 data  #AUTHOR_TAG, and more than 20 % in each of 10 languages among the 54 in ud 2. 1 with training treebanks.', 'but modeling non - projectivity is computationally costly ( mc  #AUTHOR_TAG.', 'some transition - based dependency parsers have deduction systems that use dynamic programming to enable exact inference in polynomial time and space  #AUTHOR_TAG.', 'for non - projective parsing, though, the only tabularization of a transition - based parser is, to our knowledge, that of  #AUTHOR_TAG.', 'they define a deduction system for ( an isomorphic variant of )  #TAUTHOR_TAG transition system, which covers a subset of non - projective trees.', 'the exact inference algorithm runs in opn 7 q time, where n denotes sentence length.', ""in this paper, we show how cohen et al.'s ( 2011 ) system can be modified to generate a new family of deduction systems with corresponding transition systems."", 'in particular, we present three novel variants of the degree - 2 attardi parser, summarized in fig. 1 ( our technique can also be applied to generalized  #TAUTHOR_TAG systems ; see § 3. 2 ).', '']",7
['good projective - parsing results  #AUTHOR_TAG  #TAUTHOR_TAG transition system'],['good projective - parsing results  #AUTHOR_TAG  #TAUTHOR_TAG transition system'],['which have yielded good projective - parsing results  #AUTHOR_TAG  #TAUTHOR_TAG transition system'],"['transition system is given by a 4 - tuple pc, t, c s, c τ q, where c is a set of configurations, t is a set of transition functions between configurations, c s is an initialization function mapping an input sentence to an initial configuration, and c τ a c defines a set of terminal configurations.', '1 faster exact inference algorithms have been defined for some sets of mildly non - projective trees ( e. g.  #AUTHOR_TAG ; see gomez - rodriguez ( 2016 ) for more ), but lack an underlying transition system.', 'having one has the practical advantage of allowing generative models, as in  #AUTHOR_TAG, and transition - based scoring functions, which have yielded good projective - parsing results  #AUTHOR_TAG  #TAUTHOR_TAG transition system of degree 2 and our variants.', 'solid arrows denote the inventory of reduce transitions ; each arrow points from the head to the modifier of the edge created by that transition.', 'the degree of a transition is the distance between the head and modifier.', 'green highlights the single degree - 3 transition.', 'thick arrows and gray dotted arrows represent additional and deleted transitions with respect to the original  #TAUTHOR_TAG system.', 'coverage refers to the percentage of nonprojective sentences ( a total of 76, 084 extracted from 604, 273 training sentences in ud 2. 1 ) that the systems are able to handle.', 'we employ a tripartite representation for configurations : pσ, β, aq, where the three elements are as follows.', 'σ and β are disjoint lists called the stack and buffer, respectively.', 'each dependency arc ph, mq in the resolved arcs set a has head h and modifier m. for a length - n input sentence w, the initial configuration is c s pwq "" prs, r0, 1,..., ns, hq where the 0 in the initial buffer denotes a special node representing the root of the parse tree.', 'all terminal configurations have an empty buffer and a stack containing only 0.', 'indexing from 0, we write s i and b j to denote item i on the stack ( starting from the right ) and item j on the buffer ( from the left ), respectively.', 'we use vertical bars to separate different parts of the buffer or stack.', 'for example, when concerned with the top three stack items and the first item on the buffer, we may write σ | s 2 | s 1 | s 0 and b 0 | β']",7
['good projective - parsing results  #AUTHOR_TAG  #TAUTHOR_TAG transition system'],['good projective - parsing results  #AUTHOR_TAG  #TAUTHOR_TAG transition system'],['which have yielded good projective - parsing results  #AUTHOR_TAG  #TAUTHOR_TAG transition system'],"['transition system is given by a 4 - tuple pc, t, c s, c τ q, where c is a set of configurations, t is a set of transition functions between configurations, c s is an initialization function mapping an input sentence to an initial configuration, and c τ a c defines a set of terminal configurations.', '1 faster exact inference algorithms have been defined for some sets of mildly non - projective trees ( e. g.  #AUTHOR_TAG ; see gomez - rodriguez ( 2016 ) for more ), but lack an underlying transition system.', 'having one has the practical advantage of allowing generative models, as in  #AUTHOR_TAG, and transition - based scoring functions, which have yielded good projective - parsing results  #AUTHOR_TAG  #TAUTHOR_TAG transition system of degree 2 and our variants.', 'solid arrows denote the inventory of reduce transitions ; each arrow points from the head to the modifier of the edge created by that transition.', 'the degree of a transition is the distance between the head and modifier.', 'green highlights the single degree - 3 transition.', 'thick arrows and gray dotted arrows represent additional and deleted transitions with respect to the original  #TAUTHOR_TAG system.', 'coverage refers to the percentage of nonprojective sentences ( a total of 76, 084 extracted from 604, 273 training sentences in ud 2. 1 ) that the systems are able to handle.', 'we employ a tripartite representation for configurations : pσ, β, aq, where the three elements are as follows.', 'σ and β are disjoint lists called the stack and buffer, respectively.', 'each dependency arc ph, mq in the resolved arcs set a has head h and modifier m. for a length - n input sentence w, the initial configuration is c s pwq "" prs, r0, 1,..., ns, hq where the 0 in the initial buffer denotes a special node representing the root of the parse tree.', 'all terminal configurations have an empty buffer and a stack containing only 0.', 'indexing from 0, we write s i and b j to denote item i on the stack ( starting from the right ) and item j on the buffer ( from the left ), respectively.', 'we use vertical bars to separate different parts of the buffer or stack.', 'for example, when concerned with the top three stack items and the first item on the buffer, we may write σ | s 2 | s 1 | s 0 and b 0 | β']",7
"['1 transitions from r into the  #TAUTHOR_TAG', 'system. surprisingly, using alldeg1']","['1 transitions from r into the  #TAUTHOR_TAG', 'system. surprisingly, using alldeg1 improves non - projective coverage from 87']","['- 1 transitions from r into the  #TAUTHOR_TAG', 'system. surprisingly, using alldeg1 improves non - projective coverage from 87. 24 % to 93. 32 %.', 'furthermore, recall that we argued above that']","['are exactly nine reduce transitions r "" tre s 0, s 1, re s 1, s 0, re s 0, s 2', ', re s 2, s 0, re s 1, s 2, re s 2, s 1, re b 0,', 's 0, re b 0, s 1, re b 0, s 2 u that can be used in cohen et al.', '\' s ( 2011 ) exact inference algorithm, without allowing a reduction with head b i for i e 1. 6 ( note that cohen et al.\'s ( 2011 ) reduce rules are precisely the first four elements of r. ) from fig. 3 we infer that the concatenation of i - computations rh 1, i, h 2, h 3, ks and rh 3, k, h 4, h 5, js yields a configuration of the form pσ | h 2 | h 4 | h 5, j | β, aq. for the application of a reduce rule to yield a valid i - computation, by condition ( 1 ) of the i - computation definition, first, the head and modifier must be selected from the "" exposed "" elements h 2, h 4, h 5, and j, corresponding to s 2, s 1, s 0, b 0, respectively ; and second, the modifier can only come from the', 'stack. r is precisely the set of rules satisfying these criteria. further, every reduce transition from r is compatible with  #AUTHOR_TAG "" hook trick "". this gives us the satisfactory result that the opn 7 q running time upper bound still holds for transitions in r, even though one of them has degree 3. next, we consider three notable variants within the family of r - based non - projective transitionbased dependency parsers. they are', 'given in fig. 1, along with their time complexities', 'and empirical coverage statistics. the latter is computed using static oracles  #AUTHOR_TAG on the', 'ud 2. 1 dataset  #AUTHOR_TAG. 7 we report the global coverage over the 76, 084 non - projective sentences from all the training treebanks. one', ""might assume that adding more degree - 1 transitions wouldn't improve coverage of trees with non - crossing edges."", ""on the other hand, since their addition doesn't affect the asymptotic run - time,"", 'we define alldeg1 to include all five degree - 1 transitions from r into the  #TAUTHOR_TAG', 'system. surprisingly, using alldeg1 improves non - projective coverage from 87. 24 % to 93. 32 %.', 'furthermore, recall that we argued above that']",5
"['the relations of interest are not specified in advance  #TAUTHOR_TAG.', 'unlabeled text is abundant in large corpora']","['the relations of interest are not specified in advance  #TAUTHOR_TAG.', 'unlabeled text is abundant in large corpora']","['the relations of interest are not specified in advance  #TAUTHOR_TAG.', 'unlabeled text is abundant in large corpora']","['', 'typically, distributional similarity is computed by comparing co - occurrence counts of extractions and seeds with various contexts found in the corpus.', 'statistical language models ( slms ) include methods for more accurately estimating co - occurrence probabilities via back - off, smoothing, and clustering techniques ( e. g.  #AUTHOR_TAG ).', 'because slms can be trained from only unlabeled text, they can be applied for ads even when the relations of interest are not specified in advance  #TAUTHOR_TAG.', 'unlabeled text is abundant in large corpora like the web, making nearly - ceaseless automated optimization of slms possible. but how fruitful is such an effort likely to be - to what extent does optimizing a language model over a fixed corpus lead to improvements in assessment accuracy?', 'in this paper, we show that an ads technique based on slms is improved substantially when the language model it employs becomes more accurate.', 'in a large - scale set of experiments, we quantify how language model perplexity correlates with ads performance over multiple data sets and slm techniques.', 'the experiments show that accuracy over unlabeled data can be used for selecting among slms - for an ads approach utilizing hidden markov models, this results in an average error reduction of 26 % over previous results in extraction and type - checking tasks']",4
['hmm - t model in  #TAUTHOR_TAG by'],['hmm - t model in  #TAUTHOR_TAG by'],['the hmm - t model in  #TAUTHOR_TAG by'],"['language models can be configured in different ways : for example, hmms require choices for the hyperparameters k and t.', 'here, we show that slm perplexity can be used to select a high - quality model configuration for ads using only unlabeled data.', 'we evaluate on the unary and binary data sets, since they have been employed in previous work on our corpora.', 'figure 2 shows that for hmms, ads performance increases as perplexity decreases across various model configurations ( a similar relationship holds for n - gram models ).', 'a model selection technique that picks the hmm model with lowest perplexity ( hmm 1 - 100 ) results in better ads performance than previous results.', 'as shown in table 2, hmm 1 - 100 reduces error over the hmm - t model in  #TAUTHOR_TAG by 26 %, on average.', 'the experiments also reveal an important difference between the hmm and n - gram approaches.', 'while kn3 is more accurate in slm than our hmm models, it performs worse in ads on average.', 'for example, hmm 1 - 25 underperforms kn3 in perpexity, at 537. 2 versus 227. 1, but wins in ads, 0. 880 to 0. 853.', 'we hypothesize that this is because the latent state distributions in the hmms provide a more informative distributional similarity measure.', 'indeed, when we compute distributional similarity for hmms using probabilistic context vectors as opposed to state distributions, ads performance for hmm 1 - 25 decreases to 5. 8 % below that of kn3']",4
['of extraction  #TAUTHOR_TAG'],['of extraction  #TAUTHOR_TAG'],"['s r.', 'for relations of arity greater than one, we consider the typechecking task, an important sub - task of extraction  #TAUTHOR_TAG.', 'the typechecking task is to rank']","['', 'let u ri denote the set of the ith arguments of the extractions in u r, and let s ri be defined similarly for the seed set s r.', 'for relations of arity greater than one, we consider the typechecking task, an important sub - task of extraction  #TAUTHOR_TAG.', 'the typechecking task is to rank extractions with arguments that are of the proper type for a relation above type errors.', 'as an example, the extraction founded ( bill gates, oracle ) is type correct, but is not correct for the extraction task']",5
"[', and were taken from  #TAUTHOR_TAG']","['unsupervised information extraction, and were taken from  #TAUTHOR_TAG']","['unsupervised information extraction, and were taken from  #TAUTHOR_TAG.', 'the first, unary, was']","['experiment with a wide range of n - gram and hmm models.', 'the n - gram models are trained using the srilm toolkit  #AUTHOR_TAG variety of hmm configurations over a large corpus requires a scalable training architecture.', 'we constructed a parallel hmm codebase using the message passing interface ( mpi ), and trained the models on a supercomputing cluster.', 'all language models were trained on a corpus of 2. 8m sentences of web text ( about 60 million tokens ).', 'slm performance is measured using the standard perplexity metric, and assessment accuracy is measured using area under the precision - recall curve ( auc ), a standard metric for ranked lists of extractions.', 'we evaluated performance on three distinct data sets.', 'the first two data sets evaluate ads for unsupervised information extraction, and were taken from  #TAUTHOR_TAG.', 'the first, unary, was an extraction task for unary relations ( company, country, language, film ) and the second, binary, was a type - checking task for binary relations ( conquered, founded, headquartered, merged ).', 'the 10 most frequent extractions served as bootstrapped seeds.', 'the two test sets contained 361 and 265 extractions, respectively.', ""the third data set, wikipedia, evaluates ads on weaklysupervised extraction, using seeds and extractions taken from wikipedia'list of'pages  #AUTHOR_TAG."", 'seed sets of various sizes ( 5, 10, 15 and 20 ) were randomly selected from each list, and we present results averaged over 10 random samplings.', 'other members of the seed list were added to a test set as correct extractions, and elements from other lists were added as errors.', 'the data set included 2264 extractions across 36 unary relations, including composers and us internet companies']",5
"['ngram models in ads  #TAUTHOR_TAG.', 'an hmm models a sentence w as a sequence of observations']","['ngram models in ads  #TAUTHOR_TAG.', 'an hmm models a sentence w as a sequence of observations']","['ngram models in ads  #TAUTHOR_TAG.', 'an hmm models a sentence w as a sequence of observations']","['statistical language model ( slm ) is a probability distribution p ( w ) over word sequences w = ( w 1,..., w r ).', 'the most common slm techniques are n - gram models, which are markov models in which the probability of a given word is dependent on only the previous n−1 words.', 'the accuracy of an n - gram model of a corpus depends on two key factors : the choice of n, and the smoothing technique employed to assign probabilities to word sequences seen infrequently in training.', 'we experiment with choices of n from 2 to 4, and two popular smoothing approaches, modified kneser - ney  #AUTHOR_TAG and witten - bell  #AUTHOR_TAG.', 'unsupervised hidden markov models ( hmms ) are an alternative slm approach previously shown to offer accuracy and scalability advantages over ngram models in ads  #TAUTHOR_TAG.', 'an hmm models a sentence w as a sequence of observations w i each generated by a hidden state variable t i.', 'here, hidden states take values from { 1,..., t }, and each hidden state variable is itself generated by some number k of previous hidden states.', 'formally, the joint distribution of a word sequence w given a corresponding state sequence t is :', 'the distributions on the right side of equation 1 are learned from the corpus in an unsupervised manner using expectation - maximization, such that words distributed similarly in the corpus tend to be generated by similar hidden states  #AUTHOR_TAG']",0
['hmms  #TAUTHOR_TAG'],['hmms  #TAUTHOR_TAG'],['hmms  #TAUTHOR_TAG'],"['assessment by distributional similarity ( ads ) technique is to rank extractions in u r in decreasing order of distributional similarity to the seeds, as estimated from the corpus.', 'in our experiments, we utilize an ads approach previously proposed for hmms  #TAUTHOR_TAG and adapt it to also apply to n - gram models, as detailed below.', 'define a context of an extraction argument e i to be a string containing the m words preceding and m words following an occurrence of e i in the corpus.', 'let c i = { c 1, c 2,..., c | c i | } be the union of all contexts of extraction arguments e i and seed arguments s i for a given relation r. we create a probabilistic context vector for each extraction e i where the j - th dimension of the vector is the probability of the context surrounding given the extraction, p ( c j | e i ), computed from the language model.', ""1 we rank the extractions in u r according to how similar their arguments'contextual distributions, p ( c | e i ), are to those of the seed arguments."", 'specifically, extractions are ranked according to :', 'where kl represents kl divergence, and the outer sum is taken over arguments e i of the extraction e. for hmms, we alternatively rank extractions using the hmm state distributions p ( t | e i ) in place of the probabilistic context vectors p ( c | e i ).', 'our experiments show that state distributions are much more accurate for ads than are hmm context vectors']",6
"[' #AUTHOR_TAG, or by integrating perceptual information  #TAUTHOR_TAG.', 'while standard dsms have been']","[' #AUTHOR_TAG, or by integrating perceptual information  #TAUTHOR_TAG.', 'while standard dsms have been']","['into neural networks  #AUTHOR_TAG, or by integrating perceptual information  #TAUTHOR_TAG.', 'while standard dsms have been applied to a variety of semantic relatedness tasks']","['semantic models ( dsms ) rely on the distributional hypothesis  #AUTHOR_TAG, that words with similar distributions have related meanings.', 'they represent a well - established tool for modelling semantic relatedness between words and phrases  #AUTHOR_TAG.', 'in the last decade, standard dsms using bag - of - words or syntactic cooccurrence counts have been enhanced by integration into neural networks  #AUTHOR_TAG, or by integrating perceptual information  #TAUTHOR_TAG.', 'while standard dsms have been applied to a variety of semantic relatedness tasks such as word sense discrimination, selectional preferences, relation distinction ( among others ), multi - modal models have predominantly been evaluated on their general ability to model semantic similarity as captured by simlex  #AUTHOR_TAG, wordsim  #AUTHOR_TAG, etc.', 'in this paper, we compare a neural network dsm relying on textual co - occurrences with a multi - modal model extension integrating visual information.', 'we focus on the prediction of compositionality for two types of german multi - word expressions : noun - noun compounds and particle verbs.', 'differently to most previous multimodal approaches, we thus address a semantically specific task that was traditionally addressed by standard dsms, mainly for english and german  #AUTHOR_TAG schulte im  #AUTHOR_TAG bott and schulte im  #AUTHOR_TAG bott and schulte im  #AUTHOR_TAG schulte im  #AUTHOR_TAG a ).', 'furthermore, we zoom into factors that might influence the quality of predictions, such as lexical and empirical target properties ( e. g., ambiguity, frequency, compositionality ) ; and filters to optimise the visual space, such as dispersion and imageability filters  #TAUTHOR_TAG, and a novel clustering filter.', 'our experiments demonstrate that the contributions of the textual and the visual models differ for predictions across the nominal vs. verbal compositions.', 'the visual modality adds complementary features in cases where ( a ) the textual modality performs poorly, and images of the most imaginable targets are added, or ( b ) the textual modality performs well, and all available - potentially noisy - images are added.', 'in addition, we demonstrate that perceptual features of verbs, such as abstractness and imageability, have a different influence on multi - modality than for nouns, presumably because they are more difficult to grasp']",0
"[' #AUTHOR_TAG, or by integrating perceptual information  #TAUTHOR_TAG.', 'while standard dsms have been']","[' #AUTHOR_TAG, or by integrating perceptual information  #TAUTHOR_TAG.', 'while standard dsms have been']","['into neural networks  #AUTHOR_TAG, or by integrating perceptual information  #TAUTHOR_TAG.', 'while standard dsms have been applied to a variety of semantic relatedness tasks']","['semantic models ( dsms ) rely on the distributional hypothesis  #AUTHOR_TAG, that words with similar distributions have related meanings.', 'they represent a well - established tool for modelling semantic relatedness between words and phrases  #AUTHOR_TAG.', 'in the last decade, standard dsms using bag - of - words or syntactic cooccurrence counts have been enhanced by integration into neural networks  #AUTHOR_TAG, or by integrating perceptual information  #TAUTHOR_TAG.', 'while standard dsms have been applied to a variety of semantic relatedness tasks such as word sense discrimination, selectional preferences, relation distinction ( among others ), multi - modal models have predominantly been evaluated on their general ability to model semantic similarity as captured by simlex  #AUTHOR_TAG, wordsim  #AUTHOR_TAG, etc.', 'in this paper, we compare a neural network dsm relying on textual co - occurrences with a multi - modal model extension integrating visual information.', 'we focus on the prediction of compositionality for two types of german multi - word expressions : noun - noun compounds and particle verbs.', 'differently to most previous multimodal approaches, we thus address a semantically specific task that was traditionally addressed by standard dsms, mainly for english and german  #AUTHOR_TAG schulte im  #AUTHOR_TAG bott and schulte im  #AUTHOR_TAG bott and schulte im  #AUTHOR_TAG schulte im  #AUTHOR_TAG a ).', 'furthermore, we zoom into factors that might influence the quality of predictions, such as lexical and empirical target properties ( e. g., ambiguity, frequency, compositionality ) ; and filters to optimise the visual space, such as dispersion and imageability filters  #TAUTHOR_TAG, and a novel clustering filter.', 'our experiments demonstrate that the contributions of the textual and the visual models differ for predictions across the nominal vs. verbal compositions.', 'the visual modality adds complementary features in cases where ( a ) the textual modality performs poorly, and images of the most imaginable targets are added, or ( b ) the textual modality performs well, and all available - potentially noisy - images are added.', 'in addition, we demonstrate that perceptual features of verbs, such as abstractness and imageability, have a different influence on multi - modality than for nouns, presumably because they are more difficult to grasp']",0
"[' #TAUTHOR_TAG.', 'the']","[' #TAUTHOR_TAG.', 'the']","[' #TAUTHOR_TAG.', 'the filter']",[' #TAUTHOR_TAG'],5
"['has been shown by  #TAUTHOR_TAG ; for gs - pv, the opposite is the']","['has been shown by  #TAUTHOR_TAG ; for gs - pv, the opposite is the']","['expect and has been shown by  #TAUTHOR_TAG ; for gs - pv, the opposite is the']","['based on the dispersion and imageability filters. note that the textual model baselines are very different for the two gold standards, ρ =. 65 for gs - nn and ρ =', '. 22 for gs - pv. regarding the nouns, the multi - modality improves the textual modality when adding the images for the ≈35 % most imaginable words, and when adding all images', '. regarding the verbs, the multi - modality improves the textual modality in most proportions, reaching its maximum when adding images for ≈80 % of the most imaginable verbs ; when adding the ≈10 % of the least imagina', '##ble verbs, the model strongly drops', 'in its performance. for the dispersion filter, the tendencies are less clear. we conclude that the visual information adds to the textual information either by adding all ( potentially noisy )', 'images because the textual information is rich by itself ; or by adding a selection of images ( unless they are overly dissimilar to each other, or for non - imaginable targets ), because the textual information by itself is', 'poor. zooming into target subsets, the predictions for monosemous targets are better than those for ambiguous targets ( significant for gs - nn ), see figure 3 ;', 'ditto for low - frequency vs. high - frequency targets. taking frequency as an indicator of ambiguity, these differences are presumably due to the difficulty of distinguishing between multiple', 'senses in vector spaces that subsume the features of all word senses within one vector, which applies to our textual and multi - modal models. the gold standard predictions strongly differ regarding the influence of target abstractness, imageability and compositionality. for gs - nn, the compositionality', 'of concrete and imaginable targets is predicted better than', 'for abstract and less imaginable targets, as one would expect and has been shown by  #TAUTHOR_TAG ; for gs - pv, the opposite is the case. similarly, while for gs - nn highly compositional targets are predicted worse than low - and mid - compositional targets, for gs - pv mid - compositional targets are predicted much worse than', 'low - and high - compositional targets. these differences in results point to questions that have still been unsolved across research fields : while humans can easily grasp intuitions about the abstractness, imageability and compositionality of nouns,', 'the categorisations are difficult to define for verbs  #AUTHOR_TAG. particle verbs add to this complexity, especially since compositionality ( rating ) is typically reduced to the semantic relatedness between the complex verb and the base verb, ignoring the particle that however contributes a', 'considerable portion of meaning to the complex verb']",3
"['##ow  #AUTHOR_TAG, skipgram  #TAUTHOR_TAG, ggm  #AUTHOR_TAG ).', 'but essentially,']","[' #AUTHOR_TAG, rnn  #AUTHOR_TAG, lbl  #AUTHOR_TAG, cbow  #AUTHOR_TAG, skipgram  #TAUTHOR_TAG, ggm  #AUTHOR_TAG ).', 'but essentially,']","['##ow  #AUTHOR_TAG, skipgram  #TAUTHOR_TAG, ggm  #AUTHOR_TAG ).', 'but essentially,']","['embedding models learn a space of continuous word representations, in which similar words are expected to be close to each other.', 'traditionally, the term similar refers to semantic similarity ( e. g. walking should be close to hiking, and happiness to joy ), hence the model performance is usually evaluated using semantic similarity datasets.', 'recently, several works introduced morphology - driven models motivated by the poor performance of traditional models on morphologically complex words.', 'such words are often rare, and there is not enough evidence to model them correctly.', 'the morphology - driven models allow pooling evidence from different words which have the same base form.', 'these models work by learning per - morpheme representations rather than just per - word ones, and compose the representing vector of each word from those of its morphemes - as derived from a supervised or unsupervised morphological analysis - and ( optionally ) its surface form ( e. g. walking = f ( v walk, v ing, v walking ) ).', 'the works differ in the way they acquire morphological knowledge ( from using linguistically derived morphological analyzers on one end, to approximating morphology using substrings while relying on the concatenative nature of morphology, on the other ) and in the model form ( cdsms  #AUTHOR_TAG, rnn  #AUTHOR_TAG, lbl  #AUTHOR_TAG, cbow  #AUTHOR_TAG, skipgram  #TAUTHOR_TAG, ggm  #AUTHOR_TAG ).', 'but essentially, they all show that breaking a word into morphological components ( base form, affixes and potentially also the complete surface form ), learning a vector for each component, and representing a word as a composition of these vectors improves the models semantic performance, especially on rare words.', 'in this work we argue that these models capture two distinct aspects of word similarity, semantic ( e. g. sim ( walking, hiking ) > sim ( walking, eating ) ) and morphological ( e. g. sim ( walking, hiking ) > sim ( walking, hiked ) ), and that these two aspects are at odds with each other ( should sim ( walking, hiking ) be lower or higher than sim ( walking, walked )? ).', 'the base form component of the compositional models is mostly responsible for semantic aspects of the similarity, while the affixes are mostly responsible for morphological similarity.', 'this analysis brings about several natural questions : is the combination of semantic and morphological components used in previous work ideal for every purpose?', ""for example, if we exclude the morphological component from the representations, wouldn't it improve the semantic performance? what is the contribution of using the surface form? and do""]",0
"['the fasttext model  #TAUTHOR_TAG, which in turn extends the skip - gram model of  #AUTHOR_TAG.', 'the skip - gram model takes a sequence of words w 1,...,']","['the fasttext model  #TAUTHOR_TAG, which in turn extends the skip - gram model of  #AUTHOR_TAG.', 'the skip - gram model takes a sequence of words w 1,...,']","['the fasttext model  #TAUTHOR_TAG, which in turn extends the skip - gram model of  #AUTHOR_TAG.', 'the skip - gram model takes a sequence of words w 1,...,']","['model form is a generalization of the fasttext model  #TAUTHOR_TAG, which in turn extends the skip - gram model of  #AUTHOR_TAG.', 'the skip - gram model takes a sequence of words w 1,..., w t and a function s assigning scores to ( word, context ) pairs, and maximizes', 'where [UNK] is the log - sigmoid loss function, c t is a set of context words, and n t is a set of negative examples sampled from the vocabulary.', 's ( w t, w c ) is defined as s ( w t, w c ) = v [UNK] wt u wc ( where v wt and u wc are the embeddings of the focus and the context words ).', ' #AUTHOR_TAG replace the word representation v wt with the set of character ngrams appearing in it : v wt = g∈g ( wt ) v g where g ( w t ) is the set of n - grams appearing in w t.', 'the n - grams are used to approximate the morphemes in the target word.', 'we generalize  #TAUTHOR_TAG by replacing the set of ngrams g ( w ) with a set p ( w ) of explicit linguistic properties.', 'each word w t is then composed as the sum of the vectors of its linguistic properties : v wt = p∈p ( wt ) v p.', ""the linguistic properties we consider are the surface form of the word ( w ), it's lemma ( l ) and its morphological tag ( m ) 1."", 'the lemma corre - sponds to the base - form, and the morphological tag encodes the grammatical properties of the word, from which its inflectional affixes are derived ( a similar approach was taken by cotterell and schutze ( 2015 ) ).', 'moving from a set of ngrams to a set of explicit linguistic properties, allows finer control of the kinds of information in the word representation.', 'we train models with different subsets of { w, l, m }']",3
"['the fasttext model  #TAUTHOR_TAG, which in turn extends the skip - gram model of  #AUTHOR_TAG.', 'the skip - gram model takes a sequence of words w 1,...,']","['the fasttext model  #TAUTHOR_TAG, which in turn extends the skip - gram model of  #AUTHOR_TAG.', 'the skip - gram model takes a sequence of words w 1,...,']","['the fasttext model  #TAUTHOR_TAG, which in turn extends the skip - gram model of  #AUTHOR_TAG.', 'the skip - gram model takes a sequence of words w 1,...,']","['model form is a generalization of the fasttext model  #TAUTHOR_TAG, which in turn extends the skip - gram model of  #AUTHOR_TAG.', 'the skip - gram model takes a sequence of words w 1,..., w t and a function s assigning scores to ( word, context ) pairs, and maximizes', 'where [UNK] is the log - sigmoid loss function, c t is a set of context words, and n t is a set of negative examples sampled from the vocabulary.', 's ( w t, w c ) is defined as s ( w t, w c ) = v [UNK] wt u wc ( where v wt and u wc are the embeddings of the focus and the context words ).', ' #AUTHOR_TAG replace the word representation v wt with the set of character ngrams appearing in it : v wt = g∈g ( wt ) v g where g ( w t ) is the set of n - grams appearing in w t.', 'the n - grams are used to approximate the morphemes in the target word.', 'we generalize  #TAUTHOR_TAG by replacing the set of ngrams g ( w ) with a set p ( w ) of explicit linguistic properties.', 'each word w t is then composed as the sum of the vectors of its linguistic properties : v wt = p∈p ( wt ) v p.', ""the linguistic properties we consider are the surface form of the word ( w ), it's lemma ( l ) and its morphological tag ( m ) 1."", 'the lemma corre - sponds to the base - form, and the morphological tag encodes the grammatical properties of the word, from which its inflectional affixes are derived ( a similar approach was taken by cotterell and schutze ( 2015 ) ).', 'moving from a set of ngrams to a set of explicit linguistic properties, allows finer control of the kinds of information in the word representation.', 'we train models with different subsets of { w, l, m }']",6
"['the fasttext model  #TAUTHOR_TAG, which in turn extends the skip - gram model of  #AUTHOR_TAG.', 'the skip - gram model takes a sequence of words w 1,...,']","['the fasttext model  #TAUTHOR_TAG, which in turn extends the skip - gram model of  #AUTHOR_TAG.', 'the skip - gram model takes a sequence of words w 1,...,']","['the fasttext model  #TAUTHOR_TAG, which in turn extends the skip - gram model of  #AUTHOR_TAG.', 'the skip - gram model takes a sequence of words w 1,...,']","['model form is a generalization of the fasttext model  #TAUTHOR_TAG, which in turn extends the skip - gram model of  #AUTHOR_TAG.', 'the skip - gram model takes a sequence of words w 1,..., w t and a function s assigning scores to ( word, context ) pairs, and maximizes', 'where [UNK] is the log - sigmoid loss function, c t is a set of context words, and n t is a set of negative examples sampled from the vocabulary.', 's ( w t, w c ) is defined as s ( w t, w c ) = v [UNK] wt u wc ( where v wt and u wc are the embeddings of the focus and the context words ).', ' #AUTHOR_TAG replace the word representation v wt with the set of character ngrams appearing in it : v wt = g∈g ( wt ) v g where g ( w t ) is the set of n - grams appearing in w t.', 'the n - grams are used to approximate the morphemes in the target word.', 'we generalize  #TAUTHOR_TAG by replacing the set of ngrams g ( w ) with a set p ( w ) of explicit linguistic properties.', 'each word w t is then composed as the sum of the vectors of its linguistic properties : v wt = p∈p ( wt ) v p.', ""the linguistic properties we consider are the surface form of the word ( w ), it's lemma ( l ) and its morphological tag ( m ) 1."", 'the lemma corre - sponds to the base - form, and the morphological tag encodes the grammatical properties of the word, from which its inflectional affixes are derived ( a similar approach was taken by cotterell and schutze ( 2015 ) ).', 'moving from a set of ngrams to a set of explicit linguistic properties, allows finer control of the kinds of information in the word representation.', 'we train models with different subsets of { w, l, m }']",6
"['implementation is based on the fasttext 2 library  #TAUTHOR_TAG, which we modify']","['implementation is based on the fasttext 2 library  #TAUTHOR_TAG, which we modify']","['implementation is based on the fasttext 2 library  #TAUTHOR_TAG, which we modify']","['implementation is based on the fasttext 2 library  #TAUTHOR_TAG, which we modify as described above.', 'we train the models on the hebrew wikipedia ( ∼4m sentences ), using a window size of 2 to each side of the focus word, and dimensionality of 200.', 'we use the morphological disambiguator of  #AUTHOR_TAG to assign words with their morphological tags, and the inflection dictionary of mila  #AUTHOR_TAG semantic evaluation measure the common datasets for semantic similarity 4 have some notable shortcomings as noted in  #AUTHOR_TAG.', 'we use the evaluation method ( and corresponding hebrew similarity dataset ) that we have introduced in a previous work  #AUTHOR_TAG ( ag ).', 'the ag method defines an annotation task which is more natural for human judges, resulting in datasets with improved annotator - agreement scores.', ""furthermore, the ag's evaluation metric takes annotator agreement into account, by putting less weight on similarities that have lower annotator agreement."", 'an ag dataset is a collection of target - groups, where each group contains a target word ( e. g. singer ) and three types of candidate words : positives which are words "" similar "" to the target ( e. g. musician ), distractors which are words "" related but dissimilar "" to the target ( e. g. microphone ), and randoms which are not related to the target at all ( e. g laptop ).', '']",6
"['n - gram based fasttext model of  #TAUTHOR_TAG that does not require morphological analysis.', 'the results ( table 2 ) highlight']","['state - of - the - art n - gram based fasttext model of  #TAUTHOR_TAG that does not require morphological analysis.', 'the results ( table 2 ) highlight']","['n - gram based fasttext model of  #TAUTHOR_TAG that does not require morphological analysis.', 'the results ( table 2 ) highlight the following :', '']","['compare the different models on the different measures, and also compare to the state - of - the - art n - gram based fasttext model of  #TAUTHOR_TAG that does not require morphological analysis.', 'the results ( table 2 ) highlight the following :', '1. there is a trade - off between semantic and morphological performance - improving one aspect comes at the expense of the other : the lemma component improves semantics but hurts morphology, while the opposite is true for the tag component.', 'the common practice of using both components together is a kind of compromise : the lm, wlm and n - grams models are not the best nor the worst on any measure.', '']",7
"['existence in hierarchical structure.', 'in particular, in  #TAUTHOR_TAG we assess the']","['existence in hierarchical structure.', 'in particular, in  #TAUTHOR_TAG we assess the']","['as evidence for the existence in hierarchical structure.', 'in particular, in  #TAUTHOR_TAG we assess the ability of lstms']","['', 'recent work examines the extent to which rnn - based models capture syntax - sensitive phenomena that are traditionally taken as evidence for the existence in hierarchical structure.', 'in particular, in  #TAUTHOR_TAG we assess the ability of lstms to learn subject - verb agreement patterns in english, and evaluate on naturally occurring wikipedia sentences.', ' #AUTHOR_TAG also consider subject - verb agreement, but in a "" colorless green ideas "" setting in which content words in naturally occurring sentences are replaced with random words with the same partof - speech and inflection, thus ensuring a focus on syntax rather than on selectional - preferences based cues.', ' #AUTHOR_TAG consider a wider range of syntactic phenomena ( subjectverb agreement, reflexive anaphora, negative polarity items ) using manually constructed stimuli, allowing for greater coverage and control than in the naturally occurring setting.', 'the bert model is based on the "" transformer "" architecture  #AUTHOR_TAG, which - in contrast to rnns - relies purely on attention mechanisms, and does not have an explicit notion of word order beyond marking each word with its absolute - position embedding.', 'this reliance on attention may lead one 1 to expect decreased performance on syntax - sensitive tasks compared to rnn ( lstm ) models that do model word order directly, and explicitly track states across the sentence.', ' #AUTHOR_TAG finds that transformerbased models perform worse than lstm models on the  #TAUTHOR_TAG agreement prediction dataset.', 'in contrast,  #AUTHOR_TAG find that self - attention performs on par with lstm for syntax sensitive dependencies in the context of machine - translation, and performance on syntactic tasks is correlated with the number of attention heads in multi - head attention.', 'i adapt the evaluation protocol and stimuli of  #TAUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG to the bidirectional setting required by bert, and evaluate the pretrained bert models ( both the large and the base models ).', 'surprisingly ( at least to me ), the out - of - the - box models ( without any task - specific fine - tuning ) perform very well on all the syntactic tasks']",4
"['##8 discarded pairs ( out of 152, 300 ). i similarly discard', '680 sentences from  #TAUTHOR_TAG where']","['##8 discarded pairs ( out of 152, 300 ). i similarly discard', '680 sentences from  #TAUTHOR_TAG where']","['##8 discarded pairs ( out of 152, 300 ). i similarly discard', '680 sentences from  #TAUTHOR_TAG where the focus verb']","['single word in the bert wordpiece - based vocabulary ( and hence cannot be predicted by the model ). this include discarding  #AUTHOR_TAG stimuli involving the words swims or adm', '##ires, resulting in 23, 368 discarded pairs ( out of 152, 300 ). i similarly discard', '680 sentences from  #TAUTHOR_TAG where the focus verb or its inflection were one of 108 out - ofvocabulary tokens, 6 and 28 sentence - pairs (', '8 tokens 7 ) from  #AUTHOR_TAG.', 'limitations the bert results are not directly comparable to the numbers reported in previous work. beyond the differences due to bidirectionality and the discarded stimuli, the bert models are also trained on a different and larger corpus ( covering', 'both wikipedia and books ). 4 https :', '/ / github. com / huggingface / pytorch - pretrained - bert 5 results are generally a bit higher when not discarding the is / are cases. 6 blames, dislike, inhabit, exclude, revolves, governs, delete, composes, overlap, edit', '##s, embrace, compose, undertakes, disagrees, redirect', ', persist, recognise, rotates, accompanies', ', attach, undertake, earn, communicates,', 'imagine, contradicts, specialize, accuses, obtain', ', caters, welcomes, interprets,', 'await, communicate, templates, qualify, reverts, achieve, achieves, govern, restricts, violate, behave, emit, contend, adopt, overlaps, reproduces, rotate,', 'defends, submit, revolve, lend, pertain, disagree, concentrate, detects, endors', '##es, detect, predate, persists, consume, locates, earns, predict, interact', ', merge, consumes, behaves, locate, predates, enhances, predicts, integrates, inhabits, satisfy, contradict, swear, activate, restrict, satisfies, redirect', '##s, excludes, violates, interacts, admires, speculate, blame, drag, qualifies, activates, criticize, assures, welcome, depart, characterizes, defend, obtains, lends', ', strives, accuse, recognises, characterize, contends, perceive, compl', '##ain, awaits 7 toss, spills, tosses, affirms, spill, melt, approves, affirm table 2 : results on the en nonce  #AUTHOR_TAG stimuli. while not strictly comparable,', 'the numbers reported by  #AUTHOR_TAG for the lstm in this condition ( on all ) is 74. 1 ± 1. 6']",4
"['use the stimuli provided by  #TAUTHOR_TAG, but change the experimental protocol to']","['use the stimuli provided by  #TAUTHOR_TAG, but change the experimental protocol to']","['use the stimuli provided by  #TAUTHOR_TAG, but change the experimental protocol to']","['use the stimuli provided by  #TAUTHOR_TAG, but change the experimental protocol to adapt it to the bidirectional nature of the bert model.', 'this requires discarding some of the stimuli, as described below.', 'thus, the numbers are not strictly comparable to those reported in previous work']",6
"['##8 discarded pairs ( out of 152, 300 ). i similarly discard', '680 sentences from  #TAUTHOR_TAG where']","['##8 discarded pairs ( out of 152, 300 ). i similarly discard', '680 sentences from  #TAUTHOR_TAG where']","['##8 discarded pairs ( out of 152, 300 ). i similarly discard', '680 sentences from  #TAUTHOR_TAG where the focus verb']","['single word in the bert wordpiece - based vocabulary ( and hence cannot be predicted by the model ). this include discarding  #AUTHOR_TAG stimuli involving the words swims or adm', '##ires, resulting in 23, 368 discarded pairs ( out of 152, 300 ). i similarly discard', '680 sentences from  #TAUTHOR_TAG where the focus verb or its inflection were one of 108 out - ofvocabulary tokens, 6 and 28 sentence - pairs (', '8 tokens 7 ) from  #AUTHOR_TAG.', 'limitations the bert results are not directly comparable to the numbers reported in previous work. beyond the differences due to bidirectionality and the discarded stimuli, the bert models are also trained on a different and larger corpus ( covering', 'both wikipedia and books ). 4 https :', '/ / github. com / huggingface / pytorch - pretrained - bert 5 results are generally a bit higher when not discarding the is / are cases. 6 blames, dislike, inhabit, exclude, revolves, governs, delete, composes, overlap, edit', '##s, embrace, compose, undertakes, disagrees, redirect', ', persist, recognise, rotates, accompanies', ', attach, undertake, earn, communicates,', 'imagine, contradicts, specialize, accuses, obtain', ', caters, welcomes, interprets,', 'await, communicate, templates, qualify, reverts, achieve, achieves, govern, restricts, violate, behave, emit, contend, adopt, overlaps, reproduces, rotate,', 'defends, submit, revolve, lend, pertain, disagree, concentrate, detects, endors', '##es, detect, predate, persists, consume, locates, earns, predict, interact', ', merge, consumes, behaves, locate, predates, enhances, predicts, integrates, inhabits, satisfy, contradict, swear, activate, restrict, satisfies, redirect', '##s, excludes, violates, interacts, admires, speculate, blame, drag, qualifies, activates, criticize, assures, welcome, depart, characterizes, defend, obtains, lends', ', strives, accuse, recognises, characterize, contends, perceive, compl', '##ain, awaits 7 toss, spills, tosses, affirms, spill, melt, approves, affirm table 2 : results on the en nonce  #AUTHOR_TAG stimuli. while not strictly comparable,', 'the numbers reported by  #AUTHOR_TAG for the lstm in this condition ( on all ) is 74. 1 ± 1. 6']",6
"['##8 discarded pairs ( out of 152, 300 ). i similarly discard', '680 sentences from  #TAUTHOR_TAG where']","['##8 discarded pairs ( out of 152, 300 ). i similarly discard', '680 sentences from  #TAUTHOR_TAG where']","['##8 discarded pairs ( out of 152, 300 ). i similarly discard', '680 sentences from  #TAUTHOR_TAG where the focus verb']","['single word in the bert wordpiece - based vocabulary ( and hence cannot be predicted by the model ). this include discarding  #AUTHOR_TAG stimuli involving the words swims or adm', '##ires, resulting in 23, 368 discarded pairs ( out of 152, 300 ). i similarly discard', '680 sentences from  #TAUTHOR_TAG where the focus verb or its inflection were one of 108 out - ofvocabulary tokens, 6 and 28 sentence - pairs (', '8 tokens 7 ) from  #AUTHOR_TAG.', 'limitations the bert results are not directly comparable to the numbers reported in previous work. beyond the differences due to bidirectionality and the discarded stimuli, the bert models are also trained on a different and larger corpus ( covering', 'both wikipedia and books ). 4 https :', '/ / github. com / huggingface / pytorch - pretrained - bert 5 results are generally a bit higher when not discarding the is / are cases. 6 blames, dislike, inhabit, exclude, revolves, governs, delete, composes, overlap, edit', '##s, embrace, compose, undertakes, disagrees, redirect', ', persist, recognise, rotates, accompanies', ', attach, undertake, earn, communicates,', 'imagine, contradicts, specialize, accuses, obtain', ', caters, welcomes, interprets,', 'await, communicate, templates, qualify, reverts, achieve, achieves, govern, restricts, violate, behave, emit, contend, adopt, overlaps, reproduces, rotate,', 'defends, submit, revolve, lend, pertain, disagree, concentrate, detects, endors', '##es, detect, predate, persists, consume, locates, earns, predict, interact', ', merge, consumes, behaves, locate, predates, enhances, predicts, integrates, inhabits, satisfy, contradict, swear, activate, restrict, satisfies, redirect', '##s, excludes, violates, interacts, admires, speculate, blame, drag, qualifies, activates, criticize, assures, welcome, depart, characterizes, defend, obtains, lends', ', strives, accuse, recognises, characterize, contends, perceive, compl', '##ain, awaits 7 toss, spills, tosses, affirms, spill, melt, approves, affirm table 2 : results on the en nonce  #AUTHOR_TAG stimuli. while not strictly comparable,', 'the numbers reported by  #AUTHOR_TAG for the lstm in this condition ( on all ) is 74. 1 ± 1. 6']",6
"['grammar  #TAUTHOR_TAG.', 'dp inference tackles this problem by exploring']","['grammar  #TAUTHOR_TAG.', 'dp inference tackles this problem by exploring']","['the grammar  #TAUTHOR_TAG.', 'dp inference tackles this problem by']","['is a deep tension in statistical modeling of grammatical structure between providing good expressivity - to allow accurate modeling of the data with sparse grammars - and low complexitymaking induction of the grammars and parsing of novel sentences computationally practical.', 'recent work that incorporated dirichlet process ( dp ) nonparametric models into tsgs has provided an efficient solution to the problem of segmenting training data trees into elementary parse tree fragments to form the grammar  #TAUTHOR_TAG.', 'dp inference tackles this problem by exploring the space of all possible segmentations of the data, in search for fragments that are on the one hand large enough so that they incorporate the useful dependencies, and on the other small enough so that they recur and have a chance to be useful in analyzing unseen data.', 'the elementary trees combined in a tsg are, intuitively, primitives of the language, yet certain linguistic phenomena ( notably various forms of modification ) "" split them up "", preventing their reuse, leading to less sparse grammars than might be ideal.', 'for instance, imagine modeling the following set of structures :', '• a natural recurring structure here would be the structure "" [ n p the [ n n president ] ] "", yet it occurs not at all in the data.', '']",0
"['occur  #TAUTHOR_TAG.', 'we extend this model by adding specialized dps for']","['occur  #TAUTHOR_TAG.', 'we extend this model by adding specialized dps for']","['substitutions should occur  #TAUTHOR_TAG.', 'we extend this model by adding specialized dps for']","['the basic nonparametric tsg model, there is an independent dp for every grammar category ( such as c = n p ), each of which uses a base distribution p 0 that generates an initial tree by making stepwise decisions.', 'the canonical p 0 uses a probabilistic cfgp that is fixed a priori to sample cfg rules top - down and bernoulli variables for determining where substitutions should occur  #TAUTHOR_TAG.', 'we extend this model by adding specialized dps for left and right auxiliary trees.', '3', 'therefore, we have an exchangeable process for generating right auxiliary trees', 'as for initial trees in tsg.', 'we must define three distinct base distributions for initial trees, left auxiliary trees, and right auxiliary trees.', 'p init 0 generates an initial tree with root label c by sampling cfg rules fromp and making a binary decision at every node generated whether to leave it as a frontier node or further expand ( with probability β c )  #AUTHOR_TAG.', 'similarly, our p right 0 generates a right auxiliary tree with root label c by first making a binary decision whether to generate an immediate foot or not ( with probability γ right c ), and then sampling an appropriate cfg rule 3 we use right insertions for illustration ; the symmetric analog applies to left insertions']",0
"['occur  #TAUTHOR_TAG.', 'we extend this model by adding specialized dps for']","['occur  #TAUTHOR_TAG.', 'we extend this model by adding specialized dps for']","['substitutions should occur  #TAUTHOR_TAG.', 'we extend this model by adding specialized dps for']","['the basic nonparametric tsg model, there is an independent dp for every grammar category ( such as c = n p ), each of which uses a base distribution p 0 that generates an initial tree by making stepwise decisions.', 'the canonical p 0 uses a probabilistic cfgp that is fixed a priori to sample cfg rules top - down and bernoulli variables for determining where substitutions should occur  #TAUTHOR_TAG.', 'we extend this model by adding specialized dps for left and right auxiliary trees.', '3', 'therefore, we have an exchangeable process for generating right auxiliary trees', 'as for initial trees in tsg.', 'we must define three distinct base distributions for initial trees, left auxiliary trees, and right auxiliary trees.', 'p init 0 generates an initial tree with root label c by sampling cfg rules fromp and making a binary decision at every node generated whether to leave it as a frontier node or further expand ( with probability β c )  #AUTHOR_TAG.', 'similarly, our p right 0 generates a right auxiliary tree with root label c by first making a binary decision whether to generate an immediate foot or not ( with probability γ right c ), and then sampling an appropriate cfg rule 3 we use right insertions for illustration ; the symmetric analog applies to left insertions']",6
"['in a joint fashion  #TAUTHOR_TAG.', 'this is']","['in a joint fashion  #TAUTHOR_TAG.', 'this is']","['in a joint fashion  #TAUTHOR_TAG.', 'this is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept / reject to achieve convergence into the correct posterior  #AUTHOR_TAG.', 'since our base distributions factor']","['this model, our inference task is to explore optimal derivations underlying the data.', 'since tig derivations are highly structured objects, a basic sampling strategy based on local node - level moves such as gibbs sampling  #AUTHOR_TAG would not hold much promise.', 'following previous work, we design a blocked metropolis - hastings sampler that samples derivations per entire parse trees all at once in a joint fashion  #TAUTHOR_TAG.', 'this is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept / reject to achieve convergence into the correct posterior  #AUTHOR_TAG.', 'since our base distributions factorize over levels of tree, cfg is the most convenient choice for a cfg rule cfg probability proposal distribution.', ' #AUTHOR_TAG provide an ( exact ) transformation from a fully general tig into a tsg that generates the same string languages.', 'it is then straightforward to represent this tsg as a cfg using the goodman transform  #TAUTHOR_TAG.', 'figure 4 lists the additional cfg productions we have designed, as well as the rules used that trigger them']",5
"['in a joint fashion  #TAUTHOR_TAG.', 'this is']","['in a joint fashion  #TAUTHOR_TAG.', 'this is']","['in a joint fashion  #TAUTHOR_TAG.', 'this is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept / reject to achieve convergence into the correct posterior  #AUTHOR_TAG.', 'since our base distributions factor']","['this model, our inference task is to explore optimal derivations underlying the data.', 'since tig derivations are highly structured objects, a basic sampling strategy based on local node - level moves such as gibbs sampling  #AUTHOR_TAG would not hold much promise.', 'following previous work, we design a blocked metropolis - hastings sampler that samples derivations per entire parse trees all at once in a joint fashion  #TAUTHOR_TAG.', 'this is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept / reject to achieve convergence into the correct posterior  #AUTHOR_TAG.', 'since our base distributions factorize over levels of tree, cfg is the most convenient choice for a cfg rule cfg probability proposal distribution.', ' #AUTHOR_TAG provide an ( exact ) transformation from a fully general tig into a tsg that generates the same string languages.', 'it is then straightforward to represent this tsg as a cfg using the goodman transform  #TAUTHOR_TAG.', 'figure 4 lists the additional cfg productions we have designed, as well as the rules used that trigger them']",5
['system of  #TAUTHOR_TAG ('],['implementation of the tsg system of  #TAUTHOR_TAG ( referred'],['implementation of the tsg system of  #TAUTHOR_TAG ('],[' #TAUTHOR_TAG'],5
['system of  #TAUTHOR_TAG ('],['implementation of the tsg system of  #TAUTHOR_TAG ( referred'],['implementation of the tsg system of  #TAUTHOR_TAG ('],[' #TAUTHOR_TAG'],3
"[' #TAUTHOR_TAG additionally, we', 'will also introduce other technical focuses']","['processing and generation natural language.  #TAUTHOR_TAG additionally, we', 'will also introduce other technical focuses']","[' #TAUTHOR_TAG additionally, we', 'will also introduce other technical focuses']","['recent years 1. in contrast to the focus of gans in computer vision, natural language processing researchers have taken a', 'broader approach to adversarial learning. for example, three core technical subareas for adversarial learning include : • adversarial examples, where researchers focus on learning or creating advers', '##arial examples or rules to improve the robustness of nlp systems.  #AUTHOR_TAG a, b ;  #AUTHOR_TAG b ;  #AUTHOR_TAG • adversarial training, which focuses on adding noise', ', randomness, or adversarial loss during optimization.  #AUTHOR_TAG a ;  #AUTHOR_TAG c ;  #AUTHOR_TAG • adversarial generation, which primarily includes practical', 'solutions of gans for processing and generation natural language.  #TAUTHOR_TAG additionally, we', 'will also introduce other technical focuses such as negative sampling and contrastive estimation  #AUTHOR_TAG, adversarial evaluation  #AUTHOR_TAG reward learning  #AUTHOR_TAG c ).', 'in particular, we will also provide a gentle introduction to the applications of adversarial learning in different nlp problems, including social media  #AUTHOR_TAG a ;', ' #AUTHOR_TAG, domain adaptation  #AUTHOR_TAG b ), data cleaning  #AUTHOR_TAG, information extraction  #AUTHOR_TAG b ;  #AUTHOR_TAG, and information retrieval  #AUTHOR_TAG. adversarial learning', 'methods could easily combine any representation learning based neural networks, and optimize for complex problems in nlp. however, a key challenge for applying deep adversarial', 'learning techniques to real - world sized nlp problems is the model design issue. this tutorial draws connections from theories of deep adversarial learning to practical applications in nlp. in particular, we start with the gentle introduction to the fundamentals of adversarial learning. we further', 'discuss their modern deep learning extensions such as generative adversarial networks  #AUTHOR_TAG. in the first part of the tutorial, we', 'also outline various applications of deep adversarial learning in nlp listed above. in the second part of the tutorial, we will focus', 'on generation of adversarial examples and their uses in nlp tasks, including ( 1 )', 'the inclusion and creation of adversarial examples for robust nlp ; ( 2 ) the', 'usage of adversarial rules for interpretable and explainable models ; and ( 3', ') the relationship between adversarial training and adversarial examples. in the third part of the tutorial, we focus on gan', '##s. we start with the general background introduction of generative adversarial learning. we will introduce an in - depth case study of generative adversarial networks for nlp, with a focus on dialogue generation  #TAUTHOR_TAG. this tutorial aims at introducing deep adversarial learning methods to researchers in the nlp community. we do not assume any particular prior knowledge in adversarial learning. the intended', 'length of the tutorial is 3. 5 hours, including a coffee break', '']",7
"[' #TAUTHOR_TAG additionally, we', 'will also introduce other technical focuses']","['processing and generation natural language.  #TAUTHOR_TAG additionally, we', 'will also introduce other technical focuses']","[' #TAUTHOR_TAG additionally, we', 'will also introduce other technical focuses']","['recent years 1. in contrast to the focus of gans in computer vision, natural language processing researchers have taken a', 'broader approach to adversarial learning. for example, three core technical subareas for adversarial learning include : • adversarial examples, where researchers focus on learning or creating advers', '##arial examples or rules to improve the robustness of nlp systems.  #AUTHOR_TAG a, b ;  #AUTHOR_TAG b ;  #AUTHOR_TAG • adversarial training, which focuses on adding noise', ', randomness, or adversarial loss during optimization.  #AUTHOR_TAG a ;  #AUTHOR_TAG c ;  #AUTHOR_TAG • adversarial generation, which primarily includes practical', 'solutions of gans for processing and generation natural language.  #TAUTHOR_TAG additionally, we', 'will also introduce other technical focuses such as negative sampling and contrastive estimation  #AUTHOR_TAG, adversarial evaluation  #AUTHOR_TAG reward learning  #AUTHOR_TAG c ).', 'in particular, we will also provide a gentle introduction to the applications of adversarial learning in different nlp problems, including social media  #AUTHOR_TAG a ;', ' #AUTHOR_TAG, domain adaptation  #AUTHOR_TAG b ), data cleaning  #AUTHOR_TAG, information extraction  #AUTHOR_TAG b ;  #AUTHOR_TAG, and information retrieval  #AUTHOR_TAG. adversarial learning', 'methods could easily combine any representation learning based neural networks, and optimize for complex problems in nlp. however, a key challenge for applying deep adversarial', 'learning techniques to real - world sized nlp problems is the model design issue. this tutorial draws connections from theories of deep adversarial learning to practical applications in nlp. in particular, we start with the gentle introduction to the fundamentals of adversarial learning. we further', 'discuss their modern deep learning extensions such as generative adversarial networks  #AUTHOR_TAG. in the first part of the tutorial, we', 'also outline various applications of deep adversarial learning in nlp listed above. in the second part of the tutorial, we will focus', 'on generation of adversarial examples and their uses in nlp tasks, including ( 1 )', 'the inclusion and creation of adversarial examples for robust nlp ; ( 2 ) the', 'usage of adversarial rules for interpretable and explainable models ; and ( 3', ') the relationship between adversarial training and adversarial examples. in the third part of the tutorial, we focus on gan', '##s. we start with the general background introduction of generative adversarial learning. we will introduce an in - depth case study of generative adversarial networks for nlp, with a focus on dialogue generation  #TAUTHOR_TAG. this tutorial aims at introducing deep adversarial learning methods to researchers in the nlp community. we do not assume any particular prior knowledge in adversarial learning. the intended', 'length of the tutorial is 3. 5 hours, including a coffee break', '']",0
['##ing two - agent gan models for conversational ai  #TAUTHOR_TAG'],"['on policy gradient and monte carlo tree search.', 'finally, we provide an in - depth case study of deploying two - agent gan models for conversational ai  #TAUTHOR_TAG']","['##ing two - agent gan models for conversational ai  #TAUTHOR_TAG.', 'we will summarize']","['- robust representation learning, adversarial learning, and generation are three closely related research subjects in natural language processing.', 'in this tutorial, we touch the intersection of all the three research subjects, covering various aspects of the theories of modern deep adversarial learning methods, and show their successful applications in nlp.', 'this tutorial is organized in three parts :', '• foundations of deep adversarial learning.', 'first, we will provide a brief overview of adversarial learning ( rl ), and discuss the cutting - edge settings in nlp.', 'we describe methods such as adversarial training  #AUTHOR_TAG, negative sampling, and noise contrastive estimation  #AUTHOR_TAG.', 'we introduce domain - adaptation learning approaches, and the widely used data cleaning and information extraction methods  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'in this part, we also introduce the modern renovation of deep generative adversarial learning  #AUTHOR_TAG, with a focus on nlp  #AUTHOR_TAG.', '• adversarial examples for nlp second, we will focus on the designing practical adversarial examples for nlp tasks.', 'in particular, we will provide an overview of recent methods, including their categorization by whether they are white ( e. g.  #AUTHOR_TAG a )  #AUTHOR_TAG.', '• an in - depth case study of gans in nlp.', 'third, we switch from the focuses of adversarial training and adversarial examples to generative adversarial networks  #AUTHOR_TAG.', 'we will discuss why it is challenging to deploy gans for nlp problems, comparing to vision problems.', 'we then focus on introducing seq - gan  #AUTHOR_TAG, an early solution of textual models of gan, with a focus on policy gradient and monte carlo tree search.', 'finally, we provide an in - depth case study of deploying two - agent gan models for conversational ai  #TAUTHOR_TAG.', 'we will summarize the lessons learned, and how we can move forward to investigate game - theoretical approaches in advancing nlp problems']",5
['shared task  #TAUTHOR_TAG based on all representative symbols used for'],['news 2009 machine transliteration shared task  #TAUTHOR_TAG based on all representative symbols used for'],['##ting an element in s. 1 the generation task is part of the news 2009 machine transliteration shared task  #TAUTHOR_TAG based on all representative symbols used for'],"['transliteration is the automatic transformation of a word in a source language to a phonetically equivalent word in a target language that uses a different writing system.', 'transliteration is important for various natural language processing ( nlp ) applications including : cross lingual information retrieval ( clir ), and machine translation ( mt ).', 'this paper introduces a system that utilizes parameters learned for a pair hidden markov model ( pair hmm ) in a shared transliteration generation task 1.', 'the pair hmm has been used before  #AUTHOR_TAG for string similarity estimation, and is based on the notion of string edit distance ( ed ).', 'string ed is defined here as the total edit cost incurred in transforming a source language string ( s ) to a target language string ( t ) through a sequence of edit operations.', 'the edit operations include : ( m ) atching an element in s with an element in t ; ( i ) nserting an element into t, and ( d ) eleting an element in s. 1 the generation task is part of the news 2009 machine transliteration shared task  #TAUTHOR_TAG based on all representative symbols used for each of the two languages, emission costs for each of the edit operations and transition parameters can be estimated and used in measuring the similarity between two strings.', 'to generate transliterations using pair hmm parameters, wfst  #AUTHOR_TAG techniques are adopted.', 'transliteration training is based mainly on the initial orthographic representation and no explicit phonetic scheme is used.', 'instead, transliteration quality is tested for different bigram combinations including all english vowel bigram combinations and n - gram combinations specified for cyrillic romanization by the us board on geographic names and british permanent committee on geographic names ( bgn / pcgn ).', 'however, transliteration parameters can still be estimated for a pair hmm when a particular phonetic representation scheme is used.', 'the quality of transliterations generated using pair hmm parameters is evaluated against transliterations generated from training wfsts and transliterations generated using a phrase - based statistical machine translation ( pbsmt ) system.', 'section 2 describes the components of the transliteration system that uses pair hmm parameters ; section 3 gives the experimental set up and results associated with the transliterations generated ; and section 4 concludes the paper']",0
['shared task  #TAUTHOR_TAG based on all representative symbols used for'],['news 2009 machine transliteration shared task  #TAUTHOR_TAG based on all representative symbols used for'],['##ting an element in s. 1 the generation task is part of the news 2009 machine transliteration shared task  #TAUTHOR_TAG based on all representative symbols used for'],"['transliteration is the automatic transformation of a word in a source language to a phonetically equivalent word in a target language that uses a different writing system.', 'transliteration is important for various natural language processing ( nlp ) applications including : cross lingual information retrieval ( clir ), and machine translation ( mt ).', 'this paper introduces a system that utilizes parameters learned for a pair hidden markov model ( pair hmm ) in a shared transliteration generation task 1.', 'the pair hmm has been used before  #AUTHOR_TAG for string similarity estimation, and is based on the notion of string edit distance ( ed ).', 'string ed is defined here as the total edit cost incurred in transforming a source language string ( s ) to a target language string ( t ) through a sequence of edit operations.', 'the edit operations include : ( m ) atching an element in s with an element in t ; ( i ) nserting an element into t, and ( d ) eleting an element in s. 1 the generation task is part of the news 2009 machine transliteration shared task  #TAUTHOR_TAG based on all representative symbols used for each of the two languages, emission costs for each of the edit operations and transition parameters can be estimated and used in measuring the similarity between two strings.', 'to generate transliterations using pair hmm parameters, wfst  #AUTHOR_TAG techniques are adopted.', 'transliteration training is based mainly on the initial orthographic representation and no explicit phonetic scheme is used.', 'instead, transliteration quality is tested for different bigram combinations including all english vowel bigram combinations and n - gram combinations specified for cyrillic romanization by the us board on geographic names and british permanent committee on geographic names ( bgn / pcgn ).', 'however, transliteration parameters can still be estimated for a pair hmm when a particular phonetic representation scheme is used.', 'the quality of transliterations generated using pair hmm parameters is evaluated against transliterations generated from training wfsts and transliterations generated using a phrase - based statistical machine translation ( pbsmt ) system.', 'section 2 describes the components of the transliteration system that uses pair hmm parameters ; section 3 gives the experimental set up and results associated with the transliterations generated ; and section 4 concludes the paper']",5
['shared transliteration task  #TAUTHOR_TAG : a standard run and'],['shared transliteration task  #TAUTHOR_TAG : a standard run and'],['shared transliteration task  #TAUTHOR_TAG : a standard run and'],"['data used is divided according to the experimental runs that were specified for the news 2009 shared transliteration task  #TAUTHOR_TAG : a standard run and non - standard runs.', 'the standard run involved using the transliteration system described above that uses pair hmm parameters combined with transformation rules.', 'the english - russian datasets used here were provided for the news 2009 shared transliteration task  #AUTHOR_TAG : 5977 pairs of names for training, 943 pairs for development, and 1000 for testing.', 'for the non - standard runs, an additional english - russian dataset extracted from the geonames data dump was merged with the shared transliteration task data above to form 10481 pairs for training and development.', 'for a second set of experiments ( table 2 ), a different set of test data ( 1000 pairs ) extracted from the geonames data dump was used.', 'for the system used in the standard run, the training data was preprocessed to include representation of bigrams associated with cyrillic romanization and all english vowel bigram combinations']",5
"['evaluating system transliteration quality.', 'these include  #TAUTHOR_TAG :']","['evaluating system transliteration quality.', 'these include  #TAUTHOR_TAG :']","['evaluating system transliteration quality.', 'these include  #TAUTHOR_TAG :']","['measures were used for evaluating system transliteration quality.', 'these include  #TAUTHOR_TAG : accuracy ( acc ), fuzziness in top - 1 ( mean f score ), mean reciprocal rank ( mrr ), mean average precision for reference transliterations ( map _ r ), mean average precision in 10 best candidate transliterations ( map _ 10 ), mean average precision for the system ( map _ sys ).', 'table 1 shows the results obtained using only the data sets provided for the shared transliteration task.', 'the system used for the standard run is "" phmm _ rules "" described in section 2 to sub section 2. 3. "" phmm _ basic "" is the system in which pair hmm parameters are used for transliteration generation but there is no representation for bigrams as described for the system used in the standard run.', 'table 2 shows the results obtained when additional data from geonames data dump was used for training and development.', 'in table 2, "" wfst _ basic "" and "" wfst _ rules "" are systems associated with training wfsts for the "" phmm _ basic "" and "" phmm _ rules "" systems table 2 results from additional geonames data sets.', 'respectively.', 'moses _ psmt is the phrase - based statistical machine translation system.', 'the results in both tables show that the systems using pair hmm parameters perform relatively better than the systems trained on wfsts but not better than moses.', 'the low transliteration quality in the pair hmm and wfst systems as compared to moses can be attributed to lack of modeling contextual dependencies unlike the case in pbsmt']",5
"['evaluating system transliteration quality.', 'these include  #TAUTHOR_TAG :']","['evaluating system transliteration quality.', 'these include  #TAUTHOR_TAG :']","['evaluating system transliteration quality.', 'these include  #TAUTHOR_TAG :']","['measures were used for evaluating system transliteration quality.', 'these include  #TAUTHOR_TAG : accuracy ( acc ), fuzziness in top - 1 ( mean f score ), mean reciprocal rank ( mrr ), mean average precision for reference transliterations ( map _ r ), mean average precision in 10 best candidate transliterations ( map _ 10 ), mean average precision for the system ( map _ sys ).', 'table 1 shows the results obtained using only the data sets provided for the shared transliteration task.', 'the system used for the standard run is "" phmm _ rules "" described in section 2 to sub section 2. 3. "" phmm _ basic "" is the system in which pair hmm parameters are used for transliteration generation but there is no representation for bigrams as described for the system used in the standard run.', 'table 2 shows the results obtained when additional data from geonames data dump was used for training and development.', 'in table 2, "" wfst _ basic "" and "" wfst _ rules "" are systems associated with training wfsts for the "" phmm _ basic "" and "" phmm _ rules "" systems table 2 results from additional geonames data sets.', 'respectively.', 'moses _ psmt is the phrase - based statistical machine translation system.', 'the results in both tables show that the systems using pair hmm parameters perform relatively better than the systems trained on wfsts but not better than moses.', 'the low transliteration quality in the pair hmm and wfst systems as compared to moses can be attributed to lack of modeling contextual dependencies unlike the case in pbsmt']",7
"[' #TAUTHOR_TAG.', 'it is my view that']","[' #TAUTHOR_TAG.', 'it is my view that']","['suggested in three papers in this volume  #TAUTHOR_TAG.', 'it is my view that']","['', 'standards can be built on.', 'for example, if one accepts the framework of the penn treebank, it is easy to move on to representations of "" deeper "" structure as suggested in three papers in this volume  #TAUTHOR_TAG.', 'it is my view that these advantages outweigh the disadvantages.', '']",0
"[' #TAUTHOR_TAG.', 'it is my view that']","[' #TAUTHOR_TAG.', 'it is my view that']","['suggested in three papers in this volume  #TAUTHOR_TAG.', 'it is my view that']","['', 'standards can be built on.', 'for example, if one accepts the framework of the penn treebank, it is easy to move on to representations of "" deeper "" structure as suggested in three papers in this volume  #TAUTHOR_TAG.', 'it is my view that these advantages outweigh the disadvantages.', '']",0
"[' #TAUTHOR_TAG.', 'it is my view that']","[' #TAUTHOR_TAG.', 'it is my view that']","['suggested in three papers in this volume  #TAUTHOR_TAG.', 'it is my view that']","['', 'standards can be built on.', 'for example, if one accepts the framework of the penn treebank, it is easy to move on to representations of "" deeper "" structure as suggested in three papers in this volume  #TAUTHOR_TAG.', 'it is my view that these advantages outweigh the disadvantages.', '']",0
"[' #TAUTHOR_TAG.', 'it is my view that']","[' #TAUTHOR_TAG.', 'it is my view that']","['suggested in three papers in this volume  #TAUTHOR_TAG.', 'it is my view that']","['', 'standards can be built on.', 'for example, if one accepts the framework of the penn treebank, it is easy to move on to representations of "" deeper "" structure as suggested in three papers in this volume  #TAUTHOR_TAG.', 'it is my view that these advantages outweigh the disadvantages.', '']",0
,,,,0
,,,,0
['not include the additional qb data  #TAUTHOR_TAG used to improve the'],['not include the additional qb data  #TAUTHOR_TAG used to improve the'],['include the additional qb data  #TAUTHOR_TAG used to improve the performance of mstparser and maltparser'],"['', 'compared to the other statistical dependency parsers, questions ( oq ) are not well represented in our training data, since they do not include the additional qb data  #TAUTHOR_TAG used to improve the performance of mstparser and maltparser']",0
"['papers  #TAUTHOR_TAG, we']","['papers  #TAUTHOR_TAG, we']","['previous papers  #TAUTHOR_TAG, we evaluate the parser on']","['in previous papers  #TAUTHOR_TAG, we evaluate the parser on its ability to recover ldds.', 'two evaluations were done.', 'the first one was semi - automatic, performed with a modified version of the evaluation script developed in  #AUTHOR_TAG.', '']",0
"['- processing scheme of  #TAUTHOR_TAG, which']","['two nodes indirectly is precisely defined in the post - processing scheme of  #TAUTHOR_TAG, which']","['of arc, a set of equivalence classes between single arcs and paths connecting two nodes indirectly is precisely defined in the post - processing scheme of  #TAUTHOR_TAG, which']","['relax the requirement of exact match on the definition of arc, a set of equivalence classes between single arcs and paths connecting two nodes indirectly is precisely defined in the post - processing scheme of  #TAUTHOR_TAG, which applies to the stanford labelling scheme.', 'in  #TAUTHOR_TAG, the encoding of long - distance dependencies in a dependency parser is categorised as simple, complex, and indirect.', '']",0
"['- processing scheme of  #TAUTHOR_TAG, which']","['two nodes indirectly is precisely defined in the post - processing scheme of  #TAUTHOR_TAG, which']","['of arc, a set of equivalence classes between single arcs and paths connecting two nodes indirectly is precisely defined in the post - processing scheme of  #TAUTHOR_TAG, which']","['relax the requirement of exact match on the definition of arc, a set of equivalence classes between single arcs and paths connecting two nodes indirectly is precisely defined in the post - processing scheme of  #TAUTHOR_TAG, which applies to the stanford labelling scheme.', 'in  #TAUTHOR_TAG, the encoding of long - distance dependencies in a dependency parser is categorised as simple, complex, and indirect.', '']",0
"['evaluations  #TAUTHOR_TAG.', '8 these papers compare']","['evaluations  #TAUTHOR_TAG.', '8 these papers compare']","['to the relevant ones of those reported in previous evaluations  #TAUTHOR_TAG.', '8 these papers compare several statistical parsers.', '']","['and manual results ( percent recall ) are shown in table 1, where we compare our results to the relevant ones of those reported in previous evaluations  #TAUTHOR_TAG.', '8 these papers compare several statistical parsers.', 'some parsers like nguyen, the c & c parser  #AUTHOR_TAG and enju  #AUTHOR_TAG are based on rich grammatical formalisms, and others others are representative of statistical dependency parsers ( mst, malt, ( mc  #AUTHOR_TAG these last two parsers constitute the relevant comparison for our approach.', '9 like the other parsers discussed in  #AUTHOR_TAG and  #TAUTHOR_TAG, the overall performance on these long - distance constructions is much lower than the overall scores for this parser.', 'however, the parser recovers long - distance dependencies at least as well as standard statistical dependency parsers that use a post - processing step, and better than standard statistical parsers.', '10 the differences in recall between manual and automatic evaluation in table 1 show that the automatic evaluation is sometimes too strict and sometimes too lenient.', 'the former cases arise primarily in small clause dependencies and dependency recovery by coordination across all ldd constructions, which were taken into account in the manual evaluation, but not in the automatic evaluation, because, as indicated above, scoring coordination automatically is too difficult.', 'this explains the recall difference between the two evaluation methods in src and semb.', 'the latter case is due to the stricter definition of head in the manual evaluation.', 'this is the main reason why ored and oq have lower recall in this evaluation.', 'table 2 reports some of the labelled error counts of the most frequent labels.', 'in general, the confusion matrix shows that the labelled correspondence is accurate, and that it corresponds to meaningful generalisations.', 'as can also be observed, a single grammatical function label corresponds to several different semantic relations and vice versa.', 'full recovery of argument structure, then, requires both grammatical syntactic relations and semantic role labelling']",0
,,,,1
['not include the additional qb data  #TAUTHOR_TAG used to improve the'],['not include the additional qb data  #TAUTHOR_TAG used to improve the'],['include the additional qb data  #TAUTHOR_TAG used to improve the performance of mstparser and maltparser'],"['', 'compared to the other statistical dependency parsers, questions ( oq ) are not well represented in our training data, since they do not include the additional qb data  #TAUTHOR_TAG used to improve the performance of mstparser and maltparser']",4
"['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG, the parser was not trained on the same data']","['the dependency parser in  #TAUTHOR_TAG, the parser was not trained on the same data or tree representations as those used in the test data.', 'the parser is trained on the data derived by merging a dependency transformation of the penn treebank with propbank and nombank  #AUTHOR_TAG.', 'an illustrative example of the kind of labelled structures that we need to parse was given in figure 3.', 'training and development data follow the usual partition as sections 02 - 21, 24 of the penn treebank.', 'more details and references on the data, and the conversion of the penn treebank format to dependencies are given in  #AUTHOR_TAG.', 'like for standard statistical and dependency parsers, the syntactic representation used by the two - level parser has been stripped of all traces.', 'the predicates of the argument structures and their locations are not provided at testing, unlike some of the conll shared tasks.', 'unlike  #TAUTHOR_TAG, we did not use an external part - of - speech tagger to annotate the data of the development set.', 'to minimize preprocessing of the data, we choose to have part - ofspeech tagging as an internal part of the parsing model, which therefore, takes raw input.', 'in order for our results to be comparable to those reported in previous evaluations  #TAUTHOR_TAG, we ran the parser "" out of the box "" directly on the test sentences, without using the development sentences to finetune.', 'we were able to parse all the sentences in the test suites without any adjustments to the parser.', '']",4
"['evaluations  #TAUTHOR_TAG.', '8 these papers compare']","['evaluations  #TAUTHOR_TAG.', '8 these papers compare']","['to the relevant ones of those reported in previous evaluations  #TAUTHOR_TAG.', '8 these papers compare several statistical parsers.', '']","['and manual results ( percent recall ) are shown in table 1, where we compare our results to the relevant ones of those reported in previous evaluations  #TAUTHOR_TAG.', '8 these papers compare several statistical parsers.', 'some parsers like nguyen, the c & c parser  #AUTHOR_TAG and enju  #AUTHOR_TAG are based on rich grammatical formalisms, and others others are representative of statistical dependency parsers ( mst, malt, ( mc  #AUTHOR_TAG these last two parsers constitute the relevant comparison for our approach.', '9 like the other parsers discussed in  #AUTHOR_TAG and  #TAUTHOR_TAG, the overall performance on these long - distance constructions is much lower than the overall scores for this parser.', 'however, the parser recovers long - distance dependencies at least as well as standard statistical dependency parsers that use a post - processing step, and better than standard statistical parsers.', '10 the differences in recall between manual and automatic evaluation in table 1 show that the automatic evaluation is sometimes too strict and sometimes too lenient.', 'the former cases arise primarily in small clause dependencies and dependency recovery by coordination across all ldd constructions, which were taken into account in the manual evaluation, but not in the automatic evaluation, because, as indicated above, scoring coordination automatically is too difficult.', 'this explains the recall difference between the two evaluation methods in src and semb.', 'the latter case is due to the stricter definition of head in the manual evaluation.', 'this is the main reason why ored and oq have lower recall in this evaluation.', 'table 2 reports some of the labelled error counts of the most frequent labels.', 'in general, the confusion matrix shows that the labelled correspondence is accurate, and that it corresponds to meaningful generalisations.', 'as can also be observed, a single grammatical function label corresponds to several different semantic relations and vice versa.', 'full recovery of argument structure, then, requires both grammatical syntactic relations and semantic role labelling']",4
['on the development set based on  #TAUTHOR_TAG'],['on the development set based on  #TAUTHOR_TAG'],['on the development set based on  #TAUTHOR_TAG one'],"['classify the errors made by our parser on the development set based on  #TAUTHOR_TAG one which occurs when the parser fails to assign the correct functional relation ( e. g., subject, object ), while a sem error is one in which the parser fails to assign the correct semantic relation ( e. g., a1, a2 ).', '']",4
"['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG, the parser was not trained on the same data']","['the dependency parser in  #TAUTHOR_TAG, the parser was not trained on the same data or tree representations as those used in the test data.', 'the parser is trained on the data derived by merging a dependency transformation of the penn treebank with propbank and nombank  #AUTHOR_TAG.', 'an illustrative example of the kind of labelled structures that we need to parse was given in figure 3.', 'training and development data follow the usual partition as sections 02 - 21, 24 of the penn treebank.', 'more details and references on the data, and the conversion of the penn treebank format to dependencies are given in  #AUTHOR_TAG.', 'like for standard statistical and dependency parsers, the syntactic representation used by the two - level parser has been stripped of all traces.', 'the predicates of the argument structures and their locations are not provided at testing, unlike some of the conll shared tasks.', 'unlike  #TAUTHOR_TAG, we did not use an external part - of - speech tagger to annotate the data of the development set.', 'to minimize preprocessing of the data, we choose to have part - ofspeech tagging as an internal part of the parsing model, which therefore, takes raw input.', 'in order for our results to be comparable to those reported in previous evaluations  #TAUTHOR_TAG, we ran the parser "" out of the box "" directly on the test sentences, without using the development sentences to finetune.', 'we were able to parse all the sentences in the test suites without any adjustments to the parser.', '']",3
"['papers  #TAUTHOR_TAG, we']","['papers  #TAUTHOR_TAG, we']","['previous papers  #TAUTHOR_TAG, we evaluate the parser on']","['in previous papers  #TAUTHOR_TAG, we evaluate the parser on its ability to recover ldds.', 'two evaluations were done.', 'the first one was semi - automatic, performed with a modified version of the evaluation script developed in  #AUTHOR_TAG.', '']",3
"['- processing scheme of  #TAUTHOR_TAG, which']","['two nodes indirectly is precisely defined in the post - processing scheme of  #TAUTHOR_TAG, which']","['of arc, a set of equivalence classes between single arcs and paths connecting two nodes indirectly is precisely defined in the post - processing scheme of  #TAUTHOR_TAG, which']","['relax the requirement of exact match on the definition of arc, a set of equivalence classes between single arcs and paths connecting two nodes indirectly is precisely defined in the post - processing scheme of  #TAUTHOR_TAG, which applies to the stanford labelling scheme.', 'in  #TAUTHOR_TAG, the encoding of long - distance dependencies in a dependency parser is categorised as simple, complex, and indirect.', '']",3
"['evaluations  #TAUTHOR_TAG.', '8 these papers compare']","['evaluations  #TAUTHOR_TAG.', '8 these papers compare']","['to the relevant ones of those reported in previous evaluations  #TAUTHOR_TAG.', '8 these papers compare several statistical parsers.', '']","['and manual results ( percent recall ) are shown in table 1, where we compare our results to the relevant ones of those reported in previous evaluations  #TAUTHOR_TAG.', '8 these papers compare several statistical parsers.', 'some parsers like nguyen, the c & c parser  #AUTHOR_TAG and enju  #AUTHOR_TAG are based on rich grammatical formalisms, and others others are representative of statistical dependency parsers ( mst, malt, ( mc  #AUTHOR_TAG these last two parsers constitute the relevant comparison for our approach.', '9 like the other parsers discussed in  #AUTHOR_TAG and  #TAUTHOR_TAG, the overall performance on these long - distance constructions is much lower than the overall scores for this parser.', 'however, the parser recovers long - distance dependencies at least as well as standard statistical dependency parsers that use a post - processing step, and better than standard statistical parsers.', '10 the differences in recall between manual and automatic evaluation in table 1 show that the automatic evaluation is sometimes too strict and sometimes too lenient.', 'the former cases arise primarily in small clause dependencies and dependency recovery by coordination across all ldd constructions, which were taken into account in the manual evaluation, but not in the automatic evaluation, because, as indicated above, scoring coordination automatically is too difficult.', 'this explains the recall difference between the two evaluation methods in src and semb.', 'the latter case is due to the stricter definition of head in the manual evaluation.', 'this is the main reason why ored and oq have lower recall in this evaluation.', 'table 2 reports some of the labelled error counts of the most frequent labels.', 'in general, the confusion matrix shows that the labelled correspondence is accurate, and that it corresponds to meaningful generalisations.', 'as can also be observed, a single grammatical function label corresponds to several different semantic relations and vice versa.', 'full recovery of argument structure, then, requires both grammatical syntactic relations and semantic role labelling']",3
"['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG, the parser was not trained on the same data']","['the dependency parser in  #TAUTHOR_TAG, the parser was not trained on the same data or tree representations as those used in the test data.', 'the parser is trained on the data derived by merging a dependency transformation of the penn treebank with propbank and nombank  #AUTHOR_TAG.', 'an illustrative example of the kind of labelled structures that we need to parse was given in figure 3.', 'training and development data follow the usual partition as sections 02 - 21, 24 of the penn treebank.', 'more details and references on the data, and the conversion of the penn treebank format to dependencies are given in  #AUTHOR_TAG.', 'like for standard statistical and dependency parsers, the syntactic representation used by the two - level parser has been stripped of all traces.', 'the predicates of the argument structures and their locations are not provided at testing, unlike some of the conll shared tasks.', 'unlike  #TAUTHOR_TAG, we did not use an external part - of - speech tagger to annotate the data of the development set.', 'to minimize preprocessing of the data, we choose to have part - ofspeech tagging as an internal part of the parsing model, which therefore, takes raw input.', 'in order for our results to be comparable to those reported in previous evaluations  #TAUTHOR_TAG, we ran the parser "" out of the box "" directly on the test sentences, without using the development sentences to finetune.', 'we were able to parse all the sentences in the test suites without any adjustments to the parser.', '']",7
['on the development set based on  #TAUTHOR_TAG'],['on the development set based on  #TAUTHOR_TAG'],['on the development set based on  #TAUTHOR_TAG one'],"['classify the errors made by our parser on the development set based on  #TAUTHOR_TAG one which occurs when the parser fails to assign the correct functional relation ( e. g., subject, object ), while a sem error is one in which the parser fails to assign the correct semantic relation ( e. g., a1, a2 ).', '']",5
"['corpus.', 'using a multilayer convolutional encoder - decoder neural network gec approach  #TAUTHOR_TAG, we evaluate the']","['corpus.', 'using a multilayer convolutional encoder - decoder neural network gec approach  #TAUTHOR_TAG, we evaluate the']","['in the gold corpus.', 'using a multilayer convolutional encoder - decoder neural network gec approach  #TAUTHOR_TAG, we evaluate the contribution of wikipedia edits and find that carefully selected wikipedia edits increase performance by over 5 %']","['develop a grammatical error correction ( gec ) system for german using a small gold gec corpus augmented with edits extracted from wikipedia revision history.', 'we extend the automatic error annotation tool errant  #AUTHOR_TAG for german and use it to analyze both gold gec corrections and wikipedia edits ( grundkiewicz and junczys  #AUTHOR_TAG in order to select as additional training data wikipedia edits containing grammatical corrections similar to those in the gold corpus.', 'using a multilayer convolutional encoder - decoder neural network gec approach  #TAUTHOR_TAG, we evaluate the contribution of wikipedia edits and find that carefully selected wikipedia edits increase performance by over 5 %']",5
"['following sections describe the data and resources used in our experiments on gec for german.', 'we create a new gec corpus for german along with the models needed for the neural gec approach presented in  #TAUTHOR_TAG.', '']","['following sections describe the data and resources used in our experiments on gec for german.', 'we create a new gec corpus for german along with the models needed for the neural gec approach presented in  #TAUTHOR_TAG.', '']","['following sections describe the data and resources used in our experiments on gec for german.', 'we create a new gec corpus for german along with the models needed for the neural gec approach presented in  #TAUTHOR_TAG.', '']","['following sections describe the data and resources used in our experiments on gec for german.', 'we create a new gec corpus for german along with the models needed for the neural gec approach presented in  #TAUTHOR_TAG.', 'throughout this paper we will refer to the source sentence as the original and the target sentence as the correction']",5
"['- merlin training data plus the complete plain wikipedia article text.', 'as suggested by  #TAUTHOR_TAG, we encode the wikipedia article text using the bpe model and learn fasttext embeddings  #AUTHOR_TAG with 500 dimensions']","['a byte pair encoding ( bpe )  #AUTHOR_TAG with 30k symbols using the corrections from the falko - merlin training data plus the complete plain wikipedia article text.', 'as suggested by  #TAUTHOR_TAG, we encode the wikipedia article text using the bpe model and learn fasttext embeddings  #AUTHOR_TAG with 500 dimensions']","[')  #AUTHOR_TAG with 30k symbols using the corrections from the falko - merlin training data plus the complete plain wikipedia article text.', 'as suggested by  #TAUTHOR_TAG, we encode the wikipedia article text using the bpe model and learn fasttext embeddings  #AUTHOR_TAG with 500 dimensions']","['learn a byte pair encoding ( bpe )  #AUTHOR_TAG with 30k symbols using the corrections from the falko - merlin training data plus the complete plain wikipedia article text.', 'as suggested by  #TAUTHOR_TAG, we encode the wikipedia article text using the bpe model and learn fasttext embeddings  #AUTHOR_TAG with 500 dimensions']",5
"['multilayer convolutional encoder - decoder neural network approach from  #TAUTHOR_TAG, using the']","['multilayer convolutional encoder - decoder neural network approach from  #TAUTHOR_TAG, using the']","['evaluate the effect of extending the falko - merlin gec corpus with wikipedia edits for a german gec system using the multilayer convolutional encoder - decoder neural network approach from  #TAUTHOR_TAG, using the same parameters as for english.', '8 we train a single model for each condition']","['evaluate the effect of extending the falko - merlin gec corpus with wikipedia edits for a german gec system using the multilayer convolutional encoder - decoder neural network approach from  #TAUTHOR_TAG, using the same parameters as for english.', '8 we train a single model for each condition and evaluate on the falko - merlin test set using m 2 scorer  #AUTHOR_TAG the results, presented in table 3, show that the addition of both unfiltered and filtered wikipedia edits to the falko - merlin gec training data lead to improvements in performance, however larger numbers of unfiltered edits ( > 250k ) do not consistently lead to improvements, similar to the results for english in grundkiewicz and junczys  #AUTHOR_TAG.', 'however for filtered edits, increasing the number of additional edits from 100k to 1m continues to lead to improvements, with an overall improvement of 5. 2 f 0. 5 for 1m edits over the baseline without additional reranking.', 'in contrast to the results for english in  #TAUTHOR_TAG in order to explore the possibility of developing gec systems for languages with fewer resources, we trained models solely on wikipedia edits, which leads to a huge drop in performance ( 45. 22 vs. 24. 37 f 0. 5 ).', 'however, the genre differences may be too large to draw solid conclusions and this approach may benefit from further work on wikipedia edit selection, such as using a language model to exclude some wikipedia edits that introduce ( rather than correct ) grammatical errors']",5
"['', 'we evaluate our method using the multilayer convolutional encoder - decoder neural network gec approach from  #TAUTHOR_TAG and find that augmenting a small']","['', 'we evaluate our method using the multilayer convolutional encoder - decoder neural network gec approach from  #TAUTHOR_TAG and find that augmenting a small']","['', 'we evaluate our method using the multilayer convolutional encoder - decoder neural network gec approach from  #TAUTHOR_TAG and find that augmenting a small gold german gec corpus with one million filtered wikipedia edits improves the performance from']","['', 'wikipedia edits are extracted using wiki edits ( grundkiewicz and junczys -  #AUTHOR_TAG, profiled with er - rant, and filtered with reference to the gold gec data.', 'we evaluate our method using the multilayer convolutional encoder - decoder neural network gec approach from  #TAUTHOR_TAG and find that augmenting a small gold german gec corpus with one million filtered wikipedia edits improves the performance from 39. 22 to 44. 47 f 0. 5 and additional language model reranking increases performance to 45. 22.', 'the data and source code for this paper are available at : https : / / github. com / adrianeboyd / boyd - wnut2018']",5
['systems use ensembles of neural mt models  #TAUTHOR_TAG and hybrid systems with both statistical and neural mt models ( grund'],['gec systems use ensembles of neural mt models  #TAUTHOR_TAG and hybrid systems with both statistical and neural mt models'],['use ensembles of neural mt models  #TAUTHOR_TAG and hybrid systems with both statistical and neural mt models ( grund'],"['the past decade, there has been a great deal of research on grammatical error correction for english including a series of shared tasks, helping our own in 2011 and 2012  #AUTHOR_TAG and the conll 2013 and 2014 shared tasks  #AUTHOR_TAG ( ng et al.,, 2014, which have contributed to the development of larger english gec corpora.', 'on the basis of these resources along with advances in machine translation, the current state - of - the - art english gec systems use ensembles of neural mt models  #TAUTHOR_TAG and hybrid systems with both statistical and neural mt models ( grundkiewicz and junczys -  #AUTHOR_TAG.', 'in addition to using gold gec corpora, which are typically fairly small in the context of mtbased approaches, research in gec has taken a number of alternate data sources into consideration such as artificially generated errors ( e. g.,  #AUTHOR_TAG, crowd - sourced corrections ( e. g.,  #AUTHOR_TAG, or errors from native language resources ( e. g.,  #AUTHOR_TAG grundkiewicz and junczys -  #AUTHOR_TAG.', ""for english, grundkiewicz and junczys  #AUTHOR_TAG extracted pairs of edited sentences from the wikipedia revision history and filtered them based on a profile of gold gec data in order to extend the training data for a statistical mt gec system and found that the addition of filtered edits improved the system's f 0. 5 score by 2 %."", 'for languages with more limited resources, native language resources such as wikipedia offer an easily accessible source of additional data.', 'using a similar approach that extends existing gold gec data with wikipedia edits, we develop a neural machine translation grammatical error correction system for a new language, in this instance german, for which there are only small gold gec corpora but plentiful native language resources']",0
"['multilayer convolutional encoder - decoder neural network approach from  #TAUTHOR_TAG, using the']","['multilayer convolutional encoder - decoder neural network approach from  #TAUTHOR_TAG, using the']","['evaluate the effect of extending the falko - merlin gec corpus with wikipedia edits for a german gec system using the multilayer convolutional encoder - decoder neural network approach from  #TAUTHOR_TAG, using the same parameters as for english.', '8 we train a single model for each condition']","['evaluate the effect of extending the falko - merlin gec corpus with wikipedia edits for a german gec system using the multilayer convolutional encoder - decoder neural network approach from  #TAUTHOR_TAG, using the same parameters as for english.', '8 we train a single model for each condition and evaluate on the falko - merlin test set using m 2 scorer  #AUTHOR_TAG the results, presented in table 3, show that the addition of both unfiltered and filtered wikipedia edits to the falko - merlin gec training data lead to improvements in performance, however larger numbers of unfiltered edits ( > 250k ) do not consistently lead to improvements, similar to the results for english in grundkiewicz and junczys  #AUTHOR_TAG.', 'however for filtered edits, increasing the number of additional edits from 100k to 1m continues to lead to improvements, with an overall improvement of 5. 2 f 0. 5 for 1m edits over the baseline without additional reranking.', 'in contrast to the results for english in  #TAUTHOR_TAG in order to explore the possibility of developing gec systems for languages with fewer resources, we trained models solely on wikipedia edits, which leads to a huge drop in performance ( 45. 22 vs. 24. 37 f 0. 5 ).', 'however, the genre differences may be too large to draw solid conclusions and this approach may benefit from further work on wikipedia edit selection, such as using a language model to exclude some wikipedia edits that introduce ( rather than correct ) grammatical errors']",7
"['', '.  #TAUTHOR_TAG followed the']","['', '.  #TAUTHOR_TAG followed the']","['', '.  #TAUTHOR_TAG followed the']","['', '.  #TAUTHOR_TAG followed the method of [ 8 ], but tried to resolve the ambiguous relative problem by using just unambiguous relatives.', '']",0
"['', '.  #TAUTHOR_TAG followed the']","['', '.  #TAUTHOR_TAG followed the']","['', '.  #TAUTHOR_TAG followed the']","['', '.  #TAUTHOR_TAG followed the method of [ 8 ], but tried to resolve the ambiguous relative problem by using just unambiguous relatives.', '']",0
"['', '.  #TAUTHOR_TAG followed the']","['', '.  #TAUTHOR_TAG followed the']","['', '.  #TAUTHOR_TAG followed the']","['', '.  #TAUTHOR_TAG followed the method of [ 8 ], but tried to resolve the ambiguous relative problem by using just unambiguous relatives.', '']",0
"['', '.  #TAUTHOR_TAG followed the']","['', '.  #TAUTHOR_TAG followed the']","['', '.  #TAUTHOR_TAG followed the']","['', '.  #TAUTHOR_TAG followed the method of [ 8 ], but tried to resolve the ambiguous relative problem by using just unambiguous relatives.', '']",0
"['', '.  #TAUTHOR_TAG followed the']","['', '.  #TAUTHOR_TAG followed the']","['', '.  #TAUTHOR_TAG followed the']","['', '.  #TAUTHOR_TAG followed the method of [ 8 ], but tried to resolve the ambiguous relative problem by using just unambiguous relatives.', '']",0
"['8 ] and  #TAUTHOR_TAG, respectively. it is']","['8 ] and  #TAUTHOR_TAG, respectively. it is observed in the table that']","['8 ] and  #TAUTHOR_TAG, respectively. it is observed in the table that']",[' #TAUTHOR_TAG'],0
"['8 ] and  #TAUTHOR_TAG, respectively. it is']","['8 ] and  #TAUTHOR_TAG, respectively. it is observed in the table that']","['8 ] and  #TAUTHOR_TAG, respectively. it is observed in the table that']",[' #TAUTHOR_TAG'],0
"['8 ] and  #TAUTHOR_TAG, respectively. it is']","['8 ] and  #TAUTHOR_TAG, respectively. it is observed in the table that']","['8 ] and  #TAUTHOR_TAG, respectively. it is observed in the table that']",[' #TAUTHOR_TAG'],0
['use of ambiguous relatives as well as unambiguous relatives unlike  #TAUTHOR_TAG and'],['use of ambiguous relatives as well as unambiguous relatives unlike  #TAUTHOR_TAG and'],['use of ambiguous relatives as well as unambiguous relatives unlike  #TAUTHOR_TAG and'],"['', 'in this example, the relative stork is selected with the highest probability and the proper sense is determined as crane # 1, which is related to the selected relative stork.', 'our method makes use of ambiguous relatives as well as unambiguous relatives unlike  #TAUTHOR_TAG and hence overcomes the shortage problem of relatives and also reduces the problem of ambiguous relatives in [ 8 ] by handling relatives separately instead of putting example sentences of the relatives together into a pool']",4
"['8 ] and  #TAUTHOR_TAG, respectively. it is']","['8 ] and  #TAUTHOR_TAG, respectively. it is observed in the table that']","['8 ] and  #TAUTHOR_TAG, respectively. it is observed in the table that']",[' #TAUTHOR_TAG'],4
"['8 ] and  #TAUTHOR_TAG, respectively. it is']","['8 ] and  #TAUTHOR_TAG, respectively. it is observed in the table that']","['8 ] and  #TAUTHOR_TAG, respectively. it is observed in the table that']",[' #TAUTHOR_TAG'],4
"['8 ] and  #TAUTHOR_TAG, respectively. it is']","['8 ] and  #TAUTHOR_TAG, respectively. it is observed in the table that']","['8 ] and  #TAUTHOR_TAG, respectively. it is observed in the table that']",[' #TAUTHOR_TAG'],4
['8 ] and  #TAUTHOR_TAG'],['correctly disambiguates senses than [ 8 ] and  #TAUTHOR_TAG'],"['8 ] and  #TAUTHOR_TAG.', 'furthermore,']","['have proposed a simple and novel method that determines senses of all contents words in sentences by selecting a relative of the target words in wordnet.', 'the relative is selected by using a co - occurrence frequency between the relative and the words surrounding the target word in a given sentence.', 'the cooccurrence frequencies are obtained from a raw corpus, not from a sense tagged corpus that is often required by other approaches.', 'we tested the proposed method on semcor data and senseval data, which are publicly available.', 'the experimental results show that the proposed method effectively disambiguates many ambiguous words in semcor and in test data for senseval all words task, as well as a small number of ambiguous words in test data for senseval lexical sample task.', 'also our method more correctly disambiguates senses than [ 8 ] and  #TAUTHOR_TAG.', 'furthermore, the proposed method achieved comparable performance with the top 3 ranked systems at senseval - 2 & 3.', 'in consequence, our method has two advantages over the previous methods ( [ 8 ] and  #TAUTHOR_TAG : our method 1 ) handles the ambiguous relatives and unambiguous relatives more effectively, and 2 ) utilizes only one co - occurrence matrix for disambiguating all contents words instead of collecting training data of the content words.', '']",4
['8 ] and  #TAUTHOR_TAG'],['correctly disambiguates senses than [ 8 ] and  #TAUTHOR_TAG'],"['8 ] and  #TAUTHOR_TAG.', 'furthermore,']","['have proposed a simple and novel method that determines senses of all contents words in sentences by selecting a relative of the target words in wordnet.', 'the relative is selected by using a co - occurrence frequency between the relative and the words surrounding the target word in a given sentence.', 'the cooccurrence frequencies are obtained from a raw corpus, not from a sense tagged corpus that is often required by other approaches.', 'we tested the proposed method on semcor data and senseval data, which are publicly available.', 'the experimental results show that the proposed method effectively disambiguates many ambiguous words in semcor and in test data for senseval all words task, as well as a small number of ambiguous words in test data for senseval lexical sample task.', 'also our method more correctly disambiguates senses than [ 8 ] and  #TAUTHOR_TAG.', 'furthermore, the proposed method achieved comparable performance with the top 3 ranked systems at senseval - 2 & 3.', 'in consequence, our method has two advantages over the previous methods ( [ 8 ] and  #TAUTHOR_TAG : our method 1 ) handles the ambiguous relatives and unambiguous relatives more effectively, and 2 ) utilizes only one co - occurrence matrix for disambiguating all contents words instead of collecting training data of the content words.', '']",4
['of  #AUTHOR_TAG to polylingual topic models  #TAUTHOR_TAG'],['of  #AUTHOR_TAG to polylingual topic models  #TAUTHOR_TAG'],"['', 'the need for bilingual training data in many language pairs and domains also makes it attractive to mitigate the quadratic runtime of brute force translation detection.', 'we begin in § 2 by extending the online variational bayes approach of  #AUTHOR_TAG to polylingual topic models  #TAUTHOR_TAG.', 'then, in § 3, we']","['', 'representing documents as points in a lowdimensional shared latent space abstracts away from the specific words used in each document, thereby facilitating the analysis of relationships between documents written using different vocabularies.', 'for instance, topic models have been used to identify scientific communities working on related problems in different disciplines, e. g., work on cancer funded by multiple institutes within the nih  #AUTHOR_TAG.', 'while vocabulary mismatch occurs within the realm of one language, naturally this mismatch occurs across different languages.', 'therefore, mapping documents in different languages into a common latent topic space can be of great benefit when detecting document translation pairs  #AUTHOR_TAG.', 'aside from the benefits that it offers in the task of detecting document translation pairs, topic models offer potential benefits to the task of creating translation lexica, aligning passages, etc.', 'the process of discovering relationship between documents using topic models involves : ( 1 ) representing documents in the latent space by inferring their topic distributions and ( 2 ) comparing pairs of topic distributions to find close matches.', 'many widely used techniques do not scale efficiently, however, as the size of the document collection grows.', 'posterior inference by gibbs sampling, for instance, may make thousands of passes through the data.', 'for the task of comparing topic distributions, recent work has also resorted to comparing all pairs of documents  #AUTHOR_TAG.', 'this paper presents efficient methods for both of these steps and performs empirical evaluations on the task of detected translated document pairs embedded in a large multilingual corpus.', 'unlike some more exploratory applications of topic models, translation detection is easy to evaluate.', 'the need for bilingual training data in many language pairs and domains also makes it attractive to mitigate the quadratic runtime of brute force translation detection.', 'we begin in § 2 by extending the online variational bayes approach of  #AUTHOR_TAG to polylingual topic models  #TAUTHOR_TAG.', 'then, in § 3, we']",5
"[' #TAUTHOR_TAG.', 'given a multilingual']","[' #TAUTHOR_TAG.', 'given a multilingual']","[' #TAUTHOR_TAG.', 'given a multilingual set of aligned']","['generative bayesian models, such as topic models, have proven to be very effective for modeling document collections and discovering underlying latent semantic structures.', 'most current topic models are based on latent dirichlet allocation ( lda ).', 'in some early work on the subject, showed the usefulness of lda on the task of automatic annotation of images.', ' #AUTHOR_TAG used lda to analyze historical trends in the scientific literature ;  #AUTHOR_TAG showed improvements on an information retrieval task.', 'more recently  #AUTHOR_TAG modeled geographic linguistic variation using twitter data.', 'aside from their widespread use on monolingual text, topic models have also been used to model multilingual data ( boyd -  #AUTHOR_TAG jagarlamudi and daume, 2010 ;  #AUTHOR_TAG, to name a few.', 'in this paper, we focus on the polylingual topic model, introduced by  #TAUTHOR_TAG.', 'given a multilingual set of aligned documents, the pltm assumes that across an aligned multilingual document tuple, there exists a single, tuple - specific, distribution across topics.', 'in addition, pltm assumes that for each language - topic pair, there exists a distribution over words in that language β l.', '']",5
"['', ' #TAUTHOR_TAG introduced polylingual topic models ( pltm ), an extension']","['( clir ) tasks has been explored through various techniques.', ' #TAUTHOR_TAG introduced polylingual topic models ( pltm ), an extension']","['( clir ) tasks has been explored through various techniques.', ' #TAUTHOR_TAG introduced polylingual topic models ( pltm ), an extension']","['multilingual documents into a common, language - independent vector space for the purpose of improving machine translation ( mt ) and performing cross - language information retrieval ( clir ) tasks has been explored through various techniques.', ' #TAUTHOR_TAG introduced polylingual topic models ( pltm ), an extension of latent dirichlet allocation ( lda ), and, more recently,  #AUTHOR_TAG proposed extensions of principal component analysis ( pca ) and probabilistic latent semantic indexing ( plsi ).', 'both the pltm and plsi represent bilingual documents in the probability simplex, and thus the task of finding document translation pairs is formulated as finding similar probability distributions.', 'while the nature of both works was exploratory, results shown on fairly large collections of bilingual documents ( less than 20k documents ) offer convincing argument of their potential.', 'expanding these approaches to much large collections of multilingual documents would require utilizing fast nn search for computing similarity in the probability simplex.', 'while there are many other proposed approaches to the task of finding document translation pairs that represent documents in metric space, such as  #AUTHOR_TAG which utilizes lsh for cosine distance, there is no evidence that they yield good results on documents of small lengths such as paragraphs and even sen - tences.', 'in this section, we empirically show how to utilize approaches that deal with representing documents in the probability simplex without a significant loss in accuracy while significantly improving the processing time.', 'we use pltm representations of bilingual documents.', 'in addition, we show how the results as reported by  #AUTHOR_TAG can be obtained using the pltm representation with a significant speed improvement.', 'as in  #AUTHOR_TAG and  #AUTHOR_TAG the task is to find document translation pairs in a multilingual collection of documents by representing documents in the probability simplex and computing similarity between their probability distribution representation across all document pairs.', 'for this experimental setup, accuracy is defined as the number of times ( in percentage ) that the target language document was discovered at rank 1 ( i. e. % @ rank 1. ) across the whole test collection']",5
"['##l  #AUTHOR_TAG  #TAUTHOR_TAG, these performance comparisons are not']","[' #AUTHOR_TAG  #TAUTHOR_TAG, these performance comparisons are not']","['used in  #AUTHOR_TAG.', 'that paper used the europarl  #AUTHOR_TAG  #TAUTHOR_TAG, these performance comparisons are not']","[""use mallet's ( mc  #AUTHOR_TAG implementation of the pltm to train and infer topics on the same data set used in  #AUTHOR_TAG."", 'that paper used the europarl  #AUTHOR_TAG  #TAUTHOR_TAG, these performance comparisons are not done on the same training and test sets - a gap that we fill below.', 'we train pltm models with number of topics t set to 50, 100, 200, and 500.', 'in order to compare exactly the same topic distributions when computing speed vs. accuracy of various approximate and exhaustive all - pairs comparisons we focus only on one inference approach - the gibbs sampling and ignore the online vb approach as it yields similar performance.', 'for all four topic models, we use the same settings for pltm ( hyperparameter values and number of gibbs sampling iterations ) as in  #TAUTHOR_TAG 2.', 'topic distributions were then inferred on the test collection using the trained topics.', 'we then performed all - pairs comparison using js divergence, hellinger distance, and approximate, lsh and kd - trees based, hellinger distance.', 'we measured the total time that it takes to perform exhaustive all - pairs comparison using js divergence, the lsh and kdtrees version on a single machine consisting of a core 2 duo quad processors with a clock speed of 2. 66ghz on each core and a total of 8gb of memory.', 'since the time performance of the e2lsh depends on the radius r of data set points considered for each query point  #AUTHOR_TAG, we performed measurements with different values of r. for this task, the all - pairs js code implementation first reads both source and target sets of documents and stores them in hash tables.', 'we then go over each entry in the source table and compute divergence against all target table entries.', 'we refer to this code implementation as hash map implementation']",5
"['##l  #AUTHOR_TAG  #TAUTHOR_TAG, these performance comparisons are not']","[' #AUTHOR_TAG  #TAUTHOR_TAG, these performance comparisons are not']","['used in  #AUTHOR_TAG.', 'that paper used the europarl  #AUTHOR_TAG  #TAUTHOR_TAG, these performance comparisons are not']","[""use mallet's ( mc  #AUTHOR_TAG implementation of the pltm to train and infer topics on the same data set used in  #AUTHOR_TAG."", 'that paper used the europarl  #AUTHOR_TAG  #TAUTHOR_TAG, these performance comparisons are not done on the same training and test sets - a gap that we fill below.', 'we train pltm models with number of topics t set to 50, 100, 200, and 500.', 'in order to compare exactly the same topic distributions when computing speed vs. accuracy of various approximate and exhaustive all - pairs comparisons we focus only on one inference approach - the gibbs sampling and ignore the online vb approach as it yields similar performance.', 'for all four topic models, we use the same settings for pltm ( hyperparameter values and number of gibbs sampling iterations ) as in  #TAUTHOR_TAG 2.', 'topic distributions were then inferred on the test collection using the trained topics.', 'we then performed all - pairs comparison using js divergence, hellinger distance, and approximate, lsh and kd - trees based, hellinger distance.', 'we measured the total time that it takes to perform exhaustive all - pairs comparison using js divergence, the lsh and kdtrees version on a single machine consisting of a core 2 duo quad processors with a clock speed of 2. 66ghz on each core and a total of 8gb of memory.', 'since the time performance of the e2lsh depends on the radius r of data set points considered for each query point  #AUTHOR_TAG, we performed measurements with different values of r. for this task, the all - pairs js code implementation first reads both source and target sets of documents and stores them in hash tables.', 'we then go over each entry in the source table and compute divergence against all target table entries.', 'we refer to this code implementation as hash map implementation']",5
"[' #TAUTHOR_TAG.', 'given a multilingual']","[' #TAUTHOR_TAG.', 'given a multilingual']","[' #TAUTHOR_TAG.', 'given a multilingual set of aligned']","['generative bayesian models, such as topic models, have proven to be very effective for modeling document collections and discovering underlying latent semantic structures.', 'most current topic models are based on latent dirichlet allocation ( lda ).', 'in some early work on the subject, showed the usefulness of lda on the task of automatic annotation of images.', ' #AUTHOR_TAG used lda to analyze historical trends in the scientific literature ;  #AUTHOR_TAG showed improvements on an information retrieval task.', 'more recently  #AUTHOR_TAG modeled geographic linguistic variation using twitter data.', 'aside from their widespread use on monolingual text, topic models have also been used to model multilingual data ( boyd -  #AUTHOR_TAG jagarlamudi and daume, 2010 ;  #AUTHOR_TAG, to name a few.', 'in this paper, we focus on the polylingual topic model, introduced by  #TAUTHOR_TAG.', 'given a multilingual set of aligned documents, the pltm assumes that across an aligned multilingual document tuple, there exists a single, tuple - specific, distribution across topics.', 'in addition, pltm assumes that for each language - topic pair, there exists a distribution over words in that language β l.', '']",0
"['', ' #TAUTHOR_TAG introduced polylingual topic models ( pltm ), an extension']","['( clir ) tasks has been explored through various techniques.', ' #TAUTHOR_TAG introduced polylingual topic models ( pltm ), an extension']","['( clir ) tasks has been explored through various techniques.', ' #TAUTHOR_TAG introduced polylingual topic models ( pltm ), an extension']","['multilingual documents into a common, language - independent vector space for the purpose of improving machine translation ( mt ) and performing cross - language information retrieval ( clir ) tasks has been explored through various techniques.', ' #TAUTHOR_TAG introduced polylingual topic models ( pltm ), an extension of latent dirichlet allocation ( lda ), and, more recently,  #AUTHOR_TAG proposed extensions of principal component analysis ( pca ) and probabilistic latent semantic indexing ( plsi ).', 'both the pltm and plsi represent bilingual documents in the probability simplex, and thus the task of finding document translation pairs is formulated as finding similar probability distributions.', 'while the nature of both works was exploratory, results shown on fairly large collections of bilingual documents ( less than 20k documents ) offer convincing argument of their potential.', 'expanding these approaches to much large collections of multilingual documents would require utilizing fast nn search for computing similarity in the probability simplex.', 'while there are many other proposed approaches to the task of finding document translation pairs that represent documents in metric space, such as  #AUTHOR_TAG which utilizes lsh for cosine distance, there is no evidence that they yield good results on documents of small lengths such as paragraphs and even sen - tences.', 'in this section, we empirically show how to utilize approaches that deal with representing documents in the probability simplex without a significant loss in accuracy while significantly improving the processing time.', 'we use pltm representations of bilingual documents.', 'in addition, we show how the results as reported by  #AUTHOR_TAG can be obtained using the pltm representation with a significant speed improvement.', 'as in  #AUTHOR_TAG and  #AUTHOR_TAG the task is to find document translation pairs in a multilingual collection of documents by representing documents in the probability simplex and computing similarity between their probability distribution representation across all document pairs.', 'for this experimental setup, accuracy is defined as the number of times ( in percentage ) that the target language document was discovered at rank 1 ( i. e. % @ rank 1. ) across the whole test collection']",0
"['##l  #AUTHOR_TAG  #TAUTHOR_TAG, these performance comparisons are not']","[' #AUTHOR_TAG  #TAUTHOR_TAG, these performance comparisons are not']","['used in  #AUTHOR_TAG.', 'that paper used the europarl  #AUTHOR_TAG  #TAUTHOR_TAG, these performance comparisons are not']","[""use mallet's ( mc  #AUTHOR_TAG implementation of the pltm to train and infer topics on the same data set used in  #AUTHOR_TAG."", 'that paper used the europarl  #AUTHOR_TAG  #TAUTHOR_TAG, these performance comparisons are not done on the same training and test sets - a gap that we fill below.', 'we train pltm models with number of topics t set to 50, 100, 200, and 500.', 'in order to compare exactly the same topic distributions when computing speed vs. accuracy of various approximate and exhaustive all - pairs comparisons we focus only on one inference approach - the gibbs sampling and ignore the online vb approach as it yields similar performance.', 'for all four topic models, we use the same settings for pltm ( hyperparameter values and number of gibbs sampling iterations ) as in  #TAUTHOR_TAG 2.', 'topic distributions were then inferred on the test collection using the trained topics.', 'we then performed all - pairs comparison using js divergence, hellinger distance, and approximate, lsh and kd - trees based, hellinger distance.', 'we measured the total time that it takes to perform exhaustive all - pairs comparison using js divergence, the lsh and kdtrees version on a single machine consisting of a core 2 duo quad processors with a clock speed of 2. 66ghz on each core and a total of 8gb of memory.', 'since the time performance of the e2lsh depends on the radius r of data set points considered for each query point  #AUTHOR_TAG, we performed measurements with different values of r. for this task, the all - pairs js code implementation first reads both source and target sets of documents and stores them in hash tables.', 'we then go over each entry in the source table and compute divergence against all target table entries.', 'we refer to this code implementation as hash map implementation']",0
"[' #TAUTHOR_TAG.', 'given a multilingual']","[' #TAUTHOR_TAG.', 'given a multilingual']","[' #TAUTHOR_TAG.', 'given a multilingual set of aligned']","['generative bayesian models, such as topic models, have proven to be very effective for modeling document collections and discovering underlying latent semantic structures.', 'most current topic models are based on latent dirichlet allocation ( lda ).', 'in some early work on the subject, showed the usefulness of lda on the task of automatic annotation of images.', ' #AUTHOR_TAG used lda to analyze historical trends in the scientific literature ;  #AUTHOR_TAG showed improvements on an information retrieval task.', 'more recently  #AUTHOR_TAG modeled geographic linguistic variation using twitter data.', 'aside from their widespread use on monolingual text, topic models have also been used to model multilingual data ( boyd -  #AUTHOR_TAG jagarlamudi and daume, 2010 ;  #AUTHOR_TAG, to name a few.', 'in this paper, we focus on the polylingual topic model, introduced by  #TAUTHOR_TAG.', 'given a multilingual set of aligned documents, the pltm assumes that across an aligned multilingual document tuple, there exists a single, tuple - specific, distribution across topics.', 'in addition, pltm assumes that for each language - topic pair, there exists a distribution over words in that language β l.', '']",4
"['emergence in neural agents  #TAUTHOR_TAG. in this paper, we study']","['emergence in neural agents  #TAUTHOR_TAG. in this paper, we study']","['emergence in neural agents  #TAUTHOR_TAG. in this paper, we study']","['spectrum, and most relevant to us, some work has found that languages will emerge to enable communication - centric tasks to be solved without direct or even indirect language supervision  #AUTHOR_TAG', 'b ). unlike regimes where agents are trained to learn an existing language, languages that emerge in this sort of setting are not necessarily easy to understand for a human', "". even attempts to translate these emerged languages for other agents are not completely successful  #AUTHOR_TAG, possibly because the target languages can't express the same concepts"", 'as the source', 'languages. this desire for structure motivates the previously mentioned', 'work on compositional language emergence in neural agents  #TAUTHOR_TAG. in this paper, we study the following question - what', '']",5
"['emergence in neural agents  #TAUTHOR_TAG. in this paper, we study']","['emergence in neural agents  #TAUTHOR_TAG. in this paper, we study']","['emergence in neural agents  #TAUTHOR_TAG. in this paper, we study']","['spectrum, and most relevant to us, some work has found that languages will emerge to enable communication - centric tasks to be solved without direct or even indirect language supervision  #AUTHOR_TAG', 'b ). unlike regimes where agents are trained to learn an existing language, languages that emerge in this sort of setting are not necessarily easy to understand for a human', "". even attempts to translate these emerged languages for other agents are not completely successful  #AUTHOR_TAG, possibly because the target languages can't express the same concepts"", 'as the source', 'languages. this desire for structure motivates the previously mentioned', 'work on compositional language emergence in neural agents  #TAUTHOR_TAG. in this paper, we study the following question - what', '']",5
"['', 'in  #TAUTHOR_TAG it was used to generate a somewhat compositional language given algorithm 1 : training with replacement and multiple agents', 'policy gradient update w.']","['approach - summarized in the black lines ( 4 - 9 ) of algorithm 1 - is our starting point.', 'in  #TAUTHOR_TAG it was used to generate a somewhat compositional language given algorithm 1 : training with replacement and multiple agents', 'policy gradient update w. r. t.', 'both q - bot and a - bot parameters appropriate agent and vocabulary configurations']","['- is our starting point.', 'in  #TAUTHOR_TAG it was used to generate a somewhat compositional language given algorithm 1 : training with replacement and multiple agents', 'policy gradient update w. r. t.', 'both q - bot and a - bot parameters appropriate agent and vocabulary configurations']","['approach - summarized in the black lines ( 4 - 9 ) of algorithm 1 - is our starting point.', 'in  #TAUTHOR_TAG it was used to generate a somewhat compositional language given algorithm 1 : training with replacement and multiple agents', 'policy gradient update w. r. t.', 'both q - bot and a - bot parameters appropriate agent and vocabulary configurations']",5
"['emergence in neural agents  #TAUTHOR_TAG. in this paper, we study']","['emergence in neural agents  #TAUTHOR_TAG. in this paper, we study']","['emergence in neural agents  #TAUTHOR_TAG. in this paper, we study']","['spectrum, and most relevant to us, some work has found that languages will emerge to enable communication - centric tasks to be solved without direct or even indirect language supervision  #AUTHOR_TAG', 'b ). unlike regimes where agents are trained to learn an existing language, languages that emerge in this sort of setting are not necessarily easy to understand for a human', "". even attempts to translate these emerged languages for other agents are not completely successful  #AUTHOR_TAG, possibly because the target languages can't express the same concepts"", 'as the source', 'languages. this desire for structure motivates the previously mentioned', 'work on compositional language emergence in neural agents  #TAUTHOR_TAG. in this paper, we study the following question - what', '']",0
"['emergence in neural agents  #TAUTHOR_TAG. in this paper, we study']","['emergence in neural agents  #TAUTHOR_TAG. in this paper, we study']","['emergence in neural agents  #TAUTHOR_TAG. in this paper, we study']","['spectrum, and most relevant to us, some work has found that languages will emerge to enable communication - centric tasks to be solved without direct or even indirect language supervision  #AUTHOR_TAG', 'b ). unlike regimes where agents are trained to learn an existing language, languages that emerge in this sort of setting are not necessarily easy to understand for a human', "". even attempts to translate these emerged languages for other agents are not completely successful  #AUTHOR_TAG, possibly because the target languages can't express the same concepts"", 'as the source', 'languages. this desire for structure motivates the previously mentioned', 'work on compositional language emergence in neural agents  #TAUTHOR_TAG. in this paper, we study the following question - what', '']",0
"['', 'in  #TAUTHOR_TAG it was used to generate a somewhat compositional language given algorithm 1 : training with replacement and multiple agents', 'policy gradient update w.']","['approach - summarized in the black lines ( 4 - 9 ) of algorithm 1 - is our starting point.', 'in  #TAUTHOR_TAG it was used to generate a somewhat compositional language given algorithm 1 : training with replacement and multiple agents', 'policy gradient update w. r. t.', 'both q - bot and a - bot parameters appropriate agent and vocabulary configurations']","['- is our starting point.', 'in  #TAUTHOR_TAG it was used to generate a somewhat compositional language given algorithm 1 : training with replacement and multiple agents', 'policy gradient update w. r. t.', 'both q - bot and a - bot parameters appropriate agent and vocabulary configurations']","['approach - summarized in the black lines ( 4 - 9 ) of algorithm 1 - is our starting point.', 'in  #TAUTHOR_TAG it was used to generate a somewhat compositional language given algorithm 1 : training with replacement and multiple agents', 'policy gradient update w. r. t.', 'both q - bot and a - bot parameters appropriate agent and vocabulary configurations']",0
"['generations.', 'single agent.', 'in  #TAUTHOR_TAG there is only one pair of agents ( n q = n a = 1 ) so we cannot replace both agents at']","['generations.', 'single agent.', 'in  #TAUTHOR_TAG there is only one pair of agents ( n q = n a = 1 ) so we cannot replace both agents at']","['previous generations.', 'single agent.', 'in  #TAUTHOR_TAG there is only one pair of agents ( n q = n a = 1 ) so we cannot replace both agents at']","['we add cultural transmission to neural dialog models by considering an implicit model of cultural transmission.', 'implicit cultural transmission does not use word - level supervision, as opposed to explicit cultural transmission in which students are told which words refer to which objects.', 'in implicit cultural transmission shared language emerges from shared goals.', 'we develop an implicit model of cultural transmission 2 that periodically replaces agents.', 'consequentially, older agents remember the old language while new agents learn it, favoring more easily compressible compositional language.', 'replacement.', 'one open choice in this approach is how to select which agents to re - initialize - we explore different options in this section.', 'every e epochs, replacement policy π is called and returns a list of agents to be re - initialized, as seen at the blue lines ( 10 - 12 ) of algorithm 1 3.', 'this process creates generations of agents such that each generation learns languages that are slightly different but eventually improve upon those of previous generations.', 'single agent.', 'in  #TAUTHOR_TAG there is only one pair of agents ( n q = n a = 1 ) so we cannot replace both agents at the same round because all existing language would be lost.', 'instead, we consider two strategies that only replace one bot at a time :', '- random.', 'sample q - bot or a - bot uniformly - alternate.', 'alternate between q - bot and a - bot multi agent.', 'q - bot and a - bot have different roles due to the asymmetry in information, so they use different parts of the language.', 'replacing a - bot ( alt. q - bot ) means q - bot ( alt. a - bot ) has to remember what a - bot says or else knowledge about that part of the language is lost.', 'if a - bot was replaced but other a - bots speaking the same language were present then q - bot would have incentive to remember the original language because of the other bots, preventing language loss.', 'furthermore, a multi agent environment may add compressibility pressure  #AUTHOR_TAG as bots have more to remember if there is any difference between the languages of their conversational partners.', 'finally, having multiple agents per type could introduce more variations in language, providing an opportunity to favor even better languages.', 'thus we introduce multiple a - bots and q - bots.', 'more concretely, we consider a population of qbots { q 1,..., q n q } and a population of a - bots { a 1,..., a n a }.', 'each member of the populations has a different set of parameters, but any q - bot - a -']",0
"['to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the']","['to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the']","['out instances ( e. g., it has unique words for purple', 'square which both agents understand ) then it is compositional. this is simply measured by accuracy on the', 'test set. previous work', 'also measures generalization to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the goal of compositional language. for a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. this dis']","['thing )', '. if the language created by interaction between agents can identify the held out instances ( e. g., it has unique words for purple', 'square which both agents understand ) then it is compositional. this is simply measured by accuracy on the', 'test set. previous work', 'also measures generalization to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the goal of compositional language. for a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. this disallows opportunities for non - compositional generalization. without this', 'constraint, agents could generalize perfectly using words for attribute pairs like "" red triangle "" and "" filled star "" instead of words for "" red, "" "" triangle, "" "" filled, ""', 'and "" star. "" the drop in accuracy 5 between', 'test and validation ( which does not hold out attribute pairs ) is roughly 20 points.', 'architecture and training. our a - bots and q - bots have the same architecture and hyperparameter variations as', 'in  #TAUTHOR_TAG, but with our cultural transmission training procedure and some other differences identified below. like  #TAUTHOR_TAG, our', 'hyperparameter variations consider the number of vocab words q -', 'bot ( v q ) and a - bot ( v a ) may utter and whether or not a - bot has memory between dialog rounds. the memoryless version of a - bot simply sets h t a = 0 between each round of dialog. this means a - bot cannot represent which attributes it has already communicated. when there are too', 'many vocab words available', ""there is less pressure to develop a compositional language because for every new object there is always an unused sequence of words which isn't too similar to existing words, an effect also"", 'noticed elsewhere  #AUTHOR_TAG. we add one setting where a - bot has no memory yet the number of vocab words is still overcomplete to help understand and disentangle these two factors. specifically, we consider the following settings', ': all agents are trained for e = 5000 epochs with a batch size of 1000 ( so 1 batch per', 'epoch ) and the adam  #AUTHOR_TAG optimizer with learning rate 0. 01', '. in the multi agent setting we use n a = n q = 5. to decide when to stop we measure validation set accuracy averaged over all q - bot - abot pairs and choose the first population whose validation accuracy did not improve for', '200k epochs. 7 this differs from  #TAUTHOR_TAG, which stopped once train accuracy reached 100 %. furthermore, we do not mine negatives for each training batch']",0
['in  #TAUTHOR_TAG. our'],['in  #TAUTHOR_TAG. our'],['in  #TAUTHOR_TAG. our'],"['', 'agent uniform random, and in a few small vocab settings. this suggests that while some agent replacement needs to occur, it does not much', 'matter whether agents with worse language are replaced or whether there is', 'a pool of similarly typed agents to remember knowledge lost from older generations. the main factor is that new agents learn in the presence of others who already know a language.', 'test set accuracies ( with standard deviations ) are reported against our new harder dataset using models similar to those in  #TAUTHOR_TAG. our variations on cultural transmission outperform the baselines ( lighter two green and lighter two blue bars )', '']",0
[' #TAUTHOR_TAG show cultural transmission'],[' #TAUTHOR_TAG show cultural transmission'],[' #AUTHOR_TAG and deep learning  #TAUTHOR_TAG show cultural transmission'],"['are two main veins of related work.', 'in the first, evolutionary linguistics explains compositionality with cultural evolution.', 'in the second, neural language models create languages to communicate and achieve their goals.', 'language evolution causes structure.', 'researchers have spent decades studying how unique properties of human language like compositionality could have emerged.', 'there is general agreement that people acquire language using a combination of innate cognitive capacity and learning from other language speakers ( cultural transmission ), with the degree of each being widely disputed  #AUTHOR_TAG.', 'most agree the answer is something in between.', 'both innate cognitive capacity and specific modern human languages like english coevolved  #AUTHOR_TAG via biological  #AUTHOR_TAG and cultural  #AUTHOR_TAG evolution, respectively.', 'thus an explanation of these evolutionary processes is essential to explaining human language.', 'in particular, explanations of how the cultural evolution of languages themselves could cause those languages to develop structure like compositionality are in abundance  #AUTHOR_TAG.', 'an important piece of the explanation of linguistic structure is the iterated learning model  #AUTHOR_TAG described in section 1.', 'this model focuses on the cultural transmission and bottlenecks that restrict how languages can be learned, showing compositional language emerges in computational  #AUTHOR_TAG b ;  #AUTHOR_TAG and human  #AUTHOR_TAG scott -  #AUTHOR_TAG experiments.', 'even though cultural transmission may aid the emergence of compositionality, recent results in evolutionary linguistics  #AUTHOR_TAG and deep learning  #TAUTHOR_TAG show cultural transmission may not be necessary for compositionality to emerge.', 'while existing work in deep learning has focused on biases that encourage compositionality, it has not considered settings where language is permitted to evolve as it is passed down over generations of agents.', '']",0
[' #TAUTHOR_TAG show cultural transmission'],[' #TAUTHOR_TAG show cultural transmission'],[' #AUTHOR_TAG and deep learning  #TAUTHOR_TAG show cultural transmission'],"['are two main veins of related work.', 'in the first, evolutionary linguistics explains compositionality with cultural evolution.', 'in the second, neural language models create languages to communicate and achieve their goals.', 'language evolution causes structure.', 'researchers have spent decades studying how unique properties of human language like compositionality could have emerged.', 'there is general agreement that people acquire language using a combination of innate cognitive capacity and learning from other language speakers ( cultural transmission ), with the degree of each being widely disputed  #AUTHOR_TAG.', 'most agree the answer is something in between.', 'both innate cognitive capacity and specific modern human languages like english coevolved  #AUTHOR_TAG via biological  #AUTHOR_TAG and cultural  #AUTHOR_TAG evolution, respectively.', 'thus an explanation of these evolutionary processes is essential to explaining human language.', 'in particular, explanations of how the cultural evolution of languages themselves could cause those languages to develop structure like compositionality are in abundance  #AUTHOR_TAG.', 'an important piece of the explanation of linguistic structure is the iterated learning model  #AUTHOR_TAG described in section 1.', 'this model focuses on the cultural transmission and bottlenecks that restrict how languages can be learned, showing compositional language emerges in computational  #AUTHOR_TAG b ;  #AUTHOR_TAG and human  #AUTHOR_TAG scott -  #AUTHOR_TAG experiments.', 'even though cultural transmission may aid the emergence of compositionality, recent results in evolutionary linguistics  #AUTHOR_TAG and deep learning  #TAUTHOR_TAG show cultural transmission may not be necessary for compositionality to emerge.', 'while existing work in deep learning has focused on biases that encourage compositionality, it has not considered settings where language is permitted to evolve as it is passed down over generations of agents.', '']",0
[' #TAUTHOR_TAG show cultural transmission'],[' #TAUTHOR_TAG show cultural transmission'],[' #AUTHOR_TAG and deep learning  #TAUTHOR_TAG show cultural transmission'],"['are two main veins of related work.', 'in the first, evolutionary linguistics explains compositionality with cultural evolution.', 'in the second, neural language models create languages to communicate and achieve their goals.', 'language evolution causes structure.', 'researchers have spent decades studying how unique properties of human language like compositionality could have emerged.', 'there is general agreement that people acquire language using a combination of innate cognitive capacity and learning from other language speakers ( cultural transmission ), with the degree of each being widely disputed  #AUTHOR_TAG.', 'most agree the answer is something in between.', 'both innate cognitive capacity and specific modern human languages like english coevolved  #AUTHOR_TAG via biological  #AUTHOR_TAG and cultural  #AUTHOR_TAG evolution, respectively.', 'thus an explanation of these evolutionary processes is essential to explaining human language.', 'in particular, explanations of how the cultural evolution of languages themselves could cause those languages to develop structure like compositionality are in abundance  #AUTHOR_TAG.', 'an important piece of the explanation of linguistic structure is the iterated learning model  #AUTHOR_TAG described in section 1.', 'this model focuses on the cultural transmission and bottlenecks that restrict how languages can be learned, showing compositional language emerges in computational  #AUTHOR_TAG b ;  #AUTHOR_TAG and human  #AUTHOR_TAG scott -  #AUTHOR_TAG experiments.', 'even though cultural transmission may aid the emergence of compositionality, recent results in evolutionary linguistics  #AUTHOR_TAG and deep learning  #TAUTHOR_TAG show cultural transmission may not be necessary for compositionality to emerge.', 'while existing work in deep learning has focused on biases that encourage compositionality, it has not considered settings where language is permitted to evolve as it is passed down over generations of agents.', '']",0
"['emergence in neural agents  #TAUTHOR_TAG. in this paper, we study']","['emergence in neural agents  #TAUTHOR_TAG. in this paper, we study']","['emergence in neural agents  #TAUTHOR_TAG. in this paper, we study']","['spectrum, and most relevant to us, some work has found that languages will emerge to enable communication - centric tasks to be solved without direct or even indirect language supervision  #AUTHOR_TAG', 'b ). unlike regimes where agents are trained to learn an existing language, languages that emerge in this sort of setting are not necessarily easy to understand for a human', "". even attempts to translate these emerged languages for other agents are not completely successful  #AUTHOR_TAG, possibly because the target languages can't express the same concepts"", 'as the source', 'languages. this desire for structure motivates the previously mentioned', 'work on compositional language emergence in neural agents  #TAUTHOR_TAG. in this paper, we study the following question - what', '']",1
"['.', 'as in  #TAUTHOR_TAG, we']","['if both attributes are correct.', 'as in  #TAUTHOR_TAG, we']","['.', 'as in  #TAUTHOR_TAG, we implement q, a, and u as neural networks.', 'our model is trained to maximize the reward using policy gradients  #AUTHOR_TAG.', 'unlike an approach supervised by human dialogues, nothing']","['', 'a ).', 'after the conversation, q - bot tries to solve the given task by predicting u ( e. g., corresponding to red square ) as a function of its observation and final memory : u = u ( x q, h t q ).', 'both agents are rewarded if both attributes are correct.', 'as in  #TAUTHOR_TAG, we implement q, a, and u as neural networks.', 'our model is trained to maximize the reward using policy gradients  #AUTHOR_TAG.', 'unlike an approach supervised by human dialogues, nothing orients the agents toward specific meanings for specific words, so they must create their own appropriately grounded language to solve the task']",3
"['to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the']","['to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the']","['out instances ( e. g., it has unique words for purple', 'square which both agents understand ) then it is compositional. this is simply measured by accuracy on the', 'test set. previous work', 'also measures generalization to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the goal of compositional language. for a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. this dis']","['thing )', '. if the language created by interaction between agents can identify the held out instances ( e. g., it has unique words for purple', 'square which both agents understand ) then it is compositional. this is simply measured by accuracy on the', 'test set. previous work', 'also measures generalization to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the goal of compositional language. for a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. this disallows opportunities for non - compositional generalization. without this', 'constraint, agents could generalize perfectly using words for attribute pairs like "" red triangle "" and "" filled star "" instead of words for "" red, "" "" triangle, "" "" filled, ""', 'and "" star. "" the drop in accuracy 5 between', 'test and validation ( which does not hold out attribute pairs ) is roughly 20 points.', 'architecture and training. our a - bots and q - bots have the same architecture and hyperparameter variations as', 'in  #TAUTHOR_TAG, but with our cultural transmission training procedure and some other differences identified below. like  #TAUTHOR_TAG, our', 'hyperparameter variations consider the number of vocab words q -', 'bot ( v q ) and a - bot ( v a ) may utter and whether or not a - bot has memory between dialog rounds. the memoryless version of a - bot simply sets h t a = 0 between each round of dialog. this means a - bot cannot represent which attributes it has already communicated. when there are too', 'many vocab words available', ""there is less pressure to develop a compositional language because for every new object there is always an unused sequence of words which isn't too similar to existing words, an effect also"", 'noticed elsewhere  #AUTHOR_TAG. we add one setting where a - bot has no memory yet the number of vocab words is still overcomplete to help understand and disentangle these two factors. specifically, we consider the following settings', ': all agents are trained for e = 5000 epochs with a batch size of 1000 ( so 1 batch per', 'epoch ) and the adam  #AUTHOR_TAG optimizer with learning rate 0. 01', '. in the multi agent setting we use n a = n q = 5. to decide when to stop we measure validation set accuracy averaged over all q - bot - abot pairs and choose the first population whose validation accuracy did not improve for', '200k epochs. 7 this differs from  #TAUTHOR_TAG, which stopped once train accuracy reached 100 %. furthermore, we do not mine negatives for each training batch']",3
"['to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the']","['to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the']","['out instances ( e. g., it has unique words for purple', 'square which both agents understand ) then it is compositional. this is simply measured by accuracy on the', 'test set. previous work', 'also measures generalization to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the goal of compositional language. for a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. this dis']","['thing )', '. if the language created by interaction between agents can identify the held out instances ( e. g., it has unique words for purple', 'square which both agents understand ) then it is compositional. this is simply measured by accuracy on the', 'test set. previous work', 'also measures generalization to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the goal of compositional language. for a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. this disallows opportunities for non - compositional generalization. without this', 'constraint, agents could generalize perfectly using words for attribute pairs like "" red triangle "" and "" filled star "" instead of words for "" red, "" "" triangle, "" "" filled, ""', 'and "" star. "" the drop in accuracy 5 between', 'test and validation ( which does not hold out attribute pairs ) is roughly 20 points.', 'architecture and training. our a - bots and q - bots have the same architecture and hyperparameter variations as', 'in  #TAUTHOR_TAG, but with our cultural transmission training procedure and some other differences identified below. like  #TAUTHOR_TAG, our', 'hyperparameter variations consider the number of vocab words q -', 'bot ( v q ) and a - bot ( v a ) may utter and whether or not a - bot has memory between dialog rounds. the memoryless version of a - bot simply sets h t a = 0 between each round of dialog. this means a - bot cannot represent which attributes it has already communicated. when there are too', 'many vocab words available', ""there is less pressure to develop a compositional language because for every new object there is always an unused sequence of words which isn't too similar to existing words, an effect also"", 'noticed elsewhere  #AUTHOR_TAG. we add one setting where a - bot has no memory yet the number of vocab words is still overcomplete to help understand and disentangle these two factors. specifically, we consider the following settings', ': all agents are trained for e = 5000 epochs with a batch size of 1000 ( so 1 batch per', 'epoch ) and the adam  #AUTHOR_TAG optimizer with learning rate 0. 01', '. in the multi agent setting we use n a = n q = 5. to decide when to stop we measure validation set accuracy averaged over all q - bot - abot pairs and choose the first population whose validation accuracy did not improve for', '200k epochs. 7 this differs from  #TAUTHOR_TAG, which stopped once train accuracy reached 100 %. furthermore, we do not mine negatives for each training batch']",3
"['to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the']","['to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the']","['out instances ( e. g., it has unique words for purple', 'square which both agents understand ) then it is compositional. this is simply measured by accuracy on the', 'test set. previous work', 'also measures generalization to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the goal of compositional language. for a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. this dis']","['thing )', '. if the language created by interaction between agents can identify the held out instances ( e. g., it has unique words for purple', 'square which both agents understand ) then it is compositional. this is simply measured by accuracy on the', 'test set. previous work', 'also measures generalization to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the goal of compositional language. for a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. this disallows opportunities for non - compositional generalization. without this', 'constraint, agents could generalize perfectly using words for attribute pairs like "" red triangle "" and "" filled star "" instead of words for "" red, "" "" triangle, "" "" filled, ""', 'and "" star. "" the drop in accuracy 5 between', 'test and validation ( which does not hold out attribute pairs ) is roughly 20 points.', 'architecture and training. our a - bots and q - bots have the same architecture and hyperparameter variations as', 'in  #TAUTHOR_TAG, but with our cultural transmission training procedure and some other differences identified below. like  #TAUTHOR_TAG, our', 'hyperparameter variations consider the number of vocab words q -', 'bot ( v q ) and a - bot ( v a ) may utter and whether or not a - bot has memory between dialog rounds. the memoryless version of a - bot simply sets h t a = 0 between each round of dialog. this means a - bot cannot represent which attributes it has already communicated. when there are too', 'many vocab words available', ""there is less pressure to develop a compositional language because for every new object there is always an unused sequence of words which isn't too similar to existing words, an effect also"", 'noticed elsewhere  #AUTHOR_TAG. we add one setting where a - bot has no memory yet the number of vocab words is still overcomplete to help understand and disentangle these two factors. specifically, we consider the following settings', ': all agents are trained for e = 5000 epochs with a batch size of 1000 ( so 1 batch per', 'epoch ) and the adam  #AUTHOR_TAG optimizer with learning rate 0. 01', '. in the multi agent setting we use n a = n q = 5. to decide when to stop we measure validation set accuracy averaged over all q - bot - abot pairs and choose the first population whose validation accuracy did not improve for', '200k epochs. 7 this differs from  #TAUTHOR_TAG, which stopped once train accuracy reached 100 %. furthermore, we do not mine negatives for each training batch']",3
['in  #TAUTHOR_TAG. our'],['in  #TAUTHOR_TAG. our'],['in  #TAUTHOR_TAG. our'],"['', 'agent uniform random, and in a few small vocab settings. this suggests that while some agent replacement needs to occur, it does not much', 'matter whether agents with worse language are replaced or whether there is', 'a pool of similarly typed agents to remember knowledge lost from older generations. the main factor is that new agents learn in the presence of others who already know a language.', 'test set accuracies ( with standard deviations ) are reported against our new harder dataset using models similar to those in  #TAUTHOR_TAG. our variations on cultural transmission outperform the baselines ( lighter two green and lighter two blue bars )', '']",3
['in  #TAUTHOR_TAG. our'],['in  #TAUTHOR_TAG. our'],['in  #TAUTHOR_TAG. our'],"['', 'agent uniform random, and in a few small vocab settings. this suggests that while some agent replacement needs to occur, it does not much', 'matter whether agents with worse language are replaced or whether there is', 'a pool of similarly typed agents to remember knowledge lost from older generations. the main factor is that new agents learn in the presence of others who already know a language.', 'test set accuracies ( with standard deviations ) are reported against our new harder dataset using models similar to those in  #TAUTHOR_TAG. our variations on cultural transmission outperform the baselines ( lighter two green and lighter two blue bars )', '']",3
['in  #TAUTHOR_TAG. our'],['in  #TAUTHOR_TAG. our'],['in  #TAUTHOR_TAG. our'],"['', 'agent uniform random, and in a few small vocab settings. this suggests that while some agent replacement needs to occur, it does not much', 'matter whether agents with worse language are replaced or whether there is', 'a pool of similarly typed agents to remember knowledge lost from older generations. the main factor is that new agents learn in the presence of others who already know a language.', 'test set accuracies ( with standard deviations ) are reported against our new harder dataset using models similar to those in  #TAUTHOR_TAG. our variations on cultural transmission outperform the baselines ( lighter two green and lighter two blue bars )', '']",3
"['to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the']","['to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the']","['out instances ( e. g., it has unique words for purple', 'square which both agents understand ) then it is compositional. this is simply measured by accuracy on the', 'test set. previous work', 'also measures generalization to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the goal of compositional language. for a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. this dis']","['thing )', '. if the language created by interaction between agents can identify the held out instances ( e. g., it has unique words for purple', 'square which both agents understand ) then it is compositional. this is simply measured by accuracy on the', 'test set. previous work', 'also measures generalization to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the goal of compositional language. for a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. this disallows opportunities for non - compositional generalization. without this', 'constraint, agents could generalize perfectly using words for attribute pairs like "" red triangle "" and "" filled star "" instead of words for "" red, "" "" triangle, "" "" filled, ""', 'and "" star. "" the drop in accuracy 5 between', 'test and validation ( which does not hold out attribute pairs ) is roughly 20 points.', 'architecture and training. our a - bots and q - bots have the same architecture and hyperparameter variations as', 'in  #TAUTHOR_TAG, but with our cultural transmission training procedure and some other differences identified below. like  #TAUTHOR_TAG, our', 'hyperparameter variations consider the number of vocab words q -', 'bot ( v q ) and a - bot ( v a ) may utter and whether or not a - bot has memory between dialog rounds. the memoryless version of a - bot simply sets h t a = 0 between each round of dialog. this means a - bot cannot represent which attributes it has already communicated. when there are too', 'many vocab words available', ""there is less pressure to develop a compositional language because for every new object there is always an unused sequence of words which isn't too similar to existing words, an effect also"", 'noticed elsewhere  #AUTHOR_TAG. we add one setting where a - bot has no memory yet the number of vocab words is still overcomplete to help understand and disentangle these two factors. specifically, we consider the following settings', ': all agents are trained for e = 5000 epochs with a batch size of 1000 ( so 1 batch per', 'epoch ) and the adam  #AUTHOR_TAG optimizer with learning rate 0. 01', '. in the multi agent setting we use n a = n q = 5. to decide when to stop we measure validation set accuracy averaged over all q - bot - abot pairs and choose the first population whose validation accuracy did not improve for', '200k epochs. 7 this differs from  #TAUTHOR_TAG, which stopped once train accuracy reached 100 %. furthermore, we do not mine negatives for each training batch']",4
"['to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the']","['to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the']","['out instances ( e. g., it has unique words for purple', 'square which both agents understand ) then it is compositional. this is simply measured by accuracy on the', 'test set. previous work', 'also measures generalization to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the goal of compositional language. for a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. this dis']","['thing )', '. if the language created by interaction between agents can identify the held out instances ( e. g., it has unique words for purple', 'square which both agents understand ) then it is compositional. this is simply measured by accuracy on the', 'test set. previous work', 'also measures generalization to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the goal of compositional language. for a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. this disallows opportunities for non - compositional generalization. without this', 'constraint, agents could generalize perfectly using words for attribute pairs like "" red triangle "" and "" filled star "" instead of words for "" red, "" "" triangle, "" "" filled, ""', 'and "" star. "" the drop in accuracy 5 between', 'test and validation ( which does not hold out attribute pairs ) is roughly 20 points.', 'architecture and training. our a - bots and q - bots have the same architecture and hyperparameter variations as', 'in  #TAUTHOR_TAG, but with our cultural transmission training procedure and some other differences identified below. like  #TAUTHOR_TAG, our', 'hyperparameter variations consider the number of vocab words q -', 'bot ( v q ) and a - bot ( v a ) may utter and whether or not a - bot has memory between dialog rounds. the memoryless version of a - bot simply sets h t a = 0 between each round of dialog. this means a - bot cannot represent which attributes it has already communicated. when there are too', 'many vocab words available', ""there is less pressure to develop a compositional language because for every new object there is always an unused sequence of words which isn't too similar to existing words, an effect also"", 'noticed elsewhere  #AUTHOR_TAG. we add one setting where a - bot has no memory yet the number of vocab words is still overcomplete to help understand and disentangle these two factors. specifically, we consider the following settings', ': all agents are trained for e = 5000 epochs with a batch size of 1000 ( so 1 batch per', 'epoch ) and the adam  #AUTHOR_TAG optimizer with learning rate 0. 01', '. in the multi agent setting we use n a = n q = 5. to decide when to stop we measure validation set accuracy averaged over all q - bot - abot pairs and choose the first population whose validation accuracy did not improve for', '200k epochs. 7 this differs from  #TAUTHOR_TAG, which stopped once train accuracy reached 100 %. furthermore, we do not mine negatives for each training batch']",4
"['to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the']","['to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the']","['out instances ( e. g., it has unique words for purple', 'square which both agents understand ) then it is compositional. this is simply measured by accuracy on the', 'test set. previous work', 'also measures generalization to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the goal of compositional language. for a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. this dis']","['thing )', '. if the language created by interaction between agents can identify the held out instances ( e. g., it has unique words for purple', 'square which both agents understand ) then it is compositional. this is simply measured by accuracy on the', 'test set. previous work', 'also measures generalization to held out compositions of attributes to measure compositionality  #TAUTHOR_TAG. unlike  #TAUTHOR_TAG, we use a slightly harder', 'version of their dataset which aligns better with the goal of compositional language. for a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. this disallows opportunities for non - compositional generalization. without this', 'constraint, agents could generalize perfectly using words for attribute pairs like "" red triangle "" and "" filled star "" instead of words for "" red, "" "" triangle, "" "" filled, ""', 'and "" star. "" the drop in accuracy 5 between', 'test and validation ( which does not hold out attribute pairs ) is roughly 20 points.', 'architecture and training. our a - bots and q - bots have the same architecture and hyperparameter variations as', 'in  #TAUTHOR_TAG, but with our cultural transmission training procedure and some other differences identified below. like  #TAUTHOR_TAG, our', 'hyperparameter variations consider the number of vocab words q -', 'bot ( v q ) and a - bot ( v a ) may utter and whether or not a - bot has memory between dialog rounds. the memoryless version of a - bot simply sets h t a = 0 between each round of dialog. this means a - bot cannot represent which attributes it has already communicated. when there are too', 'many vocab words available', ""there is less pressure to develop a compositional language because for every new object there is always an unused sequence of words which isn't too similar to existing words, an effect also"", 'noticed elsewhere  #AUTHOR_TAG. we add one setting where a - bot has no memory yet the number of vocab words is still overcomplete to help understand and disentangle these two factors. specifically, we consider the following settings', ': all agents are trained for e = 5000 epochs with a batch size of 1000 ( so 1 batch per', 'epoch ) and the adam  #AUTHOR_TAG optimizer with learning rate 0. 01', '. in the multi agent setting we use n a = n q = 5. to decide when to stop we measure validation set accuracy averaged over all q - bot - abot pairs and choose the first population whose validation accuracy did not improve for', '200k epochs. 7 this differs from  #TAUTHOR_TAG, which stopped once train accuracy reached 100 %. furthermore, we do not mine negatives for each training batch']",4
"['no replacement baseline 6 this is slightly different from small vocab in  #TAUTHOR_TAG.', '7 there are few objects in the environment, so each batch contains all objects and is an entire epoch.', 'are not simply due to luck.', 'in the multi agent setting we increased e from 5000 to 25000']","['no replacement baseline 6 this is slightly different from small vocab in  #TAUTHOR_TAG.', '7 there are few objects in the environment, so each batch contains all objects and is an entire epoch.', 'are not simply due to luck.', 'in the multi agent setting we increased e from 5000 to 25000']","['the no replacement baseline 6 this is slightly different from small vocab in  #TAUTHOR_TAG.', '7 there are few objects in the environment, so each batch contains all objects and is an entire epoch.', 'are not simply due to luck.', 'in the multi agent setting we increased e from 5000 to 25000']","['no replacement.', 'never replace any agent ( i. e., algorithm 1 without blue lines ).', '- replace all.', 'always replace every agent ( i. e., with b = all agents at line 11 of algorithm 1 ).', 'comparing to the no replacement baseline establishes the main result by measuring the difference replacement makes.', 'however, each time lines 11 and 12 of algorithm 1 are executed there is one more chance of getting a lucky random initialization.', 'since the no replacement baseline never does this it has a smaller chance of running in to one such lucky agent.', 'thus we compare to the replace all baseline, which has the greatest chance of seeing a lucky initialization and thereby ensures that gains over the no replacement baseline 6 this is slightly different from small vocab in  #TAUTHOR_TAG.', '7 there are few objects in the environment, so each batch contains all objects and is an entire epoch.', 'are not simply due to luck.', 'in the multi agent setting we increased e from 5000 to 25000 because agents were slower to converge']",4
"['( ner ) tasks  #TAUTHOR_TAG.', 'to represent input words,']","['( ner ) tasks  #TAUTHOR_TAG.', 'to represent input words,']","['( ner ) tasks  #TAUTHOR_TAG.', 'to represent input words, the embedding layer weights of the model was pre - initialized with values']","['subtask 2, a deep learning approach was taken.', 'specifically, a bidirectional long short - term memory ( bilstm ) coupled with a conditional random field ( crf ) layer neural network architecture was used to perform named entity recognition to identify the adverse drug reaction mentions.', 'this architecture has been empirically shown to perform well at named entity recognition ( ner ) tasks  #TAUTHOR_TAG.', 'to represent input words, the embedding layer weights of the model was pre - initialized with values obtained from a word2vec model that was trained on the mimic - iii dataset  #AUTHOR_TAG']",0
"[', given the demonstrated effectiveness of such an architecture in this domain  #TAUTHOR_TAG.', 'we expect to improve']","['task, we utilized a deep learning approach, given the demonstrated effectiveness of such an architecture in this domain  #TAUTHOR_TAG.', 'we expect to improve']","['into feature importance.', 'in the named entity recognition task, we utilized a deep learning approach, given the demonstrated effectiveness of such an architecture in this domain  #TAUTHOR_TAG.', 'we expect to improve']","[', our systems for tasks 1 and 2 consisted of a combination of ( 1 ) lexicon selection and domain - specific feature engineering ; ( 2 ) classical machine learning techniques such as logistic regression ; and ( 3 ) neural architectures, including biobert and bilstm - crf models.', 'we found simpler models consisting of lexicon selection and classical machine learning models ( such as the logistic regression model discussed previously ) performed better with limited datasets and offered explainability into feature importance.', 'in the named entity recognition task, we utilized a deep learning approach, given the demonstrated effectiveness of such an architecture in this domain  #TAUTHOR_TAG.', 'we expect to improve the performance of our systems through further refinement of our feature engineering and tuning of our model parameters']",1
"[', given the demonstrated effectiveness of such an architecture in this domain  #TAUTHOR_TAG.', 'we expect to improve']","['task, we utilized a deep learning approach, given the demonstrated effectiveness of such an architecture in this domain  #TAUTHOR_TAG.', 'we expect to improve']","['into feature importance.', 'in the named entity recognition task, we utilized a deep learning approach, given the demonstrated effectiveness of such an architecture in this domain  #TAUTHOR_TAG.', 'we expect to improve']","[', our systems for tasks 1 and 2 consisted of a combination of ( 1 ) lexicon selection and domain - specific feature engineering ; ( 2 ) classical machine learning techniques such as logistic regression ; and ( 3 ) neural architectures, including biobert and bilstm - crf models.', 'we found simpler models consisting of lexicon selection and classical machine learning models ( such as the logistic regression model discussed previously ) performed better with limited datasets and offered explainability into feature importance.', 'in the named entity recognition task, we utilized a deep learning approach, given the demonstrated effectiveness of such an architecture in this domain  #TAUTHOR_TAG.', 'we expect to improve the performance of our systems through further refinement of our feature engineering and tuning of our model parameters']",5
"['this system bias  #TAUTHOR_TAG.', 'though widely used, length']","['this system bias  #TAUTHOR_TAG.', 'though widely used, length']","['this system bias  #TAUTHOR_TAG.', 'though widely used, length normalization is not']","['the past few years, neural machine translation ( nmt ) has achieved state - of - the - art performance in many translation tasks.', 'it models the translation problem using neural networks with no assumption of the hidden structures between two languages, and learns the model parameters from bilingual texts in an end - to - end fashion  #AUTHOR_TAG.', 'in such systems, target words are generated over a sequence of time steps.', 'the model score is simply defined as the sum of the log - scale word probabilities : log p ( y | x ) = | y | j = 1 log p ( y j | y < j, x )', 'where x and y are the source and target sentences, and p ( y j | y < j, x ) is the probability of generating the j - th word y j given the previously - generated words y < j and the source sentence x. however, the straightforward implementation of this model suffers from many problems, the most obvious one being the bias that the system tends to choose shorter translations because the log - probability is added over time steps.', 'the situation is worse when we use beam search where the shorter translations have more chances to beat the longer ones.', 'it is in general to normalize the model score by translation length ( say length normalization ) to eliminate this system bias  #TAUTHOR_TAG.', '']",0
"['this system bias  #TAUTHOR_TAG.', 'though widely used, length']","['this system bias  #TAUTHOR_TAG.', 'though widely used, length']","['this system bias  #TAUTHOR_TAG.', 'though widely used, length normalization is not']","['the past few years, neural machine translation ( nmt ) has achieved state - of - the - art performance in many translation tasks.', 'it models the translation problem using neural networks with no assumption of the hidden structures between two languages, and learns the model parameters from bilingual texts in an end - to - end fashion  #AUTHOR_TAG.', 'in such systems, target words are generated over a sequence of time steps.', 'the model score is simply defined as the sum of the log - scale word probabilities : log p ( y | x ) = | y | j = 1 log p ( y j | y < j, x )', 'where x and y are the source and target sentences, and p ( y j | y < j, x ) is the probability of generating the j - th word y j given the previously - generated words y < j and the source sentence x. however, the straightforward implementation of this model suffers from many problems, the most obvious one being the bias that the system tends to choose shorter translations because the log - probability is added over time steps.', 'the situation is worse when we use beam search where the shorter translations have more chances to beat the longer ones.', 'it is in general to normalize the model score by translation length ( say length normalization ) to eliminate this system bias  #TAUTHOR_TAG.', '']",0
"['statistical machine translation  #AUTHOR_TAG.', 'in nmt, several good methods have been developed.', 'the simplest of these is length normalization which penalizes short translations in decoding  #TAUTHOR_TAG.', 'more sophisticated methods focus on modeling the coverage problem with extra sub - modules in nm']","['statistical machine translation  #AUTHOR_TAG.', 'in nmt, several good methods have been developed.', 'the simplest of these is length normalization which penalizes short translations in decoding  #TAUTHOR_TAG.', 'more sophisticated methods focus on modeling the coverage problem with extra sub - modules in nmt and require a training process  #AUTHOR_TAG.', 'perhaps']","['statistical machine translation  #AUTHOR_TAG.', 'in nmt, several good methods have been developed.', 'the simplest of these is length normalization which penalizes short translations in decoding  #TAUTHOR_TAG.', 'more sophisticated methods focus on modeling the coverage problem with extra sub - modules in nm']","['length preference and coverage problems have been discussed for years since the rise of statistical machine translation  #AUTHOR_TAG.', 'in nmt, several good methods have been developed.', 'the simplest of these is length normalization which penalizes short translations in decoding  #TAUTHOR_TAG.', 'more sophisticated methods focus on modeling the coverage problem with extra sub - modules in nmt and require a training process  #AUTHOR_TAG.', 'perhaps the most related work to this paper is  #TAUTHOR_TAG.', 'in their work, the coverage problem can be interpreted in a probability story.', 'however, it fails to account for the cases that one source word is translated into multiple target words and is thus of a total attention score > 1.', 'to address this issue, we remove the probability constraint and make the coverage score interpretable for different cases.', ""another difference lies in that our coverage model is applied to every beam search step, while  #TAUTHOR_TAG's model affects only a small number of translation outputs."", 'previous work have pointed out that bleu scores of nmt systems drop as beam size increases  #AUTHOR_TAG, and the existing length normalization and coverage models can alleviate this problem to some extent.', 'in this work we show that our method can do this much better.', '']",0
"['statistical machine translation  #AUTHOR_TAG.', 'in nmt, several good methods have been developed.', 'the simplest of these is length normalization which penalizes short translations in decoding  #TAUTHOR_TAG.', 'more sophisticated methods focus on modeling the coverage problem with extra sub - modules in nm']","['statistical machine translation  #AUTHOR_TAG.', 'in nmt, several good methods have been developed.', 'the simplest of these is length normalization which penalizes short translations in decoding  #TAUTHOR_TAG.', 'more sophisticated methods focus on modeling the coverage problem with extra sub - modules in nmt and require a training process  #AUTHOR_TAG.', 'perhaps']","['statistical machine translation  #AUTHOR_TAG.', 'in nmt, several good methods have been developed.', 'the simplest of these is length normalization which penalizes short translations in decoding  #TAUTHOR_TAG.', 'more sophisticated methods focus on modeling the coverage problem with extra sub - modules in nm']","['length preference and coverage problems have been discussed for years since the rise of statistical machine translation  #AUTHOR_TAG.', 'in nmt, several good methods have been developed.', 'the simplest of these is length normalization which penalizes short translations in decoding  #TAUTHOR_TAG.', 'more sophisticated methods focus on modeling the coverage problem with extra sub - modules in nmt and require a training process  #AUTHOR_TAG.', 'perhaps the most related work to this paper is  #TAUTHOR_TAG.', 'in their work, the coverage problem can be interpreted in a probability story.', 'however, it fails to account for the cases that one source word is translated into multiple target words and is thus of a total attention score > 1.', 'to address this issue, we remove the probability constraint and make the coverage score interpretable for different cases.', ""another difference lies in that our coverage model is applied to every beam search step, while  #TAUTHOR_TAG's model affects only a small number of translation outputs."", 'previous work have pointed out that bleu scores of nmt systems drop as beam size increases  #AUTHOR_TAG, and the existing length normalization and coverage models can alleviate this problem to some extent.', 'in this work we show that our method can do this much better.', '']",0
"['| j a ij  #TAUTHOR_TAG. then,', 'the coverage score of the sentence pair ( x, y ) is defined as the sum of the truncated coverage over all positions ( see figure 1']","['| j a ij  #TAUTHOR_TAG. then,', 'the coverage score of the sentence pair ( x, y ) is defined as the sum of the truncated coverage over all positions ( see figure 1']","['| j a ij  #TAUTHOR_TAG. then,', 'the coverage score of the sentence pair ( x, y ) is defined as the sum of the truncated coverage over all positions ( see figure 1 for an 1 as the discussion of', 'the attention mechanism is out of the scope of this']","['the past attention probabilities c i = | y | j a ij  #TAUTHOR_TAG. then,', 'the coverage score of the sentence pair ( x, y ) is defined as the sum of the truncated coverage over all positions ( see figure 1 for an 1 as the discussion of', 'the attention mechanism is out of the scope of this work, we refer the reader to  #AUTHOR_TAG ;  #AUTHOR_TAG', 'for more details. illustration ) : where β is a parameter that', 'can be tuned on a development set. this model has two properties : • non - linearity eq.', '( 2 ) is a log - linear model. it is desirable because this model does not benefit too much from the received attention when the coverage of a source word is high. this can prevent the cases that the system puts too much attention on a few words while others only receive', 'a little attention to have relatively high scores. beyond this, the log - scale scoring fits into the nm', '##t model where word probabilities are represented in the logarithm manner ( see eq. ( 1 ) ). • truncation at the early stage of decoding, the', 'coverage of the most source words is close to 0. this may result in a negative infinity value after the logarithm function, and discard hypotheses with sharp', 'attention distributions, which is not necessarily bad. the truncation with the lowest value β can ensure that the coverage score has a reasonable value. here β is similar to model warm - up, which makes the model easy to run in the first few decoding steps. note that our way of truncation is different from  #TAUTHOR_TAG', ""' s,"", 'where they clip the coverage into [ 0, 1 ] and ignore the', 'fact that a source word may', 'be translated into multiple target words and its coverage should be of a value larger than', '1. for decoding, we incorporate the coverage score into beam search via linear combination with the nmt model score as below,', 'where y is a partial translation generated during decoding, log p ( y | x ) is the model score, and α is the coefficient for linear', 'interpolation. in standard implementation of nmt systems, once a hypothesis is finished, it is removed from the beam and the beam shrinks accordingly. here we choose', 'a different decoding strategy. we keep the finished hypotheses in the beam until the decoding completes, which means that', 'we compare the finished hypotheses with partial translations at each step. this method helps because it can dynamically determine whether a finished hypothesis is kept in', 'beam through the entire decoding process, and thus reduce search errors. it enables the decoder to', 'throw away finished hypotheses if they have very low coverage but are of high likelihood values']",3
"['coverage penalty ( cp ) methods  #TAUTHOR_TAG.', '']","['coverage penalty ( cp ) methods  #TAUTHOR_TAG.', '']","['coverage penalty ( cp ) methods  #TAUTHOR_TAG.', 'we used grid search']","['', 'base = base system, ln = length normalization, cp = coverage penalty, and cs = our coverage score.', '30k entries for both source and target vocabularies.', 'for the english - german task, bpe  #AUTHOR_TAG was used for better performance.', 'for comparison, we re - implemented the length normalization ( ln ) and coverage penalty ( cp ) methods  #TAUTHOR_TAG.', 'we used grid search to tune all hyperparameters on the development set as  #TAUTHOR_TAG.', 'specifically, weights for both cp and our cs are evaluated in interval [ 0, 1 ] with step 0. 1, while the weight for ln is in interval [ 0. 5, 1. 5 ].', 'we found that the settings determined with beam size 10 can be reliably applied to larger beam sizes in the preliminary experiments and thus we tuned all systems with beam size 10.', 'for chinese - english translation, we used a weight of 1. 0 for both ln and cp, and set α = 0. 6 and β = 0. 4.', '']",3
"['coverage penalty ( cp ) methods  #TAUTHOR_TAG.', '']","['coverage penalty ( cp ) methods  #TAUTHOR_TAG.', '']","['coverage penalty ( cp ) methods  #TAUTHOR_TAG.', 'we used grid search']","['', 'base = base system, ln = length normalization, cp = coverage penalty, and cs = our coverage score.', '30k entries for both source and target vocabularies.', 'for the english - german task, bpe  #AUTHOR_TAG was used for better performance.', 'for comparison, we re - implemented the length normalization ( ln ) and coverage penalty ( cp ) methods  #TAUTHOR_TAG.', 'we used grid search to tune all hyperparameters on the development set as  #TAUTHOR_TAG.', 'specifically, weights for both cp and our cs are evaluated in interval [ 0, 1 ] with step 0. 1, while the weight for ln is in interval [ 0. 5, 1. 5 ].', 'we found that the settings determined with beam size 10 can be reliably applied to larger beam sizes in the preliminary experiments and thus we tuned all systems with beam size 10.', 'for chinese - english translation, we used a weight of 1. 0 for both ln and cp, and set α = 0. 6 and β = 0. 4.', '']",3
"['| j a ij  #TAUTHOR_TAG. then,', 'the coverage score of the sentence pair ( x, y ) is defined as the sum of the truncated coverage over all positions ( see figure 1']","['| j a ij  #TAUTHOR_TAG. then,', 'the coverage score of the sentence pair ( x, y ) is defined as the sum of the truncated coverage over all positions ( see figure 1']","['| j a ij  #TAUTHOR_TAG. then,', 'the coverage score of the sentence pair ( x, y ) is defined as the sum of the truncated coverage over all positions ( see figure 1 for an 1 as the discussion of', 'the attention mechanism is out of the scope of this']","['the past attention probabilities c i = | y | j a ij  #TAUTHOR_TAG. then,', 'the coverage score of the sentence pair ( x, y ) is defined as the sum of the truncated coverage over all positions ( see figure 1 for an 1 as the discussion of', 'the attention mechanism is out of the scope of this work, we refer the reader to  #AUTHOR_TAG ;  #AUTHOR_TAG', 'for more details. illustration ) : where β is a parameter that', 'can be tuned on a development set. this model has two properties : • non - linearity eq.', '( 2 ) is a log - linear model. it is desirable because this model does not benefit too much from the received attention when the coverage of a source word is high. this can prevent the cases that the system puts too much attention on a few words while others only receive', 'a little attention to have relatively high scores. beyond this, the log - scale scoring fits into the nm', '##t model where word probabilities are represented in the logarithm manner ( see eq. ( 1 ) ). • truncation at the early stage of decoding, the', 'coverage of the most source words is close to 0. this may result in a negative infinity value after the logarithm function, and discard hypotheses with sharp', 'attention distributions, which is not necessarily bad. the truncation with the lowest value β can ensure that the coverage score has a reasonable value. here β is similar to model warm - up, which makes the model easy to run in the first few decoding steps. note that our way of truncation is different from  #TAUTHOR_TAG', ""' s,"", 'where they clip the coverage into [ 0, 1 ] and ignore the', 'fact that a source word may', 'be translated into multiple target words and its coverage should be of a value larger than', '1. for decoding, we incorporate the coverage score into beam search via linear combination with the nmt model score as below,', 'where y is a partial translation generated during decoding, log p ( y | x ) is the model score, and α is the coefficient for linear', 'interpolation. in standard implementation of nmt systems, once a hypothesis is finished, it is removed from the beam and the beam shrinks accordingly. here we choose', 'a different decoding strategy. we keep the finished hypotheses in the beam until the decoding completes, which means that', 'we compare the finished hypotheses with partial translations at each step. this method helps because it can dynamically determine whether a finished hypothesis is kept in', 'beam through the entire decoding process, and thus reduce search errors. it enables the decoder to', 'throw away finished hypotheses if they have very low coverage but are of high likelihood values']",4
"['statistical machine translation  #AUTHOR_TAG.', 'in nmt, several good methods have been developed.', 'the simplest of these is length normalization which penalizes short translations in decoding  #TAUTHOR_TAG.', 'more sophisticated methods focus on modeling the coverage problem with extra sub - modules in nm']","['statistical machine translation  #AUTHOR_TAG.', 'in nmt, several good methods have been developed.', 'the simplest of these is length normalization which penalizes short translations in decoding  #TAUTHOR_TAG.', 'more sophisticated methods focus on modeling the coverage problem with extra sub - modules in nmt and require a training process  #AUTHOR_TAG.', 'perhaps']","['statistical machine translation  #AUTHOR_TAG.', 'in nmt, several good methods have been developed.', 'the simplest of these is length normalization which penalizes short translations in decoding  #TAUTHOR_TAG.', 'more sophisticated methods focus on modeling the coverage problem with extra sub - modules in nm']","['length preference and coverage problems have been discussed for years since the rise of statistical machine translation  #AUTHOR_TAG.', 'in nmt, several good methods have been developed.', 'the simplest of these is length normalization which penalizes short translations in decoding  #TAUTHOR_TAG.', 'more sophisticated methods focus on modeling the coverage problem with extra sub - modules in nmt and require a training process  #AUTHOR_TAG.', 'perhaps the most related work to this paper is  #TAUTHOR_TAG.', 'in their work, the coverage problem can be interpreted in a probability story.', 'however, it fails to account for the cases that one source word is translated into multiple target words and is thus of a total attention score > 1.', 'to address this issue, we remove the probability constraint and make the coverage score interpretable for different cases.', ""another difference lies in that our coverage model is applied to every beam search step, while  #TAUTHOR_TAG's model affects only a small number of translation outputs."", 'previous work have pointed out that bleu scores of nmt systems drop as beam size increases  #AUTHOR_TAG, and the existing length normalization and coverage models can alleviate this problem to some extent.', 'in this work we show that our method can do this much better.', '']",4
"['coverage penalty ( cp ) methods  #TAUTHOR_TAG.', '']","['coverage penalty ( cp ) methods  #TAUTHOR_TAG.', '']","['coverage penalty ( cp ) methods  #TAUTHOR_TAG.', 'we used grid search']","['', 'base = base system, ln = length normalization, cp = coverage penalty, and cs = our coverage score.', '30k entries for both source and target vocabularies.', 'for the english - german task, bpe  #AUTHOR_TAG was used for better performance.', 'for comparison, we re - implemented the length normalization ( ln ) and coverage penalty ( cp ) methods  #TAUTHOR_TAG.', 'we used grid search to tune all hyperparameters on the development set as  #TAUTHOR_TAG.', 'specifically, weights for both cp and our cs are evaluated in interval [ 0, 1 ] with step 0. 1, while the weight for ln is in interval [ 0. 5, 1. 5 ].', 'we found that the settings determined with beam size 10 can be reliably applied to larger beam sizes in the preliminary experiments and thus we tuned all systems with beam size 10.', 'for chinese - english translation, we used a weight of 1. 0 for both ln and cp, and set α = 0. 6 and β = 0. 4.', '']",5
"['coverage penalty ( cp ) methods  #TAUTHOR_TAG.', '']","['coverage penalty ( cp ) methods  #TAUTHOR_TAG.', '']","['coverage penalty ( cp ) methods  #TAUTHOR_TAG.', 'we used grid search']","['', 'base = base system, ln = length normalization, cp = coverage penalty, and cs = our coverage score.', '30k entries for both source and target vocabularies.', 'for the english - german task, bpe  #AUTHOR_TAG was used for better performance.', 'for comparison, we re - implemented the length normalization ( ln ) and coverage penalty ( cp ) methods  #TAUTHOR_TAG.', 'we used grid search to tune all hyperparameters on the development set as  #TAUTHOR_TAG.', 'specifically, weights for both cp and our cs are evaluated in interval [ 0, 1 ] with step 0. 1, while the weight for ln is in interval [ 0. 5, 1. 5 ].', 'we found that the settings determined with beam size 10 can be reliably applied to larger beam sizes in the preliminary experiments and thus we tuned all systems with beam size 10.', 'for chinese - english translation, we used a weight of 1. 0 for both ln and cp, and set α = 0. 6 and β = 0. 4.', '']",5
"['existing foil dataset  #TAUTHOR_TAG.', '']","['existing foil dataset  #TAUTHOR_TAG.', '']","['interpretation, overlooking difficulties in understanding other parts - of - speech.', 'our paper expands the existing foil dataset  #TAUTHOR_TAG.', '']","['', 'progress in this area has seemed swift and impressive, but the community is now scrutinising the results to understand whether enthusiasm is warranted.', 'several diagnostic datasets have been proposed with this goal in mind, highlighting various flaws in existing tasks  #AUTHOR_TAG.', 'our paper is a contribution to these efforts, showing that the field may have moved too fast from noun to sentence interpretation, overlooking difficulties in understanding other parts - of - speech.', 'our paper expands the existing foil dataset  #TAUTHOR_TAG.', 'foil consists of a set of images matched with captions containing one single mistake.', 'the mistakes are always nouns referring to objects not actually present in the image.', 'the work demonstrates that the language and vision modalities are not truly integrated in current computational models, as they fail to spot the mistake in the caption and to correct it appropriately ( humans, on the other hand, obtain almost 100 % accuracy on those tasks ).', 'in the present paper, we exploit the foil strategy to evaluate language and vision models on a larger set of possible mismatches between language and vision.', ""beside considering nouns as possible'foil'words, we also consider verbs, adjectives, adverbs and prepositions, as illustrated in figure 1."", 'the results obtained by state - of - the - art systems on this data demonstrate that current models are indeed little able to move beyond object understanding.', 'figure 1 : sample image, corresponding original caption and the generated foil caption for the different parts of speech.', ""the model has to be able to classify the caption as'correct'or'foil'( task 1 ) ; detect the foil word in the foil caption ( see words highlighted in red ) ( task 2 ) ; and correct the foil word with an appropriate replacement ( see words highlighted in green ) ( task 3 )""]",5
"['##3, we regress over all  #TAUTHOR_TAG.', 'the target words on the position of the foil word and select the one which generates the caption with the highest probability to be']","['with the highest probability. for t3, we regress over all  #TAUTHOR_TAG.', 'the target words on the position of the foil word and select the one which generates the caption with the highest probability to be "" good']","['##3, we regress over all  #TAUTHOR_TAG.', 'the target words on the position of the foil word and select the one which generates the caption with the highest probability to be "" good']","['', ""for t1, the models are directly trained to classify a given caption as'good'vs.'foil '. for"", 't2 and t3, the model trained on t1 is adopted. for t2, we subsequently occlude one word (  #AUTHOR_TAG ) at a time and calculate the probability of the new caption', 'to be good vs. foil. the model selects as foil word, the one which has generated the caption with the highest probability. for t3, we regress over all  #TAUTHOR_TAG.', 'the target words on the position of the foil word and select the one which generates the caption with the highest probability to be "" good "". due to the generative nature of ic models,', 'adapting ic - wang for the classification purpose is less straightforward.', 'for t1, we generate all possible captions by subsequently predicting one word at a time provided all other words in the cap', '##tion and the image. we compare the probability of these generated captions with the given', 'caption. when the test caption probability is higher than generated captions probabilities,', 'we classify the given caption as good caption, else as foil caption']",5
"['##3, we regress over all  #TAUTHOR_TAG.', 'the target words on the position of the foil word and select the one which generates the caption with the highest probability to be']","['with the highest probability. for t3, we regress over all  #TAUTHOR_TAG.', 'the target words on the position of the foil word and select the one which generates the caption with the highest probability to be "" good']","['##3, we regress over all  #TAUTHOR_TAG.', 'the target words on the position of the foil word and select the one which generates the caption with the highest probability to be "" good']","['', ""for t1, the models are directly trained to classify a given caption as'good'vs.'foil '. for"", 't2 and t3, the model trained on t1 is adopted. for t2, we subsequently occlude one word (  #AUTHOR_TAG ) at a time and calculate the probability of the new caption', 'to be good vs. foil. the model selects as foil word, the one which has generated the caption with the highest probability. for t3, we regress over all  #TAUTHOR_TAG.', 'the target words on the position of the foil word and select the one which generates the caption with the highest probability to be "" good "". due to the generative nature of ic models,', 'adapting ic - wang for the classification purpose is less straightforward.', 'for t1, we generate all possible captions by subsequently predicting one word at a time provided all other words in the cap', '##tion and the image. we compare the probability of these generated captions with the given', 'caption. when the test caption probability is higher than generated captions probabilities,', 'we classify the given caption as good caption, else as foil caption']",5
"[' #TAUTHOR_TAG, we aim at creating a dataset of images associated with both correct']","[' #TAUTHOR_TAG, we aim at creating a dataset of images associated with both correct']","[' #TAUTHOR_TAG, we aim at creating a dataset of images associated with both correct']","[' #TAUTHOR_TAG, we aim at creating a dataset of images associated with both correct and foil captions, where the latter are obtained by replacing one word in the original text.', 'expanding on the original paper, our target / foil pairs do not merely consist of nouns.', ""the introduced error can also be an adjective ( an object's attribute ), a verb ( an action ), a preposition ( a relation between objects ) or an adverb ( a manner of action )."", 'in total, we produce 196, 284 datapoints, each corresponding to an < image, original, foil > triple.', ""the starting point for images and correct captions is microsoft's common objects in context ( ms - coco ) (  #AUTHOR_TAG )""]",5
"['across the various pos, we only use a subset of the foil - coco dataset of  #TAUTHOR_TAG.', 'from the foil']","['across the various pos, we only use a subset of the foil - coco dataset of  #TAUTHOR_TAG.', 'from the foil dataset, we retain the']","['to the same category in ms - coco ( e. g., bird / dog, from the ms - coco category animal ).', 'in order to obtain a balanced dataset across the various pos, we only use a subset of the foil - coco dataset of  #TAUTHOR_TAG.', 'from the foil dataset, we retain the 37, 536 images for which foil captions could be generated, using the target / foil pairs']","['', 'nouns : the target / foil noun pairs are built using words that belong to the same category in ms - coco ( e. g., bird / dog, from the ms - coco category animal ).', 'in order to obtain a balanced dataset across the various pos, we only use a subset of the foil - coco dataset of  #TAUTHOR_TAG.', 'from the foil dataset, we retain the 37, 536 images for which foil captions could be generated, using the target / foil pairs extracted from the resources mentioned above.', 'of the foil datapoints generated for the noun pairs, only those containing images used for the other pos are selected.', 'hence, the number of unique images of the whole dataset is the same of those used for nouns ( see']",5
"['from the foil dataset by  #TAUTHOR_TAG.', 'in this']","['from the foil dataset by  #TAUTHOR_TAG.', 'in this case, for']","['from the foil dataset by  #TAUTHOR_TAG.', 'in this case, for each original ms - coco caption, several foil ones are generated and subsequently filtered using several heuristics.', 'the aim of filtering is']","['the word pair lists above, foil captions are generated from ms - coco original captions.', 'the foil captions are generated by replacing nouns are directly extracted from the foil dataset by  #TAUTHOR_TAG.', 'in this case, for each original ms - coco caption, several foil ones are generated and subsequently filtered using several heuristics.', 'the aim of filtering is to prioritise salient objects in the image, and to minimise the language bias in the data.', 'ideally, these filters would have to be applied also for the generation of the foil caption for the other pos, but we found that they reduced the size of our data in an unacceptably small size.', ""as a consequence, the results we report are obviously affected by the language bias, as shown by the reasonable performance of a'blind'model without access to visual data."", 'however, as we will see, our broad claim is not affected by this heightened baseline.', 'details on the number of the unique images and of of datapoints generated for each pos are reported in table 1.', 'table 2 reports the accuracy of the various models described in § 2 for task t1.', '']",5
"['from the foil dataset by  #TAUTHOR_TAG.', 'in this']","['from the foil dataset by  #TAUTHOR_TAG.', 'in this case, for']","['from the foil dataset by  #TAUTHOR_TAG.', 'in this case, for each original ms - coco caption, several foil ones are generated and subsequently filtered using several heuristics.', 'the aim of filtering is']","['the word pair lists above, foil captions are generated from ms - coco original captions.', 'the foil captions are generated by replacing nouns are directly extracted from the foil dataset by  #TAUTHOR_TAG.', 'in this case, for each original ms - coco caption, several foil ones are generated and subsequently filtered using several heuristics.', 'the aim of filtering is to prioritise salient objects in the image, and to minimise the language bias in the data.', 'ideally, these filters would have to be applied also for the generation of the foil caption for the other pos, but we found that they reduced the size of our data in an unacceptably small size.', ""as a consequence, the results we report are obviously affected by the language bias, as shown by the reasonable performance of a'blind'model without access to visual data."", 'however, as we will see, our broad claim is not affected by this heightened baseline.', 'details on the number of the unique images and of of datapoints generated for each pos are reported in table 1.', 'table 2 reports the accuracy of the various models described in § 2 for task t1.', '']",5
"[' #TAUTHOR_TAG, we aim at creating a dataset of images associated with both correct']","[' #TAUTHOR_TAG, we aim at creating a dataset of images associated with both correct']","[' #TAUTHOR_TAG, we aim at creating a dataset of images associated with both correct']","[' #TAUTHOR_TAG, we aim at creating a dataset of images associated with both correct and foil captions, where the latter are obtained by replacing one word in the original text.', 'expanding on the original paper, our target / foil pairs do not merely consist of nouns.', ""the introduced error can also be an adjective ( an object's attribute ), a verb ( an action ), a preposition ( a relation between objects ) or an adverb ( a manner of action )."", 'in total, we produce 196, 284 datapoints, each corresponding to an < image, original, foil > triple.', ""the starting point for images and correct captions is microsoft's common objects in context ( ms - coco ) (  #AUTHOR_TAG )""]",6
"['from the foil dataset by  #TAUTHOR_TAG.', 'in this']","['from the foil dataset by  #TAUTHOR_TAG.', 'in this case, for']","['from the foil dataset by  #TAUTHOR_TAG.', 'in this case, for each original ms - coco caption, several foil ones are generated and subsequently filtered using several heuristics.', 'the aim of filtering is']","['the word pair lists above, foil captions are generated from ms - coco original captions.', 'the foil captions are generated by replacing nouns are directly extracted from the foil dataset by  #TAUTHOR_TAG.', 'in this case, for each original ms - coco caption, several foil ones are generated and subsequently filtered using several heuristics.', 'the aim of filtering is to prioritise salient objects in the image, and to minimise the language bias in the data.', 'ideally, these filters would have to be applied also for the generation of the foil caption for the other pos, but we found that they reduced the size of our data in an unacceptably small size.', ""as a consequence, the results we report are obviously affected by the language bias, as shown by the reasonable performance of a'blind'model without access to visual data."", 'however, as we will see, our broad claim is not affected by this heightened baseline.', 'details on the number of the unique images and of of datapoints generated for each pos are reported in table 1.', 'table 2 reports the accuracy of the various models described in § 2 for task t1.', '']",0
"['##ner 2  #TAUTHOR_TAG, a tool for misalign']","['with bicleaner 2  #TAUTHOR_TAG, a tool for misalignment']","['with bicleaner 2  #TAUTHOR_TAG, a tool for misalign']","['', 'translation quality both on a domain - specific test set and on a more generic test set. next, our experiments confirm that nmt translation quality for ga→en can be significantly improved using', 'back - translation. due to a lack of irish monolingual data, backtranslation was less useful for en→ga nmt. finally, a set of experiments was performed in which the synthetic parallel corpus, obtained via back - translation, was filtered with bicleaner 2  #TAUTHOR_TAG, a tool for misalignment detection', '. we show that applying misalignment detection on a synthetic corpus before adding it to the parallel training data results in small increases in bleu score and could be a useful strategy in terms of data selection. filtering of parallel data has', 'been the subject of various studies ( axelrod et al. 2011 ; van der wees et al', '. 2017 ), but such data selection methods have only been scarcely investigated in the context of back - translation.  #AUTHOR_TAG suggest several sampling strategies for synthetic data obtained via back - translation, targeting difficult', 'to predict words. more closely related to our filtering technique is the method proposed by  #AUTHOR_TAG. they present a method in', 'which a synthetic corpus was filtered by calculating the bleu score between the target monolingual sentence and the translation of the', 'synthetic source sentence in the target language and report small increases in translation quality in a lowresource setting']",5
['##ner  #TAUTHOR_TAG was applied to'],['bicleaner  #TAUTHOR_TAG was applied to'],[' #TAUTHOR_TAG was applied to'],"['this section, we give an overview of the data used for training our nmt systems.', 'both bilingual and monolingual data are used.', 'in table 1 an overview of the parallel data is shown.', 'three types of data were collected : 1 ) baseline data, i. e. a collection of publicly available resources ; 2 ) web - crawled data, i. e. data scraped from two bilingual websites, and 3 ) paracrawl data.', 'the baseline data has been described in detail in previous publications ( dowling et al. 2015 ; arcan et al. 2016 ).', 'we note that there are some other parallel corpora available for the en - ga language pair, the largest of which are the kde 3 and gnome 4 corpora.', 'however, due to the very specific nature of these corpora, they were not included in the training data.', 'the web - crawled dataset consists of sentence pairs we scraped and aligned ourselves from two bilingual websites.', 'this data was scraped using scrapy 5 and then document - aligned using malign, 6 a tool for document alignment that makes use of mt.', 'sentence alignment of these document pairs was subsequently performed using hunalign 7 ( varga et al. 2005 ).', 'finally, the misalignment detection tool bicleaner  #TAUTHOR_TAG was applied to these aligned sentences ( the bicleaner threshold was set to 0. 5 8 ).', 'we also used the paracrawl corpus as a bilingual resource.', 'we used the raw en - ga paracrawl corpus v4. 0 15 consisting of 156m sentence pairs.', 'paracrawl is known to contain a diversity of noise such as misalignments, untranslated sentences, non - linguistic characters, wrong encoding, language errors, short segments etc.', 'that may harm nmt performance ( khayrallah et al.']",5
"['##ner  #TAUTHOR_TAG.', 'we show that our approach']","['misalignment detection, bicleaner  #TAUTHOR_TAG.', 'we show that our approach']","['misalignment detection, bicleaner  #TAUTHOR_TAG.', 'we show that our approach results in small increases']","['', 'such data is, to the best of our knowledge, not publicly available.', 'the corpus of contemporary irish, a monolingual collection of irish - language texts in digital format, 22 containing around 24. 7m words, may be a possible candidate.', 'however, this corpus is only searchable and we could therefore not use it in the present study.', 'finally, we presented a lightweight method for filtering synthetic sentence pairs obtained via back - translation, using a tool for misalignment detection, bicleaner  #TAUTHOR_TAG.', 'we show that our approach results in small increases in bleu score, while requiring less training data.', 'in future work we will investigate to what extent our proposed methodology can be applied to other languages with a similar amount of data available.', 'another interesting research direction would be the development of a multilingual mt system which includes not only irish but also other gaelic languages, and which is based on methods such as the one described by  #AUTHOR_TAG.', 'it should also be investigated whether unsupervised mt approaches like the one of lample et al. 22 https : / / www. gaois. ie / g3m / en ( 2019 ) can be used to increase the translation quality of en↔ga mt systems']",5
"[', sentences are scored based on fluency and diversity.', 'more details are provided by  #TAUTHOR_TAG.', 'in order to clean the en - ga web - crawled corpus and the synthetic']","['detected by means of an automatic classifier.', 'finally, sentences are scored based on fluency and diversity.', 'more details are provided by  #TAUTHOR_TAG.', 'in order to clean the en - ga web - crawled corpus and the synthetic']","[', sentences are scored based on fluency and diversity.', 'more details are provided by  #TAUTHOR_TAG.', 'in order to clean the en - ga web - crawled corpus and the synthetic data']","['##leaner detects noisy sentence pairs in a parallel corpus by estimating the likelihood of a pair of sentences being mutual translations ( value near 1 ) or not ( value near 0 ).', 'very noisy sentences are given the score 0 and detected by means of hand - crafted hard rules.', 'this set of hand - crafted rules tries to detect evident flaws such as language errors, encoding errors, short segments and very different lengths in pairs of parallel sentences.', 'in a second step, misalignments are detected by means of an automatic classifier.', 'finally, sentences are scored based on fluency and diversity.', 'more details are provided by  #TAUTHOR_TAG.', 'in order to clean the en - ga web - crawled corpus and the synthetic data obtained via backtranslation we used a pre - trained classifier provided by the authors.', '']",7
"['.', 'to our knowledge,  #TAUTHOR_TAG is the only']","['semantic frame.', 'to our knowledge,  #TAUTHOR_TAG is the only']","['appropriate semantic frame.', 'to our knowledge,  #TAUTHOR_TAG is the only work that uses']","['', 'to our knowledge,  #TAUTHOR_TAG is the only work that uses framenet to build a statistical semantic classifier.', '']",3
"['test set as that used in  #TAUTHOR_TAG.', 'using only extracted']","['test set as that used in  #TAUTHOR_TAG.', 'using only extracted']","['', 'as a further analysis, we have examined the performance of our base me model on the same test set as that used in  #TAUTHOR_TAG.', 'using only extracted']","['', 'in the training data, a syntactic pattern of np - ext, target, np - obj, given the predicate bend, was associated 100 % of the time with the frame element pattern : "" agent target bodypart "", thus, providing powerful evidence as to the classification of those frame elements.', 'we exploit these sentence - level patterns by implementing a re - ranking system that chooses among the n - best tagger outputs.', 'the re - ranker was trained on a development corpus, which was first tagged using the metagger described above.', ""for each sentence in the development corpus, the 10 best tag sequences are output by the classifier and described by three probabilities : 3 1 ) the sequence's probability given by the me classifier ( me ) ; 2 ) the conditional probability of that sequence given the syntactic pattern and the target predicate ( pat + target ) ; 3 ) a back off conditional probability of the tag sequence given just the syntactic pattern ( pat )."", 'a me model is then used to combine the log of these probabilities to give a model of the form : p ( tag - seq | me, pat + target, pat ) figure 2 shows the performance of the base me model, the base model within a tagging framework, and the base model within a tagging framework plus the reranker.', 'results are shown for data sets trained and tested using human annotated syntactic features and trained and tested using automatically extracted syntactic features.', 'in both cases the training and test sets are identical.', 'for both the extracted and human conditions, adopting a tagging framework improves results by over 1 %.', 'however, while the syntactic pattern based reranker increases performance using human annotations by nearly 2 %, the effect when using automatically extracted information is only 0. 5 %.', ""this is reasonable considering that the re - ranker's effectiveness is correlated with the level of noise in the syntactic patterns upon which it is based."", 'the difference in performance between the models under both human and extracted conditions was relatively consistent : averaging 8. 7 % with a standard deviation of 0. 7.', 'as a further analysis, we have examined the performance of our base me model on the same test set as that used in  #TAUTHOR_TAG.', 'using only extracted']",3
"['.', 'to our knowledge,  #TAUTHOR_TAG is the only']","['semantic frame.', 'to our knowledge,  #TAUTHOR_TAG is the only']","['appropriate semantic frame.', 'to our knowledge,  #TAUTHOR_TAG is the only work that uses']","['', 'to our knowledge,  #TAUTHOR_TAG is the only work that uses framenet to build a statistical semantic classifier.', '']",0
"['.', 'to our knowledge,  #TAUTHOR_TAG is the only']","['semantic frame.', 'to our knowledge,  #TAUTHOR_TAG is the only']","['appropriate semantic frame.', 'to our knowledge,  #TAUTHOR_TAG is the only work that uses']","['', 'to our knowledge,  #TAUTHOR_TAG is the only work that uses framenet to build a statistical semantic classifier.', '']",4
"['2002 framenet release following the divisions used in  #TAUTHOR_TAG 1.', 'because human - annotated syntactic information could only be obtained for a subset of their data, the training']","['2002 framenet release following the divisions used in  #TAUTHOR_TAG 1.', 'because human - annotated syntactic information could only be obtained for a subset of their data, the training, development, and test sets used here are approximately']","['framenet release following the divisions used in  #TAUTHOR_TAG 1.', 'because human - annotated syntactic information could only be obtained for a subset of their data, the training, development, and test sets used here are approximately']","['( 32, 251 sentences ), development ( 3, 491 sentences ), and held out test sets ( 3, 398 sentences ) were generated from the june 2002 framenet release following the divisions used in  #TAUTHOR_TAG 1.', 'because human - annotated syntactic information could only be obtained for a subset of their data, the training, development, and test sets used here are approximately 10 % smaller than those used in  #TAUTHOR_TAG.', '2 there are on average 2. 2 frame elements per sentence, falling into one of 126 unique classes']",4
"['target predicate.', 'due to data sparsity issues, we do not calculate this model directly, but rather, model various feature combinations as described in  #TAUTHOR_TAG.', 'the classifier was trained, using only features that had a frequency in training of']","['target predicate.', 'due to data sparsity issues, we do not calculate this model directly, but rather, model various feature combinations as described in  #TAUTHOR_TAG.', 'the classifier was trained, using only features that had a frequency in training of']","['r indicates the element type, pt the phrase type, gf the grammatical function, h the head word, and target the target predicate.', 'due to data sparsity issues, we do not calculate this model directly, but rather, model various feature combinations as described in  #TAUTHOR_TAG.', 'the classifier was trained, using only features that had a frequency in training of']","['r indicates the element type, pt the phrase type, gf the grammatical function, h the head word, and target the target predicate.', 'due to data sparsity issues, we do not calculate this model directly, but rather, model various feature combinations as described in  #TAUTHOR_TAG.', 'the classifier was trained, using only features that had a frequency in training of one or more, and until performance on the development set ceased to improve.', 'feature weights were smoothed using a bayesian method, such that weight limits are gaussian distributed with mean 0 and standard deviation 1']",4
"['via personal communication.', '2  #TAUTHOR_TAG use']","['via personal communication.', '2  #TAUTHOR_TAG use 36995 training, 4000 development,']","['divisions given by dan gildea via personal communication.', '2  #TAUTHOR_TAG use 36995 training, 4000 development,']","['divisions given by dan gildea via personal communication.', '2  #TAUTHOR_TAG use 36995 training, 4000 development, and 3865 test sentences.', 'they do not report results using hand annotated syntactic information.', 'augmented to include information about the tags of the previous one and two frame elements in the sentence']",4
"['from parse trees do not have access to rich grammatical information.', 'following  #TAUTHOR_TAG, automatic extraction of grammatical information here is limited']","['from parse trees do not have access to rich grammatical information.', 'following  #TAUTHOR_TAG, automatic extraction of grammatical information here is limited']","['from parse trees do not have access to rich grammatical information.', 'following  #TAUTHOR_TAG, automatic extraction of grammatical information here is limited']","['', 'to compensate for noisy parser output, our current work is focusing on two strategies.', 'first, we are looking at using shallower but more reliable methods for syntactic feature generation, such as part of speech tagging and text chunking, to either replace or augment the syntactic parser.', 'second, we are using ontological information, such as word classes and synonyms, in the hopes that semantic information may supplement the noisy syntactic information.', 'the models trained on features extracted from parse trees do not have access to rich grammatical information.', 'following  #TAUTHOR_TAG, automatic extraction of grammatical information here is limited to the governing category of a noun phrase.', 'the framenet annotations, however, are much richer and include information about complements, modifiers, etc.', 'we are looking at ways to include such information either by using alternative parsers  #AUTHOR_TAG or as a post processing task  #AUTHOR_TAG.', 'in future work, we will extend the strategies outlined here to incorporate frame element identification into our model.', 'by treating semantic classification as a single tagging problem, we hope to create a unified, practical, and high performance system for frame element tagging']",4
"['.', 'to our knowledge,  #TAUTHOR_TAG is the only']","['semantic frame.', 'to our knowledge,  #TAUTHOR_TAG is the only']","['appropriate semantic frame.', 'to our knowledge,  #TAUTHOR_TAG is the only work that uses']","['', 'to our knowledge,  #TAUTHOR_TAG is the only work that uses framenet to build a statistical semantic classifier.', '']",6
"['target predicate.', 'due to data sparsity issues, we do not calculate this model directly, but rather, model various feature combinations as described in  #TAUTHOR_TAG.', 'the classifier was trained, using only features that had a frequency in training of']","['target predicate.', 'due to data sparsity issues, we do not calculate this model directly, but rather, model various feature combinations as described in  #TAUTHOR_TAG.', 'the classifier was trained, using only features that had a frequency in training of']","['r indicates the element type, pt the phrase type, gf the grammatical function, h the head word, and target the target predicate.', 'due to data sparsity issues, we do not calculate this model directly, but rather, model various feature combinations as described in  #TAUTHOR_TAG.', 'the classifier was trained, using only features that had a frequency in training of']","['r indicates the element type, pt the phrase type, gf the grammatical function, h the head word, and target the target predicate.', 'due to data sparsity issues, we do not calculate this model directly, but rather, model various feature combinations as described in  #TAUTHOR_TAG.', 'the classifier was trained, using only features that had a frequency in training of one or more, and until performance on the development set ceased to improve.', 'feature weights were smoothed using a bayesian method, such that weight limits are gaussian distributed with mean 0 and standard deviation 1']",6
"['2002 framenet release following the divisions used in  #TAUTHOR_TAG 1.', 'because human - annotated syntactic information could only be obtained for a subset of their data, the training']","['2002 framenet release following the divisions used in  #TAUTHOR_TAG 1.', 'because human - annotated syntactic information could only be obtained for a subset of their data, the training, development, and test sets used here are approximately']","['framenet release following the divisions used in  #TAUTHOR_TAG 1.', 'because human - annotated syntactic information could only be obtained for a subset of their data, the training, development, and test sets used here are approximately']","['( 32, 251 sentences ), development ( 3, 491 sentences ), and held out test sets ( 3, 398 sentences ) were generated from the june 2002 framenet release following the divisions used in  #TAUTHOR_TAG 1.', 'because human - annotated syntactic information could only be obtained for a subset of their data, the training, development, and test sets used here are approximately 10 % smaller than those used in  #TAUTHOR_TAG.', '2 there are on average 2. 2 frame elements per sentence, falling into one of 126 unique classes']",5
"['test set as that used in  #TAUTHOR_TAG.', 'using only extracted']","['test set as that used in  #TAUTHOR_TAG.', 'using only extracted']","['', 'as a further analysis, we have examined the performance of our base me model on the same test set as that used in  #TAUTHOR_TAG.', 'using only extracted']","['', 'in the training data, a syntactic pattern of np - ext, target, np - obj, given the predicate bend, was associated 100 % of the time with the frame element pattern : "" agent target bodypart "", thus, providing powerful evidence as to the classification of those frame elements.', 'we exploit these sentence - level patterns by implementing a re - ranking system that chooses among the n - best tagger outputs.', 'the re - ranker was trained on a development corpus, which was first tagged using the metagger described above.', ""for each sentence in the development corpus, the 10 best tag sequences are output by the classifier and described by three probabilities : 3 1 ) the sequence's probability given by the me classifier ( me ) ; 2 ) the conditional probability of that sequence given the syntactic pattern and the target predicate ( pat + target ) ; 3 ) a back off conditional probability of the tag sequence given just the syntactic pattern ( pat )."", 'a me model is then used to combine the log of these probabilities to give a model of the form : p ( tag - seq | me, pat + target, pat ) figure 2 shows the performance of the base me model, the base model within a tagging framework, and the base model within a tagging framework plus the reranker.', 'results are shown for data sets trained and tested using human annotated syntactic features and trained and tested using automatically extracted syntactic features.', 'in both cases the training and test sets are identical.', 'for both the extracted and human conditions, adopting a tagging framework improves results by over 1 %.', 'however, while the syntactic pattern based reranker increases performance using human annotations by nearly 2 %, the effect when using automatically extracted information is only 0. 5 %.', ""this is reasonable considering that the re - ranker's effectiveness is correlated with the level of noise in the syntactic patterns upon which it is based."", 'the difference in performance between the models under both human and extracted conditions was relatively consistent : averaging 8. 7 % with a standard deviation of 0. 7.', 'as a further analysis, we have examined the performance of our base me model on the same test set as that used in  #TAUTHOR_TAG.', 'using only extracted']",5
"[', following  #TAUTHOR_TAG,']","['obtained, following  #TAUTHOR_TAG,']","[', following  #TAUTHOR_TAG, the following equations are then used to compute the compositionality of']",[' #TAUTHOR_TAG'],5
"['wikipedia dumps - following  #TAUTHOR_TAG - from 20 january 2018.', 'the raw dumps']","['wikipedia dumps - following  #TAUTHOR_TAG - from 20 january 2018.', 'the raw dumps']","['and german wikipedia dumps - following  #TAUTHOR_TAG - from 20 january 2018.', 'the raw dumps']","['train language models over a portion of english and german wikipedia dumps - following  #TAUTHOR_TAG - from 20 january 2018.', 'the raw dumps are preprocessed using wp2txt 6 to remove wikimarkup, metadata, and xml and html tags.', 'the text from wikipedia contains many characters that are not typically found in mwes, for example, non - ascii characters.', 'such characters drastically increase the size of the vocabulary of the language model, which leads to very long training times.', 'we therefore remove all non - ascii characters from the english dump, and all non - ascii characters other thana, a, o, o, u, u, ß from the german dump.', 'training the character - level language model over the wikipedia dumps in their entirety would take a prohibitively long time due to their size.', 'we therefore instead carry out experiments training on a 1 % sample of the english dump, and a 2 % sample of the german dump ( to give a corpus of similar size to the english one ).', 'details of the resulting training corpora are provided in table 1']",5
['three datasets as  #TAUTHOR_TAG cover two languages ( english and'],['three datasets as  #TAUTHOR_TAG cover two languages ( english and german ) and two'],['proposed model is evaluated over the same three datasets as  #TAUTHOR_TAG cover two languages ( english and'],"['proposed model is evaluated over the same three datasets as  #TAUTHOR_TAG cover two languages ( english and german ) and two kinds of mwes ( noun compounds and verb - particle constructions ).', 'enc this dataset contains 90 english noun compounds ( e. g., game plan, gravy train ) which are annotated on a scale of [ 0, 5 ] for both their overall compositionality, and the compositionality of each of their component words  #AUTHOR_TAG.', ' #AUTHOR_TAG, are also shown.', 'evpc this dataset consists of 160 english verb - particle constructions ( e. g., add up, figure out ) which are rated on a binary scale for the compositionality of each of the verb and particle component words  #AUTHOR_TAG by multiple annotators ; no ratings for the overall compositionality of mwes are provided in this dataset.', 'the binary compositionality judgements are converted to continuous values as in  #TAUTHOR_TAG by dividing the number of judgements that an expression is compositional by the total number of judgements.', ""gnc this dataset contains 244 german noun compounds ( e. g., ahornblatt'maple leaf ', knoblauch'garlic') which are annotated on a scale of [ 1, 7 ] for their overall compositionality, and the compositionality of each component word ( von der  #AUTHOR_TAG""]",5
['three datasets as  #TAUTHOR_TAG cover two languages ( english and'],['three datasets as  #TAUTHOR_TAG cover two languages ( english and german ) and two'],['proposed model is evaluated over the same three datasets as  #TAUTHOR_TAG cover two languages ( english and'],"['proposed model is evaluated over the same three datasets as  #TAUTHOR_TAG cover two languages ( english and german ) and two kinds of mwes ( noun compounds and verb - particle constructions ).', 'enc this dataset contains 90 english noun compounds ( e. g., game plan, gravy train ) which are annotated on a scale of [ 0, 5 ] for both their overall compositionality, and the compositionality of each of their component words  #AUTHOR_TAG.', ' #AUTHOR_TAG, are also shown.', 'evpc this dataset consists of 160 english verb - particle constructions ( e. g., add up, figure out ) which are rated on a binary scale for the compositionality of each of the verb and particle component words  #AUTHOR_TAG by multiple annotators ; no ratings for the overall compositionality of mwes are provided in this dataset.', 'the binary compositionality judgements are converted to continuous values as in  #TAUTHOR_TAG by dividing the number of judgements that an expression is compositional by the total number of judgements.', ""gnc this dataset contains 244 german noun compounds ( e. g., ahornblatt'maple leaf ', knoblauch'garlic') which are annotated on a scale of [ 1, 7 ] for their overall compositionality, and the compositionality of each component word ( von der  #AUTHOR_TAG""]",5
"[""following  #TAUTHOR_TAG by computing pearson's correlation between the predicted compositional""]","[""following  #TAUTHOR_TAG by computing pearson's correlation between the predicted compositionality ( i. e., from""]","[""proposed approach following  #TAUTHOR_TAG by computing pearson's correlation between the predicted compositionality (""]","[""evaluate our proposed approach following  #TAUTHOR_TAG by computing pearson's correlation between the predicted compositionality ( i. e., from either comp 1 or comp 2 ) and human ratings for overall compositionality."", 'for evpc, no overall compositionality ratings are provided.', 'in this case we report the correlation between the predicted compositionality scores and both the verb and particle compositionality judgements.', '']",5
['of  #TAUTHOR_TAG - typically'],['predicting compositionality of  #TAUTHOR_TAG - typically'],['of  #TAUTHOR_TAG - typically'],"['', '. word embedding models - such as that used in the approach to predicting compositionality of  #TAUTHOR_TAG - typically do not learn representations for low frequency items', '. 9 these results demonstrate that the proposed model is able to predict the compositionality for low frequency items, that would not typically be in', '- vocabulary for word embedding models, and for which compositionality models based only on word embeddings would not be able to make predictions. 10 for gnc, and the verb component of evpc, in line with the previous results over', 'the entire dataset, neither compositionality measure gives significant correlations, with the', 'exception of the verb component of evpc using comp 2 for unattested expressions, although', 'again the number of expressions here is relatively small', '. in an effort to improve on', 'the default setup we considered a range of model variations. in particular we considered an rnn and gru ( instead of an lstm ), character embeddings of size 25 and 50 (', 'instead of a one - hot representation ), increasing the batch size to 100 ( from 20 ), using dropout between', '0. 2 - 0. 6, and using a bi - directional lstm. none of these variations led to consistent improvements over the default', 'setup']",5
['of  #TAUTHOR_TAG - typically'],['predicting compositionality of  #TAUTHOR_TAG - typically'],['of  #TAUTHOR_TAG - typically'],"['', '. word embedding models - such as that used in the approach to predicting compositionality of  #TAUTHOR_TAG - typically do not learn representations for low frequency items', '. 9 these results demonstrate that the proposed model is able to predict the compositionality for low frequency items, that would not typically be in', '- vocabulary for word embedding models, and for which compositionality models based only on word embeddings would not be able to make predictions. 10 for gnc, and the verb component of evpc, in line with the previous results over', 'the entire dataset, neither compositionality measure gives significant correlations, with the', 'exception of the verb component of evpc using comp 2 for unattested expressions, although', 'again the number of expressions here is relatively small', '. in an effort to improve on', 'the default setup we considered a range of model variations. in particular we considered an rnn and gru ( instead of an lstm ), character embeddings of size 25 and 50 (', 'instead of a one - hot representation ), increasing the batch size to 100 ( from 20 ), using dropout between', '0. 2 - 0. 6, and using a bi - directional lstm. none of these variations led to consistent improvements over the default', 'setup']",4
['of  #TAUTHOR_TAG - typically'],['predicting compositionality of  #TAUTHOR_TAG - typically'],['of  #TAUTHOR_TAG - typically'],"['', '. word embedding models - such as that used in the approach to predicting compositionality of  #TAUTHOR_TAG - typically do not learn representations for low frequency items', '. 9 these results demonstrate that the proposed model is able to predict the compositionality for low frequency items, that would not typically be in', '- vocabulary for word embedding models, and for which compositionality models based only on word embeddings would not be able to make predictions. 10 for gnc, and the verb component of evpc, in line with the previous results over', 'the entire dataset, neither compositionality measure gives significant correlations, with the', 'exception of the verb component of evpc using comp 2 for unattested expressions, although', 'again the number of expressions here is relatively small', '. in an effort to improve on', 'the default setup we considered a range of model variations. in particular we considered an rnn and gru ( instead of an lstm ), character embeddings of size 25 and 50 (', 'instead of a one - hot representation ), increasing the batch size to 100 ( from 20 ), using dropout between', '0. 2 - 0. 6, and using a bi - directional lstm. none of these variations led to consistent improvements over the default', 'setup']",4
"['system,  #TAUTHOR_TAG explain that :', '']","['system,  #TAUTHOR_TAG explain that :', '[ c ] oncepts could be words, named entities, syntactic subtrees']","['system,  #TAUTHOR_TAG explain that :', '']","['- of - the - art approaches to extractive summarization are based on the notion of coverage maximization ( berg -  #AUTHOR_TAG.', 'the assumption is that a good summary is a selection of sentences from the document that contains as many of the important concepts as possible.', 'the importance of concepts is implemented by assigning weights w i to each concept i with binary variable c i, yielding the following coverage maximization objective, subject to the appropriate constraints :', 'in proposing bigrams as concepts for their system,  #TAUTHOR_TAG explain that :', '[ c ] oncepts could be words, named entities, syntactic subtrees or semantic relations, for example.', 'while deeper semantics make more appealing concepts, their extraction and weighting are much more error - prone.', 'any error in concept extraction can result in a biased objective function, leading to poor sentence selection.', ' #AUTHOR_TAG several authors, e. g.,  #AUTHOR_TAG, and  #AUTHOR_TAG, have followed  #TAUTHOR_TAG in assuming that bigrams would lead to better practical performance than more syntactic or semantic concepts, even though bigrams serve as only an approximation of these.', 'in this paper, we revisit this assumption and evaluate the maximum coverage objective for extractive text summarization with syntactic and semantic concepts.', 'specifically, we replace bigram concepts with new ones based on syntactic dependencies, semantic frames, as well as named entities.', 'we show that using such concepts can lead to significant improvements in text summarization performance outside of the newswire domain.', 'we evaluate coverage maximization incorporating syntactic and semantic concepts across three different domains : newswire, legal judgments, and wikipedia articles']",0
"['system,  #TAUTHOR_TAG explain that :', '']","['system,  #TAUTHOR_TAG explain that :', '[ c ] oncepts could be words, named entities, syntactic subtrees']","['system,  #TAUTHOR_TAG explain that :', '']","['- of - the - art approaches to extractive summarization are based on the notion of coverage maximization ( berg -  #AUTHOR_TAG.', 'the assumption is that a good summary is a selection of sentences from the document that contains as many of the important concepts as possible.', 'the importance of concepts is implemented by assigning weights w i to each concept i with binary variable c i, yielding the following coverage maximization objective, subject to the appropriate constraints :', 'in proposing bigrams as concepts for their system,  #TAUTHOR_TAG explain that :', '[ c ] oncepts could be words, named entities, syntactic subtrees or semantic relations, for example.', 'while deeper semantics make more appealing concepts, their extraction and weighting are much more error - prone.', 'any error in concept extraction can result in a biased objective function, leading to poor sentence selection.', ' #AUTHOR_TAG several authors, e. g.,  #AUTHOR_TAG, and  #AUTHOR_TAG, have followed  #TAUTHOR_TAG in assuming that bigrams would lead to better practical performance than more syntactic or semantic concepts, even though bigrams serve as only an approximation of these.', 'in this paper, we revisit this assumption and evaluate the maximum coverage objective for extractive text summarization with syntactic and semantic concepts.', 'specifically, we replace bigram concepts with new ones based on syntactic dependencies, semantic frames, as well as named entities.', 'we show that using such concepts can lead to significant improvements in text summarization performance outside of the newswire domain.', 'we evaluate coverage maximization incorporating syntactic and semantic concepts across three different domains : newswire, legal judgments, and wikipedia articles']",0
"['to the concept coverage optimization problem, we resort to fast solvers for integer linear programming, under some appropriate constraints.', 'bigrams.  #TAUTHOR_TAG proposed to use bigrams as concepts, and']","['to the concept coverage optimization problem, we resort to fast solvers for integer linear programming, under some appropriate constraints.', 'bigrams.  #TAUTHOR_TAG proposed to use bigrams as concepts, and']","['', 'moreover, due the np - hardness of coverage maximization, for an exact solution to the concept coverage optimization problem, we resort to fast solvers for integer linear programming, under some appropriate constraints.', 'bigrams.  #TAUTHOR_TAG proposed to use bigrams as concepts, and']","['extractive summarization, the unsupervised version of the task is sometimes set up as that of finding a subset of sentences in a document, within some relatively small budget, that covers as many of the important concepts in the document as possible.', 'in the maximum coverage objective, concepts are considered as independent of each other.', 'concepts are weighted by the number of times they appear in a document.', 'moreover, due the np - hardness of coverage maximization, for an exact solution to the concept coverage optimization problem, we resort to fast solvers for integer linear programming, under some appropriate constraints.', 'bigrams.  #TAUTHOR_TAG proposed to use bigrams as concepts, and to weight their contribution to the objective function in equation ( 1 ) by the frequency with which they occur in the document.', 'some pre - processing is first carried out to these bigrams : all bigrams consisting uniquely of stop - words are removed from consideration, and each word is stemmed.', 'they also require bigrams to occur with a minimal frequency ( cf. section 3. 2 ).', 'named entities.', 'we consider three new types of concepts, all suggested, but subsequently rejected by  #TAUTHOR_TAG semantic frames.', 'the intuition behind our use of frame semantics is that a summary should represent the most central semantic frames  #AUTHOR_TAG present in the corresponding document - indeed, we consider these frames to be actual types of concepts.', 'we extract frame names from sentences for a further type of concepts under consideration.', 'we use se - mafor 3 to augment documents with semantic frames']",0
"[' #TAUTHOR_TAG, icsis']","[' #TAUTHOR_TAG, icsisumm 7.', 'their system was originally intended for multi - document update']","[' #TAUTHOR_TAG, icsis']","['baseline is the bigram - based extraction summarization system of  #TAUTHOR_TAG, icsisumm 7.', 'their system was originally intended for multi - document update summarization, and summaries are extracted from document sentences that share more than k content words with some query.', 'we follow this approach for the tac08 data.', 'for echr and wikipedia, the task is single document summarization, and the now irrelevant topic - document intersection preprocessing step is eliminated.', 'the original system uses the gnu linear programming kit 8 with a time limit of 100 seconds.', 'for all experiments presented in this paper, we double this time limit ; we experimented with longer time limits on the development set for the echr data, without any performance improvements.', 'once the summarizer reaches the time limit, a summary is output based on the current feasible solution, whether the solution is optimal or not.', 'moreover, the current icsisumm ( v1 ) distribution prunes sentences shorter than 10 words.', 'we note that we also tried replacing glpk by gurobi 9, for which no time limit was necessary, but found poorer results on the development set of the echr data.', 'the original system takes several important input parameters.', '1. summary length, for tac08, is specified by the tac 2008 conference guidelines as 100 words.', '']",0
"['to the concept coverage optimization problem, we resort to fast solvers for integer linear programming, under some appropriate constraints.', 'bigrams.  #TAUTHOR_TAG proposed to use bigrams as concepts, and']","['to the concept coverage optimization problem, we resort to fast solvers for integer linear programming, under some appropriate constraints.', 'bigrams.  #TAUTHOR_TAG proposed to use bigrams as concepts, and']","['', 'moreover, due the np - hardness of coverage maximization, for an exact solution to the concept coverage optimization problem, we resort to fast solvers for integer linear programming, under some appropriate constraints.', 'bigrams.  #TAUTHOR_TAG proposed to use bigrams as concepts, and']","['extractive summarization, the unsupervised version of the task is sometimes set up as that of finding a subset of sentences in a document, within some relatively small budget, that covers as many of the important concepts in the document as possible.', 'in the maximum coverage objective, concepts are considered as independent of each other.', 'concepts are weighted by the number of times they appear in a document.', 'moreover, due the np - hardness of coverage maximization, for an exact solution to the concept coverage optimization problem, we resort to fast solvers for integer linear programming, under some appropriate constraints.', 'bigrams.  #TAUTHOR_TAG proposed to use bigrams as concepts, and to weight their contribution to the objective function in equation ( 1 ) by the frequency with which they occur in the document.', 'some pre - processing is first carried out to these bigrams : all bigrams consisting uniquely of stop - words are removed from consideration, and each word is stemmed.', 'they also require bigrams to occur with a minimal frequency ( cf. section 3. 2 ).', 'named entities.', 'we consider three new types of concepts, all suggested, but subsequently rejected by  #TAUTHOR_TAG semantic frames.', 'the intuition behind our use of frame semantics is that a summary should represent the most central semantic frames  #AUTHOR_TAG present in the corresponding document - indeed, we consider these frames to be actual types of concepts.', 'we extract frame names from sentences for a further type of concepts under consideration.', 'we use se - mafor 3 to augment documents with semantic frames']",7
['in  #TAUTHOR_TAG ( see'],['in  #TAUTHOR_TAG ( see'],"['in  #TAUTHOR_TAG ( see table 1, bigrams )']","['evaluate output summaries using rouge - 1, rouge - 2, and rouge - su4  #AUTHOR_TAG, with no stemming and retaining all stopwords.', 'these measures have been shown to correlate best with human judgments in general, but among the automatic measures, rouge - 1 and rouge - 2 also correlate best with the pyramid  #AUTHOR_TAG and responsiveness manual metrics  #AUTHOR_TAG.', 'moreover, rouge - 1 has been shown to best reflect human - automatic summary comparisons  #AUTHOR_TAG.', 'for single concept systems, the results are shown in table 1, and concept combination system results are given in table 2.', 'we first note that our runs of the current distribution of icsisumm yield significantly worse rouge - 2 results than reported in  #TAUTHOR_TAG ( see table 1, bigrams ) : 0. 081 compared to 0. 110 respectively.', 'on the tac08 data, we observe no improvements over the baseline bigram system for any rouge metric here.', 'hence,  #TAUTHOR_TAG were right in their assumption that syntactic and semantic concepts would not lead to performance improvements, when restricting ourselves to this dataset.', 'however, when we change domain to the legal judgments or wikipedia articles, using syntactic and semantic concepts leads to significant gains across all the rouge metrics.', '']",7
"[' #TAUTHOR_TAG, icsis']","[' #TAUTHOR_TAG, icsisumm 7.', 'their system was originally intended for multi - document update']","[' #TAUTHOR_TAG, icsis']","['baseline is the bigram - based extraction summarization system of  #TAUTHOR_TAG, icsisumm 7.', 'their system was originally intended for multi - document update summarization, and summaries are extracted from document sentences that share more than k content words with some query.', 'we follow this approach for the tac08 data.', 'for echr and wikipedia, the task is single document summarization, and the now irrelevant topic - document intersection preprocessing step is eliminated.', 'the original system uses the gnu linear programming kit 8 with a time limit of 100 seconds.', 'for all experiments presented in this paper, we double this time limit ; we experimented with longer time limits on the development set for the echr data, without any performance improvements.', 'once the summarizer reaches the time limit, a summary is output based on the current feasible solution, whether the solution is optimal or not.', 'moreover, the current icsisumm ( v1 ) distribution prunes sentences shorter than 10 words.', 'we note that we also tried replacing glpk by gurobi 9, for which no time limit was necessary, but found poorer results on the development set of the echr data.', 'the original system takes several important input parameters.', '1. summary length, for tac08, is specified by the tac 2008 conference guidelines as 100 words.', '']",5
['in  #TAUTHOR_TAG ( see'],['in  #TAUTHOR_TAG ( see'],"['in  #TAUTHOR_TAG ( see table 1, bigrams )']","['evaluate output summaries using rouge - 1, rouge - 2, and rouge - su4  #AUTHOR_TAG, with no stemming and retaining all stopwords.', 'these measures have been shown to correlate best with human judgments in general, but among the automatic measures, rouge - 1 and rouge - 2 also correlate best with the pyramid  #AUTHOR_TAG and responsiveness manual metrics  #AUTHOR_TAG.', 'moreover, rouge - 1 has been shown to best reflect human - automatic summary comparisons  #AUTHOR_TAG.', 'for single concept systems, the results are shown in table 1, and concept combination system results are given in table 2.', 'we first note that our runs of the current distribution of icsisumm yield significantly worse rouge - 2 results than reported in  #TAUTHOR_TAG ( see table 1, bigrams ) : 0. 081 compared to 0. 110 respectively.', 'on the tac08 data, we observe no improvements over the baseline bigram system for any rouge metric here.', 'hence,  #TAUTHOR_TAG were right in their assumption that syntactic and semantic concepts would not lead to performance improvements, when restricting ourselves to this dataset.', 'however, when we change domain to the legal judgments or wikipedia articles, using syntactic and semantic concepts leads to significant gains across all the rouge metrics.', '']",4
"['from syntax trees  #TAUTHOR_TAG,']","['from syntax trees  #TAUTHOR_TAG,']","['from syntax trees  #TAUTHOR_TAG,']","['- parametric bayesian methods have been successfully applied to directly learn phrase pairs from a bilingual corpus with little or no dependence on word alignments  #AUTHOR_TAG de  #AUTHOR_TAG.', 'because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many - to - one word - level alignments produced by the ibm series models  #AUTHOR_TAG or the hidden markov model ( hmm )  #AUTHOR_TAG.', 'we wish to apply this direct, bayesian approach to learn better translation rules for syntaxbased statistical mt ( ssmt ), by which we specifically refer to mt systems using tree - to - string ( tts ) translation templates derived from syntax trees  #TAUTHOR_TAG, as opposed to formally syntactic systems such as hiero  #AUTHOR_TAG.', 'the stumbling block preventing us from taking this approach is the extremely large space of possible tts templates when no word alignments are given.', 'given a sentence pair and syntax tree over one side, there are an exponential number of potential tts templates and a polynomial number of phrase pairs.', 'in this paper, we explore methods for restricting the space of possible tts templates under consideration, while still allowing good templates to emerge directly from the data as much as possible.', 'we find an improvement in translation accuracy through, first, using constraints to limit the number of new templates, second, using bayesian methods to limit which of these new templates are favored when re - analyzing the training data with em, and, third, experimenting with different renormalization techniques for the em re - analysis.', 'we introduce two constraints to limit the number of tts templates that we extract directly from tree / string pairs without using word alignments.', 'the first constraint is to limit direct tts template extraction to the part of the corpus where word alignment tools such as giza + + do poorly.', 'there is no reason not to re - use the good alignments from giza + +, which holds a very competitive baseline performance.', 'as already mentioned, the noisy alignments from giza + + are likely to cross the boundaries of the tree constituents, which leads to comparatively big tts templates.', 'we use this fact as a heuristic to roughly distinguish noisy from good word alignments.', '1 here we define big templates as those with more than 8 symbols in their right hand sides ( rhss ).', 'the word alignments in big templates are considered to be noisy and will be recomposed by extracting smaller tts templates.', 'another reason to do extraction on big templates is that the applicability of big templates to new sentences is very limited due to their size, and the portion of the training data from']",5
"['##n )  #TAUTHOR_TAG, corresponding to']","['lhs ( rootn )  #TAUTHOR_TAG, corresponding to']","['##n )  #TAUTHOR_TAG, corresponding to']",[' #TAUTHOR_TAG'],5
"[' #TAUTHOR_TAG, where']","[' #TAUTHOR_TAG, where tts templates']","['', 'the first is synchronous parsing  #TAUTHOR_TAG, where']","['', 'the variables in a tts template are further transformed using other tts templates, and the recursive process continues until there are no variables left.', 'there are two ways that tts templates are commonly used in machine translation.', 'the first is synchronous parsing  #TAUTHOR_TAG, where tts templates are used to construct synchronous parse trees for an input sentence, and the translations will be generated once the synchronous trees are built up.', 'the other way is the tts transducer  #AUTHOR_TAG, where tts templates are used just as their name indicates : to transform a source parse tree ( or forest ) into the proper target string.', 'since synchronous parsing considers all possible synchronous parse trees of the source sentence, it is less constrained than tts transducers and hence requires more computational power.', 'in this paper, we use a tts transducer to test the performance of different tts templates, but our techniques could also be applied to ssmt systems based on synchronous parsing']",0
"['##n )  #TAUTHOR_TAG, corresponding to']","['lhs ( rootn )  #TAUTHOR_TAG, corresponding to']","['##n )  #TAUTHOR_TAG, corresponding to']",[' #TAUTHOR_TAG'],0
"['##n )  #TAUTHOR_TAG, corresponding to']","['lhs ( rootn )  #TAUTHOR_TAG, corresponding to']","['##n )  #TAUTHOR_TAG, corresponding to']",[' #TAUTHOR_TAG'],0
"[' #TAUTHOR_TAG, where']","[' #TAUTHOR_TAG, where tts templates']","['', 'the first is synchronous parsing  #TAUTHOR_TAG, where']","['', 'the variables in a tts template are further transformed using other tts templates, and the recursive process continues until there are no variables left.', 'there are two ways that tts templates are commonly used in machine translation.', 'the first is synchronous parsing  #TAUTHOR_TAG, where tts templates are used to construct synchronous parse trees for an input sentence, and the translations will be generated once the synchronous trees are built up.', 'the other way is the tts transducer  #AUTHOR_TAG, where tts templates are used just as their name indicates : to transform a source parse tree ( or forest ) into the proper target string.', 'since synchronous parsing considers all possible synchronous parse trees of the source sentence, it is less constrained than tts transducers and hence requires more computational power.', 'in this paper, we use a tts transducer to test the performance of different tts templates, but our techniques could also be applied to ssmt systems based on synchronous parsing']",3
"[' #AUTHOR_TAG and is widely used in ssmt systems  #TAUTHOR_TAG.', 'the word alignment used in gh']","['ghkm  #AUTHOR_TAG and is widely used in ssmt systems  #TAUTHOR_TAG.', 'the word alignment used in ghkm is usually computed independent of the syntactic structure, and as de  #AUTHOR_TAG and  #AUTHOR_TAG have noted,', 'ch - en en - ch union heuristic 28. 6 % 33. 0 %']","[' #AUTHOR_TAG and is widely used in ssmt systems  #TAUTHOR_TAG.', 'the word alignment used in gh']","['##s templates are commonly generated by decomposing a pair of aligned source syntax tree and target string into smaller pairs of tree fragments and target string ( i. e., the tts templates ).', 'to keep the number of tts templates to a manageable scale, only the non - decomposable tts templates are generated.', 'this algorithm is referred to as ghkm  #AUTHOR_TAG and is widely used in ssmt systems  #TAUTHOR_TAG.', 'the word alignment used in ghkm is usually computed independent of the syntactic structure, and as de  #AUTHOR_TAG and  #AUTHOR_TAG have noted,', 'ch - en en - ch union heuristic 28. 6 % 33. 0 % 45. 9 % 20. 1 % table 2 : in the selected big templates, the distribution of words in the templates of different sizes, which are measured based on the number of symbols in their rhss is not the best for ssmt systems.', 'in fact, noisy word alignments cause more damage to a ssmt system than to a phrase based smt system, because the tts templates can only be derived from tree constituents.', '']",3
"['. g.,  #TAUTHOR_TAG, we']","['( e. g.,  #TAUTHOR_TAG, we']","['. g.,  #TAUTHOR_TAG, we frame token']","['##word expressions ( mwes ) are combinations of multiple words that exhibit some degree of idiomaticity  #AUTHOR_TAG.', 'verb - noun combinations ( vncs ), consisting of a verb with a noun in its direct object position, are a common type of semantically - idiomatic mwe in english and cross - lingually  #AUTHOR_TAG.', 'many vncs are ambiguous between mwes and literal combinations, as in the following examples of see stars, in which 1 is an idiomatic usage ( i. e., an mwe ), while 2 is a literal combination.', '1 1. hereford united were seeing stars at gillingham after letting in 2 early goals 2. look into the night sky to see the stars mwe identification is the task of automatically determining which word combinations at the token - level form mwes  #AUTHOR_TAG, and must be able to make such distinctions.', 'this is particularly important for applications such as machine translation  #AUTHOR_TAG, where the appropriate meaning of word combinations in context must be preserved for accurate translation.', 'in this paper, following prior work ( e. g.,  #TAUTHOR_TAG, we frame token - level identification of vncs as a supervised binary classification problem, i. e., idiomatic vs. literal.', 'we consider a range of approaches to forming distributed representations of the context in which a vnc occurs, including word embeddings  #AUTHOR_TAG, word embeddings tailored to representing sentences  #AUTHOR_TAG, and skip - thoughts sentence embeddings  #AUTHOR_TAG.', '']",4
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']",[' #TAUTHOR_TAG'],4
"['( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent']","['on mwe identification has focused on specific kinds of mwes ( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent']","['( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent work has considered the identification of a broad range of kinds of mwes ( e. g.,  #AUTHOR_TAG.', '']","['research on mwe identification has focused on specific kinds of mwes ( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent work has considered the identification of a broad range of kinds of mwes ( e. g.,  #AUTHOR_TAG.', 'work on mwe identification has leveraged rich linguistic knowledge of the constructions under consideration ( e. g.,  #AUTHOR_TAG, treated literal and idiomatic as two senses of an expression and applied approaches similar to word - sense disambiguation ( e. g.,  #AUTHOR_TAG, incorporated topic models ( e. g.,  #AUTHOR_TAG, and made use of distributed representations of words  #AUTHOR_TAG.', 'in the most closely related work to ours,  #TAUTHOR_TAG represent token instances of vncs by embedding the sentence that they occur in using skip - thoughts  #AUTHOR_TAG - an encoderdecoder model that can be viewed as a sentencelevel counterpart to the word2vec  #AUTHOR_TAG skip - gram model.', '']",0
"['( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent']","['on mwe identification has focused on specific kinds of mwes ( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent']","['( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent work has considered the identification of a broad range of kinds of mwes ( e. g.,  #AUTHOR_TAG.', '']","['research on mwe identification has focused on specific kinds of mwes ( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent work has considered the identification of a broad range of kinds of mwes ( e. g.,  #AUTHOR_TAG.', 'work on mwe identification has leveraged rich linguistic knowledge of the constructions under consideration ( e. g.,  #AUTHOR_TAG, treated literal and idiomatic as two senses of an expression and applied approaches similar to word - sense disambiguation ( e. g.,  #AUTHOR_TAG, incorporated topic models ( e. g.,  #AUTHOR_TAG, and made use of distributed representations of words  #AUTHOR_TAG.', 'in the most closely related work to ours,  #TAUTHOR_TAG represent token instances of vncs by embedding the sentence that they occur in using skip - thoughts  #AUTHOR_TAG - an encoderdecoder model that can be viewed as a sentencelevel counterpart to the word2vec  #AUTHOR_TAG skip - gram model.', '']",0
"['( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent']","['on mwe identification has focused on specific kinds of mwes ( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent']","['( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent work has considered the identification of a broad range of kinds of mwes ( e. g.,  #AUTHOR_TAG.', '']","['research on mwe identification has focused on specific kinds of mwes ( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent work has considered the identification of a broad range of kinds of mwes ( e. g.,  #AUTHOR_TAG.', 'work on mwe identification has leveraged rich linguistic knowledge of the constructions under consideration ( e. g.,  #AUTHOR_TAG, treated literal and idiomatic as two senses of an expression and applied approaches similar to word - sense disambiguation ( e. g.,  #AUTHOR_TAG, incorporated topic models ( e. g.,  #AUTHOR_TAG, and made use of distributed representations of words  #AUTHOR_TAG.', 'in the most closely related work to ours,  #TAUTHOR_TAG represent token instances of vncs by embedding the sentence that they occur in using skip - thoughts  #AUTHOR_TAG - an encoderdecoder model that can be viewed as a sentencelevel counterpart to the word2vec  #AUTHOR_TAG skip - gram model.', '']",0
"['( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent']","['on mwe identification has focused on specific kinds of mwes ( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent']","['( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent work has considered the identification of a broad range of kinds of mwes ( e. g.,  #AUTHOR_TAG.', '']","['research on mwe identification has focused on specific kinds of mwes ( e. g.,  #AUTHOR_TAG, including english vncs ( e. g.,  #TAUTHOR_TAG, although some recent work has considered the identification of a broad range of kinds of mwes ( e. g.,  #AUTHOR_TAG.', 'work on mwe identification has leveraged rich linguistic knowledge of the constructions under consideration ( e. g.,  #AUTHOR_TAG, treated literal and idiomatic as two senses of an expression and applied approaches similar to word - sense disambiguation ( e. g.,  #AUTHOR_TAG, incorporated topic models ( e. g.,  #AUTHOR_TAG, and made use of distributed representations of words  #AUTHOR_TAG.', 'in the most closely related work to ours,  #TAUTHOR_TAG represent token instances of vncs by embedding the sentence that they occur in using skip - thoughts  #AUTHOR_TAG - an encoderdecoder model that can be viewed as a sentencelevel counterpart to the word2vec  #AUTHOR_TAG skip - gram model.', '']",1
[' #AUTHOR_TAG and  #TAUTHOR_TAG - to train'],[' #AUTHOR_TAG and  #TAUTHOR_TAG - to train'],[' #AUTHOR_TAG and  #TAUTHOR_TAG - to train'],"['use the vnc - tokens dataset  #AUTHOR_TAG - the same dataset used by  #AUTHOR_TAG and  #TAUTHOR_TAG - to train and evaluate our models.', 'this dataset consists of sentences containing vnc usages drawn from the british national corpus  #AUTHOR_TAG, 7 along with a label indicating whether the vnc is an idiomatic or literal usage ( or whether this cannot be determined, in which case it is labelled "" unknown "" ).', 'vnc - tokens is divided into dev and test sets that each include fourteen vnc types and a total of roughly six hundred instances of these types annotated as literal or idiomatic.', 'following  #TAUTHOR_TAG, we use dev and test, and ignore all token instances annotated as "" unknown "".', ' #AUTHOR_TAG and  #TAUTHOR_TAG structured their experiments differently.', 'fazly et al. report results over dev and test separately.', 'in this setup test consists of expressions that were not seen during model development ( done on dev ).', ' #TAUTHOR_TAG, on the other hand, merge dev and test, and create new training and testing sets, such that each expression is present in the training and testing data, and the ratio of idiomatic to literal usages of each expression in the training data is roughly equal to that in the testing data.', 'we borrowed ideas from both of these approaches in structuring our experiments.', 'we retain we then divide each of these into training and testing sets, using the same ratios of idiomatic to literal usages for each expression as  #TAUTHOR_TAG.', 'this allows us to develop and tune a model on dev, and then determine whether, when retrained on instances of unseen vncs in ( the training portion of ) test, that model is able to generalize to new vncs without further tuning to the specific expressions in test']",3
[' #AUTHOR_TAG and  #TAUTHOR_TAG - to train'],[' #AUTHOR_TAG and  #TAUTHOR_TAG - to train'],[' #AUTHOR_TAG and  #TAUTHOR_TAG - to train'],"['use the vnc - tokens dataset  #AUTHOR_TAG - the same dataset used by  #AUTHOR_TAG and  #TAUTHOR_TAG - to train and evaluate our models.', 'this dataset consists of sentences containing vnc usages drawn from the british national corpus  #AUTHOR_TAG, 7 along with a label indicating whether the vnc is an idiomatic or literal usage ( or whether this cannot be determined, in which case it is labelled "" unknown "" ).', 'vnc - tokens is divided into dev and test sets that each include fourteen vnc types and a total of roughly six hundred instances of these types annotated as literal or idiomatic.', 'following  #TAUTHOR_TAG, we use dev and test, and ignore all token instances annotated as "" unknown "".', ' #AUTHOR_TAG and  #TAUTHOR_TAG structured their experiments differently.', 'fazly et al. report results over dev and test separately.', 'in this setup test consists of expressions that were not seen during model development ( done on dev ).', ' #TAUTHOR_TAG, on the other hand, merge dev and test, and create new training and testing sets, such that each expression is present in the training and testing data, and the ratio of idiomatic to literal usages of each expression in the training data is roughly equal to that in the testing data.', 'we borrowed ideas from both of these approaches in structuring our experiments.', 'we retain we then divide each of these into training and testing sets, using the same ratios of idiomatic to literal usages for each expression as  #TAUTHOR_TAG.', 'this allows us to develop and tune a model on dev, and then determine whether, when retrained on instances of unseen vncs in ( the training portion of ) test, that model is able to generalize to new vncs without further tuning to the specific expressions in test']",3
"[' #TAUTHOR_TAG, 2, 3, 4,']","[' #TAUTHOR_TAG, 2, 3, 4, 5, 6 ], little']","['merging both spaces have been proposed [  #TAUTHOR_TAG, 2, 3, 4,']","['', 'currently based on deep neural net representations, where an image embedding ( e. g., obtained from a convolutional neural network or cnn ) and a text embedding ( e', '. g., obtained from a recurrent neural network or rnn ) are combined into a unique multimodal embedding space. while several techniques for merging both spaces have been proposed [  #TAUTHOR_TAG, 2, 3, 4, 5, 6 ], little effort has been made in finding the most appropriate image embeddings to be used in that process. in', 'fact,  #TAUTHOR_TAG simply use a one - layer cnn embedding [ 7, 8 ]. in this paper we explore the impact of using a full - network embedding ( fne ) [ 9 ] to generate the required image', 'embedding, replacing the one - layer embedding. we do so by integrating the fne into', '']",0
"[' #TAUTHOR_TAG, 2, 3, 4,']","[' #TAUTHOR_TAG, 2, 3, 4, 5, 6 ], little']","['merging both spaces have been proposed [  #TAUTHOR_TAG, 2, 3, 4,']","['', 'currently based on deep neural net representations, where an image embedding ( e. g., obtained from a convolutional neural network or cnn ) and a text embedding ( e', '. g., obtained from a recurrent neural network or rnn ) are combined into a unique multimodal embedding space. while several techniques for merging both spaces have been proposed [  #TAUTHOR_TAG, 2, 3, 4, 5, 6 ], little effort has been made in finding the most appropriate image embeddings to be used in that process. in', 'fact,  #TAUTHOR_TAG simply use a one - layer cnn embedding [ 7, 8 ]. in this paper we explore the impact of using a full - network embedding ( fne ) [ 9 ] to generate the required image', 'embedding, replacing the one - layer embedding. we do so by integrating the fne into', '']",0
"[' #TAUTHOR_TAG, 2, 3, 4,']","[' #TAUTHOR_TAG, 2, 3, 4, 5, 6 ], little']","['merging both spaces have been proposed [  #TAUTHOR_TAG, 2, 3, 4,']","['', 'currently based on deep neural net representations, where an image embedding ( e. g., obtained from a convolutional neural network or cnn ) and a text embedding ( e', '. g., obtained from a recurrent neural network or rnn ) are combined into a unique multimodal embedding space. while several techniques for merging both spaces have been proposed [  #TAUTHOR_TAG, 2, 3, 4, 5, 6 ], little effort has been made in finding the most appropriate image embeddings to be used in that process. in', 'fact,  #TAUTHOR_TAG simply use a one - layer cnn embedding [ 7, 8 ]. in this paper we explore the impact of using a full - network embedding ( fne ) [ 9 ] to generate the required image', 'embedding, replacing the one - layer embedding. we do so by integrating the fne into', '']",0
"['the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross - domain search [  #TAUTHOR_TAG, 2, 3, 4, 5 ].', 'this paper builds upon']","['the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross - domain search [  #TAUTHOR_TAG, 2, 3, 4, 5 ].', 'this paper builds upon']","['the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross - domain search [  #TAUTHOR_TAG, 2, 3, 4, 5 ].', 'this paper builds upon']","['the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross - domain search [  #TAUTHOR_TAG, 2, 3, 4, 5 ].', 'this paper builds upon the methodology described by  #TAUTHOR_TAG is in turn based on previous works in the area of neural machine translation [ 14 ].', 'in their work,  #TAUTHOR_TAG define a vectorized representation of an input text by using gru rnns.', 'in this setting, each word in the text is codified into a vector using a word dictionary, vectors which are then fed one by one into the grus.', '']",0
"['the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross - domain search [  #TAUTHOR_TAG, 2, 3, 4, 5 ].', 'this paper builds upon']","['the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross - domain search [  #TAUTHOR_TAG, 2, 3, 4, 5 ].', 'this paper builds upon']","['the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross - domain search [  #TAUTHOR_TAG, 2, 3, 4, 5 ].', 'this paper builds upon']","['the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross - domain search [  #TAUTHOR_TAG, 2, 3, 4, 5 ].', 'this paper builds upon the methodology described by  #TAUTHOR_TAG is in turn based on previous works in the area of neural machine translation [ 14 ].', 'in their work,  #TAUTHOR_TAG define a vectorized representation of an input text by using gru rnns.', 'in this setting, each word in the text is codified into a vector using a word dictionary, vectors which are then fed one by one into the grus.', '']",0
"['the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross - domain search [  #TAUTHOR_TAG, 2, 3, 4, 5 ].', 'this paper builds upon']","['the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross - domain search [  #TAUTHOR_TAG, 2, 3, 4, 5 ].', 'this paper builds upon']","['the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross - domain search [  #TAUTHOR_TAG, 2, 3, 4, 5 ].', 'this paper builds upon']","['the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross - domain search [  #TAUTHOR_TAG, 2, 3, 4, 5 ].', 'this paper builds upon the methodology described by  #TAUTHOR_TAG is in turn based on previous works in the area of neural machine translation [ 14 ].', 'in their work,  #TAUTHOR_TAG define a vectorized representation of an input text by using gru rnns.', 'in this setting, each word in the text is codified into a vector using a word dictionary, vectors which are then fed one by one into the grus.', '']",0
"['multimodal embedding pipeline of  #TAUTHOR_TAG.', 'to do so we use']","['multimodal embedding pipeline of  #TAUTHOR_TAG.', 'to do so we use']","['our approach, we integrate the fne with the multimodal embedding pipeline of  #TAUTHOR_TAG.', 'to do so we use']","['our approach, we integrate the fne with the multimodal embedding pipeline of  #TAUTHOR_TAG.', '']",0
"['multimodal embedding pipeline of  #TAUTHOR_TAG.', 'to do so we use']","['multimodal embedding pipeline of  #TAUTHOR_TAG.', 'to do so we use']","['our approach, we integrate the fne with the multimodal embedding pipeline of  #TAUTHOR_TAG.', 'to do so we use']","['our approach, we integrate the fne with the multimodal embedding pipeline of  #TAUTHOR_TAG.', '']",0
"[' #TAUTHOR_TAG, 2, 3, 4,']","[' #TAUTHOR_TAG, 2, 3, 4, 5, 6 ], little']","['merging both spaces have been proposed [  #TAUTHOR_TAG, 2, 3, 4,']","['', 'currently based on deep neural net representations, where an image embedding ( e. g., obtained from a convolutional neural network or cnn ) and a text embedding ( e', '. g., obtained from a recurrent neural network or rnn ) are combined into a unique multimodal embedding space. while several techniques for merging both spaces have been proposed [  #TAUTHOR_TAG, 2, 3, 4, 5, 6 ], little effort has been made in finding the most appropriate image embeddings to be used in that process. in', 'fact,  #TAUTHOR_TAG simply use a one - layer cnn embedding [ 7, 8 ]. in this paper we explore the impact of using a full - network embedding ( fne ) [ 9 ] to generate the required image', 'embedding, replacing the one - layer embedding. we do so by integrating the fne into', '']",6
"['the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross - domain search [  #TAUTHOR_TAG, 2, 3, 4, 5 ].', 'this paper builds upon']","['the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross - domain search [  #TAUTHOR_TAG, 2, 3, 4, 5 ].', 'this paper builds upon']","['the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross - domain search [  #TAUTHOR_TAG, 2, 3, 4, 5 ].', 'this paper builds upon']","['the last few years, several solutions have been proposed to the problem of building common representations for images and text with the goal of enabling cross - domain search [  #TAUTHOR_TAG, 2, 3, 4, 5 ].', 'this paper builds upon the methodology described by  #TAUTHOR_TAG is in turn based on previous works in the area of neural machine translation [ 14 ].', 'in their work,  #TAUTHOR_TAG define a vectorized representation of an input text by using gru rnns.', 'in this setting, each word in the text is codified into a vector using a word dictionary, vectors which are then fed one by one into the grus.', '']",6
"['multimodal embedding pipeline of  #TAUTHOR_TAG.', 'to do so we use']","['multimodal embedding pipeline of  #TAUTHOR_TAG.', 'to do so we use']","['our approach, we integrate the fne with the multimodal embedding pipeline of  #TAUTHOR_TAG.', 'to do so we use']","['our approach, we integrate the fne with the multimodal embedding pipeline of  #TAUTHOR_TAG.', '']",6
"[' #TAUTHOR_TAG ( cnn - mme ).', 'additionally, we define a second baseline by using the  #TAUTHOR_TAG with a training configuration closer to']","[' #TAUTHOR_TAG ( cnn - mme ).', 'additionally, we define a second baseline by using the  #TAUTHOR_TAG with a training configuration closer to']","['of the  #TAUTHOR_TAG ( cnn - mme ).', 'additionally, we define a second baseline by using the  #TAUTHOR_TAG with a training configuration closer to the one used for the fne experiments (']","['this section we evaluate the impact of using the fne in a multimodal pipeline ( fn - mme ) for both image annotation and image retrieval tasks.', 'to properly measure the relevance of the fne, we compare the results of the fn - mme with those of the  #TAUTHOR_TAG ( cnn - mme ).', 'additionally, we define a second baseline by using the  #TAUTHOR_TAG with a training configuration closer to the one used for the fne experiments ( i. e., same source cnn, same mme dimensionality, etc. ).', 'we refer to this second baseline as cnn - mme *']",6
"[' #TAUTHOR_TAG, 2, 3, 4,']","[' #TAUTHOR_TAG, 2, 3, 4, 5, 6 ], little']","['merging both spaces have been proposed [  #TAUTHOR_TAG, 2, 3, 4,']","['', 'currently based on deep neural net representations, where an image embedding ( e. g., obtained from a convolutional neural network or cnn ) and a text embedding ( e', '. g., obtained from a recurrent neural network or rnn ) are combined into a unique multimodal embedding space. while several techniques for merging both spaces have been proposed [  #TAUTHOR_TAG, 2, 3, 4, 5, 6 ], little effort has been made in finding the most appropriate image embeddings to be used in that process. in', 'fact,  #TAUTHOR_TAG simply use a one - layer cnn embedding [ 7, 8 ]. in this paper we explore the impact of using a full - network embedding ( fne ) [ 9 ] to generate the required image', 'embedding, replacing the one - layer embedding. we do so by integrating the fne into', '']",5
"['test.', 'these splits are the same ones used by  #TAUTHOR_TAG and by karpathy and fei - fei [ 22 ].', 'the mscoco dataset']","['test.', 'these splits are the same ones used by  #TAUTHOR_TAG and by karpathy and fei - fei [ 22 ].', 'the mscoco dataset [ 13 ] includes images of everyday scenes containing common objects in their natural context.', 'for captioning, 82, 783 images and']","['test.', 'these splits are the same ones used by  #TAUTHOR_TAG and by karpathy and fei - fei [ 22 ].', 'the mscoco dataset [ 13 ] includes images of everyday scenes containing common objects in their natural context.', 'for cap']","['', 'these splits are the same ones used by  #TAUTHOR_TAG and by karpathy and fei - fei [ 22 ].', 'the mscoco dataset [ 13 ] includes images of everyday scenes containing common objects in their natural context.', 'for captioning, 82, 783 images and 413, 915 captions are available for training, while 40, 504 images and 202, 520 captions are available for validation.', 'captions from the test set are not publicly available.', 'previous contributions consider using a subset of the validation set for validation and a different subset for test.', 'in most cases, such subsets are composed by either 1, 000 or 5, 000 images for each set, with their corresponding 5 captions per image.', 'in our experiments we consider both settings']",5
"[' #TAUTHOR_TAG, 2, 3, 4,']","[' #TAUTHOR_TAG, 2, 3, 4, 5, 6 ], little']","['merging both spaces have been proposed [  #TAUTHOR_TAG, 2, 3, 4,']","['', 'currently based on deep neural net representations, where an image embedding ( e. g., obtained from a convolutional neural network or cnn ) and a text embedding ( e', '. g., obtained from a recurrent neural network or rnn ) are combined into a unique multimodal embedding space. while several techniques for merging both spaces have been proposed [  #TAUTHOR_TAG, 2, 3, 4, 5, 6 ], little effort has been made in finding the most appropriate image embeddings to be used in that process. in', 'fact,  #TAUTHOR_TAG simply use a one - layer cnn embedding [ 7, 8 ]. in this paper we explore the impact of using a full - network embedding ( fne ) [ 9 ] to generate the required image', 'embedding, replacing the one - layer embedding. we do so by integrating the fne into', '']",1
"['of  #TAUTHOR_TAG, using the full -']","['of  #TAUTHOR_TAG, using the full - network image embedding results in consistently higher']","['the multimodal pipeline of  #TAUTHOR_TAG, using the full - network image embedding results in consistently higher performances than using a one - layer image embedding.', 'these results suggest that the visual representation']","['the multimodal pipeline of  #TAUTHOR_TAG, using the full - network image embedding results in consistently higher performances than using a one - layer image embedding.', 'these results suggest that the visual representation provided by the fne is superior to the current standard for the construction of most multimodal embeddings.', 'when compared to the current state - of - the - art, the results obtained by the fn - mme are significantly less competitive than problem - specific methods.', 'since this happens for all models using the same pipeline ( cnn - mme, cnn - mme †, cnn - mme * ), these results indicate that the original architecture of  #TAUTHOR_TAG is itself outperformed in general by more problem - specific techniques.', 'since the fne is compatible with most multimodal pipelines based on cnn embeddings, as future work of this paper we intend to evaluate the performance of the fne when integrated into the current state - of - the - art on image annotation ( w2vv [ 18 ] ) and image retrieval ( fv [ 4 ] ).', 'if the boost in performance obtained by the fne on the  #TAUTHOR_TAG pipeline translates to these other methods, such combination would be likely to define new state - of - the - art results on both tasks']",2
"[' #TAUTHOR_TAG, with the difference being that we focus on un - named']","[' #TAUTHOR_TAG, with the difference being that we focus on un - named entities.', 'contribution.', '']","['of entity recognition  #TAUTHOR_TAG, with the difference being that we focus on un - named entities.', 'contribution.', '']","['', 'however, progress on the common sense front, as opposed to named entities such as locations, and people, is still limited  #AUTHOR_TAG.', 'in this paper, we study entity recognition of common sense concepts.', 'our goal is to detect mentions of concepts that are discernible by sense.', 'for example, recognize that "" chirping birds "" is a mention of an audible concept ( sound ), and "" burning rubber "" is a mention of an olfactible concept ( smell ).', 'we aim to detect mentions of concepts without performing co - reference resolution or clustering mentions.', 'therefore, our setting resembles the established task of entity recognition  #TAUTHOR_TAG, with the difference being that we focus on un - named entities.', 'contribution.', 'one of the factors impeding progress in common sense information extraction is the lack of training data.', 'it is relatively easy to obtain labeled data for named entities such as companies and people.', 'examples of such named entities can be found in structured forms on the web, such as html lists and tables, and wikipedia infoboxes  #AUTHOR_TAG.', 'this is not the case for common sense concepts.', 'we therefore propose a data labeling method, that leverages crowdsourcing and large corpora.', 'this approach provides the flexibility to control the size and accuracy of the available labeled data for model training.', 'additionally, we propose and train several sequence models including variations of recurrent neural networks that learn to recognize mentions of sound and smell concepts in text.', 'in our experiments, we show that the combination of our mixture labeling approach, and a suitable learning model are an effective solution to sense recognition in text']",0
"[': words, prefix / suffices', ', and part - of - speech tag  #TAUTHOR_TAG. we can see that for']","[': words, prefix / suffices', ', and part - of - speech tag  #TAUTHOR_TAG. we can see that for']","['words, prefix / suffices', ', and part - of - speech tag  #TAUTHOR_TAG. we can see that for both senses, the model that uses both character embedding features', ', and']","['', 'at the previous time t - 1. formally, we have : we illustrate the model in figure 4. we found this model to consistently perform well on the senses', 'of sound and smell. model evaluation. to evaluate the models,', 'we set aside 200 of the 1000 crowd - annotated phrases', 'as test data, meaning we have 100 test instances for each sense type ( sound /', 'smell ). the rest of the data, 400 per sense type was used for generating training data using', 'the combined crowd and pattern approach described in section 3. 3. we set α = 0. 6 and', 'α = 0. 4, based on', 'figure 3, for audible', 'and olfactible concepts respectively. with these α values, the combination approach produced 1, 962 and 1, 702 training instances for audible and olfactible concepts respectively', 'performance of the various models is shown in table 4. the abbreviations denote the following : lstm refers to a vanilla lstm model, using only word embeddings as features, + or refers to the lstm', 'plus the output recurrence, + char refers to the lstm', 'plus the character embeddings as features. + or + char refers to the lstm plus the output recurrence and character embeddings as', 'features. for the crf, we use the commonly used features for named entity recognition : words, prefix / suffices', ', and part - of - speech tag  #TAUTHOR_TAG. we can see that for both senses, the model that uses both character embedding features', ', and an output recurrence', '']",0
"['a crf, and involve feature engineering  #TAUTHOR_TAG.', 'more recently, neural']","['a crf, and involve feature engineering  #TAUTHOR_TAG.', 'more recently, neural']","['a crf, and involve feature engineering  #TAUTHOR_TAG.', 'more recently, neural approaches have been used']","['task is related to entity recognition however in this paper we focused on novel types of entities, which can be used to improve extraction of common sense knowledge.', 'entity recognition systems are traditionally based on a sequential model, for example a crf, and involve feature engineering  #TAUTHOR_TAG.', 'more recently, neural approaches have been used for named entity recognition  #AUTHOR_TAG dos santos and guimaraes, 2015 ;  #AUTHOR_TAG.', 'like other neural approaches, our approach does not require feature engineering, the only features we use are word and character embeddings.', 'related to our proposed recurrence in the output layer is the work of  #AUTHOR_TAG which introduced a crf on top of lstm for the task of named entity recognition']",0
"[' #TAUTHOR_TAG, with the difference being that we focus on un - named']","[' #TAUTHOR_TAG, with the difference being that we focus on un - named entities.', 'contribution.', '']","['of entity recognition  #TAUTHOR_TAG, with the difference being that we focus on un - named entities.', 'contribution.', '']","['', 'however, progress on the common sense front, as opposed to named entities such as locations, and people, is still limited  #AUTHOR_TAG.', 'in this paper, we study entity recognition of common sense concepts.', 'our goal is to detect mentions of concepts that are discernible by sense.', 'for example, recognize that "" chirping birds "" is a mention of an audible concept ( sound ), and "" burning rubber "" is a mention of an olfactible concept ( smell ).', 'we aim to detect mentions of concepts without performing co - reference resolution or clustering mentions.', 'therefore, our setting resembles the established task of entity recognition  #TAUTHOR_TAG, with the difference being that we focus on un - named entities.', 'contribution.', 'one of the factors impeding progress in common sense information extraction is the lack of training data.', 'it is relatively easy to obtain labeled data for named entities such as companies and people.', 'examples of such named entities can be found in structured forms on the web, such as html lists and tables, and wikipedia infoboxes  #AUTHOR_TAG.', 'this is not the case for common sense concepts.', 'we therefore propose a data labeling method, that leverages crowdsourcing and large corpora.', 'this approach provides the flexibility to control the size and accuracy of the available labeled data for model training.', 'additionally, we propose and train several sequence models including variations of recurrent neural networks that learn to recognize mentions of sound and smell concepts in text.', 'in our experiments, we show that the combination of our mixture labeling approach, and a suitable learning model are an effective solution to sense recognition in text']",4
"['sentence is a sequence of tokens labeled using the bio tagging scheme  #TAUTHOR_TAG.', 'the bio labels denote tokens at']","['sentence is a sequence of tokens labeled using the bio tagging scheme  #TAUTHOR_TAG.', 'the bio labels denote tokens at']","['##s of audible ( sound ) and olfactible ( smell ) concepts.', 'we treat sense recognition in text as a sequence labeling task where each sentence is a sequence of tokens labeled using the bio tagging scheme  #TAUTHOR_TAG.', 'the bio labels denote tokens at the beginning, inside, and outside of a relevant mention, respectively.', 'example bio tagged sentences are shown in figure 1']","['would like to detect mentions of concepts discernible by sense.', 'in this paper, we focus on mentions of audible ( sound ) and olfactible ( smell ) concepts.', 'we treat sense recognition in text as a sequence labeling task where each sentence is a sequence of tokens labeled using the bio tagging scheme  #TAUTHOR_TAG.', 'the bio labels denote tokens at the beginning, inside, and outside of a relevant mention, respectively.', 'example bio tagged sentences are shown in figure 1']",5
"[': words, prefix / suffices', ', and part - of - speech tag  #TAUTHOR_TAG. we can see that for']","[': words, prefix / suffices', ', and part - of - speech tag  #TAUTHOR_TAG. we can see that for']","['words, prefix / suffices', ', and part - of - speech tag  #TAUTHOR_TAG. we can see that for both senses, the model that uses both character embedding features', ', and']","['', 'at the previous time t - 1. formally, we have : we illustrate the model in figure 4. we found this model to consistently perform well on the senses', 'of sound and smell. model evaluation. to evaluate the models,', 'we set aside 200 of the 1000 crowd - annotated phrases', 'as test data, meaning we have 100 test instances for each sense type ( sound /', 'smell ). the rest of the data, 400 per sense type was used for generating training data using', 'the combined crowd and pattern approach described in section 3. 3. we set α = 0. 6 and', 'α = 0. 4, based on', 'figure 3, for audible', 'and olfactible concepts respectively. with these α values, the combination approach produced 1, 962 and 1, 702 training instances for audible and olfactible concepts respectively', 'performance of the various models is shown in table 4. the abbreviations denote the following : lstm refers to a vanilla lstm model, using only word embeddings as features, + or refers to the lstm', 'plus the output recurrence, + char refers to the lstm', 'plus the character embeddings as features. + or + char refers to the lstm plus the output recurrence and character embeddings as', 'features. for the crf, we use the commonly used features for named entity recognition : words, prefix / suffices', ', and part - of - speech tag  #TAUTHOR_TAG. we can see that for both senses, the model that uses both character embedding features', ', and an output recurrence', '']",5
[' #TAUTHOR_TAG and show that model'],[' #TAUTHOR_TAG and show that model'],['the nli task  #TAUTHOR_TAG and show that model performance is'],"['', 'to model difficulty we use item response theory ( irt ) from psychometrics  #AUTHOR_TAG.', 'irt models characteristics such as difficulty and discrimination ability of specific examples ( called "" items "" 1 ) in order to estimate a latent ability trait of test - takers.', 'here we use irt to model the difficulty of test items to determine how dnns learn items of varying difficulty.', 'irt provides a well - studied methodology for modeling item difficulty as opposed to more heuristic - based difficulty estimates such as sentence length.', 'irt was previously used to build a new test set for the nli task  #TAUTHOR_TAG and show that model performance is dependent on test set difficulty.', 'in this work we use irt to probe specific items to try to analyze model performance at a more finegrained level, and expand the analysis to include the task of sa.', 'we train three dnns models with varying training set sizes to compare performance on two nlp tasks : nli and sentiment analysis ( sa ).', '']",0
[' #TAUTHOR_TAG and show that model'],[' #TAUTHOR_TAG and show that model'],['the nli task  #TAUTHOR_TAG and show that model performance is'],"['', 'to model difficulty we use item response theory ( irt ) from psychometrics  #AUTHOR_TAG.', 'irt models characteristics such as difficulty and discrimination ability of specific examples ( called "" items "" 1 ) in order to estimate a latent ability trait of test - takers.', 'here we use irt to model the difficulty of test items to determine how dnns learn items of varying difficulty.', 'irt provides a well - studied methodology for modeling item difficulty as opposed to more heuristic - based difficulty estimates such as sentence length.', 'irt was previously used to build a new test set for the nli task  #TAUTHOR_TAG and show that model performance is dependent on test set difficulty.', 'in this work we use irt to probe specific items to try to analyze model performance at a more finegrained level, and expand the analysis to include the task of sa.', 'we train three dnns models with varying training set sizes to compare performance on two nlp tasks : nli and sentiment analysis ( sa ).', '']",6
"[' #TAUTHOR_TAG.', ""the 3pl model in irt models an individual's latent ability ( θ ) on a task as a""]","['model item difficulty we use the three parameter logistic ( 3pl ) model from irt  #TAUTHOR_TAG.', ""the 3pl model in irt models an individual's latent ability ( θ ) on a task as a""]","['model item difficulty we use the three parameter logistic ( 3pl ) model from irt  #TAUTHOR_TAG.', ""the 3pl model in irt models an individual's latent ability ( θ ) on a task as a function""]","['model item difficulty we use the three parameter logistic ( 3pl ) model from irt  #TAUTHOR_TAG.', ""the 3pl model in irt models an individual's latent ability ( θ ) on a task as a function of three item characteristics : discrimination ability ( a ), difficulty ( b ), and guessing ( c )."", '']",5
"['models of  #TAUTHOR_TAG and extracted the difficulty item parameters.', 'the data consists of approximately']","['estimate item difficulties for nli, we used the pre - trained irt models of  #TAUTHOR_TAG and extracted the difficulty item parameters.', 'the data consists of approximately']","['estimate item difficulties for nli, we used the pre - trained irt models of  #TAUTHOR_TAG and extracted the difficulty item parameters.', 'the data consists of approximately 1000 human annotator responses from amazon mechanical turk ( amt )']","['estimate item difficulties for nli, we used the pre - trained irt models of  #TAUTHOR_TAG and extracted the difficulty item parameters.', 'the data consists of approximately 1000 human annotator responses from amazon mechanical turk ( amt ) for a selection of 180 premise - hypothesis pairs from the snli data set  #AUTHOR_TAG.', 'each amt worker ( turker ) was shown the premisehypothesis pairs and was asked to indicate whether, if the premise was taken to be true, the hypothesis was ( a ) definitely true ( entailment ), ( b ) maybe true ( neutral ), or ( c ) definitely not true ( contradiction ).', 'for sa, we collected a new data set of labels for 134 examples randomly selected from the stanford sentiment treebank ( sstb )  #AUTHOR_TAG, using a similar amt setup as  #TAUTHOR_TAG.', 'for each randomly selected example, we had 1000 turkers label the sentence as very negative, negative, neutral, positive, or very positive.', 'we converted these responses to binary positive / negative labels and fit a new irt 3pl model ( § 2. 1 ) using the mirt r package  #AUTHOR_TAG.', 'very negative and negative labels were binned together, and neutral, positive, and very positive were binned together.', 'tables 1 and 2 show examples of the items in our data sets, and the difficulty values estimated from the irt models.', 'the first example in table 1 is a clear case of entailment, where if we assume that the premise is true, we can infer that the hypothesis is also true.', 'the label of the second example in snli is contradiction, but in this case the result is not as clear.', 'there are sports stadiums that offer lawn seating, and therefore this could potentially be a case of entailment ( or neutral ).', 'either way, one could argue that the second example here is more difficult than the first.', 'similarly, the first two examples of table 2 are interesting.', 'both of these items are labeled as negative examples in the data set.', 'the first example is clear, but the second one is more ambiguous.', 'it could be considered a mild complement, since the author still endorses renting the movie.', 'therefore you could argue again that the second example is more difficult than the first.', 'the learned difficulty parameters reflect this difference negative - 2. 46 still, it gets the job done - a sleepy afternoon rental.', 'negative 1. 78 an endlessly fascinating, landmark movie that is as bold as anything the cinema has seen in years']",5
"['3.', 'scores for the nli annotations were calculated when the original dataset was collected and are reproduced here  #TAUTHOR_TAG.', 'human annotations for the sa annotations were converted to binary before calculating the agreement.', 'we see that the agreement scores are in the range']","['3.', 'scores for the nli annotations were calculated when the original dataset was collected and are reproduced here  #TAUTHOR_TAG.', 'human annotations for the sa annotations were converted to binary before calculating the agreement.', 'we see that the agreement scores are in the range']","['- rater reliability scores for the collected annotations are showin in table 3.', 'scores for the nli annotations were calculated when the original dataset was collected and are reproduced here  #TAUTHOR_TAG.', 'human annotations for the sa annotations were converted to binary before calculating the agreement.', 'we see that the agreement scores are in the range']","['no picture ever made has more literally showed that the road to hell is paved with good intentions.', 'positive 2. 05 in difficulty in both cases.', 'inter - rater reliability scores for the collected annotations are showin in table 3.', 'scores for the nli annotations were calculated when the original dataset was collected and are reproduced here  #TAUTHOR_TAG.', 'human annotations for the sa annotations were converted to binary before calculating the agreement.', 'we see that the agreement scores are in the range of 0. 4 to 0. 6 which is considered moderate agreement  #AUTHOR_TAG.', 'with the large number of annotators it is to be expected that there is some disagreement in the labels.', 'however this disagreement can be interpreted as varying difficulty of the items, which is what we expect when we fit the irt models']",5
"['models of  #TAUTHOR_TAG and extracted the difficulty item parameters.', 'the data consists of approximately']","['estimate item difficulties for nli, we used the pre - trained irt models of  #TAUTHOR_TAG and extracted the difficulty item parameters.', 'the data consists of approximately']","['estimate item difficulties for nli, we used the pre - trained irt models of  #TAUTHOR_TAG and extracted the difficulty item parameters.', 'the data consists of approximately 1000 human annotator responses from amazon mechanical turk ( amt )']","['estimate item difficulties for nli, we used the pre - trained irt models of  #TAUTHOR_TAG and extracted the difficulty item parameters.', 'the data consists of approximately 1000 human annotator responses from amazon mechanical turk ( amt ) for a selection of 180 premise - hypothesis pairs from the snli data set  #AUTHOR_TAG.', 'each amt worker ( turker ) was shown the premisehypothesis pairs and was asked to indicate whether, if the premise was taken to be true, the hypothesis was ( a ) definitely true ( entailment ), ( b ) maybe true ( neutral ), or ( c ) definitely not true ( contradiction ).', 'for sa, we collected a new data set of labels for 134 examples randomly selected from the stanford sentiment treebank ( sstb )  #AUTHOR_TAG, using a similar amt setup as  #TAUTHOR_TAG.', 'for each randomly selected example, we had 1000 turkers label the sentence as very negative, negative, neutral, positive, or very positive.', 'we converted these responses to binary positive / negative labels and fit a new irt 3pl model ( § 2. 1 ) using the mirt r package  #AUTHOR_TAG.', 'very negative and negative labels were binned together, and neutral, positive, and very positive were binned together.', 'tables 1 and 2 show examples of the items in our data sets, and the difficulty values estimated from the irt models.', 'the first example in table 1 is a clear case of entailment, where if we assume that the premise is true, we can infer that the hypothesis is also true.', 'the label of the second example in snli is contradiction, but in this case the result is not as clear.', 'there are sports stadiums that offer lawn seating, and therefore this could potentially be a case of entailment ( or neutral ).', 'either way, one could argue that the second example here is more difficult than the first.', 'similarly, the first two examples of table 2 are interesting.', 'both of these items are labeled as negative examples in the data set.', 'the first example is clear, but the second one is more ambiguous.', 'it could be considered a mild complement, since the author still endorses renting the movie.', 'therefore you could argue again that the second example is more difficult than the first.', 'the learned difficulty parameters reflect this difference negative - 2. 46 still, it gets the job done - a sleepy afternoon rental.', 'negative 1. 78 an endlessly fascinating, landmark movie that is as bold as anything the cinema has seen in years']",3
"['encoder stack.', ' #TAUTHOR_TAG introduced two unsupervised tasks to pretrain this']","['encoder stack.', ' #TAUTHOR_TAG introduced two unsupervised tasks to pretrain this architecture, next']","['by performing multi - headed attention over the encoder stack.', ' #TAUTHOR_TAG introduced two unsupervised tasks to pretrain this architecture, next sentence prediction']","['2  #AUTHOR_TAG originally came out as a machine translation architecture and it uses the idea of self attention mechanism  #AUTHOR_TAG.', 'it has an encoderdecoder design and both parts use the same novel multi - head attention mechanism.', 'the encoder part takes an input sentence and derives a representation from it using this attention mechanism.', 'afterwards, the decoder generates the target sentence by performing multi - headed attention over the encoder stack.', ' #TAUTHOR_TAG introduced two unsupervised tasks to pretrain this architecture, next sentence prediction and masked language modeling.', '']",3
['presented a  #TAUTHOR_TAG baseline for the hyperpartisan news'],['presented a  #TAUTHOR_TAG baseline for the hyperpartisan news'],"['presented a  #TAUTHOR_TAG baseline for the hyperpartisan news detection task.', 'we demonstrated that pretraining  #TAUTHOR_TAG in an unseen domain']","['presented a  #TAUTHOR_TAG baseline for the hyperpartisan news detection task.', 'we demonstrated that pretraining  #TAUTHOR_TAG in an unseen domain improves the performance of the model on the domain specific supervised task.', 'we also showed that the difference in news source affects the generalization.', '']",2
['for an imageguessing task as in  #TAUTHOR_TAG. it is'],"['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, flu']","['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, fluent and informative answers for the human - ai team']","['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, fluent and informative answers for the human - ai team to do well. furthermore, they have to be robust to their own mistakes, i. e.', ', if an agent makes an error at a particular round, that error is now part of its conversation history, and it must be able to correct itself rather than be', ""consistently inaccurate. similarly, human players must also learn to adapt to alice's sometime noisy and inaccurate responses. at its core, guesswhich is a game - with - a - purpose ( gwap"", ') that leverages human computation to evaluate visual conversational agents. traditionally, gwap ( von ahn and dabbish 2008 )', 'have focused on human - human collaboration, i. e. collecting data by', 'making humans play games to label images ( von ah', '##n and dabbish 2004 ), music ( law et al. 2007 ) and movies ( michelucci 2013 ). we extend this to human - ai teams and to the best of our knowledge', ', our work is the first to evaluate visual conversational agents in an interactive setting where humans are', 'continuously engaging with agents to succeed at a cooperative game. contributions. more concretely, we make the following contributions', 'in this work : • we design an interactive image - guessing game (', 'guesswhich ) for evaluating human - ai team performance in the specific context of the ais being visual conversational agents. guesswhich pairs humans with alice, an ai capable of answering a sequence of questions about images. alice is assigned a secret image and answers questions asked about', 'that image from a human for 9 rounds to help them identify the secret image ( sec. 4 ). • we evaluate human - ai team performance on this game for both supervised learning', '( sl ) and reinforcement learning ( rl ) versions of alice. our main experimental finding is that despite significant differences between sl and rl agents reported in previous work  #TAUTHOR_TAG, we find no', 'significant difference in performance between alice sl or alice rl when paired with human partners ( sec. 6. 1 ). this suggests that while self - talk and rl', 'are interesting directions to pursue for building better visual conversational agents, there appears to be a disconnect between ai - ai and human - ai evaluations - progress on former does not seem predictive of progress on latter. this is an important finding to', '']",0
['for an imageguessing task as in  #TAUTHOR_TAG. it is'],"['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, flu']","['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, fluent and informative answers for the human - ai team']","['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, fluent and informative answers for the human - ai team to do well. furthermore, they have to be robust to their own mistakes, i. e.', ', if an agent makes an error at a particular round, that error is now part of its conversation history, and it must be able to correct itself rather than be', ""consistently inaccurate. similarly, human players must also learn to adapt to alice's sometime noisy and inaccurate responses. at its core, guesswhich is a game - with - a - purpose ( gwap"", ') that leverages human computation to evaluate visual conversational agents. traditionally, gwap ( von ahn and dabbish 2008 )', 'have focused on human - human collaboration, i. e. collecting data by', 'making humans play games to label images ( von ah', '##n and dabbish 2004 ), music ( law et al. 2007 ) and movies ( michelucci 2013 ). we extend this to human - ai teams and to the best of our knowledge', ', our work is the first to evaluate visual conversational agents in an interactive setting where humans are', 'continuously engaging with agents to succeed at a cooperative game. contributions. more concretely, we make the following contributions', 'in this work : • we design an interactive image - guessing game (', 'guesswhich ) for evaluating human - ai team performance in the specific context of the ais being visual conversational agents. guesswhich pairs humans with alice, an ai capable of answering a sequence of questions about images. alice is assigned a secret image and answers questions asked about', 'that image from a human for 9 rounds to help them identify the secret image ( sec. 4 ). • we evaluate human - ai team performance on this game for both supervised learning', '( sl ) and reinforcement learning ( rl ) versions of alice. our main experimental finding is that despite significant differences between sl and rl agents reported in previous work  #TAUTHOR_TAG, we find no', 'significant difference in performance between alice sl or alice rl when paired with human partners ( sec. 6. 1 ). this suggests that while self - talk and rl', 'are interesting directions to pursue for building better visual conversational agents, there appears to be a disconnect between ai - ai and human - ai evaluations - progress on former does not seem predictive of progress on latter. this is an important finding to', '']",0
['for an imageguessing task as in  #TAUTHOR_TAG. it is'],"['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, flu']","['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, fluent and informative answers for the human - ai team']","['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, fluent and informative answers for the human - ai team to do well. furthermore, they have to be robust to their own mistakes, i. e.', ', if an agent makes an error at a particular round, that error is now part of its conversation history, and it must be able to correct itself rather than be', ""consistently inaccurate. similarly, human players must also learn to adapt to alice's sometime noisy and inaccurate responses. at its core, guesswhich is a game - with - a - purpose ( gwap"", ') that leverages human computation to evaluate visual conversational agents. traditionally, gwap ( von ahn and dabbish 2008 )', 'have focused on human - human collaboration, i. e. collecting data by', 'making humans play games to label images ( von ah', '##n and dabbish 2004 ), music ( law et al. 2007 ) and movies ( michelucci 2013 ). we extend this to human - ai teams and to the best of our knowledge', ', our work is the first to evaluate visual conversational agents in an interactive setting where humans are', 'continuously engaging with agents to succeed at a cooperative game. contributions. more concretely, we make the following contributions', 'in this work : • we design an interactive image - guessing game (', 'guesswhich ) for evaluating human - ai team performance in the specific context of the ais being visual conversational agents. guesswhich pairs humans with alice, an ai capable of answering a sequence of questions about images. alice is assigned a secret image and answers questions asked about', 'that image from a human for 9 rounds to help them identify the secret image ( sec. 4 ). • we evaluate human - ai team performance on this game for both supervised learning', '( sl ) and reinforcement learning ( rl ) versions of alice. our main experimental finding is that despite significant differences between sl and rl agents reported in previous work  #TAUTHOR_TAG, we find no', 'significant difference in performance between alice sl or alice rl when paired with human partners ( sec. 6. 1 ). this suggests that while self - talk and rl', 'are interesting directions to pursue for building better visual conversational agents, there appears to be a disconnect between ai - ai and human - ai evaluations - progress on former does not seem predictive of progress on latter. this is an important finding to', '']",0
['##a ;  #TAUTHOR_TAG ;. ( das'],['al. 2017a ;  #TAUTHOR_TAG ;. ( das'],['. 2017a ;  #TAUTHOR_TAG ;. ( das'],"['that our goal is to evaluate visual conversational agents through a human computation game, we draw connections to relevant work on visual conversational agents, human computation games, and dialog evaluation below.', 'visual conversational agents.', 'our ai agents are visual conversational models, which have recently emerged as a popular research area in visually - grounded language modeling ( das et al. 2017a ;  #TAUTHOR_TAG ;. ( das et al. 2017a ) introduced the task of visual dialog and collected the visdial dataset by pairing subjects on amazon mechanical turk ( amt ) to chat about an image ( with assigned roles of questioner and answerer ).', ' #TAUTHOR_TAG pre - trained questioner and answerer agents on this visdial dataset via supervised learning and fine - tuned them via self - talk ( reinforcement learning ), observing that rl - fine - tuned qbot - abot are better at image - guessing after interacting with each other.', 'however, ( aras et al. 2010 ; chamberlain, poesio, and kruschwitz 2008 ), movies ( michelucci 2013 ) etc.', 'while such games have traditionally focused on human - human collaboration, we extend these ideas to human - ai teams.', 'rather than collecting labeled data, our game is designed to measure the effectiveness of the ai in the context of human - ai teams.', '']",0
['##a ;  #TAUTHOR_TAG ;. ( das'],['al. 2017a ;  #TAUTHOR_TAG ;. ( das'],['. 2017a ;  #TAUTHOR_TAG ;. ( das'],"['that our goal is to evaluate visual conversational agents through a human computation game, we draw connections to relevant work on visual conversational agents, human computation games, and dialog evaluation below.', 'visual conversational agents.', 'our ai agents are visual conversational models, which have recently emerged as a popular research area in visually - grounded language modeling ( das et al. 2017a ;  #TAUTHOR_TAG ;. ( das et al. 2017a ) introduced the task of visual dialog and collected the visdial dataset by pairing subjects on amazon mechanical turk ( amt ) to chat about an image ( with assigned roles of questioner and answerer ).', ' #TAUTHOR_TAG pre - trained questioner and answerer agents on this visdial dataset via supervised learning and fine - tuned them via self - talk ( reinforcement learning ), observing that rl - fine - tuned qbot - abot are better at image - guessing after interacting with each other.', 'however, ( aras et al. 2010 ; chamberlain, poesio, and kruschwitz 2008 ), movies ( michelucci 2013 ) etc.', 'while such games have traditionally focused on human - human collaboration, we extend these ideas to human - ai teams.', 'rather than collecting labeled data, our game is designed to measure the effectiveness of the ai in the context of human - ai teams.', '']",0
['##a ;  #TAUTHOR_TAG ;. ( das'],['al. 2017a ;  #TAUTHOR_TAG ;. ( das'],['. 2017a ;  #TAUTHOR_TAG ;. ( das'],"['that our goal is to evaluate visual conversational agents through a human computation game, we draw connections to relevant work on visual conversational agents, human computation games, and dialog evaluation below.', 'visual conversational agents.', 'our ai agents are visual conversational models, which have recently emerged as a popular research area in visually - grounded language modeling ( das et al. 2017a ;  #TAUTHOR_TAG ;. ( das et al. 2017a ) introduced the task of visual dialog and collected the visdial dataset by pairing subjects on amazon mechanical turk ( amt ) to chat about an image ( with assigned roles of questioner and answerer ).', ' #TAUTHOR_TAG pre - trained questioner and answerer agents on this visdial dataset via supervised learning and fine - tuned them via self - talk ( reinforcement learning ), observing that rl - fine - tuned qbot - abot are better at image - guessing after interacting with each other.', 'however, ( aras et al. 2010 ; chamberlain, poesio, and kruschwitz 2008 ), movies ( michelucci 2013 ) etc.', 'while such games have traditionally focused on human - human collaboration, we extend these ideas to human - ai teams.', 'rather than collecting labeled data, our game is designed to measure the effectiveness of the ai in the context of human - ai teams.', '']",0
['agent abot from  #TAUTHOR_TAG as abot is the agent more likely to be deployed with a human partner in real applications ('],['agent abot from  #TAUTHOR_TAG as abot is the agent more likely to be deployed with a human partner in real applications ( e. g.'],"['of human - ai teams in the context of visual conversational agents.', 'specifically, we are considering the question - answering agent abot from  #TAUTHOR_TAG as abot is the agent more likely to be deployed with a human partner in real applications (']","['from section 1 that our goal is to evaluate how progress in ai measured through automatic evaluation translates to performance of human - ai teams in the context of visual conversational agents.', 'specifically, we are considering the question - answering agent abot from  #TAUTHOR_TAG as abot is the agent more likely to be deployed with a human partner in real applications ( e. g. to answer questions about visual content to aid a visually impaired user ).', 'for completeness, we will review this work in this section.', ' #TAUTHOR_TAG formulate a self - supervised imageguessing task between a questioner bot ( qbot ) and an answerer bot ( abot ) which plays out over multiple rounds of dialog.', 'at the start of the task, qbot and abot are shown a one sentence description ( i. e. a caption ) of an image ( unknown to qbot ).', 'the pair can then engage in question and answer based dialog for a fixed number of iterations after which qbot must try to select the secret image from a pool.', 'the goal of the qbot - abot team is two - fold, qbot should : 1 ) build a mental model of the unseen image purely from the dialog and 2 ) be able to retrieve that image from a line - up of images.', '']",0
"['to emulating human responses and generating visually discriminative responses in ai - ai dialog.', ' #TAUTHOR_TAG']","['to emulating human responses and generating visually discriminative responses in ai - ai dialog.', ' #TAUTHOR_TAG']","['to emulating human responses and generating visually discriminative responses in ai - ai dialog.', ' #TAUTHOR_TAG']","['compare the performance of the two agents alice sl and alice rl in the guesswhich game.', 'these bots are state - ofthe - art visual dialog agents with respect to emulating human responses and generating visually discriminative responses in ai - ai dialog.', ' #TAUTHOR_TAG evaluate these agents against strong baselines and report ai - ai team results that are significantly better than chance on a pool of ∼10k images ( rank ∼1000 for sl, rank ∼500 for rl ).', 'in addition to evaluating them in the context of human - ai teams we also report qbot - alice team performances for reference.', '']",0
['for an imageguessing task as in  #TAUTHOR_TAG. it is'],"['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, flu']","['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, fluent and informative answers for the human - ai team']","['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, fluent and informative answers for the human - ai team to do well. furthermore, they have to be robust to their own mistakes, i. e.', ', if an agent makes an error at a particular round, that error is now part of its conversation history, and it must be able to correct itself rather than be', ""consistently inaccurate. similarly, human players must also learn to adapt to alice's sometime noisy and inaccurate responses. at its core, guesswhich is a game - with - a - purpose ( gwap"", ') that leverages human computation to evaluate visual conversational agents. traditionally, gwap ( von ahn and dabbish 2008 )', 'have focused on human - human collaboration, i. e. collecting data by', 'making humans play games to label images ( von ah', '##n and dabbish 2004 ), music ( law et al. 2007 ) and movies ( michelucci 2013 ). we extend this to human - ai teams and to the best of our knowledge', ', our work is the first to evaluate visual conversational agents in an interactive setting where humans are', 'continuously engaging with agents to succeed at a cooperative game. contributions. more concretely, we make the following contributions', 'in this work : • we design an interactive image - guessing game (', 'guesswhich ) for evaluating human - ai team performance in the specific context of the ais being visual conversational agents. guesswhich pairs humans with alice, an ai capable of answering a sequence of questions about images. alice is assigned a secret image and answers questions asked about', 'that image from a human for 9 rounds to help them identify the secret image ( sec. 4 ). • we evaluate human - ai team performance on this game for both supervised learning', '( sl ) and reinforcement learning ( rl ) versions of alice. our main experimental finding is that despite significant differences between sl and rl agents reported in previous work  #TAUTHOR_TAG, we find no', 'significant difference in performance between alice sl or alice rl when paired with human partners ( sec. 6. 1 ). this suggests that while self - talk and rl', 'are interesting directions to pursue for building better visual conversational agents, there appears to be a disconnect between ai - ai and human - ai evaluations - progress on former does not seem predictive of progress on latter. this is an important finding to', '']",3
['for an imageguessing task as in  #TAUTHOR_TAG. it is'],"['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, flu']","['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, fluent and informative answers for the human - ai team']","['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, fluent and informative answers for the human - ai team to do well. furthermore, they have to be robust to their own mistakes, i. e.', ', if an agent makes an error at a particular round, that error is now part of its conversation history, and it must be able to correct itself rather than be', ""consistently inaccurate. similarly, human players must also learn to adapt to alice's sometime noisy and inaccurate responses. at its core, guesswhich is a game - with - a - purpose ( gwap"", ') that leverages human computation to evaluate visual conversational agents. traditionally, gwap ( von ahn and dabbish 2008 )', 'have focused on human - human collaboration, i. e. collecting data by', 'making humans play games to label images ( von ah', '##n and dabbish 2004 ), music ( law et al. 2007 ) and movies ( michelucci 2013 ). we extend this to human - ai teams and to the best of our knowledge', ', our work is the first to evaluate visual conversational agents in an interactive setting where humans are', 'continuously engaging with agents to succeed at a cooperative game. contributions. more concretely, we make the following contributions', 'in this work : • we design an interactive image - guessing game (', 'guesswhich ) for evaluating human - ai team performance in the specific context of the ais being visual conversational agents. guesswhich pairs humans with alice, an ai capable of answering a sequence of questions about images. alice is assigned a secret image and answers questions asked about', 'that image from a human for 9 rounds to help them identify the secret image ( sec. 4 ). • we evaluate human - ai team performance on this game for both supervised learning', '( sl ) and reinforcement learning ( rl ) versions of alice. our main experimental finding is that despite significant differences between sl and rl agents reported in previous work  #TAUTHOR_TAG, we find no', 'significant difference in performance between alice sl or alice rl when paired with human partners ( sec. 6. 1 ). this suggests that while self - talk and rl', 'are interesting directions to pursue for building better visual conversational agents, there appears to be a disconnect between ai - ai and human - ai evaluations - progress on former does not seem predictive of progress on latter. this is an important finding to', '']",3
['for an imageguessing task as in  #TAUTHOR_TAG. it is'],"['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, flu']","['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, fluent and informative answers for the human - ai team']","['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, fluent and informative answers for the human - ai team to do well. furthermore, they have to be robust to their own mistakes, i. e.', ', if an agent makes an error at a particular round, that error is now part of its conversation history, and it must be able to correct itself rather than be', ""consistently inaccurate. similarly, human players must also learn to adapt to alice's sometime noisy and inaccurate responses. at its core, guesswhich is a game - with - a - purpose ( gwap"", ') that leverages human computation to evaluate visual conversational agents. traditionally, gwap ( von ahn and dabbish 2008 )', 'have focused on human - human collaboration, i. e. collecting data by', 'making humans play games to label images ( von ah', '##n and dabbish 2004 ), music ( law et al. 2007 ) and movies ( michelucci 2013 ). we extend this to human - ai teams and to the best of our knowledge', ', our work is the first to evaluate visual conversational agents in an interactive setting where humans are', 'continuously engaging with agents to succeed at a cooperative game. contributions. more concretely, we make the following contributions', 'in this work : • we design an interactive image - guessing game (', 'guesswhich ) for evaluating human - ai team performance in the specific context of the ais being visual conversational agents. guesswhich pairs humans with alice, an ai capable of answering a sequence of questions about images. alice is assigned a secret image and answers questions asked about', 'that image from a human for 9 rounds to help them identify the secret image ( sec. 4 ). • we evaluate human - ai team performance on this game for both supervised learning', '( sl ) and reinforcement learning ( rl ) versions of alice. our main experimental finding is that despite significant differences between sl and rl agents reported in previous work  #TAUTHOR_TAG, we find no', 'significant difference in performance between alice sl or alice rl when paired with human partners ( sec. 6. 1 ). this suggests that while self - talk and rl', 'are interesting directions to pursue for building better visual conversational agents, there appears to be a disconnect between ai - ai and human - ai evaluations - progress on former does not seem predictive of progress on latter. this is an important finding to', '']",5
['for an imageguessing task as in  #TAUTHOR_TAG. it is'],"['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, flu']","['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, fluent and informative answers for the human - ai team']","['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, fluent and informative answers for the human - ai team to do well. furthermore, they have to be robust to their own mistakes, i. e.', ', if an agent makes an error at a particular round, that error is now part of its conversation history, and it must be able to correct itself rather than be', ""consistently inaccurate. similarly, human players must also learn to adapt to alice's sometime noisy and inaccurate responses. at its core, guesswhich is a game - with - a - purpose ( gwap"", ') that leverages human computation to evaluate visual conversational agents. traditionally, gwap ( von ahn and dabbish 2008 )', 'have focused on human - human collaboration, i. e. collecting data by', 'making humans play games to label images ( von ah', '##n and dabbish 2004 ), music ( law et al. 2007 ) and movies ( michelucci 2013 ). we extend this to human - ai teams and to the best of our knowledge', ', our work is the first to evaluate visual conversational agents in an interactive setting where humans are', 'continuously engaging with agents to succeed at a cooperative game. contributions. more concretely, we make the following contributions', 'in this work : • we design an interactive image - guessing game (', 'guesswhich ) for evaluating human - ai team performance in the specific context of the ais being visual conversational agents. guesswhich pairs humans with alice, an ai capable of answering a sequence of questions about images. alice is assigned a secret image and answers questions asked about', 'that image from a human for 9 rounds to help them identify the secret image ( sec. 4 ). • we evaluate human - ai team performance on this game for both supervised learning', '( sl ) and reinforcement learning ( rl ) versions of alice. our main experimental finding is that despite significant differences between sl and rl agents reported in previous work  #TAUTHOR_TAG, we find no', 'significant difference in performance between alice sl or alice rl when paired with human partners ( sec. 6. 1 ). this suggests that while self - talk and rl', 'are interesting directions to pursue for building better visual conversational agents, there appears to be a disconnect between ai - ai and human - ai evaluations - progress on former does not seem predictive of progress on latter. this is an important finding to', '']",5
['for an imageguessing task as in  #TAUTHOR_TAG. it is'],"['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, flu']","['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, fluent and informative answers for the human - ai team']","['- tuned via reinforcement learning for an imageguessing task as in  #TAUTHOR_TAG. it is important to appreciate the difficulty and sensitivity of the guesswhich game as an evaluation tool - agents have to understand human questions', 'and respond with accurate, consistent, fluent and informative answers for the human - ai team to do well. furthermore, they have to be robust to their own mistakes, i. e.', ', if an agent makes an error at a particular round, that error is now part of its conversation history, and it must be able to correct itself rather than be', ""consistently inaccurate. similarly, human players must also learn to adapt to alice's sometime noisy and inaccurate responses. at its core, guesswhich is a game - with - a - purpose ( gwap"", ') that leverages human computation to evaluate visual conversational agents. traditionally, gwap ( von ahn and dabbish 2008 )', 'have focused on human - human collaboration, i. e. collecting data by', 'making humans play games to label images ( von ah', '##n and dabbish 2004 ), music ( law et al. 2007 ) and movies ( michelucci 2013 ). we extend this to human - ai teams and to the best of our knowledge', ', our work is the first to evaluate visual conversational agents in an interactive setting where humans are', 'continuously engaging with agents to succeed at a cooperative game. contributions. more concretely, we make the following contributions', 'in this work : • we design an interactive image - guessing game (', 'guesswhich ) for evaluating human - ai team performance in the specific context of the ais being visual conversational agents. guesswhich pairs humans with alice, an ai capable of answering a sequence of questions about images. alice is assigned a secret image and answers questions asked about', 'that image from a human for 9 rounds to help them identify the secret image ( sec. 4 ). • we evaluate human - ai team performance on this game for both supervised learning', '( sl ) and reinforcement learning ( rl ) versions of alice. our main experimental finding is that despite significant differences between sl and rl agents reported in previous work  #TAUTHOR_TAG, we find no', 'significant difference in performance between alice sl or alice rl when paired with human partners ( sec. 6. 1 ). this suggests that while self - talk and rl', 'are interesting directions to pursue for building better visual conversational agents, there appears to be a disconnect between ai - ai and human - ai evaluations - progress on former does not seem predictive of progress on latter. this is an important finding to', '']",4
"['to emulating human responses and generating visually discriminative responses in ai - ai dialog.', ' #TAUTHOR_TAG']","['to emulating human responses and generating visually discriminative responses in ai - ai dialog.', ' #TAUTHOR_TAG']","['to emulating human responses and generating visually discriminative responses in ai - ai dialog.', ' #TAUTHOR_TAG']","['compare the performance of the two agents alice sl and alice rl in the guesswhich game.', 'these bots are state - ofthe - art visual dialog agents with respect to emulating human responses and generating visually discriminative responses in ai - ai dialog.', ' #TAUTHOR_TAG evaluate these agents against strong baselines and report ai - ai team results that are significantly better than chance on a pool of ∼10k images ( rank ∼1000 for sl, rank ∼500 for rl ).', 'in addition to evaluating them in the context of human - ai teams we also report qbot - alice team performances for reference.', '']",4
"['to emulating human responses and generating visually discriminative responses in ai - ai dialog.', ' #TAUTHOR_TAG']","['to emulating human responses and generating visually discriminative responses in ai - ai dialog.', ' #TAUTHOR_TAG']","['to emulating human responses and generating visually discriminative responses in ai - ai dialog.', ' #TAUTHOR_TAG']","['compare the performance of the two agents alice sl and alice rl in the guesswhich game.', 'these bots are state - ofthe - art visual dialog agents with respect to emulating human responses and generating visually discriminative responses in ai - ai dialog.', ' #TAUTHOR_TAG evaluate these agents against strong baselines and report ai - ai team results that are significantly better than chance on a pool of ∼10k images ( rank ∼1000 for sl, rank ∼500 for rl ).', 'in addition to evaluating them in the context of human - ai teams we also report qbot - alice team performances for reference.', '']",4
['agent abot from  #TAUTHOR_TAG as abot is the agent more likely to be deployed with a human partner in real applications ('],['agent abot from  #TAUTHOR_TAG as abot is the agent more likely to be deployed with a human partner in real applications ( e. g.'],"['of human - ai teams in the context of visual conversational agents.', 'specifically, we are considering the question - answering agent abot from  #TAUTHOR_TAG as abot is the agent more likely to be deployed with a human partner in real applications (']","['from section 1 that our goal is to evaluate how progress in ai measured through automatic evaluation translates to performance of human - ai teams in the context of visual conversational agents.', 'specifically, we are considering the question - answering agent abot from  #TAUTHOR_TAG as abot is the agent more likely to be deployed with a human partner in real applications ( e. g. to answer questions about visual content to aid a visually impaired user ).', 'for completeness, we will review this work in this section.', ' #TAUTHOR_TAG formulate a self - supervised imageguessing task between a questioner bot ( qbot ) and an answerer bot ( abot ) which plays out over multiple rounds of dialog.', 'at the start of the task, qbot and abot are shown a one sentence description ( i. e. a caption ) of an image ( unknown to qbot ).', 'the pair can then engage in question and answer based dialog for a fixed number of iterations after which qbot must try to select the secret image from a pool.', 'the goal of the qbot - abot team is two - fold, qbot should : 1 ) build a mental model of the unseen image purely from the dialog and 2 ) be able to retrieve that image from a line - up of images.', '']",7
['agent abot from  #TAUTHOR_TAG as abot is the agent more likely to be deployed with a human partner in real applications ('],['agent abot from  #TAUTHOR_TAG as abot is the agent more likely to be deployed with a human partner in real applications ( e. g.'],"['of human - ai teams in the context of visual conversational agents.', 'specifically, we are considering the question - answering agent abot from  #TAUTHOR_TAG as abot is the agent more likely to be deployed with a human partner in real applications (']","['from section 1 that our goal is to evaluate how progress in ai measured through automatic evaluation translates to performance of human - ai teams in the context of visual conversational agents.', 'specifically, we are considering the question - answering agent abot from  #TAUTHOR_TAG as abot is the agent more likely to be deployed with a human partner in real applications ( e. g. to answer questions about visual content to aid a visually impaired user ).', 'for completeness, we will review this work in this section.', ' #TAUTHOR_TAG formulate a self - supervised imageguessing task between a questioner bot ( qbot ) and an answerer bot ( abot ) which plays out over multiple rounds of dialog.', 'at the start of the task, qbot and abot are shown a one sentence description ( i. e. a caption ) of an image ( unknown to qbot ).', 'the pair can then engage in question and answer based dialog for a fixed number of iterations after which qbot must try to select the secret image from a pool.', 'the goal of the qbot - abot team is two - fold, qbot should : 1 ) build a mental model of the unseen image purely from the dialog and 2 ) be able to retrieve that image from a line - up of images.', '']",7
"[', new tasks have been introduced, one of them being  #TAUTHOR_TAG.']","['measure progress in such virtual environments, new tasks have been introduced, one of them being  #TAUTHOR_TAG. the  #TAUTHOR_TAG task requires an agent to intelligently navigate in a simulated household environment [', '25 ]']","['9, 17 ]. to foster and measure progress in such virtual environments, new tasks have been introduced, one of them being  #TAUTHOR_TAG.']","['9, 17 ]. to foster and measure progress in such virtual environments, new tasks have been introduced, one of them being  #TAUTHOR_TAG. the  #TAUTHOR_TAG task requires an agent to intelligently navigate in a simulated household environment [', '']",7
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['reinforce ) and nmc ( bc + a3c ) in the t 10 case. the difference in performance b', '/ w the nearest neighbour method and bow is primarily due to the fact that the bow method leverages validation metrics more effectively, uses distributed word representations and differs', 'in optimization. we also observe that the majority baseline achieves an accuracy of only 17. 15 %, suggesting that the other question - only baselines leverage dataset biases separate from class modes', '. for completeness, we also include a question only baseline derived directly from the  #TAUTHOR_TAG q - only ( lstm ). note that we only compare the top - 1 accuracy of', ""different methods here, and not the navigation performance since it's not directly applicable to these blindfold baselines. to better understand the exact bias exploited by the text only models we observe that ( a ) the questions"", ""from training set are largely repeated in the validation and test set, with only 2 and 6 questions being unique to them respectively. as noted earlier, this means that models don't need to generalize across unseen combinations of rooms / objects / colors to perform well on this task ( b ) despite entropy - pruning,"", '']",7
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['reinforce ) and nmc ( bc + a3c ) in the t 10 case. the difference in performance b', '/ w the nearest neighbour method and bow is primarily due to the fact that the bow method leverages validation metrics more effectively, uses distributed word representations and differs', 'in optimization. we also observe that the majority baseline achieves an accuracy of only 17. 15 %, suggesting that the other question - only baselines leverage dataset biases separate from class modes', '. for completeness, we also include a question only baseline derived directly from the  #TAUTHOR_TAG q - only ( lstm ). note that we only compare the top - 1 accuracy of', ""different methods here, and not the navigation performance since it's not directly applicable to these blindfold baselines. to better understand the exact bias exploited by the text only models we observe that ( a ) the questions"", ""from training set are largely repeated in the validation and test set, with only 2 and 6 questions being unique to them respectively. as noted earlier, this means that models don't need to generalize across unseen combinations of rooms / objects / colors to perform well on this task ( b ) despite entropy - pruning,"", '']",3
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['reinforce ) and nmc ( bc + a3c ) in the t 10 case. the difference in performance b', '/ w the nearest neighbour method and bow is primarily due to the fact that the bow method leverages validation metrics more effectively, uses distributed word representations and differs', 'in optimization. we also observe that the majority baseline achieves an accuracy of only 17. 15 %, suggesting that the other question - only baselines leverage dataset biases separate from class modes', '. for completeness, we also include a question only baseline derived directly from the  #TAUTHOR_TAG q - only ( lstm ). note that we only compare the top - 1 accuracy of', ""different methods here, and not the navigation performance since it's not directly applicable to these blindfold baselines. to better understand the exact bias exploited by the text only models we observe that ( a ) the questions"", ""from training set are largely repeated in the validation and test set, with only 2 and 6 questions being unique to them respectively. as noted earlier, this means that models don't need to generalize across unseen combinations of rooms / objects / colors to perform well on this task ( b ) despite entropy - pruning,"", '']",3
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['reinforce ) and nmc ( bc + a3c ) in the t 10 case. the difference in performance b', '/ w the nearest neighbour method and bow is primarily due to the fact that the bow method leverages validation metrics more effectively, uses distributed word representations and differs', 'in optimization. we also observe that the majority baseline achieves an accuracy of only 17. 15 %, suggesting that the other question - only baselines leverage dataset biases separate from class modes', '. for completeness, we also include a question only baseline derived directly from the  #TAUTHOR_TAG q - only ( lstm ). note that we only compare the top - 1 accuracy of', ""different methods here, and not the navigation performance since it's not directly applicable to these blindfold baselines. to better understand the exact bias exploited by the text only models we observe that ( a ) the questions"", ""from training set are largely repeated in the validation and test set, with only 2 and 6 questions being unique to them respectively. as noted earlier, this means that models don't need to generalize across unseen combinations of rooms / objects / colors to perform well on this task ( b ) despite entropy - pruning,"", '']",3
"[' #TAUTHOR_TAG task.', 'our results indicate existing models']","[' #TAUTHOR_TAG task.', 'our results indicate existing models']","['compete with existing methods on the  #TAUTHOR_TAG task.', 'our results indicate existing models are not able to convincingly use sensory inputs from the environment to perform question answering, although']","['show that simple question only baselines largely outperform or closely compete with existing methods on the  #TAUTHOR_TAG task.', 'our results indicate existing models are not able to convincingly use sensory inputs from the environment to perform question answering, although they have been demonstrated some ability navigate toward the object of interest.', 'besides providing a benchmark score for future researchers working on this task, our results suggest considerations for future dataset and task construction in  #TAUTHOR_TAG and related tasks']",2
"['is used [ 14,  #TAUTHOR_TAG']","['is used [ 14,  #TAUTHOR_TAG']","['is used [ 14,  #TAUTHOR_TAG']","['', 'is regarded as a negative ground - truth since it fails to handle the utterance. evaluating on an annotated dataset from the user', 'logs of a large - scale conversation interaction system, we show that the proposed approach significantly improves the domain classification especially when', 'hypothesis reranking is used [ 14,  #TAUTHOR_TAG']",0
"['is used [ 14,  #TAUTHOR_TAG']","['is used [ 14,  #TAUTHOR_TAG']","['is used [ 14,  #TAUTHOR_TAG']","['', 'is regarded as a negative ground - truth since it fails to handle the utterance. evaluating on an annotated dataset from the user', 'logs of a large - scale conversation interaction system, we show that the proposed approach significantly improves the domain classification especially when', 'hypothesis reranking is used [ 14,  #TAUTHOR_TAG']",4
"['higher scalabil - ity [ 14,  #TAUTHOR_TAG.', 'within']","['higher scalabil - ity [ 14,  #TAUTHOR_TAG.', 'within']","['higher scalabil - ity [ 14,  #TAUTHOR_TAG.', 'within the approach, a shortlister, which is a light - weighted domain classifier, suggests the most promising k domains as the hypotheses.', 'we train the shortlister']","['take a hypothesis reranking approach, which is widely used in large - scale domain classification for higher scalabil - ity [ 14,  #TAUTHOR_TAG.', 'within the approach, a shortlister, which is a light - weighted domain classifier, suggests the most promising k domains as the hypotheses.', 'we train the shortlister along with the added pseudo labels, leveraging negative system responses, and self - distillation, which are described in section 3. then a hypothesis reranker selects the final prediction from the k hypotheses enriched with additional input features, which is described in section 4']",3
['that is similar to  #TAUTHOR_TAG'],['that is similar to  #TAUTHOR_TAG'],"['of the hypothesis reranker that is similar to  #TAUTHOR_TAG.', 'first,']",[' #TAUTHOR_TAG'],3
['that is similar to  #TAUTHOR_TAG'],['that is similar to  #TAUTHOR_TAG'],"['of the hypothesis reranker that is similar to  #TAUTHOR_TAG.', 'first,']",[' #TAUTHOR_TAG'],3
"['higher scalabil - ity [ 14,  #TAUTHOR_TAG.', 'within']","['higher scalabil - ity [ 14,  #TAUTHOR_TAG.', 'within']","['higher scalabil - ity [ 14,  #TAUTHOR_TAG.', 'within the approach, a shortlister, which is a light - weighted domain classifier, suggests the most promising k domains as the hypotheses.', 'we train the shortlister']","['take a hypothesis reranking approach, which is widely used in large - scale domain classification for higher scalabil - ity [ 14,  #TAUTHOR_TAG.', 'within the approach, a shortlister, which is a light - weighted domain classifier, suggests the most promising k domains as the hypotheses.', 'we train the shortlister along with the added pseudo labels, leveraging negative system responses, and self - distillation, which are described in section 3. then a hypothesis reranker selects the final prediction from the k hypotheses enriched with additional input features, which is described in section 4']",5
"['the model training similarly to  #TAUTHOR_TAG and [ 18 ].', 'for example, given "" ask { ambientsounds } to { play thunderstorm sound } "",']","['the model training similarly to  #TAUTHOR_TAG and [ 18 ].', 'for example, given "" ask { ambientsounds } to { play thunderstorm sound } "",']","['the model training similarly to  #TAUTHOR_TAG and [ 18 ].', 'for example, given "" ask { ambientsounds } to { play thunderstorm sound } "", we extract']","['utilize utterances with explicit invocation patterns from an intelligent conversational system for the model training similarly to  #TAUTHOR_TAG and [ 18 ].', 'for example, given "" ask { ambientsounds } to { play thunderstorm sound } "", we extract "" play thunderstorm "" as the input utterance and ambient sounds as the ground - truth.', 'one difference from the previous work is that we utilize utterances with positive system responses as the positive train set and the dev set, and use those with the negative responses as the negative train set as described in section 3. 2.', 'we have extracted 3m positive train, 400k negative train, and 600k dev sets from 4m log data with 2, 500 most frequent domains as the ground - truths.', 'pseudo labels are added to 53k out of 3m in the positive train set as described in section 3. 1.', 'for the evaluation, we have extracted 10k random utterances from the user log data and independent annotators labeled the top three predictions of all the evaluated models for each utterance so that we can correctly compute ndcg at rank position 3.', 'table 1 shows the evaluation results of the shortlister and the hypothesis reranker with the proposed approaches.', '']",5
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['english  #TAUTHOR_TAG,']","['', 'syntax - based pre - ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as english - french ( xia and mc  #AUTHOR_TAG, german - english  #AUTHOR_TAG, chinese - english  #TAUTHOR_TAG, and english - japanese  #AUTHOR_TAG.', 'as a kind of constituent structure, hpsg  #AUTHOR_TAG parsing - based pre - ordering showed improvements in svo - sov translations, such as english - japanese  #AUTHOR_TAG and chinese - japanese  #AUTHOR_TAG.', 'since dependency parsing is more concise than constituent parsing in describing sentences, some research has used dependency parsing in pre - ordering approaches for language pairs such as arabic - english  #AUTHOR_TAG, and english - sov languages  #AUTHOR_TAG katz -  #AUTHOR_TAG.', 'the pre - ordering rules can be made manually  #TAUTHOR_TAG or extracted automatically from a parallel corpus ( xia and mc  #AUTHOR_TAG.', 'the purpose of this paper is to introduce a novel dependency - based pre - ordering approach through creating a pre - ordering rule set and applying it to the chinese - english pbsmt system.', 'experiment results showed that our pre - ordering rule set improved the bleu score on the nist 2006 evaluation data by 1. 61.', '']",0
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['english  #TAUTHOR_TAG,']","['', 'syntax - based pre - ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as english - french ( xia and mc  #AUTHOR_TAG, german - english  #AUTHOR_TAG, chinese - english  #TAUTHOR_TAG, and english - japanese  #AUTHOR_TAG.', 'as a kind of constituent structure, hpsg  #AUTHOR_TAG parsing - based pre - ordering showed improvements in svo - sov translations, such as english - japanese  #AUTHOR_TAG and chinese - japanese  #AUTHOR_TAG.', 'since dependency parsing is more concise than constituent parsing in describing sentences, some research has used dependency parsing in pre - ordering approaches for language pairs such as arabic - english  #AUTHOR_TAG, and english - sov languages  #AUTHOR_TAG katz -  #AUTHOR_TAG.', 'the pre - ordering rules can be made manually  #TAUTHOR_TAG or extracted automatically from a parallel corpus ( xia and mc  #AUTHOR_TAG.', 'the purpose of this paper is to introduce a novel dependency - based pre - ordering approach through creating a pre - ordering rule set and applying it to the chinese - english pbsmt system.', 'experiment results showed that our pre - ordering rule set improved the bleu score on the nist 2006 evaluation data by 1. 61.', '']",0
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['english  #TAUTHOR_TAG,']","['', 'syntax - based pre - ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as english - french ( xia and mc  #AUTHOR_TAG, german - english  #AUTHOR_TAG, chinese - english  #TAUTHOR_TAG, and english - japanese  #AUTHOR_TAG.', 'as a kind of constituent structure, hpsg  #AUTHOR_TAG parsing - based pre - ordering showed improvements in svo - sov translations, such as english - japanese  #AUTHOR_TAG and chinese - japanese  #AUTHOR_TAG.', 'since dependency parsing is more concise than constituent parsing in describing sentences, some research has used dependency parsing in pre - ordering approaches for language pairs such as arabic - english  #AUTHOR_TAG, and english - sov languages  #AUTHOR_TAG katz -  #AUTHOR_TAG.', 'the pre - ordering rules can be made manually  #TAUTHOR_TAG or extracted automatically from a parallel corpus ( xia and mc  #AUTHOR_TAG.', 'the purpose of this paper is to introduce a novel dependency - based pre - ordering approach through creating a pre - ordering rule set and applying it to the chinese - english pbsmt system.', 'experiment results showed that our pre - ordering rule set improved the bleu score on the nist 2006 evaluation data by 1. 61.', '']",1
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['english  #TAUTHOR_TAG,']","['', 'syntax - based pre - ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as english - french ( xia and mc  #AUTHOR_TAG, german - english  #AUTHOR_TAG, chinese - english  #TAUTHOR_TAG, and english - japanese  #AUTHOR_TAG.', 'as a kind of constituent structure, hpsg  #AUTHOR_TAG parsing - based pre - ordering showed improvements in svo - sov translations, such as english - japanese  #AUTHOR_TAG and chinese - japanese  #AUTHOR_TAG.', 'since dependency parsing is more concise than constituent parsing in describing sentences, some research has used dependency parsing in pre - ordering approaches for language pairs such as arabic - english  #AUTHOR_TAG, and english - sov languages  #AUTHOR_TAG katz -  #AUTHOR_TAG.', 'the pre - ordering rules can be made manually  #TAUTHOR_TAG or extracted automatically from a parallel corpus ( xia and mc  #AUTHOR_TAG.', 'the purpose of this paper is to introduce a novel dependency - based pre - ordering approach through creating a pre - ordering rule set and applying it to the chinese - english pbsmt system.', 'experiment results showed that our pre - ordering rule set improved the bleu score on the nist 2006 evaluation data by 1. 61.', '']",1
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['english  #TAUTHOR_TAG,']","['', 'syntax - based pre - ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as english - french ( xia and mc  #AUTHOR_TAG, german - english  #AUTHOR_TAG, chinese - english  #TAUTHOR_TAG, and english - japanese  #AUTHOR_TAG.', 'as a kind of constituent structure, hpsg  #AUTHOR_TAG parsing - based pre - ordering showed improvements in svo - sov translations, such as english - japanese  #AUTHOR_TAG and chinese - japanese  #AUTHOR_TAG.', 'since dependency parsing is more concise than constituent parsing in describing sentences, some research has used dependency parsing in pre - ordering approaches for language pairs such as arabic - english  #AUTHOR_TAG, and english - sov languages  #AUTHOR_TAG katz -  #AUTHOR_TAG.', 'the pre - ordering rules can be made manually  #TAUTHOR_TAG or extracted automatically from a parallel corpus ( xia and mc  #AUTHOR_TAG.', 'the purpose of this paper is to introduce a novel dependency - based pre - ordering approach through creating a pre - ordering rule set and applying it to the chinese - english pbsmt system.', 'experiment results showed that our pre - ordering rule set improved the bleu score on the nist 2006 evaluation data by 1. 61.', '']",4
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['english  #TAUTHOR_TAG,']","['', 'syntax - based pre - ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as english - french ( xia and mc  #AUTHOR_TAG, german - english  #AUTHOR_TAG, chinese - english  #TAUTHOR_TAG, and english - japanese  #AUTHOR_TAG.', 'as a kind of constituent structure, hpsg  #AUTHOR_TAG parsing - based pre - ordering showed improvements in svo - sov translations, such as english - japanese  #AUTHOR_TAG and chinese - japanese  #AUTHOR_TAG.', 'since dependency parsing is more concise than constituent parsing in describing sentences, some research has used dependency parsing in pre - ordering approaches for language pairs such as arabic - english  #AUTHOR_TAG, and english - sov languages  #AUTHOR_TAG katz -  #AUTHOR_TAG.', 'the pre - ordering rules can be made manually  #TAUTHOR_TAG or extracted automatically from a parallel corpus ( xia and mc  #AUTHOR_TAG.', 'the purpose of this paper is to introduce a novel dependency - based pre - ordering approach through creating a pre - ordering rule set and applying it to the chinese - english pbsmt system.', 'experiment results showed that our pre - ordering rule set improved the bleu score on the nist 2006 evaluation data by 1. 61.', '']",4
"['. similar to  #TAUTHOR_TAG, we']","['. similar to  #TAUTHOR_TAG, we']","['about more incorrect pre - orderings. similar to  #TAUTHOR_TAG, we carried out human evaluations to assess the accuracy', '']","['', 'conduct long - distance reordering. in this case, the affect of the performance of the constituent parsers on pre - ordering is larger than that of the dependency ones so that', 'the constituent parsers are likely to bring about more incorrect pre - orderings. similar to  #TAUTHOR_TAG, we carried out human evaluations to assess the accuracy', 'of our dependency - based pre - ordering rules by employing the system "" our dep 2', '"" in table 1. the evaluation set contained 200 sentences randomly selected from the development set. among them, 107 sentences contained at least one rule and the', 'rules were applied 185 times totally. since the accuracy check for dependency parse trees took great deal of time, we did not try to select error free ( 100', '% accurately parsed ) sentences. a bilingual speaker of chinese and english looked at an original chinese phrase and the pre - ordered one with their corresponding english phrase and judged whether the', '']",4
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['english  #TAUTHOR_TAG,']","['', 'syntax - based pre - ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as english - french ( xia and mc  #AUTHOR_TAG, german - english  #AUTHOR_TAG, chinese - english  #TAUTHOR_TAG, and english - japanese  #AUTHOR_TAG.', 'as a kind of constituent structure, hpsg  #AUTHOR_TAG parsing - based pre - ordering showed improvements in svo - sov translations, such as english - japanese  #AUTHOR_TAG and chinese - japanese  #AUTHOR_TAG.', 'since dependency parsing is more concise than constituent parsing in describing sentences, some research has used dependency parsing in pre - ordering approaches for language pairs such as arabic - english  #AUTHOR_TAG, and english - sov languages  #AUTHOR_TAG katz -  #AUTHOR_TAG.', 'the pre - ordering rules can be made manually  #TAUTHOR_TAG or extracted automatically from a parallel corpus ( xia and mc  #AUTHOR_TAG.', 'the purpose of this paper is to introduce a novel dependency - based pre - ordering approach through creating a pre - ordering rule set and applying it to the chinese - english pbsmt system.', 'experiment results showed that our pre - ordering rule set improved the bleu score on the nist 2006 evaluation data by 1. 61.', '']",3
"['. similar to  #TAUTHOR_TAG, we']","['. similar to  #TAUTHOR_TAG, we']","['about more incorrect pre - orderings. similar to  #TAUTHOR_TAG, we carried out human evaluations to assess the accuracy', '']","['', 'conduct long - distance reordering. in this case, the affect of the performance of the constituent parsers on pre - ordering is larger than that of the dependency ones so that', 'the constituent parsers are likely to bring about more incorrect pre - orderings. similar to  #TAUTHOR_TAG, we carried out human evaluations to assess the accuracy', 'of our dependency - based pre - ordering rules by employing the system "" our dep 2', '"" in table 1. the evaluation set contained 200 sentences randomly selected from the development set. among them, 107 sentences contained at least one rule and the', 'rules were applied 185 times totally. since the accuracy check for dependency parse trees took great deal of time, we did not try to select error free ( 100', '% accurately parsed ) sentences. a bilingual speaker of chinese and english looked at an original chinese phrase and the pre - ordered one with their corresponding english phrase and judged whether the', '']",3
"['. similar to  #TAUTHOR_TAG, we']","['. similar to  #TAUTHOR_TAG, we']","['about more incorrect pre - orderings. similar to  #TAUTHOR_TAG, we carried out human evaluations to assess the accuracy', '']","['', 'conduct long - distance reordering. in this case, the affect of the performance of the constituent parsers on pre - ordering is larger than that of the dependency ones so that', 'the constituent parsers are likely to bring about more incorrect pre - orderings. similar to  #TAUTHOR_TAG, we carried out human evaluations to assess the accuracy', 'of our dependency - based pre - ordering rules by employing the system "" our dep 2', '"" in table 1. the evaluation set contained 200 sentences randomly selected from the development set. among them, 107 sentences contained at least one rule and the', 'rules were applied 185 times totally. since the accuracy check for dependency parse trees took great deal of time, we did not try to select error free ( 100', '% accurately parsed ) sentences. a bilingual speaker of chinese and english looked at an original chinese phrase and the pre - ordered one with their corresponding english phrase and judged whether the', '']",3
"['. similar to  #TAUTHOR_TAG, we']","['. similar to  #TAUTHOR_TAG, we']","['about more incorrect pre - orderings. similar to  #TAUTHOR_TAG, we carried out human evaluations to assess the accuracy', '']","['', 'conduct long - distance reordering. in this case, the affect of the performance of the constituent parsers on pre - ordering is larger than that of the dependency ones so that', 'the constituent parsers are likely to bring about more incorrect pre - orderings. similar to  #TAUTHOR_TAG, we carried out human evaluations to assess the accuracy', 'of our dependency - based pre - ordering rules by employing the system "" our dep 2', '"" in table 1. the evaluation set contained 200 sentences randomly selected from the development set. among them, 107 sentences contained at least one rule and the', 'rules were applied 185 times totally. since the accuracy check for dependency parse trees took great deal of time, we did not try to select error free ( 100', '% accurately parsed ) sentences. a bilingual speaker of chinese and english looked at an original chinese phrase and the pre - ordered one with their corresponding english phrase and judged whether the', '']",3
"['. similar to  #TAUTHOR_TAG, we']","['. similar to  #TAUTHOR_TAG, we']","['about more incorrect pre - orderings. similar to  #TAUTHOR_TAG, we carried out human evaluations to assess the accuracy', '']","['', 'conduct long - distance reordering. in this case, the affect of the performance of the constituent parsers on pre - ordering is larger than that of the dependency ones so that', 'the constituent parsers are likely to bring about more incorrect pre - orderings. similar to  #TAUTHOR_TAG, we carried out human evaluations to assess the accuracy', 'of our dependency - based pre - ordering rules by employing the system "" our dep 2', '"" in table 1. the evaluation set contained 200 sentences randomly selected from the development set. among them, 107 sentences contained at least one rule and the', 'rules were applied 185 times totally. since the accuracy check for dependency parse trees took great deal of time, we did not try to select error free ( 100', '% accurately parsed ) sentences. a bilingual speaker of chinese and english looked at an original chinese phrase and the pre - ordered one with their corresponding english phrase and judged whether the', '']",3
"['. similar to  #TAUTHOR_TAG, we']","['. similar to  #TAUTHOR_TAG, we']","['about more incorrect pre - orderings. similar to  #TAUTHOR_TAG, we carried out human evaluations to assess the accuracy', '']","['', 'conduct long - distance reordering. in this case, the affect of the performance of the constituent parsers on pre - ordering is larger than that of the dependency ones so that', 'the constituent parsers are likely to bring about more incorrect pre - orderings. similar to  #TAUTHOR_TAG, we carried out human evaluations to assess the accuracy', 'of our dependency - based pre - ordering rules by employing the system "" our dep 2', '"" in table 1. the evaluation set contained 200 sentences randomly selected from the development set. among them, 107 sentences contained at least one rule and the', 'rules were applied 185 times totally. since the accuracy check for dependency parse trees took great deal of time, we did not try to select error free ( 100', '% accurately parsed ) sentences. a bilingual speaker of chinese and english looked at an original chinese phrase and the pre - ordered one with their corresponding english phrase and judged whether the', '']",5
"['. similar to  #TAUTHOR_TAG, we']","['. similar to  #TAUTHOR_TAG, we']","['about more incorrect pre - orderings. similar to  #TAUTHOR_TAG, we carried out human evaluations to assess the accuracy', '']","['', 'conduct long - distance reordering. in this case, the affect of the performance of the constituent parsers on pre - ordering is larger than that of the dependency ones so that', 'the constituent parsers are likely to bring about more incorrect pre - orderings. similar to  #TAUTHOR_TAG, we carried out human evaluations to assess the accuracy', 'of our dependency - based pre - ordering rules by employing the system "" our dep 2', '"" in table 1. the evaluation set contained 200 sentences randomly selected from the development set. among them, 107 sentences contained at least one rule and the', 'rules were applied 185 times totally. since the accuracy check for dependency parse trees took great deal of time, we did not try to select error free ( 100', '% accurately parsed ) sentences. a bilingual speaker of chinese and english looked at an original chinese phrase and the pre - ordered one with their corresponding english phrase and judged whether the', '']",5
"['learning  #TAUTHOR_TAG. the idea is', '']","['( nivre and mc', ' #AUTHOR_TAG. the second approach is based on multi - view learning  #TAUTHOR_TAG. the idea is', '']","['learning  #TAUTHOR_TAG. the idea is', '']","['', 'features. this method has been used for leveraging two different treebanks for word segmentation  #AUTHOR_TAG and dependency parsing ( nivre and mc', ' #AUTHOR_TAG. the second approach is based on multi - view learning  #TAUTHOR_TAG. the idea is', 'to address both annotation styles simultaneously by sharing common feature representations. in particular,  #AUTHOR_TAG trained', 'dependency parsers using the domain adaptation method of daume iii ( 2007 ), keeping a copy', 'of shared features and a separate copy of features for each treebank.  #AUTHOR_TAG trained pos tag', '##gers by coupling the labelsets from two different treebank', '##s into a single combined labelset.', 'a summary of such multi - view methods is shown in figure 1 ( b ), which demonstrates their main differences compared to stacking ( figure 1 ( a ) ). recently, neural network has gained increasing research attention', ', with highly competitive results being reported for numerous nlp tasks, including word segmentation  #AUTHOR_TAG pos - tagging  #AUTHOR_TAG, and parsing  #AUTHOR_TAG. on', 'the other hand, the aforementioned methods on heterogeneous annotations are investigated', 'mainly for discrete models. it remains an interesting research question how effective multiple tree', '##banks can be utilized by neural nlp models, and we aim to investigate this empirically. we follow  #TAUTHOR_TAG as the discrete stacking and multi - view training baselines, respectively, and building', 'neural network counterparts to their models for empirical comparison. the base tagger is a neural crf model  #AUTHOR_TAG, which gives', 'competitive accuracies to discrete crf taggers. results show that neural stacking allows deeper integration of the source model beyond one - best outputs, and further the fine - tuning of', 'the source model during the target model training. in addition, the advantage of neural multi - view learning over its discrete counterpart are many - fold. first, it is free from', 'the necessity of manual cross - labelset interactive feature engineering, which is far from trivial for', 'representing annotation correspondence  #TAUTHOR_TAG. second, compared to discrete model, parameter sharing in deep neural network eliminates the issue of exponential growth of search space, and', 'allows separated training of each label type, in the same way as multi - task learning  #AUTHOR_TAG. our neural multi - view learning model achieves not only', 'better accuracy improvements, but also an order of magnitude faster speed compared to its discrete baseline, adding little time', 'cost compared to a neural model trained on a single treebank. the c + + implementations of our neural network stacking and multi - view learning models are available under gpl', ', at https : / / github. com / chenhongshen / nnhetseq', '']",1
"['learning  #TAUTHOR_TAG. the idea is', '']","['( nivre and mc', ' #AUTHOR_TAG. the second approach is based on multi - view learning  #TAUTHOR_TAG. the idea is', '']","['learning  #TAUTHOR_TAG. the idea is', '']","['', 'features. this method has been used for leveraging two different treebanks for word segmentation  #AUTHOR_TAG and dependency parsing ( nivre and mc', ' #AUTHOR_TAG. the second approach is based on multi - view learning  #TAUTHOR_TAG. the idea is', 'to address both annotation styles simultaneously by sharing common feature representations. in particular,  #AUTHOR_TAG trained', 'dependency parsers using the domain adaptation method of daume iii ( 2007 ), keeping a copy', 'of shared features and a separate copy of features for each treebank.  #AUTHOR_TAG trained pos tag', '##gers by coupling the labelsets from two different treebank', '##s into a single combined labelset.', 'a summary of such multi - view methods is shown in figure 1 ( b ), which demonstrates their main differences compared to stacking ( figure 1 ( a ) ). recently, neural network has gained increasing research attention', ', with highly competitive results being reported for numerous nlp tasks, including word segmentation  #AUTHOR_TAG pos - tagging  #AUTHOR_TAG, and parsing  #AUTHOR_TAG. on', 'the other hand, the aforementioned methods on heterogeneous annotations are investigated', 'mainly for discrete models. it remains an interesting research question how effective multiple tree', '##banks can be utilized by neural nlp models, and we aim to investigate this empirically. we follow  #TAUTHOR_TAG as the discrete stacking and multi - view training baselines, respectively, and building', 'neural network counterparts to their models for empirical comparison. the base tagger is a neural crf model  #AUTHOR_TAG, which gives', 'competitive accuracies to discrete crf taggers. results show that neural stacking allows deeper integration of the source model beyond one - best outputs, and further the fine - tuning of', 'the source model during the target model training. in addition, the advantage of neural multi - view learning over its discrete counterpart are many - fold. first, it is free from', 'the necessity of manual cross - labelset interactive feature engineering, which is far from trivial for', 'representing annotation correspondence  #TAUTHOR_TAG. second, compared to discrete model, parameter sharing in deep neural network eliminates the issue of exponential growth of search space, and', 'allows separated training of each label type, in the same way as multi - task learning  #AUTHOR_TAG. our neural multi - view learning model achieves not only', 'better accuracy improvements, but also an order of magnitude faster speed compared to its discrete baseline, adding little time', 'cost compared to a neural model trained on a single treebank. the c + + implementations of our neural network stacking and multi - view learning models are available under gpl', ', at https : / / github. com / chenhongshen / nnhetseq', '']",5
"['learning  #TAUTHOR_TAG. the idea is', '']","['( nivre and mc', ' #AUTHOR_TAG. the second approach is based on multi - view learning  #TAUTHOR_TAG. the idea is', '']","['learning  #TAUTHOR_TAG. the idea is', '']","['', 'features. this method has been used for leveraging two different treebanks for word segmentation  #AUTHOR_TAG and dependency parsing ( nivre and mc', ' #AUTHOR_TAG. the second approach is based on multi - view learning  #TAUTHOR_TAG. the idea is', 'to address both annotation styles simultaneously by sharing common feature representations. in particular,  #AUTHOR_TAG trained', 'dependency parsers using the domain adaptation method of daume iii ( 2007 ), keeping a copy', 'of shared features and a separate copy of features for each treebank.  #AUTHOR_TAG trained pos tag', '##gers by coupling the labelsets from two different treebank', '##s into a single combined labelset.', 'a summary of such multi - view methods is shown in figure 1 ( b ), which demonstrates their main differences compared to stacking ( figure 1 ( a ) ). recently, neural network has gained increasing research attention', ', with highly competitive results being reported for numerous nlp tasks, including word segmentation  #AUTHOR_TAG pos - tagging  #AUTHOR_TAG, and parsing  #AUTHOR_TAG. on', 'the other hand, the aforementioned methods on heterogeneous annotations are investigated', 'mainly for discrete models. it remains an interesting research question how effective multiple tree', '##banks can be utilized by neural nlp models, and we aim to investigate this empirically. we follow  #TAUTHOR_TAG as the discrete stacking and multi - view training baselines, respectively, and building', 'neural network counterparts to their models for empirical comparison. the base tagger is a neural crf model  #AUTHOR_TAG, which gives', 'competitive accuracies to discrete crf taggers. results show that neural stacking allows deeper integration of the source model beyond one - best outputs, and further the fine - tuning of', 'the source model during the target model training. in addition, the advantage of neural multi - view learning over its discrete counterpart are many - fold. first, it is free from', 'the necessity of manual cross - labelset interactive feature engineering, which is far from trivial for', 'representing annotation correspondence  #TAUTHOR_TAG. second, compared to discrete model, parameter sharing in deep neural network eliminates the issue of exponential growth of search space, and', 'allows separated training of each label type, in the same way as multi - task learning  #AUTHOR_TAG. our neural multi - view learning model achieves not only', 'better accuracy improvements, but also an order of magnitude faster speed compared to its discrete baseline, adding little time', 'cost compared to a neural model trained on a single treebank. the c + + implementations of our neural network stacking and multi - view learning models are available under gpl', ', at https : / / github. com / chenhongshen / nnhetseq', '']",5
"['1 ( b ), multi - view learning  #TAUTHOR_TAG utilizes corpus a and corpus b simultaneously for training.', 'the coupled tag']","['1 ( b ), multi - view learning  #TAUTHOR_TAG utilizes corpus a and corpus b simultaneously for training.', 'the coupled tagger directly learns the logistic correspondences']","['1 ( b ), multi - view learning  #TAUTHOR_TAG utilizes corpus a and corpus b simultaneously for training.', 'the coupled tagger directly learns the logistic correspondences']","['shown in figure 1 ( b ), multi - view learning  #TAUTHOR_TAG utilizes corpus a and corpus b simultaneously for training.', 'the coupled tagger directly learns the logistic correspondences between both corpora, therefore can lead a more comprehensive usage of corpus a compared with stacking.', 'in order to better capture such correlation, specifically designed feature templates between two tag sets are essential.', 'for each training instances, both a and b labels are needed.', 'however, one type of tag is missing.', ' #AUTHOR_TAG used a mapping function to supplement the missing annotations with the help of the annotated tag.', 'the result is a set of sentence with bundled tags in both annotations, but with ambiguities on one side, due to one - to - many mappings.', ' #AUTHOR_TAG showed that speed can be significantly improved by manually restricting possible mappings between the labelsets, but a full mapping without restriction yields the highest accuracies']",5
"['from corpus a and the b tagger is randomly initialized.', 'for neural multi - view model, we follow  #TAUTHOR_TAG and']","['a tagger has been pretrained from corpus a and the b tagger is randomly initialized.', 'for neural multi - view model, we follow  #TAUTHOR_TAG and']","['the a tagger has been pretrained from corpus a and the b tagger is randomly initialized.', 'for neural multi - view model, we follow  #TAUTHOR_TAG and']","[': two training datasets : where y is the model output, s ( y | x ) = logp ( y | x ) is the log probability of y and δ ( y, y d ) is the hamming distance between y and y d.', 'we adopt online learning, updating parameters using adagrad  #AUTHOR_TAG.', 'to train the neural stacking model, we first train a base tagger using corpus a. then, we train the stacked tagger with corpus b, where the parameters of the a tagger has been pretrained from corpus a and the b tagger is randomly initialized.', 'for neural multi - view model, we follow  #TAUTHOR_TAG and take a the corpus - weighting strategy to sample a number of training instances from both corpora for each training iteration, as shown in algorithm 1.', 'at each epoch, we randomly sample from the two datasets according to a corpus weights ratio, namely the ratio between the number of sentences in each dataset used for training, to form a training set for the epoch']",5
"[')  #AUTHOR_TAG as our main corpus, with the standard data split following previous work  #TAUTHOR_TAG.', ""people's daily ( pd ) is used as second corpus with a different scheme."", 'we']","['adopt the penn chinese treebank version 5. 0 ( ctb5 )  #AUTHOR_TAG as our main corpus, with the standard data split following previous work  #TAUTHOR_TAG.', ""people's daily ( pd ) is used as second corpus with a different scheme."", 'we']","[')  #AUTHOR_TAG as our main corpus, with the standard data split following previous work  #TAUTHOR_TAG.', ""people's daily ( pd ) is used as second corpus with a different scheme."", 'we filter out pd sentences longer than 200 words.', 'details of the datasets are listed in table 1.', 'the standard token - wise pos tagging']","['adopt the penn chinese treebank version 5. 0 ( ctb5 )  #AUTHOR_TAG as our main corpus, with the standard data split following previous work  #TAUTHOR_TAG.', ""people's daily ( pd ) is used as second corpus with a different scheme."", 'we filter out pd sentences longer than 200 words.', 'details of the datasets are listed in table 1.', 'the standard token - wise pos tagging accuracy is used as the evaluation metric.', 'the systems are implemented with libn3l.', 'for all the neural models, we set the hidden layer size to 100, the initial learning rate for adagrad to 0. 01 and the regularization parameter λ to 10 −8.', 'word2vec 1 is used to pretrain word embeddings.', 'the chinese giga - word corpus version 5  #AUTHOR_TAG, segmented by zpar 2  #AUTHOR_TAG, is used for the training corpus for word embeddings.', 'the size of word embedding is 50']",5
['crf baseline  #TAUTHOR_TAG 95'],['crf baseline  #TAUTHOR_TAG 95'],"['remaining experiments, which strikes the balance between over - system accuracy crf baseline  #TAUTHOR_TAG 95']","['', '##fitting. as a result, we choose a dropout rate of 20 % for', 'the remaining experiments, which strikes the balance between over - system accuracy crf baseline  #TAUTHOR_TAG 95 table 2 : accuracies on ctb - test. fitting and underfitting. figure 5 also', '']",5
['crf baseline  #TAUTHOR_TAG 95'],['crf baseline  #TAUTHOR_TAG 95'],"['remaining experiments, which strikes the balance between over - system accuracy crf baseline  #TAUTHOR_TAG 95']","['', '##fitting. as a result, we choose a dropout rate of 20 % for', 'the remaining experiments, which strikes the balance between over - system accuracy crf baseline  #TAUTHOR_TAG 95 table 2 : accuracies on ctb - test. fitting and underfitting. figure 5 also', '']",5
"['of neural and discrete multi - view training by running our models and the model of  #TAUTHOR_TAG 4 with default configurations on the ctb5 training data.', 'the crf baseline is adapted from  #TAUTHOR_TAG.', 'all the systems are implemented in c + + running on']","['of neural and discrete multi - view training by running our models and the model of  #TAUTHOR_TAG 4 with default configurations on the ctb5 training data.', 'the crf baseline is adapted from  #TAUTHOR_TAG.', 'all the systems are implemented in c + + running on']","['compare the efficiencies of neural and discrete multi - view training by running our models and the model of  #TAUTHOR_TAG 4 with default configurations on the ctb5 training data.', 'the crf baseline is adapted from  #TAUTHOR_TAG.', 'all the systems are implemented in c + + running on']","['compare the efficiencies of neural and discrete multi - view training by running our models and the model of  #TAUTHOR_TAG 4 with default configurations on the ctb5 training data.', 'the crf baseline is adapted from  #TAUTHOR_TAG.', 'all the systems are implemented in c + + running on an intel e5 - 1620 cpu.', 'the results are shown in table 3.', 'the nn baseline model is slower than the crf baseline model.', 'this is due to the higher computation cost of a deep neural network on a cpu.', 'compared with the crf baseline, the crf multi - view model is significantly slower because of its large feature set and the multi - label search space.', 'however, the nn multi - view model achieves almost the same time cost with the nn baseline, and is much more efficient than the crf counterpart.', 'this shows the efficiency advantage of the nn multi - view model by parameter sharing and output splitting']",5
"['learning  #TAUTHOR_TAG. the idea is', '']","['( nivre and mc', ' #AUTHOR_TAG. the second approach is based on multi - view learning  #TAUTHOR_TAG. the idea is', '']","['learning  #TAUTHOR_TAG. the idea is', '']","['', 'features. this method has been used for leveraging two different treebanks for word segmentation  #AUTHOR_TAG and dependency parsing ( nivre and mc', ' #AUTHOR_TAG. the second approach is based on multi - view learning  #TAUTHOR_TAG. the idea is', 'to address both annotation styles simultaneously by sharing common feature representations. in particular,  #AUTHOR_TAG trained', 'dependency parsers using the domain adaptation method of daume iii ( 2007 ), keeping a copy', 'of shared features and a separate copy of features for each treebank.  #AUTHOR_TAG trained pos tag', '##gers by coupling the labelsets from two different treebank', '##s into a single combined labelset.', 'a summary of such multi - view methods is shown in figure 1 ( b ), which demonstrates their main differences compared to stacking ( figure 1 ( a ) ). recently, neural network has gained increasing research attention', ', with highly competitive results being reported for numerous nlp tasks, including word segmentation  #AUTHOR_TAG pos - tagging  #AUTHOR_TAG, and parsing  #AUTHOR_TAG. on', 'the other hand, the aforementioned methods on heterogeneous annotations are investigated', 'mainly for discrete models. it remains an interesting research question how effective multiple tree', '##banks can be utilized by neural nlp models, and we aim to investigate this empirically. we follow  #TAUTHOR_TAG as the discrete stacking and multi - view training baselines, respectively, and building', 'neural network counterparts to their models for empirical comparison. the base tagger is a neural crf model  #AUTHOR_TAG, which gives', 'competitive accuracies to discrete crf taggers. results show that neural stacking allows deeper integration of the source model beyond one - best outputs, and further the fine - tuning of', 'the source model during the target model training. in addition, the advantage of neural multi - view learning over its discrete counterpart are many - fold. first, it is free from', 'the necessity of manual cross - labelset interactive feature engineering, which is far from trivial for', 'representing annotation correspondence  #TAUTHOR_TAG. second, compared to discrete model, parameter sharing in deep neural network eliminates the issue of exponential growth of search space, and', 'allows separated training of each label type, in the same way as multi - task learning  #AUTHOR_TAG. our neural multi - view learning model achieves not only', 'better accuracy improvements, but also an order of magnitude faster speed compared to its discrete baseline, adding little time', 'cost compared to a neural model trained on a single treebank. the c + + implementations of our neural network stacking and multi - view learning models are available under gpl', ', at https : / / github. com / chenhongshen / nnhetseq', '']",0
['of 0. 90 brought by discrete method of  #TAUTHOR_TAG over its'],"['of 0. 90 brought by discrete method of  #TAUTHOR_TAG over its baseline, from 94. 10 to 95. 00.', 'nn multi - view training method gives relatively higher']",['of 0. 90 brought by discrete method of  #TAUTHOR_TAG over its'],"['', 'this shows that neural stacking is a preferred choice for stacking.', 'multi - view training.', 'with respect of the multiview training method, the nn model improves over the nn baseline from 94. 24 to 95. 40, by a margin of + 1. 16, which is higher than that of 0. 90 brought by discrete method of  #TAUTHOR_TAG over its baseline, from 94. 10 to 95. 00.', 'nn multi - view training method gives relatively higher improvements compared with nn stacking method.', 'this is consistent with the observation of  #TAUTHOR_TAG, who showed that discrete label coupling training gives slightly better improvement compared with discrete stacking.', 'the final accuracies of nn multi - view training is also higher than that of its crf counterpart']",4
['of 0. 90 brought by discrete method of  #TAUTHOR_TAG over its'],"['of 0. 90 brought by discrete method of  #TAUTHOR_TAG over its baseline, from 94. 10 to 95. 00.', 'nn multi - view training method gives relatively higher']",['of 0. 90 brought by discrete method of  #TAUTHOR_TAG over its'],"['', 'this shows that neural stacking is a preferred choice for stacking.', 'multi - view training.', 'with respect of the multiview training method, the nn model improves over the nn baseline from 94. 24 to 95. 40, by a margin of + 1. 16, which is higher than that of 0. 90 brought by discrete method of  #TAUTHOR_TAG over its baseline, from 94. 10 to 95. 00.', 'nn multi - view training method gives relatively higher improvements compared with nn stacking method.', 'this is consistent with the observation of  #TAUTHOR_TAG, who showed that discrete label coupling training gives slightly better improvement compared with discrete stacking.', 'the final accuracies of nn multi - view training is also higher than that of its crf counterpart']",3
"['of neural and discrete multi - view training by running our models and the model of  #TAUTHOR_TAG 4 with default configurations on the ctb5 training data.', 'the crf baseline is adapted from  #TAUTHOR_TAG.', 'all the systems are implemented in c + + running on']","['of neural and discrete multi - view training by running our models and the model of  #TAUTHOR_TAG 4 with default configurations on the ctb5 training data.', 'the crf baseline is adapted from  #TAUTHOR_TAG.', 'all the systems are implemented in c + + running on']","['compare the efficiencies of neural and discrete multi - view training by running our models and the model of  #TAUTHOR_TAG 4 with default configurations on the ctb5 training data.', 'the crf baseline is adapted from  #TAUTHOR_TAG.', 'all the systems are implemented in c + + running on']","['compare the efficiencies of neural and discrete multi - view training by running our models and the model of  #TAUTHOR_TAG 4 with default configurations on the ctb5 training data.', 'the crf baseline is adapted from  #TAUTHOR_TAG.', 'all the systems are implemented in c + + running on an intel e5 - 1620 cpu.', 'the results are shown in table 3.', 'the nn baseline model is slower than the crf baseline model.', 'this is due to the higher computation cost of a deep neural network on a cpu.', 'compared with the crf baseline, the crf multi - view model is significantly slower because of its large feature set and the multi - label search space.', 'however, the nn multi - view model achieves almost the same time cost with the nn baseline, and is much more efficient than the crf counterpart.', 'this shows the efficiency advantage of the nn multi - view model by parameter sharing and output splitting']",6
"['by  #TAUTHOR_TAG.', 'however,']","['by  #TAUTHOR_TAG.', 'however,']","['by  #TAUTHOR_TAG.', 'however,']","['now turn to the evaluation of comic - en on the texas corpus as it is a publicly available dataset.', 'as mentioned before, comic - en performs meaning comparison based on a system of categories while the texas system is a scoring approach, trying to predict a grade.', 'while the former is a classification task, the latter is better characterized as a regression problem because of the desired numerical outcome.', 'of course, one could simply pretend that individual grades are classes and treat scoring as a classification task.', ""however, a classification approach has no knowledge of numerical relationships, i. e., it does not'know'that 4 is a higher grade than 3 and a much higher grade than 1 ( assuming a 0 - 5 scale )."", 'as a result, if an evaluation metric such as pearson correlation is used, classification systems are at a disadvantage because some misclassifications are punished more than others.', 'we discuss this point further in section 4.', 'for these reasons, to obtain a more interesting comparison, we modified comic - en to perform scoring instead of meaning comparison.', 'this means that the memory - based learning approach comic - en had employed so far was no longer applicable and had to be replaced with a regression - capable learning strategy.', 'we chose support vector regression ( svr ) using libsvm 4 since that is one of the methods employed by  #TAUTHOR_TAG.', '']",6
"['.,', '). however, for the texas corpus,  #TAUTHOR_TAG have opted to use the arithmetic mean of the two']","['.,', '). however, for the texas corpus,  #TAUTHOR_TAG have opted to use the arithmetic mean of the two']","['is also supported by recent literature ( cf., e. g.,', '). however, for the texas corpus,  #TAUTHOR_TAG have opted to use the arithmetic mean of the two']","['', ', several metrics need to be reported. finally, an important point concerns the quality of gold standards. given the relatively low interannotator', 'agreement in the texas corpus ( r = 0. 586, rm se = 0.', '659 ) it seems fair to ask whether answers without perfect agreement should be used in training and testing systems at all. in the cre', '##e and creg corpora, answers with disagreement among the annotators have either been excluded from experiments or resolved by an additional judge. this approach is also supported by recent literature ( cf., e. g.,', '). however, for the texas corpus,  #TAUTHOR_TAG have opted to use the arithmetic mean of the two graders as gold standard. while mathematically a viable solution, it seems questionable', 'whether the mean is reliable with only two graders, especially if they have not operated on the grounds of explicit guidelines.', 'it would be interesting to see whether in this case, a system trained on more, singly annotated data would perform better than one on less, doubly annotated data', ', as argued for by  #AUTHOR_TAG. in any case, if many disagreements occur, one should ask the question whether the annotation task is defined well enough and', 'whether machines should really be expected to perform it consistently if humans have trouble doing so']",7
[' #TAUTHOR_TAG ; han et al'],"[' #TAUTHOR_TAG ; han et al., to appear ).', 'furthermore, social media posts']",['nlp tools over social media data  #TAUTHOR_TAG ; han et al'],"['', 'however, there have been recent successes in adapting parsers and pos taggers to social media data  #AUTHOR_TAG.', 'additionally, lexical normalisation and other preprocessing strategies have been shown to enhance the performance of nlp tools over social media data  #TAUTHOR_TAG ; han et al., to appear ).', 'furthermore, social media posts tend to be short and the content highly varied, meaning it is difficult to adapt a tool to the domain, or harness textual context to disambiguate the content.', 'there is also the engineering challenge of real - time processing of the text stream, as much of nlp research is carried out offline with only secondary concern for throughput.', 'as such, we might conclude that social media data is a foe of nlp, in that it challenges traditional assumptions made in nlp research on the nature of the target text and the requirements for real - time responsiveness.', 'however, if we look beyond the immediate text content of social media, we quickly realise that there are various non - textual data sources that can be used to enhance the robustness and accuracy of nlp models,']",0
[' #TAUTHOR_TAG will assign'],[' #TAUTHOR_TAG will assign'],[' #TAUTHOR_TAG will assign'],[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG will assign'],[' #TAUTHOR_TAG will assign'],[' #TAUTHOR_TAG will assign'],[' #TAUTHOR_TAG'],0
['model of  #TAUTHOR_TAG by jointly reasoning over the relations and entity types occurring in the paths'],['model of  #TAUTHOR_TAG by jointly reasoning over the relations and entity types occurring in the paths'],['paper extends the recurrent neural network model of  #TAUTHOR_TAG by jointly reasoning over the relations and entity types occurring in the paths'],"['paper extends the recurrent neural network model of  #TAUTHOR_TAG by jointly reasoning over the relations and entity types occurring in the paths between an entity pair.', '']",0
[' #TAUTHOR_TAG will assign'],[' #TAUTHOR_TAG will assign'],[' #TAUTHOR_TAG will assign'],[' #TAUTHOR_TAG'],6
['model of  #TAUTHOR_TAG by jointly reasoning over the relations and entity types occurring in the paths'],['model of  #TAUTHOR_TAG by jointly reasoning over the relations and entity types occurring in the paths'],['paper extends the recurrent neural network model of  #TAUTHOR_TAG by jointly reasoning over the relations and entity types occurring in the paths'],"['paper extends the recurrent neural network model of  #TAUTHOR_TAG by jointly reasoning over the relations and entity types occurring in the paths between an entity pair.', '']",6
['model of  #TAUTHOR_TAG by jointly reasoning over the relations and entity types occurring in the paths'],['model of  #TAUTHOR_TAG by jointly reasoning over the relations and entity types occurring in the paths'],['paper extends the recurrent neural network model of  #TAUTHOR_TAG by jointly reasoning over the relations and entity types occurring in the paths'],"['paper extends the recurrent neural network model of  #TAUTHOR_TAG by jointly reasoning over the relations and entity types occurring in the paths between an entity pair.', '']",6
"['for the ranking following previous work  #TAUTHOR_TAG.', '']","['for the ranking following previous work  #TAUTHOR_TAG.', '']","['( map ) score for the ranking following previous work  #TAUTHOR_TAG.', '']","['', 'we rank the entity pairs in the test set based on their scores and calculate the mean average precision ( map ) score for the ranking following previous work  #TAUTHOR_TAG.', '']",5
"['needed [ 13 ].', 'tasks can ask the user to refer to the whole image or to only certain parts or objects, and sometimes require them to annotate the regions of some relevant objects.', 'the images can be natural scenes taken by real people, or artificial scenes created with clip arts like in  #TAUTHOR_TAG']","['needed [ 13 ].', 'tasks can ask the user to refer to the whole image or to only certain parts or objects, and sometimes require them to annotate the regions of some relevant objects.', 'the images can be natural scenes taken by real people, or artificial scenes created with clip arts like in  #TAUTHOR_TAG']","['needed [ 13 ].', 'tasks can ask the user to refer to the whole image or to only certain parts or objects, and sometimes require them to annotate the regions of some relevant objects.', 'the images can be natural scenes taken by real people, or artificial scenes created with clip arts like in  #TAUTHOR_TAG']","['tasks can have the form of multiple choice questions ( closed ) or open response ones, or a combination of the two.', 'the first case includes choosing "" yes "" / "" no "" for a given option ( e. g. a pair of an action and an object ), choosing all true options from a given list ( e. g. given an object, pick its attributes or actions ) and more.', 'the second case includes supplying a full free - form sentence describing an image, picking free words to describe attributes or actions, or fill - in - the - blank a specific attribute or event [ 15 ].', 'a combined case can allowed the user to add their own option to the list if needed [ 13 ].', 'tasks can ask the user to refer to the whole image or to only certain parts or objects, and sometimes require them to annotate the regions of some relevant objects.', 'the images can be natural scenes taken by real people, or artificial scenes created with clip arts like in  #TAUTHOR_TAG']",3
['machine translation  #TAUTHOR_TAG has been receiving considerable'],['machine translation  #TAUTHOR_TAG has been receiving considerable'],"['machine translation  #TAUTHOR_TAG has been receiving considerable attention in the recent years, given']","['machine translation  #TAUTHOR_TAG has been receiving considerable attention in the recent years, given its superior performance without the demand of heavily hand crafted engineering efforts.', 'nmt often outperforms statistical machine translation ( smt ) techniques but it still struggles if the parallel data is insufficient like in the case of indian languages.', 'the bulk of research on low resource nmt has focused on exploiting monolingual data or parallel data from other language pairs.', 'some recent methods to improve nmt models that exploit monolingual data ranges from back - translation  #AUTHOR_TAG a ), dual nmt  #AUTHOR_TAG to unsupervised mt models  #AUTHOR_TAG.', 'transfer learning is also a promising approach for low resource nmt which exploits parallel data from other language pairs  #AUTHOR_TAG.', 'typically it is achieved by training a parent model in a high resource language pair, then using some of the trained weights as the initialization for a child model and further train it on the low - resource language pair.', 'other promising approach for improving translation performance for low resource languages is multilingual neural machine translation.', 'it has been shown that exploiting data from other language pairs & joint training helps in improving the translation performance of nmt models.', ' #AUTHOR_TAG.', 'this paper describes the nmt system of iiit - h for wmt19 evaluation.', 'we participated in the gujarati→english news translation task.', 'we used an attention - based encoder - decoder model as our baseline system and used byte pair encoding ( bpe ) to enable open vocabulary translation.', 'we then leverage hindi - english parallel corpus in a multilingual setting so as to improve our baseline system.', 'we basically combined hindi - english and gujarati - english parallel corpus and use it as our training corpus.', ""our multilingual system is similiar to  #AUTHOR_TAG but we don't use any artificial token at the start of source sentences to indicate the target language."", 'the reason is trivial, that is we have only english as our target language.', 'we also provide results of our experiments conducted post wmt19 shared task involving transformer models']",0
"[') as described in  #TAUTHOR_TAG.', 'the model directly estimates the posterior']","['( rnn ) as described in  #TAUTHOR_TAG.', 'the model directly estimates the posterior']","[') as described in  #TAUTHOR_TAG.', 'the model directly estimates the posterior distribution p θ ( y | x )']","['nmt model consists of an encoder and a decoder, each of which is a recurrent neural network ( rnn ) as described in  #TAUTHOR_TAG.', 'the model directly estimates the posterior distribution p θ ( y | x ) of translating a source sentence x = ( x 1,.., x n ) to a target sentence y = ( y 1,.., y m ) as :', '']",5
"[') as described in  #TAUTHOR_TAG.', 'the model directly estimates the posterior']","['( rnn ) as described in  #TAUTHOR_TAG.', 'the model directly estimates the posterior']","[') as described in  #TAUTHOR_TAG.', 'the model directly estimates the posterior distribution p θ ( y | x )']","['nmt model consists of an encoder and a decoder, each of which is a recurrent neural network ( rnn ) as described in  #TAUTHOR_TAG.', 'the model directly estimates the posterior distribution p θ ( y | x ) of translating a source sentence x = ( x 1,.., x n ) to a target sentence y = ( y 1,.., y m ) as :', '']",5
"[') as described in  #TAUTHOR_TAG.', 'the model directly estimates the posterior']","['( rnn ) as described in  #TAUTHOR_TAG.', 'the model directly estimates the posterior']","[') as described in  #TAUTHOR_TAG.', 'the model directly estimates the posterior distribution p θ ( y | x )']","['nmt model consists of an encoder and a decoder, each of which is a recurrent neural network ( rnn ) as described in  #TAUTHOR_TAG.', 'the model directly estimates the posterior distribution p θ ( y | x ) of translating a source sentence x = ( x 1,.., x n ) to a target sentence y = ( y 1,.., y m ) as :', '']",5
"['of our nmt model is same as in  #TAUTHOR_TAG, an rn']","['of our nmt model is same as in  #TAUTHOR_TAG, an rnn based encoder - decoder model with global attention mechanism.', 'we used an lstm based bi - directional encoder and a unidirectional decoder.', 'we kept 4 layers in']","['structure of our nmt model is same as in  #TAUTHOR_TAG, an rnn based encoder']","['structure of our nmt model is same as in  #TAUTHOR_TAG, an rnn based encoder - decoder model with global attention mechanism.', 'we used an lstm based bi - directional encoder and a unidirectional decoder.', 'we kept 4 layers in both the encoder & decoder with embedding size set to 512.', 'the batch size was set to 64 and a dropout rate of 0. 3.', 'we used adam optimizer  #AUTHOR_TAG for our experiments.', 'our multilingual model is trained with all the same hyperparameters as our baseline model except that the training data is a combination of hindi - english & gujarati - english parallel data']",5
['by  #TAUTHOR_TAG on speaker and'],['by  #TAUTHOR_TAG on speaker and'],[' #TAUTHOR_TAG on speaker and'],"['', 'evident in the relatively short and generic responses even though they generally capture the persona of the speaker. to overcome these limitations, we propose phredgan, a multi - modal hredgan', 'dialogue system which additionally conditions the adversarial framework proposed by  #TAUTHOR_TAG on speaker and / or utterance attributes', 'in order to maintain response quality of hredgan and still capture speaker and other modalities within a conversation. the attributes can be seen as another', 'input modality as the utterance. the attribute representation is an embedding that is learned together with the rest of model parameters, similar to [ 8 ]. the introduction of attributes allows the model to generate responses conditioned on particular attribute ( s )', 'across conversation turns. since the attributes are discrete, it also allows for exploring what - if scenarios of model responses. we train and sample the', 'proposed phredgan similar to the procedure for hredgan  #TAUTHOR_TAG.', 'to demonstrate model capability, we train on customer service related data such', 'as the ubuntu dialogue corpus ( udc ) that is strongly bimodal between', 'question poster and answerer, and character consistent tv scripts', 'from two popular series, the big bang theory and friends with quantitative and qualitative analysis', '. we demonstrate system superiority over hredgan and the state - of - the - art persona conversational model in']",1
['by  #TAUTHOR_TAG on speaker and'],['by  #TAUTHOR_TAG on speaker and'],[' #TAUTHOR_TAG on speaker and'],"['', 'evident in the relatively short and generic responses even though they generally capture the persona of the speaker. to overcome these limitations, we propose phredgan, a multi - modal hredgan', 'dialogue system which additionally conditions the adversarial framework proposed by  #TAUTHOR_TAG on speaker and / or utterance attributes', 'in order to maintain response quality of hredgan and still capture speaker and other modalities within a conversation. the attributes can be seen as another', 'input modality as the utterance. the attribute representation is an embedding that is learned together with the rest of model parameters, similar to [ 8 ]. the introduction of attributes allows the model to generate responses conditioned on particular attribute ( s )', 'across conversation turns. since the attributes are discrete, it also allows for exploring what - if scenarios of model responses. we train and sample the', 'proposed phredgan similar to the procedure for hredgan  #TAUTHOR_TAG.', 'to demonstrate model capability, we train on customer service related data such', 'as the ubuntu dialogue corpus ( udc ) that is strongly bimodal between', 'question poster and answerer, and character consistent tv scripts', 'from two popular series, the big bang theory and friends with quantitative and qualitative analysis', '. we demonstrate system superiority over hredgan and the state - of - the - art persona conversational model in']",5
['by  #TAUTHOR_TAG on speaker and'],['by  #TAUTHOR_TAG on speaker and'],[' #TAUTHOR_TAG on speaker and'],"['', 'evident in the relatively short and generic responses even though they generally capture the persona of the speaker. to overcome these limitations, we propose phredgan, a multi - modal hredgan', 'dialogue system which additionally conditions the adversarial framework proposed by  #TAUTHOR_TAG on speaker and / or utterance attributes', 'in order to maintain response quality of hredgan and still capture speaker and other modalities within a conversation. the attributes can be seen as another', 'input modality as the utterance. the attribute representation is an embedding that is learned together with the rest of model parameters, similar to [ 8 ]. the introduction of attributes allows the model to generate responses conditioned on particular attribute ( s )', 'across conversation turns. since the attributes are discrete, it also allows for exploring what - if scenarios of model responses. we train and sample the', 'proposed phredgan similar to the procedure for hredgan  #TAUTHOR_TAG.', 'to demonstrate model capability, we train on customer service related data such', 'as the ubuntu dialogue corpus ( udc ) that is strongly bimodal between', 'question poster and answerer, and character consistent tv scripts', 'from two popular series, the big bang theory and friends with quantitative and qualitative analysis', '. we demonstrate system superiority over hredgan and the state - of - the - art persona conversational model in']",5
"['of  #TAUTHOR_TAG.', 'noise injection : although  #TAUTHOR_TAG demonstrated that injecting noise at']","['of  #TAUTHOR_TAG.', 'noise injection : although  #TAUTHOR_TAG demonstrated that injecting noise at']","['. 1 of  #TAUTHOR_TAG.', 'noise injection : although  #TAUTHOR_TAG demonstrated that injecting noise at the word level seems to perform better than at']",[' #TAUTHOR_TAG'],5
"['training procedure in algorithm 1 with λ g = λ m = 1  #TAUTHOR_TAG.', 'since the encoder, word embeddings and attribute embeddings are shared, we are able to train']","['training procedure in algorithm 1 with λ g = λ m = 1  #TAUTHOR_TAG.', 'since the encoder, word embeddings and attribute embeddings are shared, we are able to train']","['phredgan using the same training procedure in algorithm 1 with λ g = λ m = 1  #TAUTHOR_TAG.', 'since the encoder, word embeddings and attribute embeddings are shared, we are able to train the system']","['train both the generator and the discriminator ( with a shared encoder ) of phredgan using the same training procedure in algorithm 1 with λ g = λ m = 1  #TAUTHOR_TAG.', 'since the encoder, word embeddings and attribute embeddings are shared, we are able to train the system end - to - end with backpropagation.', 'encoder : the encoder rnns, ern n, and ern n are bidirectional while crrn is unidirectional.', 'all rnn units are 3 - layer gru cells with a hidden state size of 512.', 'we use a word vocabulary size, v = 50, 000, with a word embedding size of 512.', 'the number of attributes, v c is dataset - dependent compute the generator output similar to eq. ( 11 ) in  #TAUTHOR_TAG.', 'sample a corresponding mini batch of utterance yi.', 'yi ∼ p θ g yi |, zi, xi, ci + 1 end for compute the adversarial discriminator accuracy dacc over n − 1 utterances', ""if dacc < accd th then update phredgan's θd with gradient of the discriminator loss. but we use an attribute embedding size of 512."", 'in this study, we only use one attribute per utterance, so there is no need to use rnns, sattrn n and tattrn n to combine the attribute embeddings.', '']",5
"['training procedure in algorithm 1 with λ g = λ m = 1  #TAUTHOR_TAG.', 'since the encoder, word embeddings and attribute embeddings are shared, we are able to train']","['training procedure in algorithm 1 with λ g = λ m = 1  #TAUTHOR_TAG.', 'since the encoder, word embeddings and attribute embeddings are shared, we are able to train']","['phredgan using the same training procedure in algorithm 1 with λ g = λ m = 1  #TAUTHOR_TAG.', 'since the encoder, word embeddings and attribute embeddings are shared, we are able to train the system']","['train both the generator and the discriminator ( with a shared encoder ) of phredgan using the same training procedure in algorithm 1 with λ g = λ m = 1  #TAUTHOR_TAG.', 'since the encoder, word embeddings and attribute embeddings are shared, we are able to train the system end - to - end with backpropagation.', 'encoder : the encoder rnns, ern n, and ern n are bidirectional while crrn is unidirectional.', 'all rnn units are 3 - layer gru cells with a hidden state size of 512.', 'we use a word vocabulary size, v = 50, 000, with a word embedding size of 512.', 'the number of attributes, v c is dataset - dependent compute the generator output similar to eq. ( 11 ) in  #TAUTHOR_TAG.', 'sample a corresponding mini batch of utterance yi.', 'yi ∼ p θ g yi |, zi, xi, ci + 1 end for compute the adversarial discriminator accuracy dacc over n − 1 utterances', ""if dacc < accd th then update phredgan's θd with gradient of the discriminator loss. but we use an attribute embedding size of 512."", 'in this study, we only use one attribute per utterance, so there is no need to use rnns, sattrn n and tattrn n to combine the attribute embeddings.', '']",5
"['training procedure in algorithm 1 with λ g = λ m = 1  #TAUTHOR_TAG.', 'since the encoder, word embeddings and attribute embeddings are shared, we are able to train']","['training procedure in algorithm 1 with λ g = λ m = 1  #TAUTHOR_TAG.', 'since the encoder, word embeddings and attribute embeddings are shared, we are able to train']","['phredgan using the same training procedure in algorithm 1 with λ g = λ m = 1  #TAUTHOR_TAG.', 'since the encoder, word embeddings and attribute embeddings are shared, we are able to train the system']","['train both the generator and the discriminator ( with a shared encoder ) of phredgan using the same training procedure in algorithm 1 with λ g = λ m = 1  #TAUTHOR_TAG.', 'since the encoder, word embeddings and attribute embeddings are shared, we are able to train the system end - to - end with backpropagation.', 'encoder : the encoder rnns, ern n, and ern n are bidirectional while crrn is unidirectional.', 'all rnn units are 3 - layer gru cells with a hidden state size of 512.', 'we use a word vocabulary size, v = 50, 000, with a word embedding size of 512.', 'the number of attributes, v c is dataset - dependent compute the generator output similar to eq. ( 11 ) in  #TAUTHOR_TAG.', 'sample a corresponding mini batch of utterance yi.', 'yi ∼ p θ g yi |, zi, xi, ci + 1 end for compute the adversarial discriminator accuracy dacc over n − 1 utterances', ""if dacc < accd th then update phredgan's θd with gradient of the discriminator loss. but we use an attribute embedding size of 512."", 'in this study, we only use one attribute per utterance, so there is no need to use rnns, sattrn n and tattrn n to combine the attribute embeddings.', '']",5
['g (. ) )  #TAUTHOR_TAG using trained models run in autoreg'],"['average discriminator loss, −logd ( g (. ) )  #TAUTHOR_TAG using trained models run in autoregressive']",['g (. ) )  #TAUTHOR_TAG using trained models run in autoreg'],"['use an inference strategy similar to the approach in olabiyi et.', 'al [ 7 ].', 'the only differences between training and inference are : ( i ) the generator is run in autoregressive mode with greedy decoding by passing the previously generated word token to the input of the drn n at the next step.', '( ii ) a modified noise sample n ( 0, αi ) is passed into the generator input.', 'for the modified noise sample, we perform a linear search for α with sample size l = 1 based on the average discriminator loss, −logd ( g (. ) )  #TAUTHOR_TAG using trained models run in autoregressive mode to reflect performance in actual deployment.', 'the optimum α value is then used for all inferences and evaluations.', 'during inference, we condition the dialogue response generation on the encoder outputs, noise samples, word embedding, and the attribute embedding of the intended responder.', ""with multiple noise samples, l = 64, we rank the generator outputs by the discriminator which is also conditioned on the encoder outputs, and the intended responder's embedding."", 'the final response is the response ranked highest by the discriminator']",5
"['in  #TAUTHOR_TAG, with 90']","['in  #TAUTHOR_TAG, with 90 %,']","['in  #TAUTHOR_TAG, with 90']","['series transcripts dataset [ 3 ].', 'we train our model on transcripts from the two popular tv drama series, big bang theory and friends.', 'following a preprocessing setup similar to [ 8 ], we collect utterances from the top 12 speakers from both series to construct a corpus of 5, 008 lines of multi - turn dialogue.', 'we split the corpus into training, development, and test sets with 94 %, 3 %, and 3 % proportions, respectively, and [ 14 ] which consists of 240, 000 dialogue triples.', 'we pre - train our model on this dataset to initialize our model parameters to avoid overfitting on a relatively small persona tv series dataset.', 'after pre - training on mtc, we reinitialize the attribute embeddings in the generator from a uniform distribution following a xavier initialization [ 12 ] for training on the combined person tv series dataset.', 'ubuntu dialogue corpus ( udc ) dataset [ 4 ].', 'we train our model on 1. 85 million conversations of multi - turn dialogue from the ubuntu community hub, with an average of 5 utterances per conversation.', 'we assign two types of speaker ids to utterances in this dataset : questioner and helper.', 'we follow the same training, development, and test split as the udc dataset in  #TAUTHOR_TAG, with 90 %, 5 %, and 5 % proportions, respectively.', 'while the overwhelming majority of utterances in udc follow two speaker types, the dataset does include utterances that are not classified under either a questioner or helper speaker type.', 'to remain consistent, we assume that there are only two speaker types within this dataset and that the first utterance of every dialogue is from a questioner.', ""this simplifying assumption does introduce a degree of noise into the model's ability to construct attribute embeddings."", 'however, our experimental results demonstrate that our model is still able to differentiate between the larger two speaker types in the dataset']",5
"['use similar evaluation metrics as in  #TAUTHOR_TAG including perplexity, ble']","['use similar evaluation metrics as in  #TAUTHOR_TAG including perplexity, bleu [ 15 ], rouge [ 16 ], and distinct n - gram [ 17 ] scores']","['use similar evaluation metrics as in  #TAUTHOR_TAG including perplexity, bleu [ 15 ], rouge [ 16 ], and distinct n - gram [ 17 ] scores']","['use similar evaluation metrics as in  #TAUTHOR_TAG including perplexity, bleu [ 15 ], rouge [ 16 ], and distinct n - gram [ 17 ] scores']",5
['to hredgan from  #TAUTHOR_TAG in'],['to hredgan from  #TAUTHOR_TAG in'],['to hredgan from  #TAUTHOR_TAG in'],"['compare our system to [ 8 ] which uses a seq2seq framework in conjunction with learnable persona embeddings.', 'their work explores two persona models to incorporate vector representations of speaker interaction and speaker attributes into the decoder of their seq2seq model, i. e., speaker and speaker - addressee models.', 'while we compare with both models quantitatively, we mostly compare with the speaker - addressee model qualitatively.', 'our quantitative comparison uses perplexity and bleu - 4 scores as those are the ones reported in [ 8 ].', 'in addition, we also measure our model performance in terms of rogue and distinct n - gram scores for the purpose of completeness.', 'for fair comparison, we use the same tv drama series dataset used in their study.', 'we also compare our system to hredgan from  #TAUTHOR_TAG in terms of perplexity, rogue, and distinct n - grams scores.', 'in  #TAUTHOR_TAG, the authors recommend the version with word - level noise injection, hredgan w, so we use this version in our comparison.', 'also for fair comparison, we use the same udc dataset as reported in  #TAUTHOR_TAG.', 'the only addition we made is to add the speaker attribute to the utterances of the dataset as described in the dataset subsection']",5
['to hredgan from  #TAUTHOR_TAG in'],['to hredgan from  #TAUTHOR_TAG in'],['to hredgan from  #TAUTHOR_TAG in'],"['compare our system to [ 8 ] which uses a seq2seq framework in conjunction with learnable persona embeddings.', 'their work explores two persona models to incorporate vector representations of speaker interaction and speaker attributes into the decoder of their seq2seq model, i. e., speaker and speaker - addressee models.', 'while we compare with both models quantitatively, we mostly compare with the speaker - addressee model qualitatively.', 'our quantitative comparison uses perplexity and bleu - 4 scores as those are the ones reported in [ 8 ].', 'in addition, we also measure our model performance in terms of rogue and distinct n - gram scores for the purpose of completeness.', 'for fair comparison, we use the same tv drama series dataset used in their study.', 'we also compare our system to hredgan from  #TAUTHOR_TAG in terms of perplexity, rogue, and distinct n - grams scores.', 'in  #TAUTHOR_TAG, the authors recommend the version with word - level noise injection, hredgan w, so we use this version in our comparison.', 'also for fair comparison, we use the same udc dataset as reported in  #TAUTHOR_TAG.', 'the only addition we made is to add the speaker attribute to the utterances of the dataset as described in the dataset subsection']",5
"['constitutes racism and sexism.', ' #TAUTHOR_TAG pointed out that hate speech is different from offensive language and released a data set of 25k tweets with the goal of distinguishing hate speech from offensive language.', 'stop saying dumb']","['constitutes racism and sexism.', ' #TAUTHOR_TAG pointed out that hate speech is different from offensive language and released a data set of 25k tweets with the goal of distinguishing hate speech from offensive language.', 'stop saying dumb blondes with pretty']","['constitutes racism and sexism.', ' #TAUTHOR_TAG pointed out that hate speech is different from offensive language and released a data set of 25k tweets with the goal of distinguishing hate speech from offensive language.', 'stop saying dumb blondes with pretty']","['social interaction involves an exchange of viewpoints and thoughts. but these views and thoughts can be caustic.', ""often we see that users resort to verbal abuse to win an argument or overshadow someone's opinion."", 'on twitter, people from every sphere have experienced online abuse.', 'be it a famous celebrity with millions of followers or someone representing a marginalized community such as lgbtq, women and more.', 'we want to channelize natural language processing ( nlp ) for social good and aid in the process of flagging abusive tweets and users.', 'detecting abuse on twitter can be challenging, particularly because the text is often noisy.', 'abuse can also have different facets.', '[ 10 ] released one of the initial data sets from twitter with the goal of identifying what constitutes racism and sexism.', ' #TAUTHOR_TAG pointed out that hate speech is different from offensive language and released a data set of 25k tweets with the goal of distinguishing hate speech from offensive language.', 'stop saying dumb blondes with pretty faces as you need a pretty face to pull them off!!! # mkr in islam women must be locked in their houses and muslims claim this is treating them well table 1 : tweets from [ 10 ] data set demonstrating online abuse they find that racist and homophobic tweets are more likely to be classified as hate speech but sexist tweets are generally classified as offensive.', '[ 4 ] introduced a large, hand - coded corpus of online harassment data for studying the nature of harassing comments and the culture of trolling.', 'keeping these motivations in mind, we make the following salient contributions :', '• we build a deep context - aware attention - based model for abusive behavior detection on twitter.', 'to the best of our knowledge ours is the first work that exploits context aware attention for this task.', '• our model is robust and achieves consistent performance gains in all the three abusive data sets • we show how context aware attention helps in focusing on certain abusive keywords when used in specific context and improve the performance of abusive behavior detection']",0
"['corpus.', ' #TAUTHOR_TAG use a similar hand']","['corpus.', ' #TAUTHOR_TAG use a similar handcrafted feature engineered model to identify offensive language and distinguish it from hate speech.', '[ 2 ] in their work, experiment with multiple deep']","['corpus.', ' #TAUTHOR_TAG use a similar hand']","['approaches to abusive text detection can be broadly divided into two categories : 1 ) feature intensive machine learning algorithms such as logistic regression ( lr ), multilayer perceptron ( mlp ) and etc.', '2 ) deep learning models which learn feature representations on their own. [ 10 ] released the popular data set of 16k tweets annotated as belonging to sexism, racism or none class 1, and provided a feature engineered model for detection of abuse in their corpus.', ' #TAUTHOR_TAG use a similar handcrafted feature engineered model to identify offensive language and distinguish it from hate speech.', '[ 2 ] in their work, experiment with multiple deep learning architectures for the task of hate speech detection on twitter using the same data set by [ 10 ].', 'their best - reported f1 - score is achieved using long short term memory networks ( lstm ) + gradient boosting.', 'on the data set released by [ 10 ], [ 5 ] experiment with a two - step approach of detecting abusive language first and then classifying them into specific types i. e. racist, sexist or none.', 'they achieve best results using a hybrid convolution neural network ( cnn ) with the intuition that character level input would counter the purposely or mistakenly misspelled words and made - up vocabularies.', '[ 6 ] in their work ran experiments on the gazetta dataset and the detox system ( [ 12 ] ) and show that a recurrent neural network ( rnn ) coupled with deep, classification - specific attention outperforms the previous state of the art in abusive comment moderation.', 'in their more recent work [ 7 ] explored how user embeddings, user - type embeddings, and user type biases can improve their previous rnn based model on the gazetta dataset.', 'attentive neural networks have been shown to perform well on a variety of nlp tasks ( [ 13 ], [ 11 ] ).', '[ 13 ] use hierarchical contextual attention for text classification ( i. e attention both at word and sentence level ) on six large scale text classification tasks and demonstrate that the proposed architecture outperform previous methods by a substantial margin.', 'we primarily focus on word level attention because most of the tweets are single sentence tweets']",0
"[' #TAUTHOR_TAG 25, 112']","['calculate the importance of the word as the similarity data set tweets count [ 10 ] 15, 844  #TAUTHOR_TAG 25, 112 [ 4 ] 20, 362 table']","[' #TAUTHOR_TAG 25, 112']","['', 'u c is our word level context vector that is randomly initialized and learned as we train our network.', 'once u i is obtained we calculate the importance of the word as the similarity data set tweets count [ 10 ] 15, 844  #TAUTHOR_TAG 25, 112 [ 4 ] 20, 362 table 2 : data sets and their total tweets count of u i with u c and get a normalized importance weight α i through a softmax function.', 'the context vector u c can be seen as a tool which filters which word is more important over all the words like that used in the lstm.', 'figure 2 shows the high - level architecture of this model.', 'w h and b h are the attention layers weights and biases.', 'more formally']",5
"['as none.', 'the  #TAUTHOR_TAG data set had']","['as sexism and 10, 862 as none.', 'the  #TAUTHOR_TAG data set had']","['as none.', 'the  #TAUTHOR_TAG data set had a total of']","['have used the 3 benchmark data sets for abusive content detection on twitter.', 'at the time of the experiment, the [ 10 ] data set had a total of 15, 844 tweets out of which 1, 924 were labelled as belonging to racism, 3, 058 as sexism and 10, 862 as none.', 'the  #TAUTHOR_TAG data set had a total of 25, 112 tweets out of which 1498 were labelled as hate speech, 19, 326 as offensive language and 4, 288 as neither.', 'for the [ 4 ] data set, there were 20, 362 tweets out of which 5, 235 were positive harassment examples and 15, 127 were negative.', 'we call [ 10 ] data set as d1, [ 9 ] data set as d2 and [ 4 ] as d3 for tweet tokenization, we use ekphrasis which is a text processing tool built specially from social platforms such as twitter.', '[ 3 ] use a big collection of twitter messages ( 330m ) to generate word embeddings, with a vocabulary size of 660k words, using glove ( [ 8 ] ).', 'we use these pre - trained word embeddings for initializing the first layer ( embedding layer ) of our neural networks']",5
"['10 ] where as the second tweet is an example of racist tweet from the same datset.', 'the third tweet is from  #TAUTHOR_TAG data set labelled as offensive language']","['to each word by the contextual attention.', 'the first tweet is a sexist tweet from [ 10 ] where as the second tweet is an example of racist tweet from the same datset.', 'the third tweet is from  #TAUTHOR_TAG data set labelled as offensive language']","['10 ] where as the second tweet is an example of racist tweet from the same datset.', 'the third tweet is from  #TAUTHOR_TAG data set labelled as offensive language']","['color intensity corresponds to the weight given to each word by the contextual attention.', 'the first tweet is a sexist tweet from [ 10 ] where as the second tweet is an example of racist tweet from the same datset.', 'the third tweet is from  #TAUTHOR_TAG data set labelled as offensive language']",5
"[' #TAUTHOR_TAG reported results using 5 fold cv.', '']","[' #TAUTHOR_TAG reported results using 5 fold cv.', '']","[' #TAUTHOR_TAG reported results using 5 fold cv.', '']","['network is trained at a learning rate of 0. 001 for 10 epochs, with a dropout of 0. 2 to prevent over - fitting.', 'the results are averaged over 10 - fold cross - validations for d1 and d3 and 5 fold cross - validations for  #TAUTHOR_TAG reported results using 5 fold cv.', 'because of class imbalance in all our data sets, we report weighted f1 scores.', 'table 3 shows our results in detail.', 'we compare our model with the best models reported in each paper.', 'because [ 4 ] is a data set paper, we cannot fill the corresponding row.', '* denotes the numbers from baseline papers.', 'all the results were reproducible except for the one marked red.', 'for  #AUTHOR_TAG data set, ( badjatiyaet al., 2017 ) claim that using gradient boosting with lstm embeddings obtained from random word embeddings boosted their performance by 12 f1 from 81. 0 to 93. 0.', 'when we tried to reproduce the result, we did not find any significant improvement over 81.', 'results show that our model is robust when it comes to the performance on all of the three data sets.', 'table 3 : data sets and the results of different models.', 'we reproduced the results for each model on three of the data sets']",1
['second tweet is a tweet from from  #TAUTHOR_TAG data set and the third from the [ 4'],['second tweet is a tweet from from  #TAUTHOR_TAG data set and the third from the [ 4'],"['10 ], the second tweet is a tweet from from  #TAUTHOR_TAG data set and the third from the [ 4']","['also share some examples from the three data sets in figure 2 which our bilstm attention model could not classify correctly.', 'on closer investigation we find that most cases where our model fails are instances where annotation is either noisy or the difference between classes are very blurred and subtle.', 'the first tweet is a tweet from [ 10 ], the second tweet is a tweet from from  #TAUTHOR_TAG data set and the third from the [ 4']",4
"['word association between man to computer programmer as woman to homemaker  #TAUTHOR_TAG.', 'pre - trained word embeddings are used in many nlp downstream tasks,']","['word association between man to computer programmer as woman to homemaker  #TAUTHOR_TAG.', 'pre - trained word embeddings are used in many nlp downstream tasks,']","[', one popular example of gender bias is the word association between man to computer programmer as woman to homemaker  #TAUTHOR_TAG.', 'pre - trained word embeddings are used in many nlp downstream tasks,']","['biases in machine learning, in general and in natural language processing ( nlp ) applications in particular, are raising the alarm of the scientific community.', 'examples of these biases are evidences such that face recognition systems or speech recognition systems works better for white men than for ethnic minorities  #AUTHOR_TAG.', 'examples in the area of nlp are the case of machine translation that systems tend to ignore the coreference information in benefit of an stereotype ( font and costa - jussa, 2019 ) or sentiment analysis where higher sentiment intensity prediction is biased for a particular gender  #AUTHOR_TAG.', 'in this work we focus on the particular nlp area of word embeddings  #AUTHOR_TAG, which represent words in a numerical vector space.', 'word embeddings representation spaces are known to present geometrical phenomena mimicking relations and analogies between words ( e. g. man is to woman as king is to queen ).', 'following this property of finding relations or analogies, one popular example of gender bias is the word association between man to computer programmer as woman to homemaker  #TAUTHOR_TAG.', 'pre - trained word embeddings are used in many nlp downstream tasks, such as natural language inference ( nli ), machine translation ( mt ) or question answering ( qa ).', 'recent progress in word embedding techniques has been achieved with contextualized word embeddings  #AUTHOR_TAG which provide different vector representations for the same word in different contexts.', 'while gender bias has been studied, detected and partially addressed for standard word embeddings techniques  #TAUTHOR_TAG a ;  #AUTHOR_TAG, it is not the case for the latest techniques of contextualized word embeddings.', 'only just recently,  #AUTHOR_TAG present a first analysis on the topic based on the proposed methods in  #TAUTHOR_TAG.', 'in this paper, we further analyse the presence of gender biases in contextualized word embeddings by means of the proposed methods in  #AUTHOR_TAG.', 'for this, in section 2 we provide an overview of the relevant work on which we build our analysis ; in section 3 we state the specific request questions addressed in this work, while in section 4 we describe the experimental framework proposed to address them and in section 5 we present the obtained and discuss the results ; finally, in section 6 we draw the conclusions of our work and propose some further research']",1
"['learned into word embeddings trained in those corpora, amplifying them  #TAUTHOR_TAG.', ' #AUTHOR_TAG studied from a geometrical']","['learned into word embeddings trained in those corpora, amplifying them  #TAUTHOR_TAG.', ' #AUTHOR_TAG studied from a geometrical']","['learned into word embeddings trained in those corpora, amplifying them  #TAUTHOR_TAG.', ' #AUTHOR_TAG studied from a geometrical point of view the presence of gender bias in word embeddings.', 'for this, they compute']","['- generated corpora suffer from social biases.', 'those biases are reflected in the cooccurrence statistics, and therefore learned into word embeddings trained in those corpora, amplifying them  #TAUTHOR_TAG.', ' #AUTHOR_TAG studied from a geometrical point of view the presence of gender bias in word embeddings.', 'for this, they compute the subspace where the gender information concentrates by computing the principal components of the difference of vector representations of male and female gender - defining word pairs.', 'with the gender subspace, the authors identify direct and indirect biases in profession words.', 'finally, they mitigate the bias by nullifying the information in the gender subspace for words that should not be associated to gender, and also equalize their distance to both elements of gender - defining word pairs.', ' #AUTHOR_TAG b ) proposed an extension to glove embeddings  #AUTHOR_TAG where the loss function used to train the embeddings is enriched with terms that confine the gender information to a specific portion of the embedded vector.', 'the authors refer to these pieces of information as protected attributes.', 'once the embeddings are trained, the gender protected attribute can be simply removed from the vector representation, therefore eliminating any gender bias present in it.', 'the transformations proposed by both  #TAUTHOR_TAG and  #AUTHOR_TAG b ) are downstream task - agnostic.', 'this fact is used in the work of  #AUTHOR_TAG to showcase that, while apparently the embedding information is removed, there is still gender information remaining in the vector representations']",1
"['learned into word embeddings trained in those corpora, amplifying them  #TAUTHOR_TAG.', ' #AUTHOR_TAG studied from a geometrical']","['learned into word embeddings trained in those corpora, amplifying them  #TAUTHOR_TAG.', ' #AUTHOR_TAG studied from a geometrical']","['learned into word embeddings trained in those corpora, amplifying them  #TAUTHOR_TAG.', ' #AUTHOR_TAG studied from a geometrical point of view the presence of gender bias in word embeddings.', 'for this, they compute']","['- generated corpora suffer from social biases.', 'those biases are reflected in the cooccurrence statistics, and therefore learned into word embeddings trained in those corpora, amplifying them  #TAUTHOR_TAG.', ' #AUTHOR_TAG studied from a geometrical point of view the presence of gender bias in word embeddings.', 'for this, they compute the subspace where the gender information concentrates by computing the principal components of the difference of vector representations of male and female gender - defining word pairs.', 'with the gender subspace, the authors identify direct and indirect biases in profession words.', 'finally, they mitigate the bias by nullifying the information in the gender subspace for words that should not be associated to gender, and also equalize their distance to both elements of gender - defining word pairs.', ' #AUTHOR_TAG b ) proposed an extension to glove embeddings  #AUTHOR_TAG where the loss function used to train the embeddings is enriched with terms that confine the gender information to a specific portion of the embedded vector.', 'the authors refer to these pieces of information as protected attributes.', 'once the embeddings are trained, the gender protected attribute can be simply removed from the vector representation, therefore eliminating any gender bias present in it.', 'the transformations proposed by both  #TAUTHOR_TAG and  #AUTHOR_TAG b ) are downstream task - agnostic.', 'this fact is used in the work of  #AUTHOR_TAG to showcase that, while apparently the embedding information is removed, there is still gender information remaining in the vector representations']",1
"[', we adapt and contrast with the evaluation measures proposed by  #TAUTHOR_TAG and  #AUTHOR_TAG']","['would be the best measure to use for gender bias detection in contextualized embeddings?', 'to address these questions, we adapt and contrast with the evaluation measures proposed by  #TAUTHOR_TAG and  #AUTHOR_TAG']","[', we adapt and contrast with the evaluation measures proposed by  #TAUTHOR_TAG and  #AUTHOR_TAG']","['##trained language models ( lm ) like ulmfit  #AUTHOR_TAG, elmo  #AUTHOR_TAG, openai gpt  #AUTHOR_TAG and bert  #AUTHOR_TAG, proposed different neural language model architectures and made their pre - trained weights available to ease the application of transfer learning to downstream tasks, where they have pushed the state - of - the - art for several benchmarks including question answering on squad, nli, cross - lingual nli and named identity recognition ( ner ).', 'while some of these pre - trained lms, like bert, use subword level tokens, elmo provides word - level representations.', 'and  #AUTHOR_TAG confirmed the viability of using elmo representations directly as features for downstream tasks without re - training the full model on the target task.', 'unlike word2vec vector representations, which are constant regardless of their context, elmo representations depend on the sentence where the word appears, and therefore the full model has to be fed with each whole sentence to get the word representations.', 'the neural architecture proposed in elmo  #AUTHOR_TAG consists of a character - level convolutional layer processing the characters of each word and creating a word representation that is then fed to a 2 - layer bidirectional lstm  #AUTHOR_TAG, trained on language modeling task on a large corpus.', 'given the high impact of contextualized word embeddings in the area of nlp and the social consequences of having biases in such embeddings, in this work we analyse the presence of bias in these contextualized word embeddings.', 'in particular, we focus on gender biases, and specifically on the following questions :', '• do contextualized word embeddings exhibit gender bias and how does this bias compare to standard and debiased word embeddings?', '• do different evaluation techniques identify bias similarly and what would be the best measure to use for gender bias detection in contextualized embeddings?', 'to address these questions, we adapt and contrast with the evaluation measures proposed by  #TAUTHOR_TAG and  #AUTHOR_TAG']",4
"['of representation of random words.', 'similarly to  #TAUTHOR_TAG,']","['of representation of random words.', 'similarly to  #TAUTHOR_TAG,']","['the principal components of representation of random words.', 'similarly to  #TAUTHOR_TAG, figure 1 shows that']","['the case of contextualized embeddings, there is not just a single representation for each word, but its representation depends on the sentence it appears in.', 'this way, in order to compute the gender subspace we take the representation of words by randomly sampling sentences that contain words from the definitional list and, for each of them, we swap the definitional word with its pair - wise equivalent from the opposite gender.', 'we then obtain the elmo representation of the definintional word in each sentence pair, computing their difference.', 'on the set of difference vectors, we compute their principal components to verify the presence of bias.', 'in order to have a reference, we computed the principal components of representation of random words.', 'similarly to  #TAUTHOR_TAG, figure 1 shows that the first eigenvalue is significantly larger than the rest and that there is also a single direction describing the majority of variance in these vectors, still the difference between the percentage of variances is less in case of contextualized embeddings, which may refer that there is less bias in such embeddings.', 'we can easily note the difference in the case of random, where there is a smooth and gradual decrease in eigenvalues, and hence the variance percentage.', 'a similar conclusion was stated in the recent work  #AUTHOR_TAG where the authors applied the same approach, but for gender swapped variants of sentences with professions.', 'they computed the difference between the vectors of occupation words in corresponding sentences and got a skewed graph where the first component represent the gender information while the second component groups the male and female related words.', 'direct bias direct bias is a measure of how close a certain set of words are to the gender vector.', 'to compute it, we extracted from the training data the sentences that contain words in the professional list.', 'we excluded the sentences that have both a professional token and definitional gender word to avoid the influence of the latter over the presence of bias in the former.', 'we applied the definition of direct bias from  #TAUTHOR_TAG on the elmo representations of the professional words in these sentences.', 'where n is the amount of gender neutral words, g the gender direction, and w the word vector of each profession.', 'we got direct bias of 0. 03, compared to 0. 08 from standard word2vec embeddings described in  #TAUTHOR_TAG.', 'this reduction on the direct bias confirms that the substantial component along the gender direction that is present in standard word embeddings is less for the contextualized word embeddings.', 'probably, this reduction comes from the fact that we are using different word embeddings for the same profession depending on the sentence which is a direct consequence and advantage of using contextualized embeddings']",4
"[' #TAUTHOR_TAG.', 'we']","[' #TAUTHOR_TAG.', 'we']","['we used a set of lists from previous work  #TAUTHOR_TAG.', 'we refer']","['follows, we define the data and resources that we are using for performing our experiments.', 'we also motivate the approach that we are using for contextualized word embeddings.', 'we worked with the english - german news corpus from the wmt18 1.', 'we used the english side with 464, 947 lines and 1, 004, 6125 tokens.', 'to perform our analysis we used a set of lists from previous work  #TAUTHOR_TAG.', ""we refer to the list of definitional pairs 2 as'definitonal list'( e. g. shehe, girl - boy )."", '']",5
"[' #TAUTHOR_TAG.', 'we']","[' #TAUTHOR_TAG.', 'we']","['we used a set of lists from previous work  #TAUTHOR_TAG.', 'we refer']","['follows, we define the data and resources that we are using for performing our experiments.', 'we also motivate the approach that we are using for contextualized word embeddings.', 'we worked with the english - german news corpus from the wmt18 1.', 'we used the english side with 464, 947 lines and 1, 004, 6125 tokens.', 'to perform our analysis we used a set of lists from previous work  #TAUTHOR_TAG.', ""we refer to the list of definitional pairs 2 as'definitonal list'( e. g. shehe, girl - boy )."", '']",5
['work  #TAUTHOR_TAG and  #AUTHOR_TAG to be'],['work  #TAUTHOR_TAG and  #AUTHOR_TAG to be'],"['is no standard measure for gender bias, and even less for such the recently proposed contextualized word embeddings.', 'in this section, we adapt gender bias measures for word embedding methods from previous work  #TAUTHOR_TAG and  #AUTHOR_TAG to be']","['is no standard measure for gender bias, and even less for such the recently proposed contextualized word embeddings.', 'in this section, we adapt gender bias measures for word embedding methods from previous work  #TAUTHOR_TAG and  #AUTHOR_TAG to be applicable to contextualized word embeddings.', 'this way, we first compute the gender subspace from the elmo vector representations of genderdefining words, then identify the presence of direct bias in the contextualized representations.', 'we then proceed to identify gender information by means of clustering and classifications techniques.', 'we compare our results to previous results from debiased and non - debiased word embeddings  #TAUTHOR_TAG.', ' #AUTHOR_TAG propose to identify gender bias in word representations by computing the direction between representations of male and female word pairs from the definitional list ( − → he - −→ she, −−→ man - − −−−− → woman ) and computing their principal components']",6
"['of representation of random words.', 'similarly to  #TAUTHOR_TAG,']","['of representation of random words.', 'similarly to  #TAUTHOR_TAG,']","['the principal components of representation of random words.', 'similarly to  #TAUTHOR_TAG, figure 1 shows that']","['the case of contextualized embeddings, there is not just a single representation for each word, but its representation depends on the sentence it appears in.', 'this way, in order to compute the gender subspace we take the representation of words by randomly sampling sentences that contain words from the definitional list and, for each of them, we swap the definitional word with its pair - wise equivalent from the opposite gender.', 'we then obtain the elmo representation of the definintional word in each sentence pair, computing their difference.', 'on the set of difference vectors, we compute their principal components to verify the presence of bias.', 'in order to have a reference, we computed the principal components of representation of random words.', 'similarly to  #TAUTHOR_TAG, figure 1 shows that the first eigenvalue is significantly larger than the rest and that there is also a single direction describing the majority of variance in these vectors, still the difference between the percentage of variances is less in case of contextualized embeddings, which may refer that there is less bias in such embeddings.', 'we can easily note the difference in the case of random, where there is a smooth and gradual decrease in eigenvalues, and hence the variance percentage.', 'a similar conclusion was stated in the recent work  #AUTHOR_TAG where the authors applied the same approach, but for gender swapped variants of sentences with professions.', 'they computed the difference between the vectors of occupation words in corresponding sentences and got a skewed graph where the first component represent the gender information while the second component groups the male and female related words.', 'direct bias direct bias is a measure of how close a certain set of words are to the gender vector.', 'to compute it, we extracted from the training data the sentences that contain words in the professional list.', 'we excluded the sentences that have both a professional token and definitional gender word to avoid the influence of the latter over the presence of bias in the former.', 'we applied the definition of direct bias from  #TAUTHOR_TAG on the elmo representations of the professional words in these sentences.', 'where n is the amount of gender neutral words, g the gender direction, and w the word vector of each profession.', 'we got direct bias of 0. 03, compared to 0. 08 from standard word2vec embeddings described in  #TAUTHOR_TAG.', 'this reduction on the direct bias confirms that the substantial component along the gender direction that is present in standard word embeddings is less for the contextualized word embeddings.', 'probably, this reduction comes from the fact that we are using different word embeddings for the same profession depending on the sentence which is a direct consequence and advantage of using contextualized embeddings']",3
"['of representation of random words.', 'similarly to  #TAUTHOR_TAG,']","['of representation of random words.', 'similarly to  #TAUTHOR_TAG,']","['the principal components of representation of random words.', 'similarly to  #TAUTHOR_TAG, figure 1 shows that']","['the case of contextualized embeddings, there is not just a single representation for each word, but its representation depends on the sentence it appears in.', 'this way, in order to compute the gender subspace we take the representation of words by randomly sampling sentences that contain words from the definitional list and, for each of them, we swap the definitional word with its pair - wise equivalent from the opposite gender.', 'we then obtain the elmo representation of the definintional word in each sentence pair, computing their difference.', 'on the set of difference vectors, we compute their principal components to verify the presence of bias.', 'in order to have a reference, we computed the principal components of representation of random words.', 'similarly to  #TAUTHOR_TAG, figure 1 shows that the first eigenvalue is significantly larger than the rest and that there is also a single direction describing the majority of variance in these vectors, still the difference between the percentage of variances is less in case of contextualized embeddings, which may refer that there is less bias in such embeddings.', 'we can easily note the difference in the case of random, where there is a smooth and gradual decrease in eigenvalues, and hence the variance percentage.', 'a similar conclusion was stated in the recent work  #AUTHOR_TAG where the authors applied the same approach, but for gender swapped variants of sentences with professions.', 'they computed the difference between the vectors of occupation words in corresponding sentences and got a skewed graph where the first component represent the gender information while the second component groups the male and female related words.', 'direct bias direct bias is a measure of how close a certain set of words are to the gender vector.', 'to compute it, we extracted from the training data the sentences that contain words in the professional list.', 'we excluded the sentences that have both a professional token and definitional gender word to avoid the influence of the latter over the presence of bias in the former.', 'we applied the definition of direct bias from  #TAUTHOR_TAG on the elmo representations of the professional words in these sentences.', 'where n is the amount of gender neutral words, g the gender direction, and w the word vector of each profession.', 'we got direct bias of 0. 03, compared to 0. 08 from standard word2vec embeddings described in  #TAUTHOR_TAG.', 'this reduction on the direct bias confirms that the substantial component along the gender direction that is present in standard word embeddings is less for the contextualized word embeddings.', 'probably, this reduction comes from the fact that we are using different word embeddings for the same profession depending on the sentence which is a direct consequence and advantage of using contextualized embeddings']",3
"['unsupervised word discovery and segmentation task, using the bilingual - rooted approach from  #TAUTHOR_TAG.', 'there, words in']","['unsupervised word discovery and segmentation task, using the bilingual - rooted approach from  #TAUTHOR_TAG.', 'there, words in']","['language documentation initiatives by automatic approaches.', 'here we investigate the unsupervised word discovery and segmentation task, using the bilingual - rooted approach from  #TAUTHOR_TAG.', 'there, words in the well - resourced language are aligned to unsegmented phonemes in']","['cambridge handbook of endangered languages  #AUTHOR_TAG estimates that at least half of the 7, 000 languages currently spoken worldwide will no longer exist by the end of this century.', 'for these endangered languages, data collection campaigns have to accommodate the challenge that many of them are from oral tradition, and producing transcriptions is costly.', 'this transcription bottleneck problem can be handled by translating into a widely spoken language to ensure subsequent interpretability of the collected recordings, and such parallel corpora have been recently created by aligning the collected audio with translations in a well - resourced language  #AUTHOR_TAG.', 'moreover, some linguists suggested that more than one translation should be collected to capture deeper layers of meaning  #AUTHOR_TAG.', 'this work is a contribution to the computational language documentation ( cld ) research field, that aims to replace part of the manual steps performed by linguists during language documentation initiatives by automatic approaches.', 'here we investigate the unsupervised word discovery and segmentation task, using the bilingual - rooted approach from  #TAUTHOR_TAG.', 'there, words in the well - resourced language are aligned to unsegmented phonemes in the endangered language in order to identify group of phonemes, and to cluster them into word - like units.', 'we experiment with the mboshi - french parallel corpus, translating the french text into four other well - resourced languages in order to investigate language impact in this cld approach.', 'our results hint that this language impact exists, and that models based on different languages will output different word - like units']",5
['from  #TAUTHOR_TAG to'],['from  #TAUTHOR_TAG to'],[') approach from  #TAUTHOR_TAG to'],"['multilingual mboshi parallel corpus : in this work we extend the bilingual mboshi - french parallel corpus  #AUTHOR_TAG, fruit of the documentation process of mboshi ( bantu c25 ), an endangered language spoken in congo - brazzaville.', 'the corpus contains 5, 130 utterances, for which it provides audio, transcriptions and translations in french.', 'we translate the french into four other well - resourced languages through the use of the deepl translator.', '1 the languages added to the dataset are : english, german, portuguese and spanish.', 'table 1 shows some statistics for the produced multilingual mboshi parallel corpus.', '2 bilingual unsupervised word segmentation / discovery approach : we use the bilingual neuralbased unsupervised word segmentation ( uws ) approach from  #TAUTHOR_TAG to discover words in mboshi.', 'in this approach, neural machine translation ( nmt ) models are trained between language pairs, using as source language the translation ( word - level ) and as target, the language to document ( unsegmented phonemic sequence ).', 'due to the attention mechanism present in these networks  #AUTHOR_TAG, posterior to training, it is possible to retrieve soft - alignment probability matrices between source and target sequences.', 'these matrices give us sentence - level source - to - target alignment information, and by using it for clustering neighbor phonemes aligned to the same translation word, we are able to create segmentation in the target side.', 'the product of this approach is a set of ( discovered - units, translation words ) pairs.', 'multilingual leveraging : in this work we apply two simple methods for including multilingual information into the bilingual models from  #TAUTHOR_TAG.', 'the first one, multilingual voting, consists of merging the information learned by models trained with different language pairs by performing a voting over the final discovered boundaries.', 'the voting is performed by applying an agreement threshold t over the output boundaries.', 'this threshold balances between accepting all boundaries from all the bilingual models ( zero agreement ) and accepting only input boundaries discovered by all these models ( total agreement ).', 'the second method is ane selection.', 'for every language pair and aligned sentence in the dataset, a soft - alignment probability matrix is generated.', 'we use average normalized entropy ( ane )  #AUTHOR_TAG a ) computed over these matrices for selecting the most confident one for segmenting each phoneme sequence.', 'this exploits the idea that models trained on different language pairs will have language - related behavior, thus differing on the resulting alignment and segmentation over the same phoneme sequence']",5
['from  #TAUTHOR_TAG to'],['from  #TAUTHOR_TAG to'],[') approach from  #TAUTHOR_TAG to'],"['multilingual mboshi parallel corpus : in this work we extend the bilingual mboshi - french parallel corpus  #AUTHOR_TAG, fruit of the documentation process of mboshi ( bantu c25 ), an endangered language spoken in congo - brazzaville.', 'the corpus contains 5, 130 utterances, for which it provides audio, transcriptions and translations in french.', 'we translate the french into four other well - resourced languages through the use of the deepl translator.', '1 the languages added to the dataset are : english, german, portuguese and spanish.', 'table 1 shows some statistics for the produced multilingual mboshi parallel corpus.', '2 bilingual unsupervised word segmentation / discovery approach : we use the bilingual neuralbased unsupervised word segmentation ( uws ) approach from  #TAUTHOR_TAG to discover words in mboshi.', 'in this approach, neural machine translation ( nmt ) models are trained between language pairs, using as source language the translation ( word - level ) and as target, the language to document ( unsegmented phonemic sequence ).', 'due to the attention mechanism present in these networks  #AUTHOR_TAG, posterior to training, it is possible to retrieve soft - alignment probability matrices between source and target sequences.', 'these matrices give us sentence - level source - to - target alignment information, and by using it for clustering neighbor phonemes aligned to the same translation word, we are able to create segmentation in the target side.', 'the product of this approach is a set of ( discovered - units, translation words ) pairs.', 'multilingual leveraging : in this work we apply two simple methods for including multilingual information into the bilingual models from  #TAUTHOR_TAG.', 'the first one, multilingual voting, consists of merging the information learned by models trained with different language pairs by performing a voting over the final discovered boundaries.', 'the voting is performed by applying an agreement threshold t over the output boundaries.', 'this threshold balances between accepting all boundaries from all the bilingual models ( zero agreement ) and accepting only input boundaries discovered by all these models ( total agreement ).', 'the second method is ane selection.', 'for every language pair and aligned sentence in the dataset, a soft - alignment probability matrix is generated.', 'we use average normalized entropy ( ane )  #AUTHOR_TAG a ) computed over these matrices for selecting the most confident one for segmenting each phoneme sequence.', 'this exploits the idea that models trained on different language pairs will have language - related behavior, thus differing on the resulting alignment and segmentation over the same phoneme sequence']",5
"[' #TAUTHOR_TAG, image caption']","[' #TAUTHOR_TAG, image caption']","[', text segmentation  #AUTHOR_TAG, information extraction  #TAUTHOR_TAG, image caption generation']","['tutorial introduces the advances in deep bayesian learning with abundant applications for natural language understanding ranging from speech recognition  #AUTHOR_TAG to document summarization  #AUTHOR_TAG, text classification  #AUTHOR_TAG, text segmentation  #AUTHOR_TAG, information extraction  #TAUTHOR_TAG, image caption generation  #AUTHOR_TAG, sentence generation  #AUTHOR_TAG b ), dialogue control  #AUTHOR_TAG a ), sentiment classification, recommendation system, question answering  #AUTHOR_TAG and machine translation, to name a few.', 'traditionally, "" deep learning "" is taken to be a learning process where the inference or optimization is based on the real - valued deterministic model.', 'the "" semantic structure "" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.', 'the "" distribution function "" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.', 'this tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced bayesian models and deep models including hierarchical dirichlet process, chinese restaurant process  #AUTHOR_TAG, hierarchical pitman - yor process  #AUTHOR_TAG, indian buffet process  #AUTHOR_TAG, recurrent neural network  #AUTHOR_TAG, long short - term memory  #AUTHOR_TAG sequence - to - sequence model  #AUTHOR_TAG, variational auto - encoder  #AUTHOR_TAG, generative adversarial network  #AUTHOR_TAG, attention mechanism  #AUTHOR_TAG, memory - augmented neural network  #AUTHOR_TAG, stochastic neural network  #AUTHOR_TAG, predictive state neural network  #AUTHOR_TAG, policy gradient  #AUTHOR_TAG and reinforcement learning  #AUTHOR_TAG.', 'we present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language.', 'the variational inference and sampling method are formulated to tackle the optimization for complicated models  #AUTHOR_TAG.', 'the word and sentence embeddings, clustering and co - clustering are merged with linguistic and semantic constraints.', 'a series of case studies are presented to tackle different issues in deep bayesian learning and understanding.', '']",5
"[' #TAUTHOR_TAG, image caption']","[' #TAUTHOR_TAG, image caption']","[', text segmentation  #AUTHOR_TAG, information extraction  #TAUTHOR_TAG, image caption generation']","['tutorial introduces the advances in deep bayesian learning with abundant applications for natural language understanding ranging from speech recognition  #AUTHOR_TAG to document summarization  #AUTHOR_TAG, text classification  #AUTHOR_TAG, text segmentation  #AUTHOR_TAG, information extraction  #TAUTHOR_TAG, image caption generation  #AUTHOR_TAG, sentence generation  #AUTHOR_TAG b ), dialogue control  #AUTHOR_TAG a ), sentiment classification, recommendation system, question answering  #AUTHOR_TAG and machine translation, to name a few.', 'traditionally, "" deep learning "" is taken to be a learning process where the inference or optimization is based on the real - valued deterministic model.', 'the "" semantic structure "" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.', 'the "" distribution function "" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.', 'this tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced bayesian models and deep models including hierarchical dirichlet process, chinese restaurant process  #AUTHOR_TAG, hierarchical pitman - yor process  #AUTHOR_TAG, indian buffet process  #AUTHOR_TAG, recurrent neural network  #AUTHOR_TAG, long short - term memory  #AUTHOR_TAG sequence - to - sequence model  #AUTHOR_TAG, variational auto - encoder  #AUTHOR_TAG, generative adversarial network  #AUTHOR_TAG, attention mechanism  #AUTHOR_TAG, memory - augmented neural network  #AUTHOR_TAG, stochastic neural network  #AUTHOR_TAG, predictive state neural network  #AUTHOR_TAG, policy gradient  #AUTHOR_TAG and reinforcement learning  #AUTHOR_TAG.', 'we present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language.', 'the variational inference and sampling method are formulated to tackle the optimization for complicated models  #AUTHOR_TAG.', 'the word and sentence embeddings, clustering and co - clustering are merged with linguistic and semantic constraints.', 'a series of case studies are presented to tackle different issues in deep bayesian learning and understanding.', '']",0
"['of jokes  #AUTHOR_TAG.', 'later,  #TAUTHOR_TAG formulated a classifier to distinguish between hum']","['of jokes  #AUTHOR_TAG.', 'later,  #TAUTHOR_TAG formulated a classifier to distinguish between humorous and']","['algorithm of the study was based on the extraction of structural patterns and peculiar structure of jokes  #AUTHOR_TAG.', 'later,  #TAUTHOR_TAG formulated a classifier to distinguish between hum']","['task of automatic humor recognition refers to deciding whether a given sentence expresses a certain degree of humor.', 'in early studies, most of them are formulated as a binary classification, based on selection on linguistic features.', 'purandare and litman analyzed humorous spoken conversations from a classic comedy television show.', 'they used standard supervised learning classifiers to identify humorous speech  #AUTHOR_TAG.', 'taylor and marlack focused on a specific type of humor, wordplays.', 'their algorithm of the study was based on the extraction of structural patterns and peculiar structure of jokes  #AUTHOR_TAG.', 'later,  #TAUTHOR_TAG formulated a classifier to distinguish between humorous and non - humorous instances, and also created computational models to discover the latent semantic structure behind humor from four perspectives : incongruity, ambiguity, interpersonal effect and phonetic style.', '']",0
"['of jokes  #AUTHOR_TAG.', 'later,  #TAUTHOR_TAG formulated a classifier to distinguish between hum']","['of jokes  #AUTHOR_TAG.', 'later,  #TAUTHOR_TAG formulated a classifier to distinguish between humorous and']","['algorithm of the study was based on the extraction of structural patterns and peculiar structure of jokes  #AUTHOR_TAG.', 'later,  #TAUTHOR_TAG formulated a classifier to distinguish between hum']","['task of automatic humor recognition refers to deciding whether a given sentence expresses a certain degree of humor.', 'in early studies, most of them are formulated as a binary classification, based on selection on linguistic features.', 'purandare and litman analyzed humorous spoken conversations from a classic comedy television show.', 'they used standard supervised learning classifiers to identify humorous speech  #AUTHOR_TAG.', 'taylor and marlack focused on a specific type of humor, wordplays.', 'their algorithm of the study was based on the extraction of structural patterns and peculiar structure of jokes  #AUTHOR_TAG.', 'later,  #TAUTHOR_TAG formulated a classifier to distinguish between humorous and non - humorous instances, and also created computational models to discover the latent semantic structure behind humor from four perspectives : incongruity, ambiguity, interpersonal effect and phonetic style.', '']",7
"['of jokes  #AUTHOR_TAG.', 'later,  #TAUTHOR_TAG formulated a classifier to distinguish between hum']","['of jokes  #AUTHOR_TAG.', 'later,  #TAUTHOR_TAG formulated a classifier to distinguish between humorous and']","['algorithm of the study was based on the extraction of structural patterns and peculiar structure of jokes  #AUTHOR_TAG.', 'later,  #TAUTHOR_TAG formulated a classifier to distinguish between hum']","['task of automatic humor recognition refers to deciding whether a given sentence expresses a certain degree of humor.', 'in early studies, most of them are formulated as a binary classification, based on selection on linguistic features.', 'purandare and litman analyzed humorous spoken conversations from a classic comedy television show.', 'they used standard supervised learning classifiers to identify humorous speech  #AUTHOR_TAG.', 'taylor and marlack focused on a specific type of humor, wordplays.', 'their algorithm of the study was based on the extraction of structural patterns and peculiar structure of jokes  #AUTHOR_TAG.', 'later,  #TAUTHOR_TAG formulated a classifier to distinguish between humorous and non - humorous instances, and also created computational models to discover the latent semantic structure behind humor from four perspectives : incongruity, ambiguity, interpersonal effect and phonetic style.', '']",5
"['construct humor recognition experiments includes four parts : pun of the day  #TAUTHOR_TAG, 16000 oneliners  #AUTHOR_TAG, short jo']","['construct humor recognition experiments includes four parts : pun of the day  #TAUTHOR_TAG, 16000 oneliners  #AUTHOR_TAG, short jokes dataset and ptt jokes.', 'the four datasets have different joke types, sentence lengths, data sizes and languages that']","['construct humor recognition experiments includes four parts : pun of the day  #TAUTHOR_TAG, 16000 oneliners  #AUTHOR_TAG, short jokes dataset and ptt jokes.', 'the four datasets have different joke types, sentence lengths, data sizes and languages that']","['fairly evaluate the performance on humor recognition, we need the dataset to consist of both humorous ( positive ) and non - humorous ( negative ) samples.', 'the datasets we use to construct humor recognition experiments includes four parts : pun of the day  #TAUTHOR_TAG, 16000 oneliners  #AUTHOR_TAG, short jokes dataset and ptt jokes.', 'the four datasets have different joke types, sentence lengths, data sizes and languages that allow us to conduct more comprehensive and comparative experiments.', 'we would like to thank yang and mihalcea for their kindly provision of two former datasets. and we depict how we collect the latter two datasets in the following subsections.', 'table 1 shows the statistics of four datasets']",5
['of  #TAUTHOR_TAG by random'],['of  #TAUTHOR_TAG by random'],"['of  #TAUTHOR_TAG by random forest with word2vec + human centric feature ( word2vec + hcf ) and  #AUTHOR_TAG by convolutional neural networks.', 'we choose a dropout rate']","['this section, we describe how we formulate humor recognition as a text classification problem and conduct experiments on four datasets which we mentioned in section 3.', 'we validate the performance of different network structure with 10 fold cross validation and compare with the performance of previous work.', 'table 2 shows the experiments on both 16000 one - liners and pun of the day.', 'we set the baseline on the previous works of  #TAUTHOR_TAG by random forest with word2vec + human centric feature ( word2vec + hcf ) and  #AUTHOR_TAG by convolutional neural networks.', ""we choose a dropout rate at 0. 5 and test our model's performance with two factors f and hn."", 'f means the increase of filter size and number as we mentioned in section 4.', ""otherwise, the window sizes would be ( 5, 6, 7 ) and filter number is 100 that is the same with  #AUTHOR_TAG's. hn indicates that we use the highway layers to train deep networks and we set the hn layers = 3 because it has better stability and accuracy in training step."", 'we could observe that when we use both f and 115 table 3 presents the result of short jokes and ptt jokes datasets.', 'as we can see, for the datasets was construed, it achieve 0. 924 on short jokes and 0. 943 on ptt jokes in terms of f1 score respectively.', 'it shows that the deep learning model can, to some extent learn the humorous meaning and structure embedded in the text automatically without human selection of features']",5
"['vectors  #TAUTHOR_TAG.', ' #AUTHOR_TAG uses attention mechanism in pointer']","['vectors  #TAUTHOR_TAG.', ' #AUTHOR_TAG uses attention mechanism in pointer']","[' #TAUTHOR_TAG.', ' #AUTHOR_TAG uses attention mechanism in pointer network']","['', 'benefiting from the availability of large - scale benchmark datasets such as squad  #AUTHOR_TAG, the attention - based neural networks has spread to machine comprehension and question answering tasks to allow the model to attend over past output vectors  #TAUTHOR_TAG.', ' #AUTHOR_TAG uses attention mechanism in pointer network to detect an answer boundary by predicting the start and the end indices in the passage.', ' #AUTHOR_TAG introduces a bi - directional attention flow network that attention models are decoupled from the recurrent neural networks.', ' #AUTHOR_TAG employs a coattention mechanism that attends to the question and document together. uses a gated attention network that includes both question and passage match and self - matching attentions.', '']",0
"['and passage encoder  #TAUTHOR_TAG,']","['and passage encoder  #TAUTHOR_TAG,']","['has exactly one question and passage encoder  #TAUTHOR_TAG,']","['concatenation of raw features as inputs are processed in fusion layers followed by encoder layers to form more abstract representations.', 'here we choose a bi - directional long short - term memory ( lstm )  #AUTHOR_TAG to obtain more abstract representations for words in passages and questions.', 'different from the commonly used approaches that every single model has exactly one question and passage encoder  #TAUTHOR_TAG, our encoder layers simultaneously calculate multiple question and passage representations, for the purpose of serving different parts of attention functions of different phases.', 'we use two types of encoders, independent encoder and shared encoder.', 'in terms of independent encoder, a bi - directional lstm is used to produce new representation v', 'where v q j ∈ r 2d are concatenated hidden states of two independent bilstm for the j - th question word and d is the hidden size.', 'in terms of shared encoder, we jointly produce new representation h', 'where h p i ∈ r 2d and u q j ∈ r 2d are concatenated hidden states of bilstm for the i - th passage word and j - th question word, sharing the same trainable bilstm parameters']",4
[' #TAUTHOR_TAG on'],[' #TAUTHOR_TAG on'],['##er  #TAUTHOR_TAG on'],[' #TAUTHOR_TAG'],7
[' #TAUTHOR_TAG on'],[' #TAUTHOR_TAG on'],['##er  #TAUTHOR_TAG on'],[' #TAUTHOR_TAG'],3
"[', there has been tremendous progress on learning parsing models from treebank data  #TAUTHOR_TAG.', '']","['the past decade, there has been tremendous progress on learning parsing models from treebank data  #TAUTHOR_TAG.', '']","['the past decade, there has been tremendous progress on learning parsing models from treebank data  #TAUTHOR_TAG.', '']","['the past decade, there has been tremendous progress on learning parsing models from treebank data  #TAUTHOR_TAG.', 'most of the early work in this area was based on postulating generative probability models of language that included parse structures  #AUTHOR_TAG.', 'learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back - off estimation tricks to cope with the sparse data problems  #AUTHOR_TAG.', 'subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such as maximum conditional likelihood ( i. e. "" maximum entropy "" ) to be applied  #AUTHOR_TAG.', 'currently, the work on conditional parsing models appears to have culminated in large margin training approaches  #AUTHOR_TAG mc  #AUTHOR_TAG, which demonstrates the state of the art performance in english dependency parsing.', 'despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models ( mc  #AUTHOR_TAG, a sufficiently unified view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches.', 'for example, smoothing methods have played a central role in probabilistic approaches  #TAUTHOR_TAG, and yet they are not being used in current large margin training algorithms.', 'another unexploited connection is that probabilistic approaches pay closer attention to the individual errors made by each component of a parse, whereas the training error minimized in the large margin approach - the "" structured margin loss "" ( mc  #AUTHOR_TAG - is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component.', 'i have addressed both of these issues, as well as others in my work']",0
"[', there has been tremendous progress on learning parsing models from treebank data  #TAUTHOR_TAG.', '']","['the past decade, there has been tremendous progress on learning parsing models from treebank data  #TAUTHOR_TAG.', '']","['the past decade, there has been tremendous progress on learning parsing models from treebank data  #TAUTHOR_TAG.', '']","['the past decade, there has been tremendous progress on learning parsing models from treebank data  #TAUTHOR_TAG.', 'most of the early work in this area was based on postulating generative probability models of language that included parse structures  #AUTHOR_TAG.', 'learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back - off estimation tricks to cope with the sparse data problems  #AUTHOR_TAG.', 'subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such as maximum conditional likelihood ( i. e. "" maximum entropy "" ) to be applied  #AUTHOR_TAG.', 'currently, the work on conditional parsing models appears to have culminated in large margin training approaches  #AUTHOR_TAG mc  #AUTHOR_TAG, which demonstrates the state of the art performance in english dependency parsing.', 'despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models ( mc  #AUTHOR_TAG, a sufficiently unified view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches.', 'for example, smoothing methods have played a central role in probabilistic approaches  #TAUTHOR_TAG, and yet they are not being used in current large margin training algorithms.', 'another unexploited connection is that probabilistic approaches pay closer attention to the individual errors made by each component of a parse, whereas the training error minimized in the large margin approach - the "" structured margin loss "" ( mc  #AUTHOR_TAG - is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component.', 'i have addressed both of these issues, as well as others in my work']",0
"[', there has been tremendous progress on learning parsing models from treebank data  #TAUTHOR_TAG.', '']","['the past decade, there has been tremendous progress on learning parsing models from treebank data  #TAUTHOR_TAG.', '']","['the past decade, there has been tremendous progress on learning parsing models from treebank data  #TAUTHOR_TAG.', '']","['the past decade, there has been tremendous progress on learning parsing models from treebank data  #TAUTHOR_TAG.', 'most of the early work in this area was based on postulating generative probability models of language that included parse structures  #AUTHOR_TAG.', 'learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back - off estimation tricks to cope with the sparse data problems  #AUTHOR_TAG.', 'subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such as maximum conditional likelihood ( i. e. "" maximum entropy "" ) to be applied  #AUTHOR_TAG.', 'currently, the work on conditional parsing models appears to have culminated in large margin training approaches  #AUTHOR_TAG mc  #AUTHOR_TAG, which demonstrates the state of the art performance in english dependency parsing.', 'despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models ( mc  #AUTHOR_TAG, a sufficiently unified view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches.', 'for example, smoothing methods have played a central role in probabilistic approaches  #TAUTHOR_TAG, and yet they are not being used in current large margin training algorithms.', 'another unexploited connection is that probabilistic approaches pay closer attention to the individual errors made by each component of a parse, whereas the training error minimized in the large margin approach - the "" structured margin loss "" ( mc  #AUTHOR_TAG - is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component.', 'i have addressed both of these issues, as well as others in my work']",1
"['', 'this formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models  #TAUTHOR_TAG as well as non - probabilistic models ( mc  #AUTHOR_TAG']","['', 'this formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models  #TAUTHOR_TAG as well as non - probabilistic models ( mc  #AUTHOR_TAG']","['', 'this formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models  #TAUTHOR_TAG as well as non - probabilistic models ( mc  #AUTHOR_TAG.', 'for the purpose of learning, the score of each link can be expressed as a weighted linear combination of features', 'where i are the weight parameters to be estimated during training']","['a sentence denote the set of all the directed, projective trees that span.', 'from an input sentence, one would like to be able to compute the best parse ; that is, a projective tree, 5 4 1 § ¢ 2 3', ', that obtains the highest "" score "".', 'in particular, i follow  #AUTHOR_TAG and mc  #AUTHOR_TAG and assume that the score of a complete spanning tree for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link ( a word pair ).', 'in which case, the parsing problem reduces to 7 6 ¡ 9 8', 'where the score s', 'can depend on any measurable property of ¤ and ¤ % $ within the tree.', 'this formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models  #TAUTHOR_TAG as well as non - probabilistic models ( mc  #AUTHOR_TAG.', 'for the purpose of learning, the score of each link can be expressed as a weighted linear combination of features', 'where i are the weight parameters to be estimated during training']",5
"['on words  #TAUTHOR_TAG.', 'the advantage of this']","['on words  #TAUTHOR_TAG.', 'the advantage of this']","['on words  #TAUTHOR_TAG.', 'the advantage of this approach is that it does not rely on part - ofspeech tags nor grammatical categories.', 'furthermore, i based training on maximizing the conditional probability']","['learn an accurate dependency parser from data, the first approach i investigated is based on a strictly lexical parsing model where all the parameters are based on words  #TAUTHOR_TAG.', 'the advantage of this approach is that it does not rely on part - ofspeech tags nor grammatical categories.', '']",5
"['provide no means of representing complex sentential structures.', 'our representation model is built on the functionalities of annotation graph  #TAUTHOR_TAG and']","['provide no means of representing complex sentential structures.', 'our representation model is built on the functionalities of annotation graph  #TAUTHOR_TAG and']","['provide no means of representing complex sentential structures.', 'our representation model is built on the functionalities of annotation graph  #TAUTHOR_TAG and']","[', we first review existing tools for annotating texts before discussing the advantages of our new tool.', 'we also present an application scenario of our tools for annotating text and explain how visualized cross - domain reference works.', 'a number of similar tools have been developed for various annotation scenarios.', 'mmax2 [ 4 ] is a customizable tool for creating and visualizing multilevel linguistic annotations that allows outputs the results of annotations according to predefined style - sheets.', 'it supports tagging of part - of - speech tags, coreference and grammatical relations, but is not capable of representing and visualizing complex discourse level structures.', 'salto [ 5 ] is a multilevel annotation tool for annotating semantic roles and treebank syntactic structures.', ""o'donnell's annotation tool for systemic functional linguistics, the uam corpustool [ 6 ], is intended for annotating multi - layered systemic functional grammar structures by a single user."", 'both tools are restrictive in terms of functionalities and do not support collaborative annotation and provide no means of representing complex sentential structures.', 'our representation model is built on the functionalities of annotation graph  #TAUTHOR_TAG and the underlying storage scheme is conceptually similar to standoff xml format [ 9 ], but we opted for a relational database structure built with an object - oriented design for efficiency, reusability and versatility.', 'several web - based annotation tools such as serengeti [ 10 ], a tool for annotating anaphoric relations and lexical chains, are limited to a particular domain and cannot be used for annotating and visualizing complex structural information without substantial modification']",5
"['provide no means of representing complex sentential structures.', 'our representation model is built on the functionalities of annotation graph  #TAUTHOR_TAG and']","['provide no means of representing complex sentential structures.', 'our representation model is built on the functionalities of annotation graph  #TAUTHOR_TAG and']","['provide no means of representing complex sentential structures.', 'our representation model is built on the functionalities of annotation graph  #TAUTHOR_TAG and']","[', we first review existing tools for annotating texts before discussing the advantages of our new tool.', 'we also present an application scenario of our tools for annotating text and explain how visualized cross - domain reference works.', 'a number of similar tools have been developed for various annotation scenarios.', 'mmax2 [ 4 ] is a customizable tool for creating and visualizing multilevel linguistic annotations that allows outputs the results of annotations according to predefined style - sheets.', 'it supports tagging of part - of - speech tags, coreference and grammatical relations, but is not capable of representing and visualizing complex discourse level structures.', 'salto [ 5 ] is a multilevel annotation tool for annotating semantic roles and treebank syntactic structures.', ""o'donnell's annotation tool for systemic functional linguistics, the uam corpustool [ 6 ], is intended for annotating multi - layered systemic functional grammar structures by a single user."", 'both tools are restrictive in terms of functionalities and do not support collaborative annotation and provide no means of representing complex sentential structures.', 'our representation model is built on the functionalities of annotation graph  #TAUTHOR_TAG and the underlying storage scheme is conceptually similar to standoff xml format [ 9 ], but we opted for a relational database structure built with an object - oriented design for efficiency, reusability and versatility.', 'several web - based annotation tools such as serengeti [ 10 ], a tool for annotating anaphoric relations and lexical chains, are limited to a particular domain and cannot be used for annotating and visualizing complex structural information without substantial modification']",4
['similar to the annotation graph model  #TAUTHOR_TAG that has been demonstrated to be capable of representing virtually'],['similar to the annotation graph model  #TAUTHOR_TAG that has been demonstrated to be capable of representing virtually'],"['.', 'the tagger is built on a generic, multifunctional relational database similar to the annotation graph model  #TAUTHOR_TAG that has been demonstrated to be capable of representing virtually all sorts of common linguistic annotations.', 'in the collaborative environment annotators can plug in certain linguistic resource that can serve as the standardized version assessable to all annotators, instead of each annotator keeping his own version, which may cause severe merging difficulties']","['', 'in view of these needs, we develop our application on a web - based infrastructure making it accessible from any web - accessible point and enabling collaborative annotation on the same data source either synchronously or asynchronously.', 'one problem that arises in collaborative annotation is that annotators often come with different sets of skills and have varying, sometimes overlapping responsibilities.', 'our goal is provide a user - friendly, intuitive interface, designed to reduce the drudgery of xml - based annotation, while enforcing annotating standards and quality functionalities for user management and versioning.', 'each stage in the annotation process is divided into several hierarchically structured steps in which each parent step can spawn child steps to be taken up by one or more annotators.', 'this gives the annotator fine - grained control over the annotating process and facilitates clear division of labor among different annotators.', 'in addition, all annotators collaborating on the same step get notified of the relevant changes in annotation in real time once a modification has been made.', 'the tagger is built on a generic, multifunctional relational database similar to the annotation graph model  #TAUTHOR_TAG that has been demonstrated to be capable of representing virtually all sorts of common linguistic annotations.', 'in the collaborative environment annotators can plug in certain linguistic resource that can serve as the standardized version assessable to all annotators, instead of each annotator keeping his own version, which may cause severe merging difficulties']",3
"['describe semantics and pragmatics, as well as syntax and behavior. my own slow progress  #TAUTHOR_TAG shows that there']","['describe semantics and pragmatics, as well as syntax and behavior. my own slow progress  #TAUTHOR_TAG shows that there']","['is that these models must describe semantics and pragmatics, as well as syntax and behavior. my own slow progress  #TAUTHOR_TAG shows that there']","['', 'response is more likely to resolve a pending task ambiguity ( de  #AUTHOR_TAG.', 'i expect many further exciting developments in our understanding of meaning and interpretation as we enrich the social intelligence of nl', '##g. modeling efforts will remain crucial to the exploration of these new capabilities. when we build and assemble models of actions and interpretations, we get systems', ""that can plan their own behavior simply by exploiting what they know about communication. these systems give new evidence about the information and problem - solving that's involved. the"", ""challenge is that these models must describe semantics and pragmatics, as well as syntax and behavior. my own slow progress  #TAUTHOR_TAG shows that there's still lots of hard work needed to develop suitable techniques"", '. i keep going because of the methodological payoffs i see on the horizon. modeling lets us take social intelligence seriously as a general implementation principle, and thus to aim for', ""systems whose multimodal behavior matches the flexibility and coordination that distinguishes our own embodied meanings. more generally, modeling replaces programming with data fitting, and a good model of action and interpretation in particular would let an agent's own"", 'experience in conversational interaction determine the repertoire of behaviors and meanings it uses to make itself understood']",2
"['describe semantics and pragmatics, as well as syntax and behavior. my own slow progress  #TAUTHOR_TAG shows that there']","['describe semantics and pragmatics, as well as syntax and behavior. my own slow progress  #TAUTHOR_TAG shows that there']","['is that these models must describe semantics and pragmatics, as well as syntax and behavior. my own slow progress  #TAUTHOR_TAG shows that there']","['', 'response is more likely to resolve a pending task ambiguity ( de  #AUTHOR_TAG.', 'i expect many further exciting developments in our understanding of meaning and interpretation as we enrich the social intelligence of nl', '##g. modeling efforts will remain crucial to the exploration of these new capabilities. when we build and assemble models of actions and interpretations, we get systems', ""that can plan their own behavior simply by exploiting what they know about communication. these systems give new evidence about the information and problem - solving that's involved. the"", ""challenge is that these models must describe semantics and pragmatics, as well as syntax and behavior. my own slow progress  #TAUTHOR_TAG shows that there's still lots of hard work needed to develop suitable techniques"", '. i keep going because of the methodological payoffs i see on the horizon. modeling lets us take social intelligence seriously as a general implementation principle, and thus to aim for', ""systems whose multimodal behavior matches the flexibility and coordination that distinguishes our own embodied meanings. more generally, modeling replaces programming with data fitting, and a good model of action and interpretation in particular would let an agent's own"", 'experience in conversational interaction determine the repertoire of behaviors and meanings it uses to make itself understood']",1
"['this technical report, we introduce fastfusionnet, an efficient variant of fusionnet  #TAUTHOR_TAG.', 'fusionnet is a high performing reading comprehension architecture, which was designed primarily']","['this technical report, we introduce fastfusionnet, an efficient variant of fusionnet  #TAUTHOR_TAG.', 'fusionnet is a high performing reading comprehension architecture, which was designed primarily']","['this technical report, we introduce fastfusionnet, an efficient variant of fusionnet  #TAUTHOR_TAG.', 'fusionnet is a high performing reading comprehension architecture, which was designed primarily']","['this technical report, we introduce fastfusionnet, an efficient variant of fusionnet  #TAUTHOR_TAG.', 'fusionnet is a high performing reading comprehension architecture, which was designed primarily for maximum retrieval accuracy with less regard towards computational requirements.', 'for fastfusionnets we remove the expensive cove layers [ 21 ] and substitute the bilstms with far more efficient sru layers [ 19 ].', 'the resulting architecture obtains state - of - the - art results on dawnbench [ 5 ] while achieving the lowest training and inference time on squad [ 25 ] to - date.', 'the code is available at https : / / github. com / felixgwu / fastfusionnet']",4
['the inference bottlenecks of fusionnet  #TAUTHOR_TAG and introduce fastfusionnet'],['the inference bottlenecks of fusionnet  #TAUTHOR_TAG and introduce fastfusionnet'],"['this technical report, we analyze the inference bottlenecks of fusionnet  #TAUTHOR_TAG and introduce fastfusionnet']","['this technical report, we analyze the inference bottlenecks of fusionnet  #TAUTHOR_TAG and introduce fastfusionnet that tackles them.', 'in our experiments, we show that fastfusionnet achieves new state - of - the - art training and inference time on squad based on the metrics of dawnbench']",4
"[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of each sru is set to 125, resulting in', 'a 250 - d output feature of each bisru regardless of the input size. in the following explanation, we use [ a ; b ] to represent concatenation in the feature dimension', '. attn ( q, k, v ) represents the attention mechanism taking the query q, the key', 'k, and the value v as inputs. assuming o being the output, we have input features. following chen et al. [ 2 ], we use 300 - dim glove [', '24 ] vectors, term - frequency, part - of - speech ( pos ) tags, and named', 'entity recognition ( ner ) tags as features for each word in the context or the question. we fine - tune the embedding vector of the padding', 'token, the unknown word token, and', 'the top 1000', 'most frequent words in the training set. like others  #TAUTHOR_TAG we use a randomly initialized the trainable embedding layer with 12 dimensions for pos tags and 8 dimensions for ner. we use question matching features proposed by chen et al. [ 2 ] as well, which contains a hard version', ""and a soft version. the hard version contains 3 binary features indicating where a context word's original form, lower case form, or lemmatized form"", '']",4
"[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of each sru is set to 125, resulting in', 'a 250 - d output feature of each bisru regardless of the input size. in the following explanation, we use [ a ; b ] to represent concatenation in the feature dimension', '. attn ( q, k, v ) represents the attention mechanism taking the query q, the key', 'k, and the value v as inputs. assuming o being the output, we have input features. following chen et al. [ 2 ], we use 300 - dim glove [', '24 ] vectors, term - frequency, part - of - speech ( pos ) tags, and named', 'entity recognition ( ner ) tags as features for each word in the context or the question. we fine - tune the embedding vector of the padding', 'token, the unknown word token, and', 'the top 1000', 'most frequent words in the training set. like others  #TAUTHOR_TAG we use a randomly initialized the trainable embedding layer with 12 dimensions for pos tags and 8 dimensions for ner. we use question matching features proposed by chen et al. [ 2 ] as well, which contains a hard version', ""and a soft version. the hard version contains 3 binary features indicating where a context word's original form, lower case form, or lemmatized form"", '']",4
"['##net without cove on squad development set  #TAUTHOR_TAG.', 'the training time']","['f1 82. 5 % of fusionnet without cove on squad development set  #TAUTHOR_TAG.', 'the training time']","['##net without cove on squad development set  #TAUTHOR_TAG.', 'the training time track aims to minimize the time to train a model up to']",[' #TAUTHOR_TAG'],4
"['##net without cove on squad development set  #TAUTHOR_TAG.', 'the training time']","['f1 82. 5 % of fusionnet without cove on squad development set  #TAUTHOR_TAG.', 'the training time']","['##net without cove on squad development set  #TAUTHOR_TAG.', 'the training time track aims to minimize the time to train a model up to']",[' #TAUTHOR_TAG'],4
"['this technical report, we introduce fastfusionnet, an efficient variant of fusionnet  #TAUTHOR_TAG.', 'fusionnet is a high performing reading comprehension architecture, which was designed primarily']","['this technical report, we introduce fastfusionnet, an efficient variant of fusionnet  #TAUTHOR_TAG.', 'fusionnet is a high performing reading comprehension architecture, which was designed primarily']","['this technical report, we introduce fastfusionnet, an efficient variant of fusionnet  #TAUTHOR_TAG.', 'fusionnet is a high performing reading comprehension architecture, which was designed primarily']","['this technical report, we introduce fastfusionnet, an efficient variant of fusionnet  #TAUTHOR_TAG.', 'fusionnet is a high performing reading comprehension architecture, which was designed primarily for maximum retrieval accuracy with less regard towards computational requirements.', 'for fastfusionnets we remove the expensive cove layers [ 21 ] and substitute the bilstms with far more efficient sru layers [ 19 ].', 'the resulting architecture obtains state - of - the - art results on dawnbench [ 5 ] while achieving the lowest training and inference time on squad [ 25 ] to - date.', 'the code is available at https : / / github. com / felixgwu / fastfusionnet']",6
['the inference bottlenecks of fusionnet  #TAUTHOR_TAG and introduce fastfusionnet'],['the inference bottlenecks of fusionnet  #TAUTHOR_TAG and introduce fastfusionnet'],"['this technical report, we analyze the inference bottlenecks of fusionnet  #TAUTHOR_TAG and introduce fastfusionnet']","['this technical report, we analyze the inference bottlenecks of fusionnet  #TAUTHOR_TAG and introduce fastfusionnet that tackles them.', 'in our experiments, we show that fastfusionnet achieves new state - of - the - art training and inference time on squad based on the metrics of dawnbench']",6
"[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of each sru is set to 125, resulting in', 'a 250 - d output feature of each bisru regardless of the input size. in the following explanation, we use [ a ; b ] to represent concatenation in the feature dimension', '. attn ( q, k, v ) represents the attention mechanism taking the query q, the key', 'k, and the value v as inputs. assuming o being the output, we have input features. following chen et al. [ 2 ], we use 300 - dim glove [', '24 ] vectors, term - frequency, part - of - speech ( pos ) tags, and named', 'entity recognition ( ner ) tags as features for each word in the context or the question. we fine - tune the embedding vector of the padding', 'token, the unknown word token, and', 'the top 1000', 'most frequent words in the training set. like others  #TAUTHOR_TAG we use a randomly initialized the trainable embedding layer with 12 dimensions for pos tags and 8 dimensions for ner. we use question matching features proposed by chen et al. [ 2 ] as well, which contains a hard version', ""and a soft version. the hard version contains 3 binary features indicating where a context word's original form, lower case form, or lemmatized form"", '']",6
"[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of each sru is set to 125, resulting in', 'a 250 - d output feature of each bisru regardless of the input size. in the following explanation, we use [ a ; b ] to represent concatenation in the feature dimension', '. attn ( q, k, v ) represents the attention mechanism taking the query q, the key', 'k, and the value v as inputs. assuming o being the output, we have input features. following chen et al. [ 2 ], we use 300 - dim glove [', '24 ] vectors, term - frequency, part - of - speech ( pos ) tags, and named', 'entity recognition ( ner ) tags as features for each word in the context or the question. we fine - tune the embedding vector of the padding', 'token, the unknown word token, and', 'the top 1000', 'most frequent words in the training set. like others  #TAUTHOR_TAG we use a randomly initialized the trainable embedding layer with 12 dimensions for pos tags and 8 dimensions for ner. we use question matching features proposed by chen et al. [ 2 ] as well, which contains a hard version', ""and a soft version. the hard version contains 3 binary features indicating where a context word's original form, lower case form, or lemmatized form"", '']",6
['##net  #TAUTHOR_TAG is reading comprehension model'],['##net  #TAUTHOR_TAG is reading comprehension model'],['##net  #TAUTHOR_TAG is reading comprehension model'],"['##net  #TAUTHOR_TAG is reading comprehension model built on top of drqa by introducing fully - aware attention layers ( context - question attention and context self - attention ), contextual embeddings [ 21 ], and more rnn layers.', 'their proposed fully - aware attention mechanism uses the concatenation of layers of hidden representations as the query and the key to compute attention weights, which shares a similar intuition as densenet [ 11 ].', 'fusionnet was the state - of - the - art reading comprehension model at the time of writing ( oct. 4th 2017 ).', '']",0
['##net  #TAUTHOR_TAG is reading comprehension model'],['##net  #TAUTHOR_TAG is reading comprehension model'],['##net  #TAUTHOR_TAG is reading comprehension model'],"['##net  #TAUTHOR_TAG is reading comprehension model built on top of drqa by introducing fully - aware attention layers ( context - question attention and context self - attention ), contextual embeddings [ 21 ], and more rnn layers.', 'their proposed fully - aware attention mechanism uses the concatenation of layers of hidden representations as the query and the key to compute attention weights, which shares a similar intuition as densenet [ 11 ].', 'fusionnet was the state - of - the - art reading comprehension model at the time of writing ( oct. 4th 2017 ).', '']",0
"[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of each sru is set to 125, resulting in', 'a 250 - d output feature of each bisru regardless of the input size. in the following explanation, we use [ a ; b ] to represent concatenation in the feature dimension', '. attn ( q, k, v ) represents the attention mechanism taking the query q, the key', 'k, and the value v as inputs. assuming o being the output, we have input features. following chen et al. [ 2 ], we use 300 - dim glove [', '24 ] vectors, term - frequency, part - of - speech ( pos ) tags, and named', 'entity recognition ( ner ) tags as features for each word in the context or the question. we fine - tune the embedding vector of the padding', 'token, the unknown word token, and', 'the top 1000', 'most frequent words in the training set. like others  #TAUTHOR_TAG we use a randomly initialized the trainable embedding layer with 12 dimensions for pos tags and 8 dimensions for ner. we use question matching features proposed by chen et al. [ 2 ] as well, which contains a hard version', ""and a soft version. the hard version contains 3 binary features indicating where a context word's original form, lower case form, or lemmatized form"", '']",3
"[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of each sru is set to 125, resulting in', 'a 250 - d output feature of each bisru regardless of the input size. in the following explanation, we use [ a ; b ] to represent concatenation in the feature dimension', '. attn ( q, k, v ) represents the attention mechanism taking the query q, the key', 'k, and the value v as inputs. assuming o being the output, we have input features. following chen et al. [ 2 ], we use 300 - dim glove [', '24 ] vectors, term - frequency, part - of - speech ( pos ) tags, and named', 'entity recognition ( ner ) tags as features for each word in the context or the question. we fine - tune the embedding vector of the padding', 'token, the unknown word token, and', 'the top 1000', 'most frequent words in the training set. like others  #TAUTHOR_TAG we use a randomly initialized the trainable embedding layer with 12 dimensions for pos tags and 8 dimensions for ner. we use question matching features proposed by chen et al. [ 2 ] as well, which contains a hard version', ""and a soft version. the hard version contains 3 binary features indicating where a context word's original form, lower case form, or lemmatized form"", '']",3
"[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of each sru is set to 125, resulting in', 'a 250 - d output feature of each bisru regardless of the input size. in the following explanation, we use [ a ; b ] to represent concatenation in the feature dimension', '. attn ( q, k, v ) represents the attention mechanism taking the query q, the key', 'k, and the value v as inputs. assuming o being the output, we have input features. following chen et al. [ 2 ], we use 300 - dim glove [', '24 ] vectors, term - frequency, part - of - speech ( pos ) tags, and named', 'entity recognition ( ner ) tags as features for each word in the context or the question. we fine - tune the embedding vector of the padding', 'token, the unknown word token, and', 'the top 1000', 'most frequent words in the training set. like others  #TAUTHOR_TAG we use a randomly initialized the trainable embedding layer with 12 dimensions for pos tags and 8 dimensions for ner. we use question matching features proposed by chen et al. [ 2 ] as well, which contains a hard version', ""and a soft version. the hard version contains 3 binary features indicating where a context word's original form, lower case form, or lemmatized form"", '']",3
"[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of']","[' #TAUTHOR_TAG, the hidden size of each sru is set to 125, resulting in', 'a 250 - d output feature of each bisru regardless of the input size. in the following explanation, we use [ a ; b ] to represent concatenation in the feature dimension', '. attn ( q, k, v ) represents the attention mechanism taking the query q, the key', 'k, and the value v as inputs. assuming o being the output, we have input features. following chen et al. [ 2 ], we use 300 - dim glove [', '24 ] vectors, term - frequency, part - of - speech ( pos ) tags, and named', 'entity recognition ( ner ) tags as features for each word in the context or the question. we fine - tune the embedding vector of the padding', 'token, the unknown word token, and', 'the top 1000', 'most frequent words in the training set. like others  #TAUTHOR_TAG we use a randomly initialized the trainable embedding layer with 12 dimensions for pos tags and 8 dimensions for ner. we use question matching features proposed by chen et al. [ 2 ] as well, which contains a hard version', ""and a soft version. the hard version contains 3 binary features indicating where a context word's original form, lower case form, or lemmatized form"", '']",5
"['( pacrr  #TAUTHOR_TAG, knrm']","['incorporated into existing neural architectures ( pacrr  #TAUTHOR_TAG, knrm [ 20 ], and drmm [ 5 ] ), allowing them', '']","['incorporated into existing neural architectures ( pacrr  #TAUTHOR_TAG, knrm [ 20 ], and drmm']","['', ""ranking [ 14 ] and question answering [ 22 ], these approaches only make use of bert's sentence classification mechanism. in contrast, we use bert's term representations"", ', and show that they can be effectively combined with existing neural ranking architectures. in summary, our contributions are as follows : - we are', 'the first to demonstrate that contextualized word representations can be successfully incorporated into existing neural architectures ( pacrr  #TAUTHOR_TAG, knrm [ 20 ], and drmm [ 5 ] ), allowing them', 'to leverage contextual information to improve ad - hoc document ranking. - we present a new joint model', ""that combines bert's classification vector with existing neural ranking architectures ( using bert"", ""' s token vectors ) to get the benefits from both approaches. - we demonstrate an approach for addressing the performance impact of computing contextualized language"", 'models by only partially computing the language model representations. - our code is available for replication and future work.']",0
"['( pacrr  #TAUTHOR_TAG, knrm']","['incorporated into existing neural architectures ( pacrr  #TAUTHOR_TAG, knrm [ 20 ], and drmm [ 5 ] ), allowing them', '']","['incorporated into existing neural architectures ( pacrr  #TAUTHOR_TAG, knrm [ 20 ], and drmm']","['', ""ranking [ 14 ] and question answering [ 22 ], these approaches only make use of bert's sentence classification mechanism. in contrast, we use bert's term representations"", ', and show that they can be effectively combined with existing neural ranking architectures. in summary, our contributions are as follows : - we are', 'the first to demonstrate that contextualized word representations can be successfully incorporated into existing neural architectures ( pacrr  #TAUTHOR_TAG, knrm [ 20 ], and drmm [ 5 ] ), allowing them', 'to leverage contextual information to improve ad - hoc document ranking. - we present a new joint model', ""that combines bert's classification vector with existing neural ranking architectures ( using bert"", ""' s token vectors ) to get the benefits from both approaches. - we demonstrate an approach for addressing the performance impact of computing contextualized language"", 'models by only partially computing the language model representations. - our code is available for replication and future work.']",7
,,,,5
,,,,5
"['##s, especially in low data regimes  #TAUTHOR_TAG']","['single - domain baselines, especially in low data regimes  #TAUTHOR_TAG']","['single - domain baselines, especially in low data regimes  #TAUTHOR_TAG.', 'in this study we explore semi - supervised slot - filling based on deep learning based approaches that can utilize natural language slot label descriptions.', '']","['', 'similar experiments on using shared feature extraction layers for slot - filling across several domains have demonstrated significant performance improvements relative to single - domain baselines, especially in low data regimes  #TAUTHOR_TAG.', 'in this study we explore semi - supervised slot - filling based on deep learning based approaches that can utilize natural language slot label descriptions.', 'this alleviates the need for large amounts of labeled or unlabeled in - domain examples or explicit schema alignment, enabling developers to quickly bootstrap new domains.', 'similar ideas have previously been shown to work for domain classification, where domain names were leveraged to generate representations in a shared space with query representations [']",0
"['standard tokenizer and lower', '- cased before use since capitalization was seen to be indicative of slot values. all digits were replaced with', 'special "" # "" tokens following  #TAUTHOR_TAG']","['standard tokenizer and lower', '- cased before use since capitalization was seen to be indicative of slot values. all digits were replaced with', 'special "" # "" tokens following  #TAUTHOR_TAG']","['standard tokenizer and lower', '- cased before use since capitalization was seen to be indicative of slot values. all digits were replaced with', 'special "" # "" tokens following  #TAUTHOR_TAG']","['', 'a verbatim match was found for the slot - values, or sent out to a second set of raters for labeling. for the labeling job the crowd workers were', 'instructed to label spans corresponding to slot values in the instantiated samples. table 1 shows the list of these domains with', 'representative example queries and the total number of training samples available. test sets were constructed using the same framework. all the collected data was tokenized using a standard tokenizer and lower', '- cased before use since capitalization was seen to be indicative of slot values. all digits were replaced with', 'special "" # "" tokens following  #TAUTHOR_TAG']",5
"['by domain specific softmax layers, following  #TAUTHOR_TAG.', 'the model was trained using a batch size of 100 with alternating batches from different domains.', 'to avoid over - training']","['each ( 128 dimensions in each direction ).', 'both lstm layers are shared across all domains, fol - lowed by domain specific softmax layers, following  #TAUTHOR_TAG.', 'the model was trained using a batch size of 100 with alternating batches from different domains.', 'to avoid over - training']","['by domain specific softmax layers, following  #TAUTHOR_TAG.', 'the model was trained using a batch size of 100 with alternating batches from different domains.', 'to avoid over - training the model on the larger domains, the number of batches chosen from each domain was proportional to the logarithm of the number of training samples from the domain.', 'the conceptual model architecture is depicted in figure 2']","['multi - task model consists of 2 stacked bidirectional lstm layers with 256 dimensions each ( 128 dimensions in each direction ).', 'both lstm layers are shared across all domains, fol - lowed by domain specific softmax layers, following  #TAUTHOR_TAG.', 'the model was trained using a batch size of 100 with alternating batches from different domains.', 'to avoid over - training the model on the larger domains, the number of batches chosen from each domain was proportional to the logarithm of the number of training samples from the domain.', 'the conceptual model architecture is depicted in figure 2']",5
"['lstm model [ 6,  #TAUTHOR_TAG • concept tagging']","['lstm model [ 6,  #TAUTHOR_TAG • concept tagging']","['lstm model [ 6,  #TAUTHOR_TAG • concept tagging model using slot label descriptions for']","['this study we explore the idea of zero - shot slot - filling, by implicitly linking slot representations across domains by using the label descriptions of the slots.', 'we compare the performance of three model architectures on varying amounts of training data :', '• single task bi - directional lstm • multi - task bi - directional stacked lstm model [ 6,  #TAUTHOR_TAG • concept tagging model using slot label descriptions for all our experiments we use 200 dimensional word2vec embeddings trained on the gnews corpus [ 23 ].', 'tokens not present in the pre - trained embeddings were replaced by a oov token.', 'each model was trained for 50000 steps using the rmsprop optimizer and tuned on the dev set performance before evaluation on the test set.', 'for evaluation, we compute the token f1 for each slot independently and report the weighted average over all slots for the target domain.', 'we use token f1 instead of the traditional slot f1 since token level evaluation results in softer penalization for mistakes, to mitigate span inconsistencies in the crowd sourced labels']",7
"['each of its component words  #AUTHOR_TAG schulte im  #TAUTHOR_TAG.', 'separately in nl']","['each of its component words  #AUTHOR_TAG schulte im  #TAUTHOR_TAG.', 'separately in nlp, there has been a recent']","['each of its component words  #AUTHOR_TAG schulte im  #TAUTHOR_TAG.', 'separately in nl']","['##word expressions ( mwes ) are word combinations that display some form of idiomaticity  #AUTHOR_TAG, including semantic idiomaticity, wherein the semantics of the mwe ( e. g. ivory tower ) cannot be predicted from the semantics of the component words ( e. g. ivory and tower ).', 'recent nlp work on semantic idiomaticity has focused on the task of "" compositionality prediction "", in the form of a regression task whereby a given mwe is mapped onto a continuous - valued compositionality score, either for the mwe as a whole or for each of its component words  #AUTHOR_TAG schulte im  #TAUTHOR_TAG.', 'separately in nlp, there has been a recent surge of interest in learning distributed representations of word meaning, in the form of "" word embeddings ""  #AUTHOR_TAG a ) and composition over distributed representations  #AUTHOR_TAG.', 'this paper is the first attempt to bring together the work on word embedding - style distributional analysis with compositionality prediction of mwes.', '']",0
"['types  #TAUTHOR_TAG.', 'word embeddings could form the basis for such an']","['of languages and mwe types  #TAUTHOR_TAG.', 'word embeddings could form the basis for such an']","['to mwes that are more broadly applicable to a wider range of languages and mwe types  #TAUTHOR_TAG.', 'word embeddings could form the basis for such an approach to predicting mwe compositionality']","['work on distributed approaches to distributional semantics has demonstrated their utility in a wide range of nlp tasks, including identifying various morphosyntactic and semantic relations  #AUTHOR_TAG a ), dependency parsing  #AUTHOR_TAG, sentiment analysis, named - entity recognition  #AUTHOR_TAG machine translation  #AUTHOR_TAG.', 'despite the wealth of research applying word embeddings within nlp, they have not yet been considered for predicting the compositionality of mwes.', 'much prior work on mwes has been tailored to specific kinds of mwes in particular languages ( e. g. english verb - noun combinations  #AUTHOR_TAG ).', 'there has however been recent interest in approaches to mwes that are more broadly applicable to a wider range of languages and mwe types  #TAUTHOR_TAG.', 'word embeddings could form the basis for such an approach to predicting mwe compositionality']",0
"['- art method for this dataset  #TAUTHOR_TAG is a supervised support vector regression model, trained over the distributional method from']","['noun  #AUTHOR_TAG.', 'the state - of - the - art method for this dataset  #TAUTHOR_TAG is a supervised support vector regression model, trained over the distributional method from']","['noun  #AUTHOR_TAG.', 'the state - of - the - art method for this dataset  #TAUTHOR_TAG is a supervised support vector regression model, trained over the distributional method from']","['evaluate our methods over three datasets : 3 ( 1 ) english noun compounds ( "" encs "", e. g. spelling bee and swimming pool ) ; ( 2 ) english verb particle constructions ( "" evpcs "", e. g. stand up and give away ) ; and ( 3 ) german noun compounds ( "" gncs "", e. g. ahornblatt "" maple leaf "" and eidechse "" lizard "" ).', 'the enc dataset consists of 90 binary english noun compounds, and is annotated on a continuous [ 0, 5 ] scale for both overall compositionality and the component - wise compositionality of each of the modifier and head noun  #AUTHOR_TAG.', 'the state - of - the - art method for this dataset  #TAUTHOR_TAG is a supervised support vector regression model, trained over the distributional method from section 3. 1 as applied to both english and 51 target languages ( under word and mwe translation ).', 'the evpc dataset consists of 160 english verb particle constructions, and is manually annotated for compositionality on a binary scale for each of the head verb and particle  #AUTHOR_TAG.', 'in order to translate the dataset into a regression task, we calculate the overall compositionality as the number of annotations of entailment for the verb, divided by the total number of verb annotations for that vpc.', 'the state - of - the - art method for this dataset  #TAUTHOR_TAG is a linear combination of : ( 1 ) the distributional method from section 3. 1 ; ( 2 ) the same method applied to 10 target languages ( under word and mwe translation, selecting the languages using supervised learning ) ; and ( 3 ) the string similarity method of  #AUTHOR_TAG.', 'the gnc dataset consists of 246 german noun compounds, and is annotated on a']",0
"['- art method for this dataset  #TAUTHOR_TAG is a supervised support vector regression model, trained over the distributional method from']","['noun  #AUTHOR_TAG.', 'the state - of - the - art method for this dataset  #TAUTHOR_TAG is a supervised support vector regression model, trained over the distributional method from']","['noun  #AUTHOR_TAG.', 'the state - of - the - art method for this dataset  #TAUTHOR_TAG is a supervised support vector regression model, trained over the distributional method from']","['evaluate our methods over three datasets : 3 ( 1 ) english noun compounds ( "" encs "", e. g. spelling bee and swimming pool ) ; ( 2 ) english verb particle constructions ( "" evpcs "", e. g. stand up and give away ) ; and ( 3 ) german noun compounds ( "" gncs "", e. g. ahornblatt "" maple leaf "" and eidechse "" lizard "" ).', 'the enc dataset consists of 90 binary english noun compounds, and is annotated on a continuous [ 0, 5 ] scale for both overall compositionality and the component - wise compositionality of each of the modifier and head noun  #AUTHOR_TAG.', 'the state - of - the - art method for this dataset  #TAUTHOR_TAG is a supervised support vector regression model, trained over the distributional method from section 3. 1 as applied to both english and 51 target languages ( under word and mwe translation ).', 'the evpc dataset consists of 160 english verb particle constructions, and is manually annotated for compositionality on a binary scale for each of the head verb and particle  #AUTHOR_TAG.', 'in order to translate the dataset into a regression task, we calculate the overall compositionality as the number of annotations of entailment for the verb, divided by the total number of verb annotations for that vpc.', 'the state - of - the - art method for this dataset  #TAUTHOR_TAG is a linear combination of : ( 1 ) the distributional method from section 3. 1 ; ( 2 ) the same method applied to 10 target languages ( under word and mwe translation, selecting the languages using supervised learning ) ; and ( 3 ) the string similarity method of  #AUTHOR_TAG.', 'the gnc dataset consists of 246 german noun compounds, and is annotated on a']",0
['first method for building vectors is that of  #TAUTHOR_TAG : the top'],['first method for building vectors is that of  #TAUTHOR_TAG : the top'],"['first method for building vectors is that of  #TAUTHOR_TAG : the top 50 most - frequent words in the training corpus are considered to be stopwords and discarded,']","['first method for building vectors is that of  #TAUTHOR_TAG : the top 50 most - frequent words in the training corpus are considered to be stopwords and discarded, and words with frequency rank 51 - 1051 are considered to be the content - bearing words, which form the dimensions for our vectors, in the manner of schutze ( 1997 ).', 'to measure the similarity of the mwe vector and the component word vectors, we considered two different approaches.', 'the first approach is based on  #AUTHOR_TAG and schulte im  #AUTHOR_TAG.', '']",5
"['), informed by the experimental results in the respective publications.', ""note that for evpc, we don't use the vector for the particle, in keeping with  #TAUTHOR_TAG ; as such, there are no results for comp 2."", 'for comp 1, α is']","['window size w ), informed by the experimental results in the respective publications.', ""note that for evpc, we don't use the vector for the particle, in keeping with  #TAUTHOR_TAG ; as such, there are no results for comp 2."", 'for comp 1, α is']","['window size w ), informed by the experimental results in the respective publications.', ""note that for evpc, we don't use the vector for the particle, in keeping with  #TAUTHOR_TAG ; as such, there are no results for comp 2."", 'for comp 1, α is set to 1']","['all experiments, we train our models over raw text wikipedia corpora for either english or german, depending on the language of the dataset.', 'the raw english and german corpora were preprocessed using the wp2txt toolbox 4 to eliminate xml and html tags and hyperlinks, and punctuation was removed.', 'finally, word - tokenisation was performed based on simple whitespace delimitation, after which we greedily identified all string occurrences of the mwes in each of our datasets and combined them into a single token.', '5 the word embedding approaches are unable to generate vector representations for tokens which occur with frequency below a fixed cutoff.', ""6 in order to table 1 : pearson's correlation ( r ) for the different methods over the three datasets ; the state - of - the - art for each dataset is described in section 4 generate a compositionality prediction back - off for the small numbers of mwes in this category, we assign a default value, which is the mean of computed compositionality scores for other instances."", '7 as a baseline, we use the translation string similarity approach of  #AUTHOR_TAG, including the cross - validation - based method for selecting the 10 best languages to use for each dataset.', 'we further include a linear combination of the string similarity method with each of the various approaches based on word embeddings.', 'table 1 shows the results for the various methods, lack of lemmatisation.', '7 we also experimented with using the string similarity approach as a back - off, which resulted in marginally lower results than what is reported in table 1.', 'over a range of hyper - parameter settings for each of word2vec ( vector dimensionality d ; we also present results for cbow vs. c - skip ) and mssg ( vector dimensionality d and window size w ), informed by the experimental results in the respective publications.', ""note that for evpc, we don't use the vector for the particle, in keeping with  #TAUTHOR_TAG ; as such, there are no results for comp 2."", 'for comp 1, α is set to 1. 0 for evpc, and 0. 7 for both enc and gnc, also based on the findings of  #TAUTHOR_TAG.', 'the results indicate that the approaches using both word2vec and mssg outperform simple distributional and string similarity by a substantial margin.', 'further, over a variety of parameteriza - tions, they surpass the state - of - the - art methods for enc and evpc ; in the case of gnc, the bestperforming method ( word2vec with d = 500 and c - skip )']",5
"['), informed by the experimental results in the respective publications.', ""note that for evpc, we don't use the vector for the particle, in keeping with  #TAUTHOR_TAG ; as such, there are no results for comp 2."", 'for comp 1, α is']","['window size w ), informed by the experimental results in the respective publications.', ""note that for evpc, we don't use the vector for the particle, in keeping with  #TAUTHOR_TAG ; as such, there are no results for comp 2."", 'for comp 1, α is']","['window size w ), informed by the experimental results in the respective publications.', ""note that for evpc, we don't use the vector for the particle, in keeping with  #TAUTHOR_TAG ; as such, there are no results for comp 2."", 'for comp 1, α is set to 1']","['all experiments, we train our models over raw text wikipedia corpora for either english or german, depending on the language of the dataset.', 'the raw english and german corpora were preprocessed using the wp2txt toolbox 4 to eliminate xml and html tags and hyperlinks, and punctuation was removed.', 'finally, word - tokenisation was performed based on simple whitespace delimitation, after which we greedily identified all string occurrences of the mwes in each of our datasets and combined them into a single token.', '5 the word embedding approaches are unable to generate vector representations for tokens which occur with frequency below a fixed cutoff.', ""6 in order to table 1 : pearson's correlation ( r ) for the different methods over the three datasets ; the state - of - the - art for each dataset is described in section 4 generate a compositionality prediction back - off for the small numbers of mwes in this category, we assign a default value, which is the mean of computed compositionality scores for other instances."", '7 as a baseline, we use the translation string similarity approach of  #AUTHOR_TAG, including the cross - validation - based method for selecting the 10 best languages to use for each dataset.', 'we further include a linear combination of the string similarity method with each of the various approaches based on word embeddings.', 'table 1 shows the results for the various methods, lack of lemmatisation.', '7 we also experimented with using the string similarity approach as a back - off, which resulted in marginally lower results than what is reported in table 1.', 'over a range of hyper - parameter settings for each of word2vec ( vector dimensionality d ; we also present results for cbow vs. c - skip ) and mssg ( vector dimensionality d and window size w ), informed by the experimental results in the respective publications.', ""note that for evpc, we don't use the vector for the particle, in keeping with  #TAUTHOR_TAG ; as such, there are no results for comp 2."", 'for comp 1, α is set to 1. 0 for evpc, and 0. 7 for both enc and gnc, also based on the findings of  #TAUTHOR_TAG.', 'the results indicate that the approaches using both word2vec and mssg outperform simple distributional and string similarity by a substantial margin.', 'further, over a variety of parameteriza - tions, they surpass the state - of - the - art methods for enc and evpc ; in the case of gnc, the bestperforming method ( word2vec with d = 500 and c - skip )']",3
"['target expression and its component words under translation into many languages, along the lines of  #TAUTHOR_TAG']","['target expression and its component words under translation into many languages, along the lines of  #TAUTHOR_TAG']","['on three compositionality datasets.', 'in future work we intend to explore the contribution of information from word embeddings of a target expression and its component words under translation into many languages, along the lines of  #TAUTHOR_TAG']","['presented the first approach to using word embeddings to predict the compositionality of mwes.', 'we showed that this approach, in combination with information from string similarity, surpassed, or was competitive with, the current state - of - the - art on three compositionality datasets.', 'in future work we intend to explore the contribution of information from word embeddings of a target expression and its component words under translation into many languages, along the lines of  #TAUTHOR_TAG']",3
"['target expression and its component words under translation into many languages, along the lines of  #TAUTHOR_TAG']","['target expression and its component words under translation into many languages, along the lines of  #TAUTHOR_TAG']","['on three compositionality datasets.', 'in future work we intend to explore the contribution of information from word embeddings of a target expression and its component words under translation into many languages, along the lines of  #TAUTHOR_TAG']","['presented the first approach to using word embeddings to predict the compositionality of mwes.', 'we showed that this approach, in combination with information from string similarity, surpassed, or was competitive with, the current state - of - the - art on three compositionality datasets.', 'in future work we intend to explore the contribution of information from word embeddings of a target expression and its component words under translation into many languages, along the lines of  #TAUTHOR_TAG']",2
"[') 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a']","['( rnns ) 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a']","[') 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a vae - rnn based architecture allows encoding']","['autoencoder ( vae )  #AUTHOR_TAG is a powerful method for learning representations of high - dimensional data.', 'however, recent attempts of applying vaes to text modelling are still far less successful compared to its application to image and speech  #AUTHOR_TAG.', 'when applying vaes for text modelling, recurrent neural networks ( rnns ) 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a vae - rnn based architecture allows encoding and generating sentences ( in the decoding phase ) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse ( or kl loss vanishing ), where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks.', '']",0
"[') 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a']","['( rnns ) 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a']","[') 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a vae - rnn based architecture allows encoding']","['autoencoder ( vae )  #AUTHOR_TAG is a powerful method for learning representations of high - dimensional data.', 'however, recent attempts of applying vaes to text modelling are still far less successful compared to its application to image and speech  #AUTHOR_TAG.', 'when applying vaes for text modelling, recurrent neural networks ( rnns ) 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a vae - rnn based architecture allows encoding and generating sentences ( in the decoding phase ) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse ( or kl loss vanishing ), where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks.', '']",0
"[') 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a']","['( rnns ) 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a']","[') 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a vae - rnn based architecture allows encoding']","['autoencoder ( vae )  #AUTHOR_TAG is a powerful method for learning representations of high - dimensional data.', 'however, recent attempts of applying vaes to text modelling are still far less successful compared to its application to image and speech  #AUTHOR_TAG.', 'when applying vaes for text modelling, recurrent neural networks ( rnns ) 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a vae - rnn based architecture allows encoding and generating sentences ( in the decoding phase ) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse ( or kl loss vanishing ), where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks.', '']",0
"[') 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a']","['( rnns ) 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a']","[') 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a vae - rnn based architecture allows encoding']","['autoencoder ( vae )  #AUTHOR_TAG is a powerful method for learning representations of high - dimensional data.', 'however, recent attempts of applying vaes to text modelling are still far less successful compared to its application to image and speech  #AUTHOR_TAG.', 'when applying vaes for text modelling, recurrent neural networks ( rnns ) 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a vae - rnn based architecture allows encoding and generating sentences ( in the decoding phase ) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse ( or kl loss vanishing ), where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks.', '']",1
"['based models in previous works  #TAUTHOR_TAG.', 'that is,']","['vanishing phenomenon.', 'our model design is motivated by one noticeable defect shared by the vae - rnn based models in previous works  #TAUTHOR_TAG.', 'that is,']","['mitigate the kl vanishing phenomenon.', 'our model design is motivated by one noticeable defect shared by the vae - rnn based models in previous works  #TAUTHOR_TAG.', 'that is,']","['this section, we discuss the technical details of the proposed holistic regularisation vae ( hr - vae ) model, a general architecture which can effectively mitigate the kl vanishing phenomenon.', 'our model design is motivated by one noticeable defect shared by the vae - rnn based models in previous works  #TAUTHOR_TAG.', 'that is, all these models, as shown in figure 1a, only impose a standard normal distribution prior on the last hidden state of the rnn encoder, which potentially leads to learning a suboptimal representation of the latent variable and results in model vulnerable to kl loss vanishing.', 'our hypothesis is that to learn a good representation of data and a good generative model, it is crucial to impose the standard normal prior on all the hidden states of the rnn - based encoder ( see figure 1b ), which allows a better regularisation of the model learning process.', 'we implement the hr - vae model using a twolayer lstm for both the encoder and decoder.', 'however, one should note that our architecture can be readily applied to other types of rnn such as gru.', '']",1
"[') 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a']","['( rnns ) 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a']","[') 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a vae - rnn based architecture allows encoding']","['autoencoder ( vae )  #AUTHOR_TAG is a powerful method for learning representations of high - dimensional data.', 'however, recent attempts of applying vaes to text modelling are still far less successful compared to its application to image and speech  #AUTHOR_TAG.', 'when applying vaes for text modelling, recurrent neural networks ( rnns ) 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a vae - rnn based architecture allows encoding and generating sentences ( in the decoding phase ) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse ( or kl loss vanishing ), where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks.', '']",5
"['for text generation  #TAUTHOR_TAG.', 'pt']","['for text generation  #TAUTHOR_TAG.', 'ptb consists of more than 40, 000 sentences from wall street journal articles whereas']","[') text generation corpus  #AUTHOR_TAG, which have been used in a number of previous works for text generation  #TAUTHOR_TAG.', 'pt']","['evaluate our model on two public datasets, namely, penn treebank ( ptb )  #AUTHOR_TAG and the end - to - end ( e2e ) text generation corpus  #AUTHOR_TAG, which have been used in a number of previous works for text generation  #TAUTHOR_TAG.', 'ptb consists of more than 40, 000 sentences from wall street journal articles whereas the e2e dataset contains over 50, 000 sen - tences of restaurant reviews.', 'the statistics of these two datasets are summarised in table 1']",5
"['test split following  #TAUTHOR_TAG.', 'for']","['split following  #TAUTHOR_TAG.', 'for']","['the ptb dataset, we used the train - test split following  #TAUTHOR_TAG.', 'for the e2e dataset, we used the train - test split from the original dataset  #AUTHOR_TAG and indexed the words with a frequency higher than 3.', 'we represent input data with 512 - dimensional word2vec embeddings  #AUTHOR_TAG.', 'we set the dimension']","['the ptb dataset, we used the train - test split following  #TAUTHOR_TAG.', 'for the e2e dataset, we used the train - test split from the original dataset  #AUTHOR_TAG and indexed the words with a frequency higher than 3.', 'we represent input data with 512 - dimensional word2vec embeddings  #AUTHOR_TAG.', 'we set the dimension of the hidden layers of both encoder and decoder to 256.', 'the adam optimiser  #AUTHOR_TAG was used for training with an initial learning rate of 0. 0001.', 'each utterance in a mini - batch was padded to the maximum length for that batch, and the maximum batch - size allowed is 128']",5
"['##f ) distribution rather than a gaussian distribution  #TAUTHOR_TAG.', 'the decoder needs to predict']","['( vmf ) distribution rather than a gaussian distribution  #TAUTHOR_TAG.', 'the decoder needs to predict']","['##f ) distribution rather than a gaussian distribution  #TAUTHOR_TAG.', 'the decoder needs to predict the entire sequence with only the help of the given latent variable z.', 'in this way, a high - quality representation abstracting the information of the input sentence is much needed for the decoder,']","['compare our hr - vae model with three strong baselines using vae for text modelling : vae - lstm - base 3 : a variational autoencoder model which uses lstm for both encoder and decoder.', 'kl annealing is used to tackled the latent variable collapse issue  #AUTHOR_TAG ; vae - cnn 4 : a variational autoencoder model with a lstm encoder and a dilated cnn decoder  #AUTHOR_TAG ; vmf - vae 5 : a variational autoencoder model using lstm for both encoder and decoder where the prior distribution is the von mises - fisher ( vmf ) distribution rather than a gaussian distribution  #TAUTHOR_TAG.', 'the decoder needs to predict the entire sequence with only the help of the given latent variable z.', 'in this way, a high - quality representation abstracting the information of the input sentence is much needed for the decoder, and hence enforcing z to learn the required information.', '']",5
['( wtmf ;  #TAUTHOR_TAG'],['( wtmf ;  #TAUTHOR_TAG'],"['weighted textual matrix factorization ( wtmf ;  #TAUTHOR_TAG.', 'next, we convert']","['the semantic similarity of short units of text is fundamental to many natural language processing tasks, from evaluating machine translation  #AUTHOR_TAG to grouping redundant event mentions in social media ( petrovic et al., 2010 ).', 'the task is challenging because of the infinitely diverse set of possible linguistic realizations for any idea  #AUTHOR_TAG, and because of the short length of individual sentences, which means that standard bag - of - words representations will be hopelessly sparse.', 'distributional methods address this problem by transforming the high - dimensional bag - of - words representation into a lower - dimensional latent space.', 'this can be accomplished by factoring a matrix or tensor of term - context counts  #AUTHOR_TAG ; proximity in the induced latent space has been shown to correlate with semantic similarity  #AUTHOR_TAG.', 'however, factoring the term - context matrix means throwing away a considerable amount of information, as the original matrix of size m × n ( number of instances by number of features ) is factored into two smaller matrices of size m × k and n × k, with k m, n.', 'if the factorization does not take into account labeled data about semantic similarity, important information can be lost.', 'in this paper, we show how labeled data can considerably improve distributional methods for measuring semantic similarity.', 'first, we develop a new discriminative term - weighting metric called tf - kld, which is applied to the term - context matrix before factorization.', 'on a standard paraphrase identification task  #AUTHOR_TAG, this method improves on both traditional tf - idf and weighted textual matrix factorization ( wtmf ;  #TAUTHOR_TAG.', 'next, we convert the latent representations of each sentence pair into a feature vector, which is used as input to a linear svm classifier.', 'this yields further improvements and substantially outperforms the current state - of - the - art on paraphrase classification.', 'we then add "" finegrained "" features about the lexical similarity of the sentence pair.', 'the combination of latent and finegrained features yields further improvements in accuracy, demonstrating that these feature sets provide complementary information on semantic similarity']",4
"[' #AUTHOR_TAG and  #TAUTHOR_TAG, who treat sentences as pseudo - documents in an lsa']","[' #AUTHOR_TAG and  #TAUTHOR_TAG, who treat sentences as pseudo - documents in an lsa framework, and identify paraphrases using similarity in']","[' #AUTHOR_TAG and  #TAUTHOR_TAG, who treat sentences as pseudo - documents in an lsa framework, and identify paraphrases using similarity in the latent space.', 'we show that the performance of such techniques can be improved dramatically by using supervised information']","['attempting to do justice to the entire literature on paraphrase identification, we note three high - level approaches : ( 1 ) string similarity metrics such as n - gram overlap and bleu score  #AUTHOR_TAG, as well as string kernels  #AUTHOR_TAG ; ( 2 ) syntactic operations on the parse structure  #AUTHOR_TAG ; and ( 3 ) distributional methods, such as latent semantic analysis ( lsa ;  #AUTHOR_TAG, which are most relevant to our work.', 'one application of distributional techniques is to replace individual words with distributionally similar alternatives  #AUTHOR_TAG.', ' #AUTHOR_TAG show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of larger units of text.', ' #AUTHOR_TAG propose a syntactically - informed approach to combine word representations, using a recursive auto - encoder to propagate meaning through the parse tree.', 'we take a different approach : rather than representing the meanings of individual words, we directly obtain a distributional representation for the entire sentence.', 'this is inspired by  #AUTHOR_TAG and  #TAUTHOR_TAG, who treat sentences as pseudo - documents in an lsa framework, and identify paraphrases using similarity in the latent space.', 'we show that the performance of such techniques can be improved dramatically by using supervised information to ( 1 ) reweight the individual distributional features and ( 2 ) learn the importance of each latent dimension']",0
"['classification using distance or similarity in the latent space  #TAUTHOR_TAG, more direct supervision']","['previous work has performed paraphrase classification using distance or similarity in the latent space  #TAUTHOR_TAG, more direct supervision']","['previous work has performed paraphrase classification using distance or similarity in the latent space  #TAUTHOR_TAG, more direct supervision can be applied.', 'specifically, we convert the latent representations']","['previous work has performed paraphrase classification using distance or similarity in the latent space  #TAUTHOR_TAG, more direct supervision can be applied.', 'specifically, we convert the latent representations of a pair of sentences v 1 and v 2 into a sample vector,', 'concatenating the element - wise sum v 1 + v 2 and ab -', '.', 'given this representation, we can use any supervised classification algorithm.', 'a further advantage of treating paraphrase as a supervised classification problem is that we can apply additional features besides the latent representation.', 'we consider a subset of features identified by  #AUTHOR_TAG, listed in table 1.', 'these features mainly capture fine - grained similarity between sentences, for example by counting specific unigram and bigram overlap']",0
"[' #AUTHOR_TAG and  #TAUTHOR_TAG, who treat sentences as pseudo - documents in an lsa']","[' #AUTHOR_TAG and  #TAUTHOR_TAG, who treat sentences as pseudo - documents in an lsa framework, and identify paraphrases using similarity in']","[' #AUTHOR_TAG and  #TAUTHOR_TAG, who treat sentences as pseudo - documents in an lsa framework, and identify paraphrases using similarity in the latent space.', 'we show that the performance of such techniques can be improved dramatically by using supervised information']","['attempting to do justice to the entire literature on paraphrase identification, we note three high - level approaches : ( 1 ) string similarity metrics such as n - gram overlap and bleu score  #AUTHOR_TAG, as well as string kernels  #AUTHOR_TAG ; ( 2 ) syntactic operations on the parse structure  #AUTHOR_TAG ; and ( 3 ) distributional methods, such as latent semantic analysis ( lsa ;  #AUTHOR_TAG, which are most relevant to our work.', 'one application of distributional techniques is to replace individual words with distributionally similar alternatives  #AUTHOR_TAG.', ' #AUTHOR_TAG show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of larger units of text.', ' #AUTHOR_TAG propose a syntactically - informed approach to combine word representations, using a recursive auto - encoder to propagate meaning through the parse tree.', 'we take a different approach : rather than representing the meanings of individual words, we directly obtain a distributional representation for the entire sentence.', 'this is inspired by  #AUTHOR_TAG and  #TAUTHOR_TAG, who treat sentences as pseudo - documents in an lsa framework, and identify paraphrases using similarity in the latent space.', 'we show that the performance of such techniques can be improved dramatically by using supervised information to ( 1 ) reweight the individual distributional features and ( 2 ) learn the importance of each latent dimension']",1
['work  #TAUTHOR_TAG'],['work  #TAUTHOR_TAG'],['- art unsupervised matrix factorization work  #TAUTHOR_TAG'],"['experiments test the utility of the tf - kld weighting towards paraphrase classification, using the microsoft research paraphrase corpus  #AUTHOR_TAG.', 'the training set contains 2753 true paraphrase pairs and 1323 false paraphrase pairs ; the test set contains 1147 and 578 pairs, respectively.', 'the tf - kld weights are constructed from only the training set, while matrix factorizations are per - formed on the entire corpus.', 'matrix factorization on both training and ( unlabeled ) test data can be viewed as a form of transductive learning  #AUTHOR_TAG, where we assume access to unlabeled test set instances.', '2 we also consider an inductive setting, where we construct the basis of the latent space from only the training set, and then project the test set onto this basis to find the corresponding latent representation.', 'the performance differences between the transductive and inductive settings were generally between 0. 5 % and 1 %, as noted in detail below.', 'we reiterate that the tf - kld weights are never computed from test set data.', 'prior work on this dataset is described in section 2.', 'to our knowledge, the current state - of - theart is a supervised system that combines several machine translation metrics  #AUTHOR_TAG, but we also compare with state - of - the - art unsupervised matrix factorization work  #TAUTHOR_TAG']",7
"[' #TAUTHOR_TAG, the threshold is tuned on held - out training data']","[' #TAUTHOR_TAG, the threshold is tuned on held - out training data.', 'we']","['cosine similarity in latent space, using a threshold for the classification boundary.', 'as in prior work  #TAUTHOR_TAG, the threshold is tuned on held - out training data.', 'we consider two distributional feature']","['the first experiment, we predict whether a pair of sentences is a paraphrase by measuring their cosine similarity in latent space, using a threshold for the classification boundary.', 'as in prior work  #TAUTHOR_TAG, the threshold is tuned on held - out training data.', 'we consider two distributional feature sets : feat 1, which includes unigrams ; and feat 2, which also includes bigrams and unlabeled dependency pairs obtained from maltparser  #AUTHOR_TAG.', 'to compare with  #TAUTHOR_TAG, we set the latent dimensionality to k = 100, which was the same in their paper.', 'both svd and nmf factorization are evaluated ; in both cases, we minimize the frobenius norm of the reconstruction error.', 'table 2 compares the accuracy of a number of different configurations.', '']",3
"[' #TAUTHOR_TAG, the threshold is tuned on held - out training data']","[' #TAUTHOR_TAG, the threshold is tuned on held - out training data.', 'we']","['cosine similarity in latent space, using a threshold for the classification boundary.', 'as in prior work  #TAUTHOR_TAG, the threshold is tuned on held - out training data.', 'we consider two distributional feature']","['the first experiment, we predict whether a pair of sentences is a paraphrase by measuring their cosine similarity in latent space, using a threshold for the classification boundary.', 'as in prior work  #TAUTHOR_TAG, the threshold is tuned on held - out training data.', 'we consider two distributional feature sets : feat 1, which includes unigrams ; and feat 2, which also includes bigrams and unlabeled dependency pairs obtained from maltparser  #AUTHOR_TAG.', 'to compare with  #TAUTHOR_TAG, we set the latent dimensionality to k = 100, which was the same in their paper.', 'both svd and nmf factorization are evaluated ; in both cases, we minimize the frobenius norm of the reconstruction error.', 'table 2 compares the accuracy of a number of different configurations.', '']",3
"[' #TAUTHOR_TAG, the threshold is tuned on held - out training data']","[' #TAUTHOR_TAG, the threshold is tuned on held - out training data.', 'we']","['cosine similarity in latent space, using a threshold for the classification boundary.', 'as in prior work  #TAUTHOR_TAG, the threshold is tuned on held - out training data.', 'we consider two distributional feature']","['the first experiment, we predict whether a pair of sentences is a paraphrase by measuring their cosine similarity in latent space, using a threshold for the classification boundary.', 'as in prior work  #TAUTHOR_TAG, the threshold is tuned on held - out training data.', 'we consider two distributional feature sets : feat 1, which includes unigrams ; and feat 2, which also includes bigrams and unlabeled dependency pairs obtained from maltparser  #AUTHOR_TAG.', 'to compare with  #TAUTHOR_TAG, we set the latent dimensionality to k = 100, which was the same in their paper.', 'both svd and nmf factorization are evaluated ; in both cases, we minimize the frobenius norm of the reconstruction error.', 'table 2 compares the accuracy of a number of different configurations.', '']",5
"[' #TAUTHOR_TAG, the threshold is tuned on held - out training data']","[' #TAUTHOR_TAG, the threshold is tuned on held - out training data.', 'we']","['cosine similarity in latent space, using a threshold for the classification boundary.', 'as in prior work  #TAUTHOR_TAG, the threshold is tuned on held - out training data.', 'we consider two distributional feature']","['the first experiment, we predict whether a pair of sentences is a paraphrase by measuring their cosine similarity in latent space, using a threshold for the classification boundary.', 'as in prior work  #TAUTHOR_TAG, the threshold is tuned on held - out training data.', 'we consider two distributional feature sets : feat 1, which includes unigrams ; and feat 2, which also includes bigrams and unlabeled dependency pairs obtained from maltparser  #AUTHOR_TAG.', 'to compare with  #TAUTHOR_TAG, we set the latent dimensionality to k = 100, which was the same in their paper.', 'both svd and nmf factorization are evaluated ; in both cases, we minimize the frobenius norm of the reconstruction error.', 'table 2 compares the accuracy of a number of different configurations.', '']",5
['generation that used these datasets  #TAUTHOR_TAG ; li et'],['on paraphrase generation that used these datasets  #TAUTHOR_TAG ; li et'],"['##ora 1, twitter  #AUTHOR_TAG and mscoco  #AUTHOR_TAG.', 'previous work on paraphrase generation that used these datasets  #TAUTHOR_TAG ; li et al., 1 https : / / data. quora. com / first - quora - dataset - release - question - pairs 2018 ;  #AUTHOR_TAG chose bleu  #AUTHOR_TAG, meteor  #AUTHOR_TAG and ter  #AUTHOR_TAG as evaluation metrics.', 'in this paper, we find that simply using the input sentence as output in an unsupervised manner ( i. e. fully parroting the input ) significantly outperforms the']","['', 'relation extraction can also benefit from incorporating paraphrase generation into its processing pipeline  #AUTHOR_TAG.', 'manually annotating translation references is expensive, and automatically generating references through paraphrasing has been shown to be effective for evaluation of machine translation  #AUTHOR_TAG.', 'datasets used for paraphrase generation include quora 1, twitter  #AUTHOR_TAG and mscoco  #AUTHOR_TAG.', 'previous work on paraphrase generation that used these datasets  #TAUTHOR_TAG ; li et al., 1 https : / / data. quora. com / first - quora - dataset - release - question - pairs 2018 ;  #AUTHOR_TAG chose bleu  #AUTHOR_TAG, meteor  #AUTHOR_TAG and ter  #AUTHOR_TAG as evaluation metrics.', 'in this paper, we find that simply using the input sentence as output in an unsupervised manner ( i. e. fully parroting the input ) significantly outperforms the state - of - the - art on two metrics for twitter, and on one metric for quora.', 'even after changing part of the input sentence ( i. e. partially parroting the input ), state - of - the - art metric scores can still be surpassed.', 'consequently, for future paraphrase generation research which achieve good evaluation scores, we suggest investigating whether their methods or models act differently from simple parroting behavior']",0
['.  #TAUTHOR_TAG sampled 4'],"[', 650', 'unique sentences that have reference paraphrases.  #TAUTHOR_TAG sampled 4']","[', 650', 'unique sentences that have reference paraphrases.  #TAUTHOR_TAG sampled 4k sentences as their test set, but did not specify which sentences they used.  #AUTHOR_TAG sampled 30k sentences as', '']","[', 650', 'unique sentences that have reference paraphrases.  #TAUTHOR_TAG sampled 4k sentences as their test set, but did not specify which sentences they used.  #AUTHOR_TAG sampled 30k sentences as', 'their test set, also not specifying which sentences they used. to avoid selecting a subset of data that is biased in favor', 'of our method, we perform evaluation on the entire quora dataset. although we evaluate on the', 'entire dataset, the size of our training set is zero due to the fully unsupervised nature of full', 'and partial parroting. we group sentences by the number of reference paraphrases they', 'have, and plot the relative counts in appendix a. it can be seen that over 64 % of entries have only a single reference', 'paraphrase, which is problematic because even if a paraphrase of good quality is generated for any one of these entries, bleu, meteor', 'and ter scores could still be inferior if the generated paraphrase differs too much from the single reference paraphrase. previous paraphrase generation work on quora  #AUTHOR_TAG did not mention removing these entries, thus we include them in our experiments for fair comparison. however, we strongly recommend future work which wishes to use bleu, meteor and ter as evaluation metrics to only consider entries that have multiple reference paraphrases. twitter. there are 114, 025', 'paraphrase sentence pairs in twitter, which were acquired by collecting tweets which contain identical urls  #AUTHOR_TAG. as with quora, prior parap', '##hrase generation work on this dataset  #AUTHOR_TAG did not provide their sampled test set sentences, so we evaluate parroting on the entire dataset to avoid bias. we', 'follow the', 'same data processing steps as quora, and plot the number of reference paraphrases in appendix a. mscoco. this is an', 'image captioning dataset, with multiple captions provided for a single image  #AUTHOR_TAG. there have been multiple works which use it as a paraphrase generation dataset by treating captions of the', 'same image as paraphrases  #TAUTHOR_TAG. the training and testing sets are available, containing 331, 163 and 162,', '016 input', 'sentences respectively. however, relevance scores for captions of the same image score only 3. 38 out', 'of 5 under human evaluation ( in contrast, the score is 4. 82 for quora )  #TAUTHOR_TAG, due to the fact that different captions', 'for the same image often vary in the semantic information conveyed. this makes the use of mscoco as a', 'paraphrase generation dataset questionable. we plot the number of reference paraphrases in appendix a']",0
['.  #TAUTHOR_TAG sampled 4'],"[', 650', 'unique sentences that have reference paraphrases.  #TAUTHOR_TAG sampled 4']","[', 650', 'unique sentences that have reference paraphrases.  #TAUTHOR_TAG sampled 4k sentences as their test set, but did not specify which sentences they used.  #AUTHOR_TAG sampled 30k sentences as', '']","[', 650', 'unique sentences that have reference paraphrases.  #TAUTHOR_TAG sampled 4k sentences as their test set, but did not specify which sentences they used.  #AUTHOR_TAG sampled 30k sentences as', 'their test set, also not specifying which sentences they used. to avoid selecting a subset of data that is biased in favor', 'of our method, we perform evaluation on the entire quora dataset. although we evaluate on the', 'entire dataset, the size of our training set is zero due to the fully unsupervised nature of full', 'and partial parroting. we group sentences by the number of reference paraphrases they', 'have, and plot the relative counts in appendix a. it can be seen that over 64 % of entries have only a single reference', 'paraphrase, which is problematic because even if a paraphrase of good quality is generated for any one of these entries, bleu, meteor', 'and ter scores could still be inferior if the generated paraphrase differs too much from the single reference paraphrase. previous paraphrase generation work on quora  #AUTHOR_TAG did not mention removing these entries, thus we include them in our experiments for fair comparison. however, we strongly recommend future work which wishes to use bleu, meteor and ter as evaluation metrics to only consider entries that have multiple reference paraphrases. twitter. there are 114, 025', 'paraphrase sentence pairs in twitter, which were acquired by collecting tweets which contain identical urls  #AUTHOR_TAG. as with quora, prior parap', '##hrase generation work on this dataset  #AUTHOR_TAG did not provide their sampled test set sentences, so we evaluate parroting on the entire dataset to avoid bias. we', 'follow the', 'same data processing steps as quora, and plot the number of reference paraphrases in appendix a. mscoco. this is an', 'image captioning dataset, with multiple captions provided for a single image  #AUTHOR_TAG. there have been multiple works which use it as a paraphrase generation dataset by treating captions of the', 'same image as paraphrases  #TAUTHOR_TAG. the training and testing sets are available, containing 331, 163 and 162,', '016 input', 'sentences respectively. however, relevance scores for captions of the same image score only 3. 38 out', 'of 5 under human evaluation ( in contrast, the score is 4. 82 for quora )  #TAUTHOR_TAG, due to the fact that different captions', 'for the same image often vary in the semantic information conveyed. this makes the use of mscoco as a', 'paraphrase generation dataset questionable. we plot the number of reference paraphrases in appendix a']",0
['.  #TAUTHOR_TAG sampled 4'],"[', 650', 'unique sentences that have reference paraphrases.  #TAUTHOR_TAG sampled 4']","[', 650', 'unique sentences that have reference paraphrases.  #TAUTHOR_TAG sampled 4k sentences as their test set, but did not specify which sentences they used.  #AUTHOR_TAG sampled 30k sentences as', '']","[', 650', 'unique sentences that have reference paraphrases.  #TAUTHOR_TAG sampled 4k sentences as their test set, but did not specify which sentences they used.  #AUTHOR_TAG sampled 30k sentences as', 'their test set, also not specifying which sentences they used. to avoid selecting a subset of data that is biased in favor', 'of our method, we perform evaluation on the entire quora dataset. although we evaluate on the', 'entire dataset, the size of our training set is zero due to the fully unsupervised nature of full', 'and partial parroting. we group sentences by the number of reference paraphrases they', 'have, and plot the relative counts in appendix a. it can be seen that over 64 % of entries have only a single reference', 'paraphrase, which is problematic because even if a paraphrase of good quality is generated for any one of these entries, bleu, meteor', 'and ter scores could still be inferior if the generated paraphrase differs too much from the single reference paraphrase. previous paraphrase generation work on quora  #AUTHOR_TAG did not mention removing these entries, thus we include them in our experiments for fair comparison. however, we strongly recommend future work which wishes to use bleu, meteor and ter as evaluation metrics to only consider entries that have multiple reference paraphrases. twitter. there are 114, 025', 'paraphrase sentence pairs in twitter, which were acquired by collecting tweets which contain identical urls  #AUTHOR_TAG. as with quora, prior parap', '##hrase generation work on this dataset  #AUTHOR_TAG did not provide their sampled test set sentences, so we evaluate parroting on the entire dataset to avoid bias. we', 'follow the', 'same data processing steps as quora, and plot the number of reference paraphrases in appendix a. mscoco. this is an', 'image captioning dataset, with multiple captions provided for a single image  #AUTHOR_TAG. there have been multiple works which use it as a paraphrase generation dataset by treating captions of the', 'same image as paraphrases  #TAUTHOR_TAG. the training and testing sets are available, containing 331, 163 and 162,', '016 input', 'sentences respectively. however, relevance scores for captions of the same image score only 3. 38 out', 'of 5 under human evaluation ( in contrast, the score is 4. 82 for quora )  #TAUTHOR_TAG, due to the fact that different captions', 'for the same image often vary in the semantic information conveyed. this makes the use of mscoco as a', 'paraphrase generation dataset questionable. we plot the number of reference paraphrases in appendix a']",0
"['2.', 'furthermore, the score deviation between different samples is small.', 'consequently, although the exact test sets used by  #TAUTHOR_TAG and  #AUTHOR_TAG are not available, it is logical to assume that parrot']","['2.', 'furthermore, the score deviation between different samples is small.', 'consequently, although the exact test sets used by  #TAUTHOR_TAG and  #AUTHOR_TAG are not available, it is logical to assume that parroting']","['to the scores in table 1, whereas the average scores for twitter are noticeably better than those in table 2.', 'furthermore, the score deviation between different samples is small.', 'consequently, although the exact test sets used by  #TAUTHOR_TAG and  #AUTHOR_TAG are not available, it is logical to assume that parrot']","['', 'of - the - art records on twitter ).', 'in total, 1200 test sets of size 4k were sampled for quora and 250 test sets of size 5k were sampled for twitter.', 'parroting performance on these sampled test sets can be found in table 4.', 'it can be observed that the average metric scores for quora are similar to the scores in table 1, whereas the average scores for twitter are noticeably better than those in table 2.', 'furthermore, the score deviation between different samples is small.', 'consequently, although the exact test sets used by  #TAUTHOR_TAG and  #AUTHOR_TAG are not available, it is logical to assume that parroting performance would still exceed or be on par with the state - of - the - art on those test sets.', 'partial parroting.', 'we also introduce lexical variation into our parroting method by replacing or cutting words of the input sentence.', ""for replacement, we substitute input words with an outof - vocabulary word not found in any of the input sentence's reference paraphrases."", 'parap']",2
['( nmt )  #TAUTHOR_TAG has gained increasing'],['( nmt )  #TAUTHOR_TAG has gained increasing'],['- end neural machine translation ( nmt )  #TAUTHOR_TAG has gained increasing'],"['- to - end neural machine translation ( nmt )  #TAUTHOR_TAG has gained increasing popularity in the machine translation community.', 'capable of capturing longdistance dependencies with gating  #AUTHOR_TAG and attention  #TAUTHOR_TAG mechanisms, nmt has proven to outperform conventional statistical machine translation systematically across a variety of language pairs ( junczys -  #AUTHOR_TAG.', '']",0
['( nmt )  #TAUTHOR_TAG has gained increasing'],['( nmt )  #TAUTHOR_TAG has gained increasing'],['- end neural machine translation ( nmt )  #TAUTHOR_TAG has gained increasing'],"['- to - end neural machine translation ( nmt )  #TAUTHOR_TAG has gained increasing popularity in the machine translation community.', 'capable of capturing longdistance dependencies with gating  #AUTHOR_TAG and attention  #TAUTHOR_TAG mechanisms, nmt has proven to outperform conventional statistical machine translation systematically across a variety of language pairs ( junczys -  #AUTHOR_TAG.', '']",0
['( nmt )  #TAUTHOR_TAG has gained increasing'],['( nmt )  #TAUTHOR_TAG has gained increasing'],['- end neural machine translation ( nmt )  #TAUTHOR_TAG has gained increasing'],"['- to - end neural machine translation ( nmt )  #TAUTHOR_TAG has gained increasing popularity in the machine translation community.', 'capable of capturing longdistance dependencies with gating  #AUTHOR_TAG and attention  #TAUTHOR_TAG mechanisms, nmt has proven to outperform conventional statistical machine translation systematically across a variety of language pairs ( junczys -  #AUTHOR_TAG.', '']",0
['( nmt )  #TAUTHOR_TAG has gained increasing'],['( nmt )  #TAUTHOR_TAG has gained increasing'],['- end neural machine translation ( nmt )  #TAUTHOR_TAG has gained increasing'],"['- to - end neural machine translation ( nmt )  #TAUTHOR_TAG has gained increasing popularity in the machine translation community.', 'capable of capturing longdistance dependencies with gating  #AUTHOR_TAG and attention  #TAUTHOR_TAG mechanisms, nmt has proven to outperform conventional statistical machine translation systematically across a variety of language pairs ( junczys -  #AUTHOR_TAG.', '']",5
['( nmt )  #TAUTHOR_TAG has gained increasing'],['( nmt )  #TAUTHOR_TAG has gained increasing'],['- end neural machine translation ( nmt )  #TAUTHOR_TAG has gained increasing'],"['- to - end neural machine translation ( nmt )  #TAUTHOR_TAG has gained increasing popularity in the machine translation community.', 'capable of capturing longdistance dependencies with gating  #AUTHOR_TAG and attention  #TAUTHOR_TAG mechanisms, nmt has proven to outperform conventional statistical machine translation systematically across a variety of language pairs ( junczys -  #AUTHOR_TAG.', '']",5
['. maximum likelihood estimation ( mle )  #TAUTHOR_TAG : the default training criterion in thum'],"['training criteria :', '1. maximum likelihood estimation ( mle )  #TAUTHOR_TAG : the default training criterion in thumt, which aims to']",['. maximum likelihood estimation ( mle )  #TAUTHOR_TAG : the default training criterion in thum'],"['##umt supports three training criteria :', '1. maximum likelihood estimation ( mle )  #TAUTHOR_TAG : the default training criterion in thumt, which aims to find a set of model parameters that maximizes the likelihood of training data.', '2. minimum risk training ( mrt ) : the recommended training criterion in thumt, which aims to find a set of model parameters that minimizes the risk ( i. e., expected loss measured by evaluation metrics ) on training data.', 'in thumt, mle is often used to initialize mrt.', 'in other words, the model trained with respect to mle serves as the initial model of mrt.', '3. semi - supervised training ( sst ) : the recommended training criterion for low - resource language translation.', 'sst is capable of exploiting abundant monolingual corpora to train source - to - target and target - to - source translation models jointly.', 'mle is also used to initialize sst']",5
"['##hog  #TAUTHOR_TAG,']","['address unknown words.', 'in our implementation, we use fastalign  #AUTHOR_TAG  #AUTHOR_TAG score.', 'our baseline system is groundhog  #TAUTHOR_TAG,']","['##hog  #TAUTHOR_TAG, a state - of - the - art open - source nmt toolkit.', 'we use']","['follow  #AUTHOR_TAG to address unknown words.', 'in our implementation, we use fastalign  #AUTHOR_TAG  #AUTHOR_TAG score.', 'our baseline system is groundhog  #TAUTHOR_TAG, a state - of - the - art open - source nmt toolkit.', 'we use the same setting of hyper - parameters for both groundhog and thumt.', 'the vocabulary size is set to 30k.', 'we set word embedding dimension to 620 for both languages.', 'the dimension of hidden layers is set to1000.', 'in training, we set the mini - batch size to 80.', 'in decoding, we set the beam size to 10.', 'during training, we set the table 1 shows the bleu scores obtained by groundhog and thumt using different training criteria and optimizers.', 'experimental results show that the translation performance of thumt is comparable to groundhog using mle.', 'due to the capability to include evaluation metrics in during, mrt obtain significant improvements over mle.', '1 another finding is that adam leads to consistent and significant improvements over adadelta.', 'table 2 shows the effect of semi - supervised training.', 'it is clear that exploiting monolingual corpora helps to improve translation quality for both directions.', '']",5
['( nmt )  #TAUTHOR_TAG has gained increasing'],['( nmt )  #TAUTHOR_TAG has gained increasing'],['- end neural machine translation ( nmt )  #TAUTHOR_TAG has gained increasing'],"['- to - end neural machine translation ( nmt )  #TAUTHOR_TAG has gained increasing popularity in the machine translation community.', 'capable of capturing longdistance dependencies with gating  #AUTHOR_TAG and attention  #TAUTHOR_TAG mechanisms, nmt has proven to outperform conventional statistical machine translation systematically across a variety of language pairs ( junczys -  #AUTHOR_TAG.', '']",4
